mcicek@harsimran:~$ cd Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_04-28-57 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 13:03:31.395267: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 13:03:31.493045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 13:03:31.493303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 13:03:31.493315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 13:03:31.648404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 13:03:31.648431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 13:03:31.648436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 13:03:31.648574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_04-28-57.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 60
eva_epoch = 6
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
All frames and annotations from 20 datasets have been read by 2019-01-29 13:03:38.448025
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:03:44.852855!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0595
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:04:05.871484!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0628
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:04:23.659639!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0855
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:04:43.394006!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0774
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:05:02.594252!
Epoch 1/1
772/772 [==============================] - 17s 23ms/step - loss: 0.0975
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:05:25.564083!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1202
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:05:44.809897!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1305
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:06:08.199111!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0825
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:06:37.678842!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0976
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:07:02.048633!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0841
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:07:25.567754!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0878
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:07:48.101703!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1109
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:08:05.225362!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1119
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:08:24.328521!
Epoch 1/1
511/511 [==============================] - 11s 21ms/step - loss: 0.0999
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:08:42.763899!
Epoch 1/1
744/744 [==============================] - 16s 21ms/step - loss: 0.0989
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:09:04.193437!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0824
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:09:20.185824!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0832
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:09:34.942081!
Epoch 1/1
542/542 [==============================] - 12s 21ms/step - loss: 0.1125
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:09:51.502953!
Epoch 1/1
485/485 [==============================] - 10s 22ms/step - loss: 0.0735
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-29 13:10:07.750015!
Epoch 1/1
572/572 [==============================] - 13s 22ms/step - loss: 0.0660
Epoch 1 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 13:10:24.994052
1. set (Dataset 6) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:10:30.208424!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1133
2. set (Dataset 11) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:10:47.826999!
Epoch 1/1
572/572 [==============================] - 13s 22ms/step - loss: 0.0620
3. set (Dataset 10) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:11:07.987300!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0892
4. set (Dataset 4) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:11:32.058099!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.0965
5. set (Dataset 23) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:11:54.642339!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1211
6. set (Dataset 13) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:12:12.493104!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0687
7. set (Dataset 17) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:12:27.557036!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0831
8. set (Dataset 1) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:12:42.040196!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1108
9. set (Dataset 8) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:13:01.118623!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0905
10. set (Dataset 7) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:13:26.681343!
Epoch 1/1
745/745 [==============================] - 17s 22ms/step - loss: 0.0924
11. set (Dataset 21) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:13:49.578012!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1431
12. set (Dataset 22) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:14:10.798968!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0633
13. set (Dataset 2) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:14:31.400470!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1033
14. set (Dataset 24) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:14:47.186268!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0692
15. set (Dataset 15) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:15:05.049298!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0889
16. set (Dataset 20) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:15:26.202594!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0720
17. set (Dataset 18) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:15:45.100621!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1054
18. set (Dataset 19) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:16:03.853898!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0803
19. set (Dataset 12) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:16:22.977819!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0842
20. set (Dataset 16) being trained for epoch 2 in Experiment 1 by 2019-01-29 13:16:49.093400!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0851
Epoch 2 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 13:17:14.638548
1. set (Dataset 19) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:17:19.531023!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0753
2. set (Dataset 16) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:17:40.383841!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0820
3. set (Dataset 21) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:18:07.758573!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1375
4. set (Dataset 15) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:18:28.194282!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0863
5. set (Dataset 13) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:18:48.821585!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0729
6. set (Dataset 12) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:19:06.800285!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0845
7. set (Dataset 18) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:19:29.643685!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1062
8. set (Dataset 22) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:19:50.277209!
Epoch 1/1
665/665 [==============================] - 16s 23ms/step - loss: 0.0653
9. set (Dataset 23) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:20:11.413924!
Epoch 1/1
569/569 [==============================] - 13s 22ms/step - loss: 0.1191
10. set (Dataset 8) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:20:31.855352!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0980
11. set (Dataset 17) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:20:54.209122!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0827
12. set (Dataset 6) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:21:08.758366!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1111
13. set (Dataset 24) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:21:25.992458!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0639
14. set (Dataset 11) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:21:43.161112!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0626
15. set (Dataset 10) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:22:03.778446!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0909
16. set (Dataset 20) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:22:26.687278!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0797
17. set (Dataset 2) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:22:44.740449!
Epoch 1/1
511/511 [==============================] - 11s 21ms/step - loss: 0.1068
18. set (Dataset 4) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:23:03.172645!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1015
19. set (Dataset 7) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:23:28.009176!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0919
20. set (Dataset 1) being trained for epoch 3 in Experiment 1 by 2019-01-29 13:23:50.630202!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1130
Epoch 3 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 13:24:06.462517
1. set (Dataset 4) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:24:13.860857!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.0962
2. set (Dataset 1) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:24:36.700602!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1115
3. set (Dataset 17) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:24:51.910144!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0871
4. set (Dataset 10) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:25:08.709663!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0889
5. set (Dataset 12) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:25:32.794743!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0821
6. set (Dataset 7) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:25:56.800517!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0918
7. set (Dataset 2) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:26:19.044766!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1020
8. set (Dataset 6) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:26:36.107069!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1180
9. set (Dataset 13) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:26:53.501490!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0698
10. set (Dataset 23) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:27:10.672699!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1232
11. set (Dataset 18) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:27:29.822272!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1076
12. set (Dataset 19) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:27:48.872109!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0815
13. set (Dataset 11) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:28:06.109735!
Epoch 1/1
572/572 [==============================] - 12s 22ms/step - loss: 0.0659
14. set (Dataset 16) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:28:27.344921!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0833
15. set (Dataset 21) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:28:54.629337!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1341
16. set (Dataset 20) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:29:14.460684!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0736
17. set (Dataset 24) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:29:32.049634!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0631
18. set (Dataset 15) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:29:49.480244!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0900
19. set (Dataset 8) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:30:12.527379!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0990
20. set (Dataset 22) being trained for epoch 4 in Experiment 1 by 2019-01-29 13:30:36.705344!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0628
Epoch 4 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 13:30:56.781411
1. set (Dataset 15) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:31:03.145618!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0860
2. set (Dataset 22) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:31:24.455312!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0613
3. set (Dataset 18) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:31:45.355303!
Epoch 1/1
614/614 [==============================] - 13s 22ms/step - loss: 0.1051
4. set (Dataset 21) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:32:04.990165!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1358
5. set (Dataset 7) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:32:27.884484!
Epoch 1/1
745/745 [==============================] - 16s 22ms/step - loss: 0.0991
6. set (Dataset 8) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:32:52.098230!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0938
7. set (Dataset 24) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:33:14.107889!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0706
8. set (Dataset 19) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:33:30.449545!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0787
9. set (Dataset 12) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:33:49.022809!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0864
10. set (Dataset 13) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:34:10.444149!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0686
11. set (Dataset 2) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:34:26.074797!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1109
12. set (Dataset 4) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:34:45.215786!
Epoch 1/1
744/744 [==============================] - 16s 21ms/step - loss: 0.1009
13. set (Dataset 16) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:35:10.073162!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0871
14. set (Dataset 1) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:35:35.859769!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1107
15. set (Dataset 17) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:35:50.888145!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0848
16. set (Dataset 20) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:36:05.444724!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0748
17. set (Dataset 11) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:36:24.117874!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0639
18. set (Dataset 10) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:36:45.064058!
Epoch 1/1
726/726 [==============================] - 16s 23ms/step - loss: 0.0884
19. set (Dataset 23) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:37:07.109230!
Epoch 1/1
569/569 [==============================] - 13s 24ms/step - loss: 0.1246
20. set (Dataset 6) being trained for epoch 5 in Experiment 1 by 2019-01-29 13:37:25.787403!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1142
Epoch 5 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 13:37:42.644479
1. set (Dataset 10) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:37:49.884699!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0880
2. set (Dataset 6) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:38:12.636943!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1141
3. set (Dataset 2) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:38:30.396998!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1017
4. set (Dataset 17) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:38:46.180608!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0854
5. set (Dataset 8) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:39:03.452252!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0893
6. set (Dataset 23) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:39:26.834958!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1210
7. set (Dataset 11) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:39:45.844022!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0639
8. set (Dataset 4) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:40:06.255144!
Epoch 1/1
744/744 [==============================] - 16s 22ms/step - loss: 0.1000
9. set (Dataset 7) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:40:30.312598!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0912
10. set (Dataset 12) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:40:55.637908!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0826
11. set (Dataset 24) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:41:17.346613!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0727
12. set (Dataset 15) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:41:35.079396!
Epoch 1/1
654/654 [==============================] - 15s 22ms/step - loss: 0.0883
13. set (Dataset 1) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:41:54.934154!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1116
14. set (Dataset 22) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:42:12.921543!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0633
15. set (Dataset 18) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:42:34.354257!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1087
16. set (Dataset 20) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:42:53.874594!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0718
17. set (Dataset 16) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:43:15.278688!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0850
18. set (Dataset 21) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:43:43.395014!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1313
19. set (Dataset 13) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:44:03.586672!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0830
20. set (Dataset 19) being trained for epoch 6 in Experiment 1 by 2019-01-29 13:44:19.003138!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0810
Epoch 6 for Experiment 1 completed!
Exp2019-01-29_13-03-34_part1.h5 has been saved.
The subjects are trained: [(10, 'M04'), (6, 'F06'), (2, 'F02'), (17, 'M10'), (8, 'M02'), (23, 'M13'), (11, 'M05'), (4, '
F04'), (7, 'M01'), (12, 'M06'), (24, 'M14'), (15, 'F03'), (1, 'F01'), (22, 'M01'), (18, 'F05'), (20, 'M12'), (16, 'M09')
, (21, 'F02'), (13, 'M07'), (19, 'M11')]
Evaluating model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 13:44:33.510751
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 9.50 Degree
        The absolute mean error on Yaw angle estimation: 24.62 Degree
        The absolute mean error on Roll angle estimation: 13.09 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 6.91 Degree
        The absolute mean error on Yaw angle estimation: 20.69 Degree
        The absolute mean error on Roll angle estimation: 4.01 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 32.43 Degree
        The absolute mean error on Yaw angle estimation: 17.47 Degree
        The absolute mean error on Roll angle estimation: 6.55 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 13.60 Degree
        The absolute mean error on Yaw angle estimation: 31.58 Degree
        The absolute mean error on Roll angle estimation: 12.89 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.61 Degree
        The absolute mean error on Yaw angle estimations: 23.59 Degree
        The absolute mean error on Roll angle estimations: 9.13 Degree
Exp2019-01-29_13-03-34_part1 completed!
Training model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
All frames and annotations from 20 datasets have been read by 2019-01-29 13:46:05.069657
1. set (Dataset 21) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:46:11.099805!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1313
2. set (Dataset 19) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:46:30.367057!
Epoch 1/1
502/502 [==============================] - 11s 21ms/step - loss: 0.0738
3. set (Dataset 24) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:46:45.938578!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0642
4. set (Dataset 18) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:47:02.630210!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1040
5. set (Dataset 23) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:47:21.882812!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.1127
6. set (Dataset 13) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:47:39.051927!
Epoch 1/1
485/485 [==============================] - 10s 22ms/step - loss: 0.0749
7. set (Dataset 16) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:47:58.367980!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0845
8. set (Dataset 15) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:48:25.852157!
Epoch 1/1
654/654 [==============================] - 14s 22ms/step - loss: 0.0853
9. set (Dataset 8) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:48:48.158857!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0983
10. set (Dataset 7) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:49:13.575604!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0907
11. set (Dataset 11) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:49:36.679327!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0656
12. set (Dataset 10) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:49:58.001709!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0886
13. set (Dataset 22) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:50:21.958712!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0636
14. set (Dataset 6) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:50:42.465625!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1152
15. set (Dataset 2) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:50:59.912830!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1065
16. set (Dataset 20) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:51:16.967557!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0737
17. set (Dataset 1) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:51:35.035104!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1256
18. set (Dataset 17) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:51:50.840721!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0809
19. set (Dataset 12) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:52:07.096151!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0802
20. set (Dataset 4) being trained for epoch 1 in Experiment 2 by 2019-01-29 13:52:31.630950!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.0990
Epoch 1 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 13:52:53.117175
1. set (Dataset 17) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:52:56.874002!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0793
2. set (Dataset 4) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:53:13.299071!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.0953
3. set (Dataset 11) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:53:36.247828!
Epoch 1/1
572/572 [==============================] - 13s 22ms/step - loss: 0.0660
4. set (Dataset 2) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:53:54.261411!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1054
5. set (Dataset 13) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:54:11.177839!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0689
6. set (Dataset 12) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:54:29.178435!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0771
7. set (Dataset 1) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:54:51.411501!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1127
8. set (Dataset 10) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:55:09.514958!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0832
9. set (Dataset 23) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:55:32.129713!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1246
10. set (Dataset 8) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:55:53.177485!
Epoch 1/1
772/772 [==============================] - 17s 23ms/step - loss: 0.0915
11. set (Dataset 16) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:56:19.484016!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0890
12. set (Dataset 21) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:56:46.764015!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1372
13. set (Dataset 6) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:57:06.845867!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1130
14. set (Dataset 19) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:57:24.690196!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0765
15. set (Dataset 24) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:57:40.885667!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0660
16. set (Dataset 20) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:57:57.571390!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0727
17. set (Dataset 22) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:58:16.853406!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0610
18. set (Dataset 18) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:58:38.096427!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1038
19. set (Dataset 7) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:59:00.546407!
Epoch 1/1
745/745 [==============================] - 16s 22ms/step - loss: 0.0994
20. set (Dataset 15) being trained for epoch 2 in Experiment 2 by 2019-01-29 13:59:23.426061!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0837
Epoch 2 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 13:59:42.774702
1. set (Dataset 18) being trained for epoch 3 in Experiment 2 by 2019-01-29 13:59:48.669309!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1059
2. set (Dataset 15) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:00:09.402788!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0817
3. set (Dataset 16) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:00:33.948204!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0814
4. set (Dataset 24) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:01:00.087279!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0644
5. set (Dataset 12) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:01:18.713732!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0873
6. set (Dataset 7) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:01:42.949657!
Epoch 1/1
745/745 [==============================] - 16s 22ms/step - loss: 0.0929
7. set (Dataset 22) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:02:05.849653!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0631
8. set (Dataset 21) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:02:27.091861!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1360
9. set (Dataset 13) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:02:46.626612!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0785
10. set (Dataset 23) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:03:03.379732!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1181
11. set (Dataset 1) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:03:22.238172!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1150
12. set (Dataset 17) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:03:37.759835!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0820
13. set (Dataset 19) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:03:52.210132!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0820
14. set (Dataset 4) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:04:11.347127!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1033
15. set (Dataset 11) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:04:33.955426!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0636
16. set (Dataset 20) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:04:53.060176!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0769
17. set (Dataset 6) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:05:11.229628!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1143
18. set (Dataset 2) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:05:28.491567!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1069
19. set (Dataset 8) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:05:48.018618!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0938
20. set (Dataset 10) being trained for epoch 3 in Experiment 2 by 2019-01-29 14:06:12.502122!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0897
Epoch 3 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 14:06:33.969702
1. set (Dataset 2) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:06:39.064809!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1017
2. set (Dataset 10) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:06:58.166081!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0852
3. set (Dataset 1) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:07:19.596921!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1114
4. set (Dataset 11) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:07:37.003496!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0678
5. set (Dataset 7) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:07:58.904133!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0941
6. set (Dataset 8) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:08:24.024933!
Epoch 1/1
772/772 [==============================] - 17s 23ms/step - loss: 0.0876
7. set (Dataset 6) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:08:46.750587!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1210
8. set (Dataset 17) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:09:02.309271!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0869
9. set (Dataset 12) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:09:19.079495!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0787
10. set (Dataset 13) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:09:41.928448!
Epoch 1/1
485/485 [==============================] - 11s 24ms/step - loss: 0.0692
11. set (Dataset 22) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:09:59.891860!
Epoch 1/1
665/665 [==============================] - 14s 21ms/step - loss: 0.0646
12. set (Dataset 18) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:10:20.197666!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1080
13. set (Dataset 4) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:10:42.048225!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1017
14. set (Dataset 15) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:11:06.321303!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0865
15. set (Dataset 16) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:11:30.890127!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0846
16. set (Dataset 20) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:11:57.361851!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0726
17. set (Dataset 19) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:12:14.659057!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0783
18. set (Dataset 24) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:12:31.332381!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0633
19. set (Dataset 23) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:12:48.194520!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1156
20. set (Dataset 21) being trained for epoch 4 in Experiment 2 by 2019-01-29 14:13:08.001254!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1319
Epoch 4 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 14:13:27.123767
1. set (Dataset 24) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:13:31.812110!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0652
2. set (Dataset 21) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:13:49.584070!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1321
3. set (Dataset 22) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:14:10.745202!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0604
4. set (Dataset 16) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:14:34.607046!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0835
5. set (Dataset 8) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:15:03.667231!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.1017
6. set (Dataset 23) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:15:27.243598!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1142
7. set (Dataset 19) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:15:45.751094!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0757
8. set (Dataset 18) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:16:03.219669!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1067
9. set (Dataset 7) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:16:24.632366!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0962
10. set (Dataset 12) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:16:50.001823!
Epoch 1/1
732/732 [==============================] - 16s 21ms/step - loss: 0.0860
11. set (Dataset 6) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:17:11.016281!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1091
12. set (Dataset 2) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:17:29.078113!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1050
13. set (Dataset 15) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:17:47.373585!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0901
14. set (Dataset 10) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:18:09.825041!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0900
15. set (Dataset 1) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:18:32.494301!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1124
16. set (Dataset 20) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:18:49.969383!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0781
17. set (Dataset 4) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:19:10.349244!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1030
18. set (Dataset 11) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:19:33.743768!
Epoch 1/1
572/572 [==============================] - 12s 22ms/step - loss: 0.0660
19. set (Dataset 13) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:19:51.120238!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0690
20. set (Dataset 17) being trained for epoch 5 in Experiment 2 by 2019-01-29 14:20:06.074403!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0838
Epoch 5 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 14:20:19.043011
1. set (Dataset 11) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:20:24.767384!
Epoch 1/1
572/572 [==============================] - 13s 22ms/step - loss: 0.0629
2. set (Dataset 17) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:20:41.235015!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0808
3. set (Dataset 6) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:20:55.404735!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1120
4. set (Dataset 1) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:21:12.756106!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1190
5. set (Dataset 23) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:21:29.832449!
Epoch 1/1
569/569 [==============================] - 13s 24ms/step - loss: 0.1180
6. set (Dataset 13) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:21:48.163084!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0655
7. set (Dataset 4) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:22:06.627699!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.0977
8. set (Dataset 2) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:22:28.923018!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1053
9. set (Dataset 8) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:22:48.914618!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0949
10. set (Dataset 7) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:23:13.719788!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0916
11. set (Dataset 19) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:23:35.591251!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0829
12. set (Dataset 24) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:23:52.372639!
Epoch 1/1
492/492 [==============================] - 11s 21ms/step - loss: 0.0710
13. set (Dataset 10) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:24:10.287387!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0921
14. set (Dataset 21) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:24:33.468225!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1412
15. set (Dataset 22) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:24:53.606217!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0628
16. set (Dataset 20) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:25:15.148543!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0770
17. set (Dataset 15) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:25:33.892142!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0836
18. set (Dataset 16) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:25:58.389754!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0819
19. set (Dataset 12) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:26:27.088258!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.0849
20. set (Dataset 18) being trained for epoch 6 in Experiment 2 by 2019-01-29 14:26:50.725662!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1077
Epoch 6 for Experiment 2 completed!
Exp2019-01-29_13-03-34_part2.h5 has been saved.
The subjects are trained: [(11, 'M05'), (17, 'M10'), (6, 'F06'), (1, 'F01'), (23, 'M13'), (13, 'M07'), (4, 'F04'), (2, '
F02'), (8, 'M02'), (7, 'M01'), (19, 'M11'), (24, 'M14'), (10, 'M04'), (21, 'F02'), (22, 'M01'), (20, 'M12'), (15, 'F03')
, (16, 'M09'), (12, 'M06'), (18, 'F05')]
Evaluating model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 14:27:08.056824
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 9.61 Degree
        The absolute mean error on Yaw angle estimation: 23.02 Degree
        The absolute mean error on Roll angle estimation: 9.50 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 7.23 Degree
        The absolute mean error on Yaw angle estimation: 22.75 Degree
        The absolute mean error on Roll angle estimation: 4.24 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 33.17 Degree
        The absolute mean error on Yaw angle estimation: 22.98 Degree
        The absolute mean error on Roll angle estimation: 6.90 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 17ms/step
        The absolute mean error on Pitch angle estimation: 14.01 Degree
        The absolute mean error on Yaw angle estimation: 30.25 Degree
        The absolute mean error on Roll angle estimation: 13.16 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.01 Degree
        The absolute mean error on Yaw angle estimations: 24.75 Degree
        The absolute mean error on Roll angle estimations: 8.45 Degree
Exp2019-01-29_13-03-34_part2 completed!
Training model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
All frames and annotations from 20 datasets have been read by 2019-01-29 14:28:39.933135
1. set (Dataset 16) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:28:48.700542!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0834
2. set (Dataset 18) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:29:15.986754!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1011
3. set (Dataset 19) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:29:34.922476!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0783
4. set (Dataset 22) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:29:52.772814!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0620
5. set (Dataset 13) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:30:13.110355!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0743
6. set (Dataset 12) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:30:31.707032!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0849
7. set (Dataset 15) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:30:54.841872!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0841
8. set (Dataset 24) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:31:15.379593!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0683
9. set (Dataset 23) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:31:32.246858!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1174
10. set (Dataset 8) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:31:53.226048!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0947
11. set (Dataset 4) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:32:18.319202!
Epoch 1/1
744/744 [==============================] - 16s 22ms/step - loss: 0.1015
12. set (Dataset 11) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:32:40.517740!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0646
13. set (Dataset 21) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:33:00.402205!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1374
14. set (Dataset 17) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:33:18.367520!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0854
15. set (Dataset 6) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:33:32.302719!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1124
16. set (Dataset 20) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:33:49.461120!
Epoch 1/1
556/556 [==============================] - 12s 21ms/step - loss: 0.0739
17. set (Dataset 10) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:34:08.758026!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0932
18. set (Dataset 1) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:34:31.394630!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1197
19. set (Dataset 7) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:34:50.566424!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0894
20. set (Dataset 2) being trained for epoch 1 in Experiment 3 by 2019-01-29 14:35:13.082862!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1060
Epoch 1 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 14:35:29.820744
1. set (Dataset 1) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:35:34.879536!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1152
2. set (Dataset 2) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:35:51.343434!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1027
3. set (Dataset 4) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:36:10.759214!
Epoch 1/1
744/744 [==============================] - 17s 22ms/step - loss: 0.1005
4. set (Dataset 6) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:36:32.673357!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1159
5. set (Dataset 12) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:36:52.306329!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0800
6. set (Dataset 7) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:37:16.590929!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0925
7. set (Dataset 10) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:37:41.797396!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0870
8. set (Dataset 11) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:38:04.465386!
Epoch 1/1
572/572 [==============================] - 13s 24ms/step - loss: 0.0645
9. set (Dataset 13) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:38:22.861479!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0716
10. set (Dataset 23) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:38:39.601507!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1228
11. set (Dataset 15) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:38:59.080127!
Epoch 1/1
654/654 [==============================] - 14s 22ms/step - loss: 0.0879
12. set (Dataset 16) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:39:22.406261!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0838
13. set (Dataset 17) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:39:47.418411!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0816
14. set (Dataset 18) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:40:02.639104!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1035
15. set (Dataset 19) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:40:22.392613!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0826
16. set (Dataset 20) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:40:39.824776!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0725
17. set (Dataset 21) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:40:58.959113!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1291
18. set (Dataset 22) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:41:20.227506!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0623
19. set (Dataset 8) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:41:42.810977!
Epoch 1/1
772/772 [==============================] - 18s 24ms/step - loss: 0.0970
20. set (Dataset 24) being trained for epoch 2 in Experiment 3 by 2019-01-29 14:42:05.801728!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0667
Epoch 2 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 14:42:21.491946
1. set (Dataset 22) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:42:27.908502!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0601
2. set (Dataset 24) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:42:47.866756!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0643
3. set (Dataset 15) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:43:05.338234!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0844
4. set (Dataset 19) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:43:25.514808!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0749
5. set (Dataset 7) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:43:44.479669!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0972
6. set (Dataset 8) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:44:09.207143!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0924
7. set (Dataset 21) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:44:33.269206!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1376
8. set (Dataset 16) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:44:55.963590!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0830
9. set (Dataset 12) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:45:24.143805!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0829
10. set (Dataset 13) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:45:45.732143!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0709
11. set (Dataset 10) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:46:04.308088!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0916
12. set (Dataset 1) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:46:26.285578!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1135
13. set (Dataset 18) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:46:43.521574!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1103
14. set (Dataset 2) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:47:03.062805!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1038
15. set (Dataset 4) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:47:22.582949!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1007
16. set (Dataset 20) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:47:45.207402!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0810
17. set (Dataset 17) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:48:02.094800!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0842
18. set (Dataset 6) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:48:16.520669!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1129
19. set (Dataset 23) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:48:34.654345!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1164
20. set (Dataset 11) being trained for epoch 3 in Experiment 3 by 2019-01-29 14:48:53.815361!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0638
Epoch 3 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 14:49:11.995673
1. set (Dataset 6) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:49:17.251319!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1062
2. set (Dataset 11) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:49:35.812507!
Epoch 1/1
572/572 [==============================] - 12s 22ms/step - loss: 0.0626
3. set (Dataset 10) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:49:55.624687!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0885
4. set (Dataset 4) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:50:20.057333!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.0995
5. set (Dataset 8) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:50:44.765521!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0911
6. set (Dataset 23) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:51:07.492438!
Epoch 1/1
569/569 [==============================] - 13s 22ms/step - loss: 0.1244
7. set (Dataset 17) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:51:24.020947!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0841
8. set (Dataset 1) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:51:38.539743!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1133
9. set (Dataset 7) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:51:57.395272!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0925
10. set (Dataset 12) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:52:22.066166!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0771
11. set (Dataset 21) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:52:44.861721!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1379
12. set (Dataset 22) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:53:06.114052!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0627
13. set (Dataset 2) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:53:26.689457!
Epoch 1/1
511/511 [==============================] - 11s 21ms/step - loss: 0.1047
14. set (Dataset 24) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:53:42.484714!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0689
15. set (Dataset 15) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:53:59.915991!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0866
16. set (Dataset 20) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:54:20.169749!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0743
17. set (Dataset 18) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:54:39.265044!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1055
18. set (Dataset 19) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:54:58.385464!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0792
19. set (Dataset 13) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:55:15.318907!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0743
20. set (Dataset 16) being trained for epoch 4 in Experiment 3 by 2019-01-29 14:55:35.286280!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0859
Epoch 4 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 14:56:00.368729
1. set (Dataset 19) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:56:05.268002!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0757
2. set (Dataset 16) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:56:25.898919!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0802
3. set (Dataset 21) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:56:53.379795!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1289
4. set (Dataset 15) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:57:14.570920!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0898
5. set (Dataset 23) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:57:35.491377!
Epoch 1/1
569/569 [==============================] - 13s 24ms/step - loss: 0.1181
6. set (Dataset 13) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:57:53.869650!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0715
7. set (Dataset 18) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:58:10.303285!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1029
8. set (Dataset 22) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:58:30.431589!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0608
9. set (Dataset 8) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:58:53.416627!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0943
10. set (Dataset 7) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:59:17.979841!
Epoch 1/1
745/745 [==============================] - 16s 22ms/step - loss: 0.0954
11. set (Dataset 17) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:59:38.234282!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0850
12. set (Dataset 6) being trained for epoch 5 in Experiment 3 by 2019-01-29 14:59:52.589172!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1100
13. set (Dataset 24) being trained for epoch 5 in Experiment 3 by 2019-01-29 15:00:10.221270!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0650
14. set (Dataset 11) being trained for epoch 5 in Experiment 3 by 2019-01-29 15:00:26.842748!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0642
15. set (Dataset 10) being trained for epoch 5 in Experiment 3 by 2019-01-29 15:00:47.469136!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0858
16. set (Dataset 20) being trained for epoch 5 in Experiment 3 by 2019-01-29 15:01:10.031689!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0767
17. set (Dataset 2) being trained for epoch 5 in Experiment 3 by 2019-01-29 15:01:28.395024!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1098
18. set (Dataset 4) being trained for epoch 5 in Experiment 3 by 2019-01-29 15:01:47.064222!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1002
19. set (Dataset 12) being trained for epoch 5 in Experiment 3 by 2019-01-29 15:02:11.367735!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0804
20. set (Dataset 1) being trained for epoch 5 in Experiment 3 by 2019-01-29 15:02:33.407123!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1152
Epoch 5 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 15:02:49.045341
1. set (Dataset 4) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:02:56.468528!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.0989
2. set (Dataset 1) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:03:18.783655!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1095
3. set (Dataset 17) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:03:34.132455!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0852
4. set (Dataset 10) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:03:50.580432!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0856
5. set (Dataset 13) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:04:12.438574!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0706
6. set (Dataset 12) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:04:31.520621!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0809
7. set (Dataset 2) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:04:53.837108!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1050
8. set (Dataset 6) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:05:11.479218!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1164
9. set (Dataset 23) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:05:29.588265!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1210
10. set (Dataset 8) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:05:50.488664!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0929
11. set (Dataset 18) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:06:14.387530!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1100
12. set (Dataset 19) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:06:33.009714!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0820
13. set (Dataset 11) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:06:50.783608!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0654
14. set (Dataset 16) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:07:12.861260!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0833
15. set (Dataset 21) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:07:39.916201!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1373
16. set (Dataset 20) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:08:00.390410!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0751
17. set (Dataset 24) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:08:17.230610!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0654
18. set (Dataset 15) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:08:34.734071!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0831
19. set (Dataset 7) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:08:57.559869!
Epoch 1/1
745/745 [==============================] - 16s 22ms/step - loss: 0.0991
20. set (Dataset 22) being trained for epoch 6 in Experiment 3 by 2019-01-29 15:09:20.486647!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0616
Epoch 6 for Experiment 3 completed!
Exp2019-01-29_13-03-34_part3.h5 has been saved.
The subjects are trained: [(4, 'F04'), (1, 'F01'), (17, 'M10'), (10, 'M04'), (13, 'M07'), (12, 'M06'), (2, 'F02'), (6, '
F06'), (23, 'M13'), (8, 'M02'), (18, 'F05'), (19, 'M11'), (11, 'M05'), (16, 'M09'), (21, 'F02'), (20, 'M12'), (24, 'M14'
), (15, 'F03'), (7, 'M01'), (22, 'M01')]
Evaluating model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 15:09:37.977354
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.56 Degree
        The absolute mean error on Yaw angle estimation: 26.07 Degree
        The absolute mean error on Roll angle estimation: 10.74 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 6.07 Degree
        The absolute mean error on Yaw angle estimation: 21.00 Degree
        The absolute mean error on Roll angle estimation: 3.80 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 35.87 Degree
        The absolute mean error on Yaw angle estimation: 25.29 Degree
        The absolute mean error on Roll angle estimation: 7.92 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 17ms/step
        The absolute mean error on Pitch angle estimation: 16.12 Degree
        The absolute mean error on Yaw angle estimation: 29.66 Degree
        The absolute mean error on Roll angle estimation: 13.25 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.66 Degree
        The absolute mean error on Yaw angle estimations: 25.50 Degree
        The absolute mean error on Roll angle estimations: 8.93 Degree
Exp2019-01-29_13-03-34_part3 completed!
Training model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
All frames and annotations from 20 datasets have been read by 2019-01-29 15:11:10.357797
1. set (Dataset 15) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:11:16.721339!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0846
2. set (Dataset 22) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:11:38.941484!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0603
3. set (Dataset 18) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:11:59.526276!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1037
4. set (Dataset 21) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:12:19.752021!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1277
5. set (Dataset 12) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:12:41.815225!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.0859
6. set (Dataset 7) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:13:07.115533!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0925
7. set (Dataset 24) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:13:28.735697!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0660
8. set (Dataset 19) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:13:44.979077!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0783
9. set (Dataset 13) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:14:01.411894!
Epoch 1/1
485/485 [==============================] - 11s 24ms/step - loss: 0.0735
10. set (Dataset 23) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:14:18.392159!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1193
11. set (Dataset 2) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:14:37.259634!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1068
12. set (Dataset 4) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:14:55.778495!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1037
13. set (Dataset 16) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:15:21.868509!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0834
14. set (Dataset 1) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:15:48.790569!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1199
15. set (Dataset 17) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:16:03.511665!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0815
16. set (Dataset 20) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:16:18.063005!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0791
17. set (Dataset 11) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:16:36.689091!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0643
18. set (Dataset 10) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:16:57.451044!
Epoch 1/1
726/726 [==============================] - 16s 23ms/step - loss: 0.0878
19. set (Dataset 8) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:17:21.758936!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0915
20. set (Dataset 6) being trained for epoch 1 in Experiment 4 by 2019-01-29 15:17:45.154962!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1141
Epoch 1 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 15:18:02.604404
1. set (Dataset 10) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:18:09.840779!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0865
2. set (Dataset 6) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:18:31.655287!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1111
3. set (Dataset 2) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:18:49.354603!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1067
4. set (Dataset 17) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:19:05.023305!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0829
5. set (Dataset 7) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:19:21.936942!
Epoch 1/1
745/745 [==============================] - 17s 22ms/step - loss: 0.0899
6. set (Dataset 8) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:19:46.421057!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0907
7. set (Dataset 11) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:20:10.140905!
Epoch 1/1
572/572 [==============================] - 13s 22ms/step - loss: 0.0613
8. set (Dataset 4) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:20:30.448011!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.0999
9. set (Dataset 12) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:20:55.438173!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0804
10. set (Dataset 13) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:21:17.084635!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0680
11. set (Dataset 24) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:21:32.851226!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0749
12. set (Dataset 15) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:21:51.164241!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0845
13. set (Dataset 1) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:22:11.498845!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1124
14. set (Dataset 22) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:22:29.289330!
Epoch 1/1
665/665 [==============================] - 16s 23ms/step - loss: 0.0659
15. set (Dataset 18) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:22:50.810600!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1092
16. set (Dataset 20) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:23:10.855202!
Epoch 1/1
556/556 [==============================] - 13s 22ms/step - loss: 0.0745
17. set (Dataset 16) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:23:32.246590!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0827
18. set (Dataset 21) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:23:59.204519!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1296
19. set (Dataset 23) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:24:19.045387!
Epoch 1/1
569/569 [==============================] - 13s 22ms/step - loss: 0.1158
20. set (Dataset 19) being trained for epoch 2 in Experiment 4 by 2019-01-29 15:24:36.769616!
Epoch 1/1
502/502 [==============================] - 11s 21ms/step - loss: 0.0786
Epoch 2 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 15:24:52.033415
1. set (Dataset 21) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:24:58.069457!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1284
2. set (Dataset 19) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:25:18.021286!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0741
3. set (Dataset 24) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:25:33.679001!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0597
4. set (Dataset 18) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:25:50.827571!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1026
5. set (Dataset 8) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:26:12.411092!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.1000
6. set (Dataset 23) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:26:35.864660!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1198
7. set (Dataset 16) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:26:57.752185!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0823
8. set (Dataset 15) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:27:25.896760!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0833
9. set (Dataset 7) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:27:48.602966!
Epoch 1/1
745/745 [==============================] - 16s 22ms/step - loss: 0.0965
10. set (Dataset 12) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:28:12.383654!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.0849
11. set (Dataset 11) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:28:35.699371!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0658
12. set (Dataset 10) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:28:56.656740!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0875
13. set (Dataset 22) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:29:18.807472!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0643
14. set (Dataset 6) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:29:39.496629!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1146
15. set (Dataset 2) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:29:57.257606!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1047
16. set (Dataset 20) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:30:15.016455!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0771
17. set (Dataset 1) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:30:32.562519!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1218
18. set (Dataset 17) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:30:48.118461!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0803
19. set (Dataset 13) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:31:02.036661!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0679
20. set (Dataset 4) being trained for epoch 3 in Experiment 4 by 2019-01-29 15:31:20.854142!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1037
Epoch 3 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 15:31:43.278700
1. set (Dataset 17) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:31:47.032599!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0851
2. set (Dataset 4) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:32:03.178223!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.0997
3. set (Dataset 11) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:32:26.219877!
Epoch 1/1
572/572 [==============================] - 13s 22ms/step - loss: 0.0674
4. set (Dataset 2) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:32:44.267953!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1082
5. set (Dataset 23) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:33:02.114513!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1219
6. set (Dataset 13) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:33:19.944811!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0655
7. set (Dataset 1) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:33:36.366684!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1182
8. set (Dataset 10) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:33:55.730994!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0843
9. set (Dataset 8) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:34:19.640492!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0902
10. set (Dataset 7) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:34:45.250502!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0909
11. set (Dataset 16) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:35:11.580406!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0876
12. set (Dataset 21) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:35:38.732373!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1389
13. set (Dataset 6) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:35:58.183296!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1101
14. set (Dataset 19) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:36:16.142198!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0771
15. set (Dataset 24) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:36:32.852779!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0626
16. set (Dataset 20) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:36:49.369749!
Epoch 1/1
556/556 [==============================] - 12s 21ms/step - loss: 0.0699
17. set (Dataset 22) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:37:07.830728!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0589
18. set (Dataset 18) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:37:29.223627!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1038
19. set (Dataset 12) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:37:50.640421!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0846
20. set (Dataset 15) being trained for epoch 4 in Experiment 4 by 2019-01-29 15:38:14.578400!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0833
Epoch 4 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 15:38:34.194620
1. set (Dataset 18) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:38:40.095896!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1036
2. set (Dataset 15) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:39:01.222486!
Epoch 1/1
654/654 [==============================] - 15s 22ms/step - loss: 0.0847
3. set (Dataset 16) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:39:24.757883!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0814
4. set (Dataset 24) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:39:50.973387!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0617
5. set (Dataset 13) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:40:07.143497!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0741
6. set (Dataset 12) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:40:25.466539!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0831
7. set (Dataset 22) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:40:48.745708!
Epoch 1/1
665/665 [==============================] - 16s 23ms/step - loss: 0.0605
8. set (Dataset 21) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:41:10.437909!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1322
9. set (Dataset 23) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:41:31.147643!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1158
10. set (Dataset 8) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:41:52.097923!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0971
11. set (Dataset 1) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:42:15.075317!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1206
12. set (Dataset 17) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:42:30.457817!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0843
13. set (Dataset 19) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:42:44.926663!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0798
14. set (Dataset 4) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:43:03.779395!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1046
15. set (Dataset 11) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:43:27.469093!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0626
16. set (Dataset 20) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:43:45.982773!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0769
17. set (Dataset 6) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:44:04.206329!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1113
18. set (Dataset 2) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:44:21.952715!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1115
19. set (Dataset 7) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:44:41.225551!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0881
20. set (Dataset 10) being trained for epoch 5 in Experiment 4 by 2019-01-29 15:45:06.068962!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0872
Epoch 5 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 15:45:26.589308
1. set (Dataset 2) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:45:31.692427!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1013
2. set (Dataset 10) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:45:51.244694!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0844
3. set (Dataset 1) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:46:13.099249!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1080
4. set (Dataset 11) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:46:30.224440!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0656
5. set (Dataset 12) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:46:50.620916!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0819
6. set (Dataset 7) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:47:14.625992!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0956
7. set (Dataset 6) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:47:37.231189!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1135
8. set (Dataset 17) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:47:53.015602!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0849
9. set (Dataset 13) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:48:07.319498!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0649
10. set (Dataset 23) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:48:24.166793!
Epoch 1/1
569/569 [==============================] - 13s 22ms/step - loss: 0.1163
11. set (Dataset 22) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:48:43.469116!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0603
12. set (Dataset 18) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:49:04.571802!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1009
13. set (Dataset 4) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:49:26.292418!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1046
14. set (Dataset 15) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:49:49.835844!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0863
15. set (Dataset 16) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:50:14.422616!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0849
16. set (Dataset 20) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:50:41.965258!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0737
17. set (Dataset 19) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:50:59.922916!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0769
18. set (Dataset 24) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:51:16.263570!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0632
19. set (Dataset 8) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:51:34.975466!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0966
20. set (Dataset 21) being trained for epoch 6 in Experiment 4 by 2019-01-29 15:51:58.365321!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1339
Epoch 6 for Experiment 4 completed!
Exp2019-01-29_13-03-34_part4.h5 has been saved.
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (12, 'M06'), (7, 'M01'), (6, 'F06'), (17, '
M10'), (13, 'M07'), (23, 'M13'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'), (20, 'M12'), (19, 'M11'
), (24, 'M14'), (8, 'M02'), (21, 'F02')]
Evaluating model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 15:52:14.962648
For the Subject 3 (F03):
730/730 [==============================] - 11s 16ms/step
        The absolute mean error on Pitch angle estimation: 7.82 Degree
        The absolute mean error on Yaw angle estimation: 23.81 Degree
        The absolute mean error on Roll angle estimation: 15.53 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.36 Degree
        The absolute mean error on Yaw angle estimation: 20.17 Degree
        The absolute mean error on Roll angle estimation: 5.86 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 31.25 Degree
        The absolute mean error on Yaw angle estimation: 23.78 Degree
        The absolute mean error on Roll angle estimation: 9.93 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 13.65 Degree
        The absolute mean error on Yaw angle estimation: 30.20 Degree
        The absolute mean error on Roll angle estimation: 13.80 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.27 Degree
        The absolute mean error on Yaw angle estimations: 24.49 Degree
        The absolute mean error on Roll angle estimations: 11.28 Degree
Exp2019-01-29_13-03-34_part4 completed!
Training model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
All frames and annotations from 20 datasets have been read by 2019-01-29 15:53:46.750866
1. set (Dataset 24) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:53:51.450469!
Epoch 1/1
492/492 [==============================] - 12s 23ms/step - loss: 0.0652
2. set (Dataset 21) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:54:09.114787!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1279
3. set (Dataset 22) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:54:30.903141!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0625
4. set (Dataset 16) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:54:55.270390!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0824
5. set (Dataset 7) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:55:24.234122!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0978
6. set (Dataset 8) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:55:49.321072!
Epoch 1/1
772/772 [==============================] - 17s 23ms/step - loss: 0.0925
7. set (Dataset 19) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:56:11.749420!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0813
8. set (Dataset 18) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:56:29.640595!
Epoch 1/1
614/614 [==============================] - 14s 24ms/step - loss: 0.1032
9. set (Dataset 12) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:56:51.491743!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0814
10. set (Dataset 13) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:57:13.029162!
Epoch 1/1
485/485 [==============================] - 11s 24ms/step - loss: 0.0695
11. set (Dataset 6) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:57:29.819095!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1132
12. set (Dataset 2) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:57:47.646184!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1029
13. set (Dataset 15) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:58:05.557781!
Epoch 1/1
654/654 [==============================] - 15s 22ms/step - loss: 0.0863
14. set (Dataset 10) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:58:27.573304!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0877
15. set (Dataset 1) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:58:50.211017!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1111
16. set (Dataset 20) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:59:07.390032!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0795
17. set (Dataset 4) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:59:28.168223!
Epoch 1/1
744/744 [==============================] - 15s 20ms/step - loss: 0.1006
18. set (Dataset 11) being trained for epoch 1 in Experiment 5 by 2019-01-29 15:59:48.831870!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0669
19. set (Dataset 23) being trained for epoch 1 in Experiment 5 by 2019-01-29 16:00:07.805980!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1211
20. set (Dataset 17) being trained for epoch 1 in Experiment 5 by 2019-01-29 16:00:24.579323!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0803
Epoch 1 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 16:00:37.490181
1. set (Dataset 11) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:00:43.207727!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0610
2. set (Dataset 17) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:01:00.223891!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0834
3. set (Dataset 6) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:01:14.233523!
Epoch 1/1
542/542 [==============================] - 12s 21ms/step - loss: 0.1100
4. set (Dataset 1) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:01:31.033185!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1151
5. set (Dataset 8) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:01:50.525308!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0955
6. set (Dataset 23) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:02:13.330376!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1178
7. set (Dataset 4) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:02:34.363457!
Epoch 1/1
744/744 [==============================] - 17s 22ms/step - loss: 0.1062
8. set (Dataset 2) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:02:56.157116!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1058
9. set (Dataset 7) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:03:16.004545!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0928
10. set (Dataset 12) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:03:41.186919!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0789
11. set (Dataset 19) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:04:03.399733!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0862
12. set (Dataset 24) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:04:20.190826!
Epoch 1/1
492/492 [==============================] - 11s 21ms/step - loss: 0.0674
13. set (Dataset 10) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:04:38.108838!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0870
14. set (Dataset 21) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:05:01.077293!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1362
15. set (Dataset 22) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:05:21.563997!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0633
16. set (Dataset 20) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:05:42.733027!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0757
17. set (Dataset 15) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:06:01.942828!
Epoch 1/1
654/654 [==============================] - 15s 24ms/step - loss: 0.0847
18. set (Dataset 16) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:06:26.247071!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0821
19. set (Dataset 13) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:06:52.441081!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0739
20. set (Dataset 18) being trained for epoch 2 in Experiment 5 by 2019-01-29 16:07:09.481207!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1078
Epoch 2 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 16:07:28.049328
1. set (Dataset 16) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:07:36.815924!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0814
2. set (Dataset 18) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:08:03.997434!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.0994
3. set (Dataset 19) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:08:23.738826!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0816
4. set (Dataset 22) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:08:41.704352!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0639
5. set (Dataset 23) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:09:02.214931!
Epoch 1/1
569/569 [==============================] - 13s 24ms/step - loss: 0.1122
6. set (Dataset 13) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:09:20.576509!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0721
7. set (Dataset 15) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:09:38.190275!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0883
8. set (Dataset 24) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:09:58.506127!
Epoch 1/1
492/492 [==============================] - 11s 21ms/step - loss: 0.0657
9. set (Dataset 8) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:10:16.952138!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0950
10. set (Dataset 7) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:10:42.234052!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0948
11. set (Dataset 4) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:11:07.183400!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1022
12. set (Dataset 11) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:11:29.837301!
Epoch 1/1
572/572 [==============================] - 13s 24ms/step - loss: 0.0684
13. set (Dataset 21) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:11:49.437531!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1340
14. set (Dataset 17) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:12:07.551771!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0861
15. set (Dataset 6) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:12:22.302690!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1138
16. set (Dataset 20) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:12:40.620681!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0706
17. set (Dataset 10) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:13:01.007965!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0895
18. set (Dataset 1) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:13:22.395103!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1136
19. set (Dataset 12) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:13:41.345900!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0797
20. set (Dataset 2) being trained for epoch 3 in Experiment 5 by 2019-01-29 16:14:02.989780!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1097
Epoch 3 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 16:14:18.775654
1. set (Dataset 1) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:14:23.826875!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1081
2. set (Dataset 2) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:14:40.778960!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1013
3. set (Dataset 4) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:15:00.308018!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1004
4. set (Dataset 6) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:15:23.376658!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1146
5. set (Dataset 13) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:15:40.217796!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0715
6. set (Dataset 12) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:15:58.886623!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0822
7. set (Dataset 10) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:16:22.834362!
Epoch 1/1
726/726 [==============================] - 16s 23ms/step - loss: 0.0864
8. set (Dataset 11) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:16:45.043365!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0672
9. set (Dataset 23) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:17:04.250948!
Epoch 1/1
569/569 [==============================] - 12s 22ms/step - loss: 0.1201
10. set (Dataset 8) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:17:24.379441!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0890
11. set (Dataset 15) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:17:48.578929!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0902
12. set (Dataset 16) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:18:12.540221!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0846
13. set (Dataset 17) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:18:37.536732!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0808
14. set (Dataset 18) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:18:52.860567!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1040
15. set (Dataset 19) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:19:12.209265!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0789
16. set (Dataset 20) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:19:28.623119!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0698
17. set (Dataset 21) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:19:47.067998!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1309
18. set (Dataset 22) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:20:08.060234!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0643
19. set (Dataset 7) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:20:30.617759!
Epoch 1/1
745/745 [==============================] - 16s 22ms/step - loss: 0.0981
20. set (Dataset 24) being trained for epoch 4 in Experiment 5 by 2019-01-29 16:20:51.485250!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0681
Epoch 4 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 16:21:07.365098
1. set (Dataset 22) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:21:13.768116!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0589
2. set (Dataset 24) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:21:34.609363!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0618
3. set (Dataset 15) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:21:51.677253!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0868
4. set (Dataset 19) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:22:11.742547!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0784
5. set (Dataset 12) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:22:30.758830!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0852
6. set (Dataset 7) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:22:55.073003!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0968
7. set (Dataset 21) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:23:18.734565!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1397
8. set (Dataset 16) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:23:41.946523!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0855
9. set (Dataset 13) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:24:08.075619!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0715
10. set (Dataset 23) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:24:24.247901!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1187
11. set (Dataset 10) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:24:44.772990!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0901
12. set (Dataset 1) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:25:05.785282!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1057
13. set (Dataset 18) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:25:23.392437!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1041
14. set (Dataset 2) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:25:42.702232!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1073
15. set (Dataset 4) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:26:01.250669!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1003
16. set (Dataset 20) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:26:24.082317!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0773
17. set (Dataset 17) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:26:41.212754!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0829
18. set (Dataset 6) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:26:55.700803!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1093
19. set (Dataset 8) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:27:15.406355!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0905
20. set (Dataset 11) being trained for epoch 5 in Experiment 5 by 2019-01-29 16:27:38.814305!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0635
Epoch 5 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 16:27:56.514867
1. set (Dataset 6) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:28:01.723367!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1076
2. set (Dataset 11) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:28:20.513897!
Epoch 1/1
572/572 [==============================] - 13s 24ms/step - loss: 0.0621
3. set (Dataset 10) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:28:41.321091!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0889
4. set (Dataset 4) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:29:05.551882!
Epoch 1/1
744/744 [==============================] - 16s 22ms/step - loss: 0.1018
5. set (Dataset 7) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:29:29.458770!
Epoch 1/1
745/745 [==============================] - 17s 22ms/step - loss: 0.1006
6. set (Dataset 8) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:29:53.980251!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0920
7. set (Dataset 17) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:30:15.409362!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0857
8. set (Dataset 1) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:30:29.420335!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1164
9. set (Dataset 12) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:30:48.542338!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0805
10. set (Dataset 13) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:31:10.448440!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0667
11. set (Dataset 21) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:31:26.975066!
Epoch 1/1
634/634 [==============================] - 22s 35ms/step - loss: 0.1390
12. set (Dataset 22) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:31:55.880905!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0607
13. set (Dataset 2) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:32:16.264745!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1030
14. set (Dataset 24) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:32:32.915067!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0693
15. set (Dataset 15) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:32:51.195955!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0894
16. set (Dataset 20) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:33:11.407430!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0716
17. set (Dataset 18) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:33:30.060501!
Epoch 1/1
614/614 [==============================] - 13s 21ms/step - loss: 0.1022
18. set (Dataset 19) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:33:48.251792!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0773
19. set (Dataset 23) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:34:05.396803!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1140
20. set (Dataset 16) being trained for epoch 6 in Experiment 5 by 2019-01-29 16:34:27.957377!
Epoch 1/1
914/914 [==============================] - 20s 22ms/step - loss: 0.0817
Epoch 6 for Experiment 5 completed!
Exp2019-01-29_13-03-34_part5.h5 has been saved.
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (7, 'M01'), (8, 'M02'), (17, 'M10'), (1, 'F
01'), (12, 'M06'), (13, 'M07'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'), (20, 'M12'), (18, 'F05')
, (19, 'M11'), (23, 'M13'), (16, 'M09')]
Evaluating model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 16:34:50.710846
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 9.27 Degree
        The absolute mean error on Yaw angle estimation: 22.35 Degree
        The absolute mean error on Roll angle estimation: 12.97 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.62 Degree
        The absolute mean error on Yaw angle estimation: 21.98 Degree
        The absolute mean error on Roll angle estimation: 4.67 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 32.96 Degree
        The absolute mean error on Yaw angle estimation: 21.14 Degree
        The absolute mean error on Roll angle estimation: 7.75 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 13.61 Degree
        The absolute mean error on Yaw angle estimation: 31.27 Degree
        The absolute mean error on Roll angle estimation: 13.23 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.11 Degree
        The absolute mean error on Yaw angle estimations: 24.18 Degree
        The absolute mean error on Roll angle estimations: 9.65 Degree
Exp2019-01-29_13-03-34_part5 completed!
Training model Exp2019-01-29_04-28-57_and_2019-01-29_13-03-34
All frames and annotations from 20 datasets have been read by 2019-01-29 16:36:22.554385
1. set (Dataset 19) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:36:27.449365!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0795
2. set (Dataset 16) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:36:47.812026!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0823
3. set (Dataset 21) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:37:15.572452!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1324
4. set (Dataset 15) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:37:36.825703!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0893
5. set (Dataset 8) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:37:59.417865!
Epoch 1/1
772/772 [==============================] - 17s 23ms/step - loss: 0.0968
6. set (Dataset 23) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:38:22.387779!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1179
7. set (Dataset 18) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:38:41.990897!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1022
8. set (Dataset 22) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:39:03.261676!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0579
9. set (Dataset 7) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:39:26.081864!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0947
10. set (Dataset 12) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:39:50.356439!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0820
11. set (Dataset 17) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:40:11.622405!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0813
12. set (Dataset 6) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:40:26.272914!
Epoch 1/1
542/542 [==============================] - 12s 21ms/step - loss: 0.1089
13. set (Dataset 24) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:40:42.735305!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0666
14. set (Dataset 11) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:41:00.395241!
Epoch 1/1
572/572 [==============================] - 12s 22ms/step - loss: 0.0646
15. set (Dataset 10) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:41:20.112581!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0906
16. set (Dataset 20) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:41:42.146800!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0723
17. set (Dataset 2) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:42:00.221490!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1127
18. set (Dataset 4) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:42:19.528798!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1014
19. set (Dataset 13) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:42:41.348710!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0701
20. set (Dataset 1) being trained for epoch 1 in Experiment 6 by 2019-01-29 16:42:57.688614!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1165
Epoch 1 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 16:43:13.676288
1. set (Dataset 4) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:43:21.083462!
Epoch 1/1
744/744 [==============================] - 16s 22ms/step - loss: 0.1016
2. set (Dataset 1) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:43:42.728386!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1078
3. set (Dataset 17) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:43:58.020103!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0853
4. set (Dataset 10) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:44:14.792242!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0867
5. set (Dataset 23) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:44:36.453456!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1239
6. set (Dataset 13) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:44:54.246058!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0709
7. set (Dataset 2) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:45:10.156899!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1071
8. set (Dataset 6) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:45:27.744005!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1178
9. set (Dataset 8) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:45:48.718853!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0900
10. set (Dataset 7) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:46:14.343309!
Epoch 1/1
745/745 [==============================] - 17s 22ms/step - loss: 0.0928
11. set (Dataset 18) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:46:36.963084!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1084
12. set (Dataset 19) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:46:56.127466!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0797
13. set (Dataset 11) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:47:13.401548!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0659
14. set (Dataset 16) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:47:35.356088!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0817
15. set (Dataset 21) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:48:02.472771!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1342
16. set (Dataset 20) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:48:22.499152!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0740
17. set (Dataset 24) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:48:40.161223!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0644
18. set (Dataset 15) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:48:58.459186!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0851
19. set (Dataset 12) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:49:20.759400!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0823
20. set (Dataset 22) being trained for epoch 2 in Experiment 6 by 2019-01-29 16:49:44.321170!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0630
Epoch 2 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 16:50:03.311730
1. set (Dataset 15) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:50:09.682799!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0876
2. set (Dataset 22) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:50:31.015403!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0619
3. set (Dataset 18) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:50:51.638107!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1047
4. set (Dataset 21) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:51:12.565857!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1333
5. set (Dataset 13) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:51:32.201591!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0743
6. set (Dataset 12) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:51:50.652878!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0853
7. set (Dataset 24) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:52:12.578114!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0659
8. set (Dataset 19) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:52:28.295254!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0754
9. set (Dataset 23) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:52:45.138092!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1168
10. set (Dataset 8) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:53:06.077713!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0945
11. set (Dataset 2) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:53:29.057583!
Epoch 1/1
511/511 [==============================] - 17s 34ms/step - loss: 0.1113
12. set (Dataset 4) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:53:53.884241!
Epoch 1/1
744/744 [==============================] - 16s 21ms/step - loss: 0.1033
13. set (Dataset 16) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:54:18.727866!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0877
14. set (Dataset 1) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:54:44.624526!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1206
15. set (Dataset 17) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:54:59.916685!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0792
16. set (Dataset 20) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:55:14.593397!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0763
17. set (Dataset 11) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:55:33.298573!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0650
18. set (Dataset 10) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:55:53.539829!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0912
19. set (Dataset 7) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:56:18.609484!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0947
20. set (Dataset 6) being trained for epoch 3 in Experiment 6 by 2019-01-29 16:56:41.539259!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1115
Epoch 3 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-29 16:56:58.432081
1. set (Dataset 10) being trained for epoch 4 in Experiment 6 by 2019-01-29 16:57:05.680919!
Epoch 1/1
194/726 [=======>......................] - ETA: 11s - loss: 0.0839^C
Model Exp2019-01-29_13-03-34_part6 has been interrupted.
Exp2019-01-29_13-03-34_part6.h5 has been saved.
Model Exp2019-01-29_13-03-34_part6 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_13-03-34_part6
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 16:58:50.491150: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 16:58:50.589391: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 16:58:50.589647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 16:58:50.589659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 16:58:50.745877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 16:58:50.745905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 16:58:50.745909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 16:58:50.746048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10453 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_13-03-34_part6.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 60
eva_epoch = 6
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_13-03-34_part6_and_2019-01-29_16-58-53
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 16:58:55.168142
For the Subject 3 (F03):
730/730 [==============================] - 12s 17ms/step
        The absolute mean error on Pitch angle estimation: 9.67 Degree
        The absolute mean error on Yaw angle estimation: 23.48 Degree
        The absolute mean error on Roll angle estimation: 10.65 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.36 Degree
        The absolute mean error on Yaw angle estimation: 20.27 Degree
        The absolute mean error on Roll angle estimation: 3.43 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 30.01 Degree
        The absolute mean error on Yaw angle estimation: 20.74 Degree
        The absolute mean error on Roll angle estimation: 8.29 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 13.86 Degree
        The absolute mean error on Yaw angle estimation: 31.66 Degree
        The absolute mean error on Roll angle estimation: 13.32 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.48 Degree
        The absolute mean error on Yaw angle estimations: 24.04 Degree
        The absolute mean error on Roll angle estimations: 8.92 Degree
subject3_Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53.png has been saved by 2019-01-29 17:00:23.021290.
subject5_Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53.png has been saved by 2019-01-29 17:00:23.219089.
subject9_Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53.png has been saved by 2019-01-29 17:00:23.415595.
subject14_Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53.png has been saved by 2019-01-29 17:00:23.679735.
Model Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53 has been evaluated successfully.
Model Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add -all
error: did you mean `--all` (with two dashes ?)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "multiple epochs do
n't effect"
[master 0e08616] multiple epochs don't effect
 8 files changed, 3331 insertions(+), 201 deletions(-)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53/output_Exp1-2
9_13-03-34_part6_and_2019-01-29_16-58-53.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53/scrollback_Ex
p1-29_13-03-34_part6_and_2019-01-29_16-58-53.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53/subject14_Exp
1-29_13-03-34_part6_and_2019-01-29_16-58-53.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53/subject3_Exp1
-29_13-03-34_part6_and_2019-01-29_16-58-53.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53/subject5_Exp1
-29_13-03-34_part6_and_2019-01-29_16-58-53.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_13-03-34_part6_and_2019-01-29_16-58-53/subject9_Exp1
-29_13-03-34_part6_and_2019-01-29_16-58-53.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_13-03-34_part6/output_Exp2019-01-29_13-03-34_p
art6.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 14, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (14/14), done.
Writing objects: 100% (14/14), 697.88 KiB | 0 bytes/s, done.
Total 14 (delta 6), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   65429f7..0e08616  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 17:23:38.343701: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 17:23:38.441048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 17:23:38.441304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 17:23:38.441316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 17:23:38.597454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 17:23:38.597479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 17:23:38.597484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 17:23:38.597620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_17-23-54 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
__________________________________________________________________________________________________
fc1024 (Dense)                  (None, 1024)         2098176     avg_pool[0][0]
__________________________________________________________________________________________________
dropout_025 (Dropout)           (None, 1024)         0           fc1024[0][0]
__________________________________________________________________________________________________
fc3 (Dense)                     (None, 3)            3075        dropout_025[0][0]
==================================================================================================
Total params: 23,904,035
Trainable params: 2,101,251
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 23904035
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False # True #
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2
019-01-29_17-23-54
All frames and annotations from 1 datasets have been read by 2019-01-29 17:23:55.912237
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-29 17:24:05.162627!
Epoch 1/1
882/882 [==============================] - 41s 47ms/step - loss: 0.2701
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100
_2019-01-29_17-23-54
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-29 17:24:48.777249
For the Subject 9 (M03):
882/882 [==============================] - 24s 27ms/step
        The absolute mean error on Pitch angle estimation: 18.43 Degree
        The absolute mean error on Yaw angle estimation: 25.75 Degree
        The absolute mean error on Roll angle estimation: 6.70 Degree
Exp2019-01-29_17-23-54_part1 completed!
Exp2019-01-29_17-23-54.h5 has been saved.
subject9_Exp2019-01-29_17-23-54.png has been saved by 2019-01-29 17:25:23.506817.
Model Exp2019-01-29_17-23-54 has been evaluated successfully.
Model Exp2019-01-29_17-23-54 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 17:26:25.627892: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 17:26:25.724369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 17:26:25.724627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.84GiB
2019-01-29 17:26:25.724640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 17:26:25.880751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 17:26:25.880778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 17:26:25.880783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 17:26:25.880920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10488 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_17-26-42 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
__________________________________________________________________________________________________
fc1024 (Dense)                  (None, 1024)         2098176     avg_pool[0][0]
__________________________________________________________________________________________________
dropout_025 (Dropout)           (None, 1024)         0           fc1024[0][0]
__________________________________________________________________________________________________
fc3 (Dense)                     (None, 3)            3075        dropout_025[0][0]
==================================================================================================
Total params: 23,904,035
Trainable params: 2,101,251
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 23904035
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.000001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False # True #
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000001_2
019-01-29_17-26-42
All frames and annotations from 1 datasets have been read by 2019-01-29 17:26:43.323100
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-29 17:26:52.508868!
Epoch 1/1
882/882 [==============================] - 32s 36ms/step - loss: 0.2489
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000001
_2019-01-29_17-26-42
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-29 17:27:26.837067
For the Subject 9 (M03):
882/882 [==============================] - 24s 27ms/step
        The absolute mean error on Pitch angle estimation: 17.78 Degree
        The absolute mean error on Yaw angle estimation: 26.57 Degree
        The absolute mean error on Roll angle estimation: 7.83 Degree
Exp2019-01-29_17-26-42_part1 completed!
Exp2019-01-29_17-26-42.h5 has been saved.
subject9_Exp2019-01-29_17-26-42.png has been saved by 2019-01-29 17:28:01.451943.
Model Exp2019-01-29_17-26-42 has been evaluated successfully.
Model Exp2019-01-29_17-26-42 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 17:28:43.426537: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 17:28:43.506074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 17:28:43.506394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.84GiB
2019-01-29 17:28:43.506410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 17:28:43.662602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 17:28:43.662628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 17:28:43.662632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 17:28:43.662811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10488 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_17-28-59 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
__________________________________________________________________________________________________
fc1024 (Dense)                  (None, 1024)         2098176     avg_pool[0][0]
__________________________________________________________________________________________________
dropout_025 (Dropout)           (None, 1024)         0           fc1024[0][0]
__________________________________________________________________________________________________
fc3 (Dense)                     (None, 3)            3075        dropout_025[0][0]
==================================================================================================
Total params: 23,904,035
Trainable params: 2,101,251
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 23904035
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00000001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False # True #
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000000_2
019-01-29_17-28-59
All frames and annotations from 1 datasets have been read by 2019-01-29 17:29:00.891006
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-29 17:29:10.081752!
Epoch 1/1
882/882 [==============================] - 32s 36ms/step - loss: 0.2715
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000000
_2019-01-29_17-28-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-29 17:29:44.612740
For the Subject 9 (M03):
882/882 [==============================] - 24s 28ms/step
        The absolute mean error on Pitch angle estimation: 18.48 Degree
        The absolute mean error on Yaw angle estimation: 28.35 Degree
        The absolute mean error on Roll angle estimation: 25.16 Degree
Exp2019-01-29_17-28-59_part1 completed!
Exp2019-01-29_17-28-59.h5 has been saved.
subject9_Exp2019-01-29_17-28-59.png has been saved by 2019-01-29 17:30:19.615424.
Model Exp2019-01-29_17-28-59 has been evaluated successfully.
Model Exp2019-01-29_17-28-59 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 17:31:45.331595: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 17:31:45.428273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 17:31:45.428532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.84GiB
2019-01-29 17:31:45.428544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 17:31:45.584492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 17:31:45.584518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 17:31:45.584523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 17:31:45.584660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10488 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_17-32-01 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
__________________________________________________________________________________________________
fc1024 (Dense)                  (None, 1024)         2098176     avg_pool[0][0]
__________________________________________________________________________________________________
dropout_025 (Dropout)           (None, 1024)         0           fc1024[0][0]
__________________________________________________________________________________________________
fc3 (Dense)                     (None, 3)            3075        dropout_025[0][0]
==================================================================================================
Total params: 23,904,035
Trainable params: 2,101,251
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 23904035
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00000001
in_epochs = 5
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False # True #
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs5_outEpochs1_AdamOpt_lr-0.000000_2
019-01-29_17-32-01
All frames and annotations from 1 datasets have been read by 2019-01-29 17:32:03.013272
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-29 17:32:12.198644!
Epoch 1/5
882/882 [==============================] - 32s 37ms/step - loss: 0.3583
Epoch 2/5
882/882 [==============================] - 28s 31ms/step - loss: 0.3462
Epoch 3/5
882/882 [==============================] - 28s 32ms/step - loss: 0.3344
Epoch 4/5
882/882 [==============================] - 28s 32ms/step - loss: 0.3224
Epoch 5/5
882/882 [==============================] - 28s 32ms/step - loss: 0.3072
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs5_outEpochs1_AdamOpt_lr-0.000000
_2019-01-29_17-32-01
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-29 17:34:38.692801
For the Subject 9 (M03):
882/882 [==============================] - 24s 28ms/step
        The absolute mean error on Pitch angle estimation: 25.59 Degree
        The absolute mean error on Yaw angle estimation: 28.45 Degree
        The absolute mean error on Roll angle estimation: 45.88 Degree
Exp2019-01-29_17-32-01_part1 completed!
Exp2019-01-29_17-32-01.h5 has been saved.
subject9_Exp2019-01-29_17-32-01.png has been saved by 2019-01-29 17:35:13.664480.
Model Exp2019-01-29_17-32-01 has been evaluated successfully.
Model Exp2019-01-29_17-32-01 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 17:41:04.004179: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 17:41:04.099604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 17:41:04.099866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 17:41:04.099880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 17:41:04.254727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 17:41:04.254752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 17:41:04.254760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 17:41:04.254902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10453 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_17-41-04 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (15, 16, 3)               138458947
_________________________________________________________________
lstm_1 (LSTM)                (15, 20)                  1920
_________________________________________________________________
dense_1 (Dense)              (15, 3)                   63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_17-
41-04
All frames and annotations from 1 datasets have been read by 2019-01-29 17:41:05.842702
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-29 17:41:14.773440!
Epoch 1/1
13/58 [=====>........................] - ETA: 54s - loss: 0.3863Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 73, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 70, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record, preprocess_input = preprocess_input)
  File "runFC_RNN_Experiment.py", line 31, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, preprocess_in
put = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 50, i
n trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, exp = exp, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 40, i
n trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_outputs, batch_
size, in_epochs, exp = exp, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 30, i
n trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1418, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py", line 217, in fit_generat
or
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1217, in train_on_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2721, in __call__
    return self._legacy_call(inputs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2693, in _legacy_c
all
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1116, in _run
    str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (11, 16, 224, 224, 3) for Tensor 'tdCNN_input:0', which has shape '(15, 16, 224,
224, 3)'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 17:44:46.906985: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 17:44:47.004741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 17:44:47.004998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 17:44:47.005011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 17:44:47.161493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 17:44:47.161518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 17:44:47.161523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 17:44:47.161664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_17-44-47 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (15, 16, 3)               138458947
_________________________________________________________________
lstm_1 (LSTM)                (15, 20)                  1920
_________________________________________________________________
dense_1 (Dense)              (15, 3)                   63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_17-
44-47
All frames and annotations from 1 datasets have been read by 2019-01-29 17:44:48.786868
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-29 17:44:57.732387!
Epoch 1/1
57/57 [==============================] - 62s 1s/step - loss: 0.3570
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_1
7-44-47
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-29 17:46:01.784098
For the Subject 9 (M03):
57/58 [============================>.] - ETA: 1sTraceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 73, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 70, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record, preprocess_input = preprocess_input)
  File "runFC_RNN_Experiment.py", line 40, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record = record, prep
rocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 124,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 85, i
n evaluateSubject
    predictions = full_model.predict_generator(test_gen, steps = int(len(test_labels)/batch_size), verbose = 1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1522, in predict_generator
    verbose=verbose)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py", line 453, in predict_gen
erator
    outs = model.predict_on_batch(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1274, in predict_on_batch
    outputs = self.predict_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2721, in __call__
    return self._legacy_call(inputs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2693, in _legacy_c
all
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1116, in _run
    str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (11, 16, 224, 224, 3) for Tensor 'tdCNN_input:0', which has shape '(15, 16, 224,
224, 3)'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 17:49:24.149175: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 17:49:24.247306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 17:49:24.247561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 17:49:24.247574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 17:49:24.403025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 17:49:24.403052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 17:49:24.403057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 17:49:24.403198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_17-49-25 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (15, 16, 3)               138458947
_________________________________________________________________
lstm_1 (LSTM)                (15, 20)                  1920
_________________________________________________________________
dense_1 (Dense)              (15, 3)                   63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_17-
49-25
All frames and annotations from 1 datasets have been read by 2019-01-29 17:49:25.984068
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-29 17:49:34.921613!
Epoch 1/1
57/57 [==============================] - 63s 1s/step - loss: 0.2869
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_1
7-49-25
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-29 17:50:39.486338
For the Subject 9 (M03):
58/58 [==============================] - 62s 1s/step
(882, 1) (870, 1)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 73, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 70, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record, preprocess_input = preprocess_input)
  File "runFC_RNN_Experiment.py", line 40, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record = record, prep
rocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 123,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 96, i
n evaluateSubject
    matrix = numpy.concatenate((test_labels[:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 17:57:04.030226: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 17:57:04.128401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 17:57:04.128662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 17:57:04.128677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 17:57:04.283782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 17:57:04.283810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 17:57:04.283819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 17:57:04.283962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 73, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 70, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 45, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateless_FC_RNN_Configuration.p
y", line 95, in getFinalModel
    rnn.add(TimeDistributed(cnn_model, batch_input_shape=(timesteps, inp[0], inp[1], inp[2]), name = 'tdCNN'))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py", line 165, in add
    layer(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py", line 457, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 229, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2922, in rnn
    outputs, _ = step_function(inputs[0], initial_states + constants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 220, in step
    output = self.layer.call(x, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/network.py", line 564, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/network.py", line 721, in run_internal_graph
    layer.call(computed_tensor, **kwargs))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py", line 171, in call
    dilation_rate=self.dilation_rate)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 3650, in conv2d
    data_format=tf_data_format)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 781, in convolution
    data_format=data_format)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 830, in __init__
    input_channels_dim = input_shape[num_spatial_dims + 1]
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py", line 532, in __
getitem__
    return self._dims[key]
IndexError: list index out of range
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 17:58:09.969098: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 17:58:10.066550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 17:58:10.066815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 17:58:10.066829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 17:58:10.222332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 17:58:10.222359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 17:58:10.222368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 17:58:10.222507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_17-58-10 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_17-
58-10
All frames and annotations from 1 datasets have been read by 2019-01-29 17:58:11.679318
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-29 17:58:20.615931!
Epoch 1/1
58/58 [==============================] - 65s 1s/step - loss: 0.4434
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_1
7-58-10
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-29 17:59:27.035471
For the Subject 9 (M03):
58/58 [==============================] - 58s 1s/step
(882, 1) (866, 1)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 73, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 70, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record, preprocess_input = preprocess_input)
  File "runFC_RNN_Experiment.py", line 40, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record = record, prep
rocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 124,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 97, i
n evaluateSubject
    matrix = numpy.concatenate((test_labels[:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 18:03:36.876549: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 18:03:36.974455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 18:03:36.974721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 18:03:36.974736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 18:03:37.129595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 18:03:37.129625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 18:03:37.129633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 18:03:37.129783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10453 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_18-03-37 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_18-
03-37
All frames and annotations from 1 datasets have been read by 2019-01-29 18:03:38.608410
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:03:47.530429!
Epoch 1/1
58/58 [==============================] - 65s 1s/step - loss: 0.3586
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_1
8-03-37
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-29 18:04:54.595450
For the Subject 9 (M03):
58/58 [==============================] - 58s 994ms/step
(866, 1) (866, 1)
        The absolute mean error on Pitch angle estimation: 30.01 Degree
(866, 1) (866, 1)
        The absolute mean error on Yaw angle estimation: 23.41 Degree
(866, 1) (866, 1)
        The absolute mean error on Roll angle estimation: 18.74 Degree
Exp2019-01-29_18-03-37_part1 completed!
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_18-
03-37
All frames and annotations from 1 datasets have been read by 2019-01-29 18:06:02.151826
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-29 18:06:11.082880!
Epoch 1/1
58/58 [==============================] - 58s 1s/step - loss: 0.3203
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_1
8-03-37
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-29 18:07:10.261092
For the Subject 9 (M03):
58/58 [==============================] - 58s 999ms/step
(866, 1) (866, 1)
        The absolute mean error on Pitch angle estimation: 13.21 Degree
(866, 1) (866, 1)
        The absolute mean error on Yaw angle estimation: 28.69 Degree
(866, 1) (866, 1)
        The absolute mean error on Roll angle estimation: 5.97 Degree
Exp2019-01-29_18-03-37_part2 completed!
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_18-
03-37
All frames and annotations from 1 datasets have been read by 2019-01-29 18:08:18.111255
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-29 18:08:27.037879!
Epoch 1/1
58/58 [==============================] - 58s 1s/step - loss: 0.2998
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_1
8-03-37
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-29 18:09:26.335489
For the Subject 9 (M03):
58/58 [==============================] - 58s 1s/step
(866, 1) (866, 1)
        The absolute mean error on Pitch angle estimation: 13.96 Degree
(866, 1) (866, 1)
        The absolute mean error on Yaw angle estimation: 26.02 Degree
(866, 1) (866, 1)
        The absolute mean error on Roll angle estimation: 5.58 Degree
Exp2019-01-29_18-03-37_part3 completed!
Exp2019-01-29_18-03-37.h5 has been saved.
subject9_Exp2019-01-29_18-03-37.png has been saved by 2019-01-29 18:10:34.091405.
Model Exp2019-01-29_18-03-37 has been evaluated successfully.
Model Exp2019-01-29_18-03-37 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 18:15:27.237136: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 18:15:27.335606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 18:15:27.335862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 18:15:27.335874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 18:15:27.491310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 18:15:27.491337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 18:15:27.491342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 18:15:27.491479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_18-15-28 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 5
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs5_outEpochs1_AdamOpt_lr-0.000100_2019-01
-29_18-15-28
All frames and annotations from 20 datasets have been read by 2019-01-29 18:15:32.609464
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:15:39.044000!
Epoch 1/5
354/665 [==============>...............] - ETA: 7s - loss: 0.3197^C
Model Exp2019-01-29_18-15-28_part1 has been interrupted.
Exp2019-01-29_18-15-28_part1.h5 has been saved.
Model Exp2019-01-29_18-15-28_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 18:16:32.140827: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 18:16:32.238979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 18:16:32.239237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 18:16:32.239250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 18:16:32.393621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 18:16:32.393649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 18:16:32.393654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 18:16:32.393799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_18-16-33 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-29_18-16-33
All frames and annotations from 20 datasets have been read by 2019-01-29 18:16:38.033377
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:16:44.448520!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.2902
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:17:06.540628!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.2145
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:17:23.866576!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.2315
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:17:43.731595!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.1966
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:18:03.089495!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.2585
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:18:26.582190!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.2695
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:18:44.805769!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.2675
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:19:08.168729!
Epoch 1/1
914/914 [==============================] - 20s 22ms/step - loss: 0.2086
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:19:35.786293!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.2619
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:20:00.075993!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.1903
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:20:24.310937!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.1989
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:20:45.994473!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.2255
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:21:03.227589!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.2326
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:21:22.960385!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1988
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:21:42.170665!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.2246
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:22:04.648930!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.1755
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:22:21.381893!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.1428
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:22:35.453679!
Epoch 1/1
542/542 [==============================] - 12s 21ms/step - loss: 0.2232
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:22:51.903533!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.1362
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:23:08.653756!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.1529
Epoch 1 for Experiment 1 completed!
Exp2019-01-29_18-16-33_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-29_18-16-33
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 18:23:24.305637
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 11.91 Degree
        The absolute mean error on Yaw angle estimation: 31.23 Degree
        The absolute mean error on Roll angle estimation: 11.09 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 16.84 Degree
        The absolute mean error on Yaw angle estimation: 30.01 Degree
        The absolute mean error on Roll angle estimation: 6.20 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 21.15 Degree
        The absolute mean error on Yaw angle estimation: 27.25 Degree
        The absolute mean error on Roll angle estimation: 6.90 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 16.80 Degree
        The absolute mean error on Yaw angle estimation: 30.81 Degree
        The absolute mean error on Roll angle estimation: 14.01 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.67 Degree
        The absolute mean error on Yaw angle estimations: 29.83 Degree
        The absolute mean error on Roll angle estimations: 9.55 Degree
Exp2019-01-29_18-16-33_part1 completed!
Exp2019-01-29_18-16-33.h5 has been saved.
subject3_Exp2019-01-29_18-16-33.png has been saved by 2019-01-29 18:24:51.926851.
subject5_Exp2019-01-29_18-16-33.png has been saved by 2019-01-29 18:24:52.128315.
subject9_Exp2019-01-29_18-16-33.png has been saved by 2019-01-29 18:24:52.327508.
subject14_Exp2019-01-29_18-16-33.png has been saved by 2019-01-29 18:24:52.547689.
Model Exp2019-01-29_18-16-33 has been evaluated successfully.
Model Exp2019-01-29_18-16-33 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "Model Exp2019-01-2
9_18-16-33 has been recorded"
[master 4fc633b] Model Exp2019-01-29_18-16-33 has been recorded
 25 files changed, 3773 insertions(+), 17 deletions(-)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_17-23-54/output_Exp2019-01-29_17-23-54.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_17-23-54/subject9_Exp2019-01-29_17-23-54.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_17-26-42/output_Exp2019-01-29_17-26-42.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_17-26-42/subject9_Exp2019-01-29_17-26-42.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_17-28-59/output_Exp2019-01-29_17-28-59.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_17-28-59/subject9_Exp2019-01-29_17-28-59.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_17-32-01/output_Exp2019-01-29_17-32-01.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_17-32-01/subject9_Exp2019-01-29_17-32-01.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_18-03-37/output_Exp2019-01-29_18-03-37.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_18-03-37/subject9_Exp2019-01-29_18-03-37.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_18-15-28_part1/output_Exp2019-01-29_18-15-28_p
art1.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_18-16-33/output_Exp2019-01-29_18-16-33.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_18-16-33/subject14_Exp2019-01-29_18-16-33.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_18-16-33/subject3_Exp2019-01-29_18-16-33.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_18-16-33/subject5_Exp2019-01-29_18-16-33.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_18-16-33/subject9_Exp2019-01-29_18-16-33.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model__/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model___/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model____/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_____/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 42, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (36/36), done.
Writing objects: 100% (42/42), 1.32 MiB | 1.15 MiB/s, done.
Total 42 (delta 17), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   0e08616..4fc633b  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 18:33:36.191406: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 18:33:36.288852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 18:33:36.289109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 18:33:36.289121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 18:33:36.443047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 18:33:36.443071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 18:33:36.443076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 18:33:36.443213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_18-33-37 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_18-
33-37
All frames and annotations from 20 datasets have been read by 2019-01-29 18:33:41.580849
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:33:48.001375!
Epoch 1/1
44/44 [==============================] - 49s 1s/step - loss: 0.3902
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:34:42.017000!
Epoch 1/1
32/32 [==============================] - 34s 1s/step - loss: 0.3117
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:35:22.564638!
Epoch 1/1
43/43 [==============================] - 45s 1s/step - loss: 0.2562
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:36:12.641561!
Epoch 1/1
33/33 [==============================] - 35s 1s/step - loss: 0.2859
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:36:55.180145!
Epoch 1/1
51/51 [==============================] - 50s 973ms/step - loss: 0.3506
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:37:50.283437!
Epoch 1/1
37/37 [==============================] - 40s 1s/step - loss: 0.3173
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:38:36.366803!
Epoch 1/1
42/42 [==============================] - 42s 1s/step - loss: 0.3062
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:39:27.234619!
Epoch 1/1
60/60 [==============================] - 59s 977ms/step - loss: 0.2598
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:40:33.429016!
Epoch 1/1
49/49 [==============================] - 51s 1s/step - loss: 0.3008
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:41:31.708912!
Epoch 1/1
48/48 [==============================] - 48s 1s/step - loss: 0.2333
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:42:27.200636!
Epoch 1/1
48/48 [==============================] - 49s 1s/step - loss: 0.2377
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:43:21.784724!
Epoch 1/1
33/33 [==============================] - 33s 1s/step - loss: 0.2602
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:44:01.145195!
Epoch 1/1
40/40 [==============================] - 40s 1s/step - loss: 0.2366
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:44:46.425125!
Epoch 1/1
33/33 [==============================] - 33s 1s/step - loss: 0.2933
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:45:27.133001!
Epoch 1/1
49/49 [==============================] - 49s 991ms/step - loss: 0.2898
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:46:21.197357!
Epoch 1/1
36/36 [==============================] - 36s 1s/step - loss: 0.1970
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:47:01.460080!
Epoch 1/1
26/26 [==============================] - 26s 983ms/step - loss: 0.1460
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:47:32.276591!
Epoch 1/1
36/36 [==============================] - 36s 993ms/step - loss: 0.2745
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:48:12.966962!
Epoch 1/1
32/32 [==============================] - 31s 981ms/step - loss: 0.1808
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-29 18:48:50.184364!
Epoch 1/1
38/38 [==============================] - 37s 984ms/step - loss: 0.1736
Epoch 1 for Experiment 1 completed!
Exp2019-01-29_18-33-37_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_1
8-33-37
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 18:49:30.024140
For the Subject 3 (F03):
48/48 [==============================] - 47s 983ms/step
(714, 1) (714, 1)
        The absolute mean error on Pitch angle estimation: 13.37 Degree
(714, 1) (714, 1)
        The absolute mean error on Yaw angle estimation: 30.98 Degree
(714, 1) (714, 1)
        The absolute mean error on Roll angle estimation: 17.86 Degree
For the Subject 5 (F05):
63/63 [==============================] - 64s 1s/step
(930, 1) (945, 1)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 73, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 70, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record, preprocess_input = preprocess_input)
  File "runFC_RNN_Experiment.py", line 40, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record = record, prep
rocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 123,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 96, i
n evaluateSubject
    matrix = numpy.concatenate((test_labels[timesteps:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 19:12:46.416307: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 19:12:46.514596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 19:12:46.514858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 19:12:46.514871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 19:12:46.669856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 19:12:46.669884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 19:12:46.669888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 19:12:46.670067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10453 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_19-12-47 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_19-
12-47
All frames and annotations from 20 datasets have been read by 2019-01-29 19:12:52.049036
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:12:58.477790!
Epoch 1/1
44/44 [==============================] - 48s 1s/step - loss: 0.2903
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:13:52.277637!
Epoch 1/1
32/32 [==============================] - 34s 1s/step - loss: 0.2699
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:14:32.427663!
Epoch 1/1
43/43 [==============================] - 45s 1s/step - loss: 0.2901
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:15:22.672957!
Epoch 1/1
33/33 [==============================] - 34s 1s/step - loss: 0.2650
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:16:04.983609!
Epoch 1/1
51/51 [==============================] - 49s 968ms/step - loss: 0.3576
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:16:59.812289!
Epoch 1/1
37/37 [==============================] - 40s 1s/step - loss: 0.3328
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:17:45.848563!
Epoch 1/1
42/42 [==============================] - 42s 1s/step - loss: 0.3036
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:18:36.737475!
Epoch 1/1
60/60 [==============================] - 61s 1s/step - loss: 0.2357
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:19:45.071558!
Epoch 1/1
49/49 [==============================] - 52s 1s/step - loss: 0.3780
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:20:44.450819!
Epoch 1/1
48/48 [==============================] - 48s 1s/step - loss: 0.3112
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:21:40.129567!
Epoch 1/1
48/48 [==============================] - 49s 1s/step - loss: 0.3113
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:22:34.679168!
Epoch 1/1
33/33 [==============================] - 34s 1s/step - loss: 0.3629
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:23:14.224620!
Epoch 1/1
40/40 [==============================] - 40s 1s/step - loss: 0.2878
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:23:59.674213!
Epoch 1/1
33/33 [==============================] - 33s 1s/step - loss: 0.3736
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:24:40.376709!
Epoch 1/1
49/49 [==============================] - 49s 1s/step - loss: 0.3244
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:25:35.130553!
Epoch 1/1
36/36 [==============================] - 36s 1s/step - loss: 0.2302
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:26:15.158616!
Epoch 1/1
26/26 [==============================] - 25s 979ms/step - loss: 0.1790
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:26:45.900531!
Epoch 1/1
36/36 [==============================] - 36s 992ms/step - loss: 0.2529
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:27:26.553026!
Epoch 1/1
32/32 [==============================] - 31s 984ms/step - loss: 0.2384
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:28:03.851536!
Epoch 1/1
38/38 [==============================] - 38s 987ms/step - loss: 0.2135
Epoch 1 for Experiment 1 completed!
Exp2019-01-29_19-12-47_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_1
9-12-47
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 19:28:43.884386
For the Subject 3 (F03):
48/48 [==============================] - 48s 990ms/step
(730, 1) (714, 1)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 73, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 70, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record, preprocess_input = preprocess_input)
  File "runFC_RNN_Experiment.py", line 40, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record = record, prep
rocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 123,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 96, i
n evaluateSubject
    matrix = numpy.concatenate((test_labels[:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 19:35:29.252106: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 19:35:29.349985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 19:35:29.350279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 19:35:29.350292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 19:35:29.504471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 19:35:29.504497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 19:35:29.504502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 19:35:29.504638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_19-35-30 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_19-
35-30
All frames and annotations from 20 datasets have been read by 2019-01-29 19:35:34.713271
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:35:41.141436!
Epoch 1/1
44/44 [==============================] - 49s 1s/step - loss: 0.2807
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:36:35.601215!
Epoch 1/1
32/32 [==============================] - 35s 1s/step - loss: 0.2247
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:37:17.155106!
Epoch 1/1
43/43 [==============================] - 45s 1s/step - loss: 0.2726
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:38:07.493033!
Epoch 1/1
33/33 [==============================] - 35s 1s/step - loss: 0.2310
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:38:50.226548!
Epoch 1/1
51/51 [==============================] - 51s 999ms/step - loss: 0.3301
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:39:46.748807!
Epoch 1/1
37/37 [==============================] - 41s 1s/step - loss: 0.2934
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:40:33.874365!
Epoch 1/1
42/42 [==============================] - 43s 1s/step - loss: 0.2915
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:41:25.901785!
Epoch 1/1
60/60 [==============================] - 60s 1s/step - loss: 0.2520
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:42:33.985700!
Epoch 1/1
49/49 [==============================] - 52s 1s/step - loss: 0.3399
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:43:33.240135!
Epoch 1/1
48/48 [==============================] - 48s 1s/step - loss: 0.2879
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:44:28.821663!
Epoch 1/1
48/48 [==============================] - 50s 1s/step - loss: 0.2746
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:45:23.577079!
Epoch 1/1
33/33 [==============================] - 33s 1s/step - loss: 0.3402
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:46:02.990160!
Epoch 1/1
40/40 [==============================] - 40s 1s/step - loss: 0.2907
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:46:48.501176!
Epoch 1/1
33/33 [==============================] - 33s 1s/step - loss: 0.3536
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:47:29.416458!
Epoch 1/1
49/49 [==============================] - 49s 999ms/step - loss: 0.3170
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:48:23.873172!
Epoch 1/1
36/36 [==============================] - 36s 1s/step - loss: 0.2222
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:49:03.988781!
Epoch 1/1
26/26 [==============================] - 25s 980ms/step - loss: 0.1976
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:49:34.768502!
Epoch 1/1
36/36 [==============================] - 36s 997ms/step - loss: 0.2697
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:50:15.624399!
Epoch 1/1
32/32 [==============================] - 32s 992ms/step - loss: 0.2362
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-29 19:50:53.190902!
Epoch 1/1
38/38 [==============================] - 37s 984ms/step - loss: 0.2101
Epoch 1 for Experiment 1 completed!
Exp2019-01-29_19-35-30_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_1
9-35-30
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 19:51:32.990962
For the Subject 3 (F03):
48/48 [==============================] - 47s 987ms/step
(714, 1) (714, 1)
        The absolute mean error on Pitch angle estimation: 12.15 Degree
(714, 1) (714, 1)
        The absolute mean error on Yaw angle estimation: 33.98 Degree
(714, 1) (714, 1)
        The absolute mean error on Roll angle estimation: 6.95 Degree
For the Subject 5 (F05):
63/63 [==============================] - 64s 1s/step
(930, 1) (945, 1)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 73, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 70, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record, preprocess_input = preprocess_input)
  File "runFC_RNN_Experiment.py", line 40, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record = record, prep
rocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 123,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 96, i
n evaluateSubject
    matrix = numpy.concatenate((test_labels[timesteps:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_19-35-30_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 20:06:56.915875: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 20:06:57.012823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 20:06:57.013089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 20:06:57.013103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 20:06:57.168159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 20:06:57.168188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 20:06:57.168196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 20:06:57.168337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_19-35-30_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_19-35-30_part1_and_2019-01-29_20-06-59
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 20:07:01.555298
For the Subject 3 (F03):
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 75, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 125,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 86, i
n evaluateSubject
    predictions = full_model.predict_generator(test_gen, steps = int(len(test_labels)/batch_size), verbose = 1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1522, in predict_generator
    verbose=verbose)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py", line 453, in predict_gen
erator
    outs = model.predict_on_batch(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1268, in predict_on_batch
    x, _, _ = self._standardize_user_data(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 751, in _standardize_user_dat
a
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py", line 138, in standardize_inp
ut_data
    str(data_shape))
ValueError: Error when checking input: expected tdCNN_input to have shape (16, 224, 224, 3) but got array with shape (1,
 224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_19-35-30_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 20:08:47.637368: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 20:08:47.735854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 20:08:47.736147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 20:08:47.736161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 20:08:47.891751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 20:08:47.891777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 20:08:47.891782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 20:08:47.891968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_19-35-30_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_19-35-30_part1_and_2019-01-29_20-08-50
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 20:08:52.692890
For the Subject 3 (F03):
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 75, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 125,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 86, i
n evaluateSubject
    predictions = full_model.predict_generator(test_gen, steps = int(len(test_labels)/batch_size), verbose = 1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1522, in predict_generator
    verbose=verbose)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py", line 453, in predict_gen
erator
    outs = model.predict_on_batch(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1268, in predict_on_batch
    x, _, _ = self._standardize_user_data(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 751, in _standardize_user_dat
a
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py", line 138, in standardize_inp
ut_data
    str(data_shape))
ValueError: Error when checking input: expected tdCNN_input to have shape (16, 224, 224, 3) but got array with shape (1,
 224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_19-35-30_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 20:12:05.008518: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 20:12:05.105568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 20:12:05.105838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 20:12:05.105851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 20:12:05.260405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 20:12:05.260431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 20:12:05.260436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 20:12:05.260576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10453 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_19-35-30_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_19-35-30_part1_and_2019-01-29_20-12-07
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 20:12:09.694443
For the Subject 3 (F03):
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 75, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 124,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 85, i
n evaluateSubject
    predictions = full_model.predict_generator(test_gen, steps = int(len(test_labels)/batch_size), verbose = 1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1522, in predict_generator
    verbose=verbose)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py", line 453, in predict_gen
erator
    outs = model.predict_on_batch(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1268, in predict_on_batch
    x, _, _ = self._standardize_user_data(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 751, in _standardize_user_dat
a
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py", line 138, in standardize_inp
ut_data
    str(data_shape))
ValueError: Error when checking input: expected tdCNN_input to have shape (16, 224, 224, 3) but got array with shape (1,
 224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_19-35-30_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 20:34:54.282258: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 20:34:54.379017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 20:34:54.379281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 20:34:54.379295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 20:34:54.533647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 20:34:54.533673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 20:34:54.533686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 20:34:54.533825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_19-35-30_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_19-35-30_part1_and_2019-01-29_20-34-56
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 20:34:58.838523
For the Subject 3 (F03):
2019-01-29 20:35:43.319056: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memo
ry trying to allocate 2.87GiB.  Current allocation summary follows.
2019-01-29 20:35:43.319159: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 113, Chunk
s in use: 113. 28.2KiB allocated for chunks. 28.2KiB in use in bin. 4.1KiB client-requested in use in bin.
2019-01-29 20:35:43.319212: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 12, Chunks
 in use: 12. 6.0KiB allocated for chunks. 6.0KiB in use in bin. 4.9KiB client-requested in use in bin.
2019-01-29 20:35:43.319251: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 18, Chunks
 in use: 18. 21.2KiB allocated for chunks. 21.2KiB in use in bin. 19.9KiB client-requested in use in bin.
2019-01-29 20:35:43.319276: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 14, Chunks
 in use: 14. 28.0KiB allocated for chunks. 28.0KiB in use in bin. 28.0KiB client-requested in use in bin.
2019-01-29 20:35:43.319319: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 17, Chunks
 in use: 16. 94.0KiB allocated for chunks. 87.2KiB in use in bin. 82.3KiB client-requested in use in bin.
2019-01-29 20:35:43.319345: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 6, Chunks
in use: 6. 71.0KiB allocated for chunks. 71.0KiB in use in bin. 66.8KiB client-requested in use in bin.
2019-01-29 20:35:43.319371: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 6,
 Chunks in use: 6. 100.5KiB allocated for chunks. 100.5KiB in use in bin. 96.0KiB client-requested in use in bin.
2019-01-29 20:35:43.319406: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 0,
 Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-29 20:35:43.319429: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 0,
 Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-29 20:35:43.319454: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 2,
 Chunks in use: 2. 288.0KiB allocated for chunks. 288.0KiB in use in bin. 288.0KiB client-requested in use in bin.
2019-01-29 20:35:43.319483: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 4,
 Chunks in use: 3. 1.29MiB allocated for chunks. 982.5KiB in use in bin. 720.0KiB client-requested in use in bin.
2019-01-29 20:35:43.319509: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks: 2,
 Chunks in use: 2. 1.12MiB allocated for chunks. 1.12MiB in use in bin. 1.12MiB client-requested in use in bin.
2019-01-29 20:35:43.319535: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks: 3,
 Chunks in use: 2. 3.38MiB allocated for chunks. 2.25MiB in use in bin. 2.25MiB client-requested in use in bin.
2019-01-29 20:35:43.319562: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks: 6,
 Chunks in use: 4. 13.50MiB allocated for chunks. 9.00MiB in use in bin. 9.00MiB client-requested in use in bin.
2019-01-29 20:35:43.319585: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks: 3,
 Chunks in use: 2. 13.50MiB allocated for chunks. 9.00MiB in use in bin. 9.00MiB client-requested in use in bin.
2019-01-29 20:35:43.319614: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks: 11
, Chunks in use: 11. 117.62MiB allocated for chunks. 117.62MiB in use in bin. 105.62MiB client-requested in use in bin.
2019-01-29 20:35:43.319643: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks: 6,
 Chunks in use: 5. 105.00MiB allocated for chunks. 80.00MiB in use in bin. 80.00MiB client-requested in use in bin.
2019-01-29 20:35:43.319667: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks: 0,
 Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-29 20:35:43.319695: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks: 2,
 Chunks in use: 2. 128.00MiB allocated for chunks. 128.00MiB in use in bin. 128.00MiB client-requested in use in bin.
2019-01-29 20:35:43.319721: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks: 1,
 Chunks in use: 0. 212.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-29 20:35:43.319747: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks: 6,
 Chunks in use: 4. 9.62GiB allocated for chunks. 6.51GiB in use in bin. 6.51GiB client-requested in use in bin.
2019-01-29 20:35:43.319772: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 2.87GiB was 256.00MiB, Chunk
State:
2019-01-29 20:35:43.319812: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 392.00MiB | Requested Size: 1
37.81MiB | in_use: 0, prev:   Size: 392.00MiB | Requested Size: 392.00MiB | in_use: 1, next:   Size: 392.00MiB | Request
ed Size: 392.00MiB | in_use: 1
2019-01-29 20:35:43.319840: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 2.73GiB | Requested Size: 1.2
1GiB | in_use: 0, prev:   Size: 2.87GiB | Requested Size: 2.87GiB | in_use: 1
2019-01-29 20:35:43.319865: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500000 of size 1280
2019-01-29 20:35:43.319885: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500500 of size 256
2019-01-29 20:35:43.319904: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500600 of size 256
2019-01-29 20:35:43.319922: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500700 of size 256
2019-01-29 20:35:43.319940: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500800 of size 256
2019-01-29 20:35:43.319958: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500900 of size 256
2019-01-29 20:35:43.319977: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500a00 of size 16384
2019-01-29 20:35:43.319996: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504a00 of size 256
2019-01-29 20:35:43.320014: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504b00 of size 256
2019-01-29 20:35:43.320033: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504c00 of size 512
2019-01-29 20:35:43.320051: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504e00 of size 256
2019-01-29 20:35:43.320069: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504f00 of size 256
2019-01-29 20:35:43.320087: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505000 of size 256
2019-01-29 20:35:43.320105: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505100 of size 256
2019-01-29 20:35:43.320124: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505200 of size 1024
2019-01-29 20:35:43.320143: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505600 of size 256
2019-01-29 20:35:43.320161: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505700 of size 256
2019-01-29 20:35:43.320180: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505800 of size 256
2019-01-29 20:35:43.320195: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505900 of size 256
2019-01-29 20:35:43.320214: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505a00 of size 256
2019-01-29 20:35:43.320233: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505b00 of size 2048
2019-01-29 20:35:43.320251: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506300 of size 256
2019-01-29 20:35:43.320269: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506400 of size 256
2019-01-29 20:35:43.320288: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506500 of size 256
2019-01-29 20:35:43.320305: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506600 of size 256
2019-01-29 20:35:43.320324: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506700 of size 256
2019-01-29 20:35:43.320343: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506800 of size 4096
2019-01-29 20:35:43.320363: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507800 of size 256
2019-01-29 20:35:43.320381: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507900 of size 256
2019-01-29 20:35:43.320400: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507a00 of size 6912
2019-01-29 20:35:43.320420: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a509500 of size 288000
2019-01-29 20:35:43.320439: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a54fa00 of size 512
2019-01-29 20:35:43.320459: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a54fc00 of size 589824
2019-01-29 20:35:43.320478: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5dfc00 of size 512
2019-01-29 20:35:43.320497: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5dfe00 of size 12288
2019-01-29 20:35:43.320516: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e2e00 of size 256
2019-01-29 20:35:43.320535: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e2f00 of size 1024
2019-01-29 20:35:43.320554: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e3300 of size 6400
2019-01-29 20:35:43.320572: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e4c00 of size 512
2019-01-29 20:35:43.320591: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e4e00 of size 256
2019-01-29 20:35:43.320610: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e4f00 of size 256
2019-01-29 20:35:43.320628: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e5000 of size 256
2019-01-29 20:35:43.320646: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e5100 of size 256
2019-01-29 20:35:43.320665: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e5200 of size 256
2019-01-29 20:35:43.320683: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e5300 of size 256
2019-01-29 20:35:43.320701: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e5400 of size 256
2019-01-29 20:35:43.320719: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e5500 of size 4096
2019-01-29 20:35:43.320738: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e6500 of size 12288
2019-01-29 20:35:43.320756: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e9500 of size 256
2019-01-29 20:35:43.320775: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e9600 of size 1024
2019-01-29 20:35:43.320793: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5e9a00 of size 6400
2019-01-29 20:35:43.320812: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5eb300 of size 512
2019-01-29 20:35:43.320830: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5eb500 of size 256
2019-01-29 20:35:43.320848: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5eb600 of size 256
2019-01-29 20:35:43.320866: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5eb700 of size 4096
2019-01-29 20:35:43.320885: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5ec700 of size 12288
2019-01-29 20:35:43.320903: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5ef700 of size 256
2019-01-29 20:35:43.320922: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5ef800 of size 1024
2019-01-29 20:35:43.320940: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5efc00 of size 6400
2019-01-29 20:35:43.320959: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f1500 of size 512
2019-01-29 20:35:43.320977: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f1700 of size 256
2019-01-29 20:35:43.320995: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f1800 of size 256
2019-01-29 20:35:43.321013: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f1900 of size 256
2019-01-29 20:35:43.321032: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f1a00 of size 256
2019-01-29 20:35:43.321050: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f1b00 of size 256
2019-01-29 20:35:43.321068: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f1c00 of size 256
2019-01-29 20:35:43.321086: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f1d00 of size 256
2019-01-29 20:35:43.321105: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f1e00 of size 256
2019-01-29 20:35:43.321123: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f1f00 of size 256
2019-01-29 20:35:43.321141: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2000 of size 256
2019-01-29 20:35:43.321159: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2100 of size 256
2019-01-29 20:35:43.321178: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2200 of size 256
2019-01-29 20:35:43.321196: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2300 of size 256
2019-01-29 20:35:43.321214: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2400 of size 256
2019-01-29 20:35:43.321232: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2500 of size 256
2019-01-29 20:35:43.321251: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2600 of size 256
2019-01-29 20:35:43.321269: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2700 of size 256
2019-01-29 20:35:43.321287: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2800 of size 256
2019-01-29 20:35:43.321305: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2900 of size 256
2019-01-29 20:35:43.321324: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2a00 of size 256
2019-01-29 20:35:43.321342: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2b00 of size 256
2019-01-29 20:35:43.321360: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2c00 of size 256
2019-01-29 20:35:43.321379: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f2d00 of size 1792
2019-01-29 20:35:43.321398: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f3400 of size 1792
2019-01-29 20:35:43.321416: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f3b00 of size 1792
2019-01-29 20:35:43.321434: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f4200 of size 1792
2019-01-29 20:35:43.321453: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a5f4900 of size 6912
2019-01-29 20:35:43.321472: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f6400 of size 256
2019-01-29 20:35:43.321491: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5f6500 of size 147456
2019-01-29 20:35:43.321509: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a61a500 of size 350464
2019-01-29 20:35:43.321528: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a66fe00 of size 6400
2019-01-29 20:35:43.321546: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a671700 of size 512
2019-01-29 20:35:43.321565: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a671900 of size 12288
2019-01-29 20:35:43.321583: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a674900 of size 147456
2019-01-29 20:35:43.321603: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a698900 of size 423168
2019-01-29 20:35:43.321622: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a6ffe00 of size 1024
2019-01-29 20:35:43.321641: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a700200 of size 2359296
2019-01-29 20:35:43.321660: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a940200 of size 1179648
2019-01-29 20:35:43.321679: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aa60200 of size 1179648
2019-01-29 20:35:43.321717: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ab80200 of size 1024
2019-01-29 20:35:43.321736: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ab80600 of size 4718592
2019-01-29 20:35:43.321755: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b000600 of size 9437184
2019-01-29 20:35:43.321775: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b900600 of size 16384000
2019-01-29 20:35:43.321795: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0c8a0600 of size 16777216
2019-01-29 20:35:43.321812: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d8a0600 of size 4718592
2019-01-29 20:35:43.321831: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0dd20600 of size 15073280
2019-01-29 20:35:43.321850: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0eb80600 of size 1024
2019-01-29 20:35:43.321869: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0eb80a00 of size 2359296
2019-01-29 20:35:43.321888: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0edc0a00 of size 2359296
2019-01-29 20:35:43.321906: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0f000a00 of size 2048
2019-01-29 20:35:43.321925: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0f001200 of size 9437184
2019-01-29 20:35:43.321944: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0f901200 of size 9437184
2019-01-29 20:35:43.321963: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb10201200 of size 9437184
2019-01-29 20:35:43.321981: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb10b01200 of size 9437184
2019-01-29 20:35:43.322000: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb11401200 of size 4718592
2019-01-29 20:35:43.322018: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11881200 of size 2359296
2019-01-29 20:35:43.322037: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb11ac1200 of size 2359296
2019-01-29 20:35:43.322056: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d01200 of size 2048
2019-01-29 20:35:43.322074: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d01a00 of size 16384
2019-01-29 20:35:43.322093: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d05a00 of size 2048
2019-01-29 20:35:43.322112: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d06200 of size 16384000
2019-01-29 20:35:43.322130: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ca6200 of size 2048
2019-01-29 20:35:43.322148: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ca6a00 of size 2048
2019-01-29 20:35:43.322167: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ca7200 of size 4096
2019-01-29 20:35:43.322186: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ca8200 of size 2048
2019-01-29 20:35:43.322204: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ca8a00 of size 9437184
2019-01-29 20:35:43.322222: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb135a8a00 of size 9437184
2019-01-29 20:35:43.322241: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13ea8a00 of size 9437184
2019-01-29 20:35:43.322260: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb147a8a00 of size 16777216
2019-01-29 20:35:43.322278: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb157a8a00 of size 16777216
2019-01-29 20:35:43.322297: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb167a8a00 of size 16777216
2019-01-29 20:35:43.342200: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb177a8a00 of size 26214400
2019-01-29 20:35:43.342210: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb190a8a00 of size 16777216
2019-01-29 20:35:43.342214: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1a0a8a00 of size 67108864
2019-01-29 20:35:43.342219: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb1e0a8a00 of size 22229811
2
2019-01-29 20:35:43.342222: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4a8a00 of size 16384
2019-01-29 20:35:43.342225: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4aca00 of size 256
2019-01-29 20:35:43.342228: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4acb00 of size 256
2019-01-29 20:35:43.342231: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4acc00 of size 6400
2019-01-29 20:35:43.342236: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ae500 of size 256
2019-01-29 20:35:43.342239: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ae600 of size 256
2019-01-29 20:35:43.342243: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ae700 of size 256
2019-01-29 20:35:43.342246: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ae800 of size 256
2019-01-29 20:35:43.342250: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ae900 of size 256
2019-01-29 20:35:43.342253: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4aea00 of size 256
2019-01-29 20:35:43.342257: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4aeb00 of size 6400
2019-01-29 20:35:43.342260: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0400 of size 512
2019-01-29 20:35:43.342264: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0600 of size 256
2019-01-29 20:35:43.342268: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0700 of size 256
2019-01-29 20:35:43.342271: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0800 of size 256
2019-01-29 20:35:43.342275: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0900 of size 256
2019-01-29 20:35:43.342278: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0a00 of size 256
2019-01-29 20:35:43.342282: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0b00 of size 256
2019-01-29 20:35:43.342286: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0c00 of size 256
2019-01-29 20:35:43.342289: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0d00 of size 256
2019-01-29 20:35:43.342293: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0e00 of size 256
2019-01-29 20:35:43.342297: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0f00 of size 256
2019-01-29 20:35:43.342300: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1000 of size 256
2019-01-29 20:35:43.342304: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1100 of size 256
2019-01-29 20:35:43.342307: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1200 of size 256
2019-01-29 20:35:43.342311: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1300 of size 256
2019-01-29 20:35:43.342315: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1400 of size 512
2019-01-29 20:35:43.342318: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1600 of size 256
2019-01-29 20:35:43.342322: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1700 of size 256
2019-01-29 20:35:43.342325: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1800 of size 256
2019-01-29 20:35:43.342329: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1900 of size 256
2019-01-29 20:35:43.342333: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1a00 of size 1024
2019-01-29 20:35:43.342336: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1e00 of size 256
2019-01-29 20:35:43.342340: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b1f00 of size 256
2019-01-29 20:35:43.342343: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2000 of size 256
2019-01-29 20:35:43.342347: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2100 of size 256
2019-01-29 20:35:43.342351: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2200 of size 2048
2019-01-29 20:35:43.342354: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2a00 of size 256
2019-01-29 20:35:43.342358: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2b00 of size 256
2019-01-29 20:35:43.342362: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2c00 of size 256
2019-01-29 20:35:43.342365: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2d00 of size 256
2019-01-29 20:35:43.342369: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2e00 of size 16384
2019-01-29 20:35:43.342372: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6e00 of size 256
2019-01-29 20:35:43.342376: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6f00 of size 256
2019-01-29 20:35:43.342380: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b7000 of size 6400
2019-01-29 20:35:43.342383: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b8900 of size 4096
2019-01-29 20:35:43.342387: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9900 of size 12288
2019-01-29 20:35:43.342391: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bc900 of size 1024
2019-01-29 20:35:43.342394: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bcd00 of size 11264
2019-01-29 20:35:43.342398: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bf900 of size 256
2019-01-29 20:35:43.342402: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bfa00 of size 1024
2019-01-29 20:35:43.342405: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bfe00 of size 256
2019-01-29 20:35:43.342409: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bff00 of size 256
2019-01-29 20:35:43.342412: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c0000 of size 256
2019-01-29 20:35:43.342416: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c0100 of size 256
2019-01-29 20:35:43.342420: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c0200 of size 6400
2019-01-29 20:35:43.342423: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c1b00 of size 512
2019-01-29 20:35:43.342427: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c1d00 of size 256
2019-01-29 20:35:43.342430: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c1e00 of size 256
2019-01-29 20:35:43.342434: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c1f00 of size 256
2019-01-29 20:35:43.342438: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c2000 of size 256
2019-01-29 20:35:43.342441: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c2100 of size 256
2019-01-29 20:35:43.342445: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c2200 of size 256
2019-01-29 20:35:43.342448: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c2300 of size 256
2019-01-29 20:35:43.342452: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c2400 of size 256
2019-01-29 20:35:43.342455: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c2500 of size 256
2019-01-29 20:35:43.342459: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c2600 of size 6656
2019-01-29 20:35:43.342463: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c4000 of size 256
2019-01-29 20:35:43.342466: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c4100 of size 256
2019-01-29 20:35:43.342470: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c4200 of size 512
2019-01-29 20:35:43.342474: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c4400 of size 512
2019-01-29 20:35:43.342477: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c4600 of size 4096
2019-01-29 20:35:43.342481: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c5600 of size 1024
2019-01-29 20:35:43.342484: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c5a00 of size 1024
2019-01-29 20:35:43.342488: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c5e00 of size 1024
2019-01-29 20:35:43.342492: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c6200 of size 2048
2019-01-29 20:35:43.342495: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c6a00 of size 2048
2019-01-29 20:35:43.342499: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c7200 of size 256
2019-01-29 20:35:43.342502: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c7300 of size 2048
2019-01-29 20:35:43.342506: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c7b00 of size 256
2019-01-29 20:35:43.342510: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c7c00 of size 2048
2019-01-29 20:35:43.342513: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c8400 of size 2048
2019-01-29 20:35:43.342517: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c8c00 of size 2048
2019-01-29 20:35:43.342520: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c9400 of size 16384
2019-01-29 20:35:43.342524: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4cd400 of size 20992
2019-01-29 20:35:43.342528: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4d2600 of size 256
2019-01-29 20:35:43.342532: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4d2700 of size 294912
2019-01-29 20:35:43.342535: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b51a700 of size 589824
2019-01-29 20:35:43.342539: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b5aa700 of size 1179648
2019-01-29 20:35:43.342543: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b6ca700 of size 2359296
2019-01-29 20:35:43.342546: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b90a700 of size 67108864
2019-01-29 20:35:43.342550: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2f90a700 of size 41104179
2
2019-01-29 20:35:43.342554: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb4810a700 of size 41104179
2
2019-01-29 20:35:43.342558: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb6090a700 of size 41104179
2
2019-01-29 20:35:43.342561: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb7910a700 of size 30828134
40
2019-01-29 20:35:43.342565: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xc30d0a700 of size 30828134
40
2019-01-29 20:35:43.342569: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xce890a700 of size 29367367
68
2019-01-29 20:35:43.342572: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size
:
2019-01-29 20:35:43.342579: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 113 Chunks of size 256 totalling 28.2
KiB
2019-01-29 20:35:43.342583: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 12 Chunks of size 512 totalling 6.0Ki
B
2019-01-29 20:35:43.342587: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 13 Chunks of size 1024 totalling 13.0
KiB
2019-01-29 20:35:43.342591: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.2Ki
B
2019-01-29 20:35:43.342595: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 1792 totalling 7.0Ki
B
2019-01-29 20:35:43.342599: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 14 Chunks of size 2048 totalling 28.0
KiB
2019-01-29 20:35:43.342603: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 6 Chunks of size 4096 totalling 24.0K
iB
2019-01-29 20:35:43.342607: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 8 Chunks of size 6400 totalling 50.0K
iB
2019-01-29 20:35:43.342611: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 6656 totalling 6.5Ki
B
2019-01-29 20:35:43.342615: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 6912 totalling 6.8Ki
B
2019-01-29 20:35:43.342619: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 11264 totalling 11.0
KiB
2019-01-29 20:35:43.342623: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 12288 totalling 60.0
KiB
2019-01-29 20:35:43.342627: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 16384 totalling 80.0
KiB
2019-01-29 20:35:43.342631: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 20992 totalling 20.5
KiB
2019-01-29 20:35:43.342635: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 147456 totalling 288
.0KiB
2019-01-29 20:35:43.342639: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 288000 totalling 281
.2KiB
2019-01-29 20:35:43.342643: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 294912 totalling 288
.0KiB
2019-01-29 20:35:43.342647: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 423168 totalling 413
.2KiB
2019-01-29 20:35:43.342651: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 589824 totalling 1.1
2MiB
2019-01-29 20:35:43.342655: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 1179648 totalling 2.
25MiB
2019-01-29 20:35:43.342659: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 2359296 totalling 9.
00MiB
2019-01-29 20:35:43.342663: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 4718592 totalling 9.
00MiB
2019-01-29 20:35:43.342667: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 8 Chunks of size 9437184 totalling 72
.00MiB
2019-01-29 20:35:43.342671: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 15073280 totalling 1
4.38MiB
2019-01-29 20:35:43.342675: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 16384000 totalling 3
1.25MiB
2019-01-29 20:35:43.342679: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 16777216 totalling 8
0.00MiB
2019-01-29 20:35:43.342683: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 67108864 totalling 1
28.00MiB
2019-01-29 20:35:43.342687: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 411041792 totalling
784.00MiB
2019-01-29 20:35:43.342691: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 3082813440 totalling
 5.74GiB
2019-01-29 20:35:43.342695: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 6.85GiB
2019-01-29 20:35:43.342700: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                 10960483124
InUse:                  7353217792
MaxInUse:               8798146048
NumAllocs:                     368
MaxAllocSize:           3082813440

2019-01-29 20:35:43.342714: W tensorflow/core/common_runtime/bfc_allocator.cc:279] ****_*****___************************
*************************************__________________________
2019-01-29 20:35:43.342728: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_ops.cc:672 : Resou
rce exhausted: OOM when allocating tensor with shape[240,64,224,224] and type float on /job:localhost/replica:0/task:0/d
evice:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_tf_
sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in __e
xit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[240,64,224,224] an
d type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdCNN_1/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], paddi
ng="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](tdCNN_1/
block1_conv1/Relu, block1_conv2_1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.

         [[Node: dense_1_1/BiasAdd/_707 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/de
vice:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_58
5_dense_1_1/BiasAdd", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 75, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 124,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 85, i
n evaluateSubject
    predictions = full_model.predict_generator(test_gen, steps = int(len(test_labels)/batch_size), verbose = 1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1522, in predict_generator
    verbose=verbose)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py", line 453, in predict_gen
erator
    outs = model.predict_on_batch(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1274, in predict_on_batch
    outputs = self.predict_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2721, in __call__
    return self._legacy_call(inputs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2693, in _legacy_c
all
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[240,64,224,224] an
d type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdCNN_1/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], paddi
ng="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](tdCNN_1/
block1_conv1/Relu, block1_conv2_1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.

         [[Node: dense_1_1/BiasAdd/_707 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/de
vice:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_58
5_dense_1_1/BiasAdd", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.


Caused by op 'tdCNN_1/block1_conv2/convolution', defined at:
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 44, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.py", line 52,
 in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py", line 419, in load_model
    model = _deserialize_model(f, custom_objects, compile)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py", line 225, in _deserialize_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py", line 458, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserialize
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 145, in deserialize_keras
_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py", line 301, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py", line 165, in add
    layer(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py", line 457, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 248, in call
    y = self.layer.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/network.py", line 564, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/network.py", line 721, in run_internal_graph
    layer.call(computed_tensor, **kwargs))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py", line 171, in call
    dilation_rate=self.dilation_rate)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 3650, in conv2d
    data_format=tf_data_format)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 782, in convolution
    return op(input, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 870, in __call__
    return self.conv_op(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 522, in __call__
    return self.call(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 206, in __call__
    name=self.name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 953, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in
_apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, in create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[240,64,224,224] and type float o
n /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdCNN_1/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], paddi
ng="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](tdCNN_1/
block1_conv1/Relu, block1_conv2_1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.

         [[Node: dense_1_1/BiasAdd/_707 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/de
vice:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_58
5_dense_1_1/BiasAdd", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.


mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_19-35-30_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 20:36:30.758415: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 20:36:30.855546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 20:36:30.855807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 20:36:30.855819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 20:36:31.010791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 20:36:31.010816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 20:36:31.010821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 20:36:31.010959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_19-35-30_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_19-35-30_part1_and_2019-01-29_20-36-33
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 20:36:35.349796
For the Subject 3 (F03):
2019-01-29 20:37:19.933877: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memo
ry trying to allocate 2.87GiB.  Current allocation summary follows.
2019-01-29 20:37:19.933971: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 113, Chunk
s in use: 113. 28.2KiB allocated for chunks. 28.2KiB in use in bin. 4.1KiB client-requested in use in bin.
2019-01-29 20:37:19.934013: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 12, Chunks
 in use: 12. 6.0KiB allocated for chunks. 6.0KiB in use in bin. 4.9KiB client-requested in use in bin.
2019-01-29 20:37:19.934043: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 18, Chunks
 in use: 18. 21.2KiB allocated for chunks. 21.2KiB in use in bin. 19.9KiB client-requested in use in bin.
2019-01-29 20:37:19.934073: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 14, Chunks
 in use: 14. 28.0KiB allocated for chunks. 28.0KiB in use in bin. 28.0KiB client-requested in use in bin.
2019-01-29 20:37:19.934102: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 16, Chunks
 in use: 16. 87.2KiB allocated for chunks. 87.2KiB in use in bin. 82.3KiB client-requested in use in bin.
2019-01-29 20:37:19.934128: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 6, Chunks
in use: 6. 71.0KiB allocated for chunks. 71.0KiB in use in bin. 66.8KiB client-requested in use in bin.
2019-01-29 20:37:19.934153: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 6,
 Chunks in use: 6. 100.5KiB allocated for chunks. 100.5KiB in use in bin. 96.0KiB client-requested in use in bin.
2019-01-29 20:37:19.934177: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 1,
 Chunks in use: 0. 42.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-29 20:37:19.934201: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 0,
 Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-29 20:37:19.934230: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 4,
 Chunks in use: 3. 576.0KiB allocated for chunks. 432.0KiB in use in bin. 432.0KiB client-requested in use in bin.
2019-01-29 20:37:19.934256: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 4,
 Chunks in use: 2. 1.12MiB allocated for chunks. 576.0KiB in use in bin. 576.0KiB client-requested in use in bin.
2019-01-29 20:37:19.934279: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks: 4,
 Chunks in use: 2. 2.25MiB allocated for chunks. 1.12MiB in use in bin. 1.12MiB client-requested in use in bin.
2019-01-29 20:37:19.934305: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks: 3,
 Chunks in use: 2. 3.38MiB allocated for chunks. 2.25MiB in use in bin. 2.25MiB client-requested in use in bin.
2019-01-29 20:37:19.934333: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks: 5,
 Chunks in use: 4. 12.38MiB allocated for chunks. 10.12MiB in use in bin. 9.00MiB client-requested in use in bin.
2019-01-29 20:37:19.934360: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks: 3,
 Chunks in use: 2. 13.50MiB allocated for chunks. 9.00MiB in use in bin. 9.00MiB client-requested in use in bin.
2019-01-29 20:37:19.934388: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks: 11
, Chunks in use: 11. 117.62MiB allocated for chunks. 117.62MiB in use in bin. 105.62MiB client-requested in use in bin.
2019-01-29 20:37:19.934417: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks: 6,
 Chunks in use: 5. 105.00MiB allocated for chunks. 80.00MiB in use in bin. 80.00MiB client-requested in use in bin.
2019-01-29 20:37:19.934441: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks: 0,
 Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-29 20:37:19.934470: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks: 2,
 Chunks in use: 2. 128.00MiB allocated for chunks. 128.00MiB in use in bin. 128.00MiB client-requested in use in bin.
2019-01-29 20:37:19.934496: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks: 1,
 Chunks in use: 0. 212.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-29 20:37:19.934532: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks: 6,
 Chunks in use: 4. 9.62GiB allocated for chunks. 6.51GiB in use in bin. 6.51GiB client-requested in use in bin.
2019-01-29 20:37:19.934557: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 2.87GiB was 256.00MiB, Chunk
State:
2019-01-29 20:37:19.934598: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 392.00MiB | Requested Size: 1
37.81MiB | in_use: 0, prev:   Size: 392.00MiB | Requested Size: 392.00MiB | in_use: 1, next:   Size: 392.00MiB | Request
ed Size: 392.00MiB | in_use: 1
2019-01-29 20:37:19.934627: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 2.73GiB | Requested Size: 1.2
1GiB | in_use: 0, prev:   Size: 2.87GiB | Requested Size: 2.87GiB | in_use: 1
2019-01-29 20:37:19.934652: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500000 of size 1280
2019-01-29 20:37:19.934673: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500500 of size 256
2019-01-29 20:37:19.934691: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500600 of size 256
2019-01-29 20:37:19.934710: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500700 of size 256
2019-01-29 20:37:19.934728: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500800 of size 256
2019-01-29 20:37:19.934746: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500900 of size 256
2019-01-29 20:37:19.934765: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500a00 of size 16384
2019-01-29 20:37:19.934784: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504a00 of size 256
2019-01-29 20:37:19.934802: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504b00 of size 256
2019-01-29 20:37:19.934821: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504c00 of size 512
2019-01-29 20:37:19.934839: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504e00 of size 256
2019-01-29 20:37:19.934858: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504f00 of size 256
2019-01-29 20:37:19.934876: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505000 of size 256
2019-01-29 20:37:19.934894: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505100 of size 256
2019-01-29 20:37:19.934912: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505200 of size 1024
2019-01-29 20:37:19.934932: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505600 of size 256
2019-01-29 20:37:19.934950: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505700 of size 256
2019-01-29 20:37:19.934968: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505800 of size 256
2019-01-29 20:37:19.934986: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505900 of size 256
2019-01-29 20:37:19.935005: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505a00 of size 256
2019-01-29 20:37:19.935023: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505b00 of size 2048
2019-01-29 20:37:19.935042: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506300 of size 256
2019-01-29 20:37:19.935060: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506400 of size 256
2019-01-29 20:37:19.935079: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506500 of size 256
2019-01-29 20:37:19.935097: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506600 of size 256
2019-01-29 20:37:19.935114: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506700 of size 256
2019-01-29 20:37:19.935134: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506800 of size 4096
2019-01-29 20:37:19.935153: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507800 of size 256
2019-01-29 20:37:19.935171: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507900 of size 256
2019-01-29 20:37:19.935189: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507a00 of size 256
2019-01-29 20:37:19.935207: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507b00 of size 256
2019-01-29 20:37:19.935226: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507c00 of size 6400
2019-01-29 20:37:19.935245: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a509500 of size 256
2019-01-29 20:37:19.935263: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a509600 of size 256
2019-01-29 20:37:19.935281: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a509700 of size 256
2019-01-29 20:37:19.935299: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a509800 of size 256
2019-01-29 20:37:19.935318: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a509900 of size 256
2019-01-29 20:37:19.935336: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a509a00 of size 256
2019-01-29 20:37:19.935354: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a509b00 of size 6400
2019-01-29 20:37:19.935373: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50b400 of size 512
2019-01-29 20:37:19.935391: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50b600 of size 256
2019-01-29 20:37:19.935409: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50b700 of size 256
2019-01-29 20:37:19.935427: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50b800 of size 256
2019-01-29 20:37:19.935446: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50b900 of size 256
2019-01-29 20:37:19.935464: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50ba00 of size 256
2019-01-29 20:37:19.935483: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50bb00 of size 256
2019-01-29 20:37:19.935501: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50bc00 of size 256
2019-01-29 20:37:19.935519: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50bd00 of size 256
2019-01-29 20:37:19.935535: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50be00 of size 256
2019-01-29 20:37:19.935553: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50bf00 of size 256
2019-01-29 20:37:19.935571: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50c000 of size 256
2019-01-29 20:37:19.935589: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50c100 of size 256
2019-01-29 20:37:19.935607: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50c200 of size 256
2019-01-29 20:37:19.935625: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50c300 of size 256
2019-01-29 20:37:19.935644: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50c400 of size 512
2019-01-29 20:37:19.935662: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50c600 of size 256
2019-01-29 20:37:19.935680: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50c700 of size 256
2019-01-29 20:37:19.935698: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50c800 of size 256
2019-01-29 20:37:19.935716: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50c900 of size 256
2019-01-29 20:37:19.935735: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50ca00 of size 1024
2019-01-29 20:37:19.935754: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50ce00 of size 256
2019-01-29 20:37:19.935772: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50cf00 of size 256
2019-01-29 20:37:19.935790: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50d000 of size 256
2019-01-29 20:37:19.935808: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50d100 of size 256
2019-01-29 20:37:19.935826: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50d200 of size 2048
2019-01-29 20:37:19.935844: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50da00 of size 256
2019-01-29 20:37:19.935863: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50db00 of size 256
2019-01-29 20:37:19.935881: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50dc00 of size 256
2019-01-29 20:37:19.935899: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50dd00 of size 256
2019-01-29 20:37:19.935918: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a50de00 of size 16384
2019-01-29 20:37:19.935936: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a511e00 of size 256
2019-01-29 20:37:19.935954: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a511f00 of size 256
2019-01-29 20:37:19.935973: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a512000 of size 6400
2019-01-29 20:37:19.935992: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a513900 of size 4096
2019-01-29 20:37:19.936010: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a514900 of size 12288
2019-01-29 20:37:19.936029: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a517900 of size 1024
2019-01-29 20:37:19.936048: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a517d00 of size 11264
2019-01-29 20:37:19.936067: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51a900 of size 256
2019-01-29 20:37:19.936085: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51aa00 of size 1024
2019-01-29 20:37:19.936104: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51ae00 of size 256
2019-01-29 20:37:19.936122: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51af00 of size 256
2019-01-29 20:37:19.936140: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51b000 of size 256
2019-01-29 20:37:19.936159: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51b100 of size 256
2019-01-29 20:37:19.936177: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51b200 of size 6400
2019-01-29 20:37:19.936195: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51cb00 of size 512
2019-01-29 20:37:19.936213: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51cd00 of size 256
2019-01-29 20:37:19.936231: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51ce00 of size 256
2019-01-29 20:37:19.936250: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51cf00 of size 256
2019-01-29 20:37:19.936268: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51d000 of size 256
2019-01-29 20:37:19.936286: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51d100 of size 256
2019-01-29 20:37:19.936304: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51d200 of size 256
2019-01-29 20:37:19.936323: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51d300 of size 256
2019-01-29 20:37:19.936341: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51d400 of size 256
2019-01-29 20:37:19.936359: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51d500 of size 256
2019-01-29 20:37:19.936378: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51d600 of size 6656
2019-01-29 20:37:19.936397: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51f000 of size 256
2019-01-29 20:37:19.936415: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51f100 of size 256
2019-01-29 20:37:19.936433: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51f200 of size 512
2019-01-29 20:37:19.936451: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51f400 of size 512
2019-01-29 20:37:19.936470: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a51f600 of size 4096
2019-01-29 20:37:19.936488: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a520600 of size 1024
2019-01-29 20:37:19.936507: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a520a00 of size 1024
2019-01-29 20:37:19.936525: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a520e00 of size 1024
2019-01-29 20:37:19.936544: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a521200 of size 2048
2019-01-29 20:37:19.936562: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a521a00 of size 2048
2019-01-29 20:37:19.936580: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a522200 of size 256
2019-01-29 20:37:19.936598: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a522300 of size 2048
2019-01-29 20:37:19.936617: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a522b00 of size 256
2019-01-29 20:37:19.936635: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a522c00 of size 2048
2019-01-29 20:37:19.936654: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a523400 of size 2048
2019-01-29 20:37:19.936672: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a523c00 of size 2048
2019-01-29 20:37:19.936691: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a524400 of size 16384
2019-01-29 20:37:19.936710: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a528400 of size 20992
2019-01-29 20:37:19.936728: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a52d600 of size 256
2019-01-29 20:37:19.936747: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a52d700 of size 147456
2019-01-29 20:37:19.936766: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a551700 of size 6400
2019-01-29 20:37:19.936784: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a553000 of size 512
2019-01-29 20:37:19.936803: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a553200 of size 12288
2019-01-29 20:37:19.936822: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a556200 of size 12288
2019-01-29 20:37:19.936840: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a559200 of size 256
2019-01-29 20:37:19.936858: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a559300 of size 1024
2019-01-29 20:37:19.936877: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a559700 of size 6400
2019-01-29 20:37:19.936896: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55b000 of size 512
2019-01-29 20:37:19.936915: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55b200 of size 256
2019-01-29 20:37:19.936933: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55b300 of size 256
2019-01-29 20:37:19.936951: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55b400 of size 256
2019-01-29 20:37:19.936969: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55b500 of size 256
2019-01-29 20:37:19.936988: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55b600 of size 256
2019-01-29 20:37:19.937006: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55b700 of size 256
2019-01-29 20:37:19.937024: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55b800 of size 256
2019-01-29 20:37:19.937042: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55b900 of size 4096
2019-01-29 20:37:19.937061: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55c900 of size 12288
2019-01-29 20:37:19.937080: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55f900 of size 256
2019-01-29 20:37:19.937098: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55fa00 of size 1024
2019-01-29 20:37:19.937116: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a55fe00 of size 6400
2019-01-29 20:37:19.957908: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a561700 of size 512
2019-01-29 20:37:19.957919: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a561900 of size 256
2019-01-29 20:37:19.957924: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a561a00 of size 256
2019-01-29 20:37:19.957928: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a561b00 of size 4096
2019-01-29 20:37:19.957932: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a562b00 of size 12288
2019-01-29 20:37:19.957936: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a565b00 of size 256
2019-01-29 20:37:19.957940: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a565c00 of size 1024
2019-01-29 20:37:19.957944: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a566000 of size 6400
2019-01-29 20:37:19.957949: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a567900 of size 512
2019-01-29 20:37:19.957952: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a567b00 of size 256
2019-01-29 20:37:19.957957: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a567c00 of size 256
2019-01-29 20:37:19.957961: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a567d00 of size 256
2019-01-29 20:37:19.957966: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a567e00 of size 256
2019-01-29 20:37:19.957969: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a567f00 of size 256
2019-01-29 20:37:19.957974: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568000 of size 256
2019-01-29 20:37:19.957978: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568100 of size 256
2019-01-29 20:37:19.957982: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568200 of size 256
2019-01-29 20:37:19.957987: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568300 of size 256
2019-01-29 20:37:19.957991: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568400 of size 256
2019-01-29 20:37:19.957996: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568500 of size 256
2019-01-29 20:37:19.958000: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568600 of size 256
2019-01-29 20:37:19.958004: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568700 of size 256
2019-01-29 20:37:19.958009: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568800 of size 256
2019-01-29 20:37:19.958013: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568900 of size 256
2019-01-29 20:37:19.958018: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568a00 of size 256
2019-01-29 20:37:19.958022: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568b00 of size 256
2019-01-29 20:37:19.958026: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568c00 of size 256
2019-01-29 20:37:19.958031: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568d00 of size 256
2019-01-29 20:37:19.958035: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568e00 of size 256
2019-01-29 20:37:19.958040: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a568f00 of size 256
2019-01-29 20:37:19.958044: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a569000 of size 256
2019-01-29 20:37:19.958049: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a569100 of size 1792
2019-01-29 20:37:19.958053: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a569800 of size 1792
2019-01-29 20:37:19.958058: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a569f00 of size 1792
2019-01-29 20:37:19.958062: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a56a600 of size 1792
2019-01-29 20:37:19.958066: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a56ad00 of size 256
2019-01-29 20:37:19.958071: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a56ae00 of size 43264
2019-01-29 20:37:19.958076: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a575700 of size 512
2019-01-29 20:37:19.958080: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a575900 of size 294912
2019-01-29 20:37:19.958085: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5bd900 of size 147456
2019-01-29 20:37:19.958089: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a5e1900 of size 147456
2019-01-29 20:37:19.958094: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a605900 of size 512
2019-01-29 20:37:19.958098: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a605b00 of size 589824
2019-01-29 20:37:19.958103: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a695b00 of size 294912
2019-01-29 20:37:19.958107: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a6ddb00 of size 294912
2019-01-29 20:37:19.958112: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a725b00 of size 1024
2019-01-29 20:37:19.958116: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a725f00 of size 2359296
2019-01-29 20:37:19.958121: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a965f00 of size 1179648
2019-01-29 20:37:19.958126: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aa85f00 of size 589824
2019-01-29 20:37:19.958130: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0ab15f00 of size 589824
2019-01-29 20:37:19.958135: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aba5f00 of size 1024
2019-01-29 20:37:19.958139: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aba6300 of size 4718592
2019-01-29 20:37:19.958144: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b026300 of size 9437184
2019-01-29 20:37:19.958149: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b926300 of size 16384000
2019-01-29 20:37:19.958154: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0c8c6300 of size 16777216
2019-01-29 20:37:19.958158: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d8c6300 of size 4718592
2019-01-29 20:37:19.958163: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0dd46300 of size 15073280
2019-01-29 20:37:19.958167: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0eba6300 of size 1024
2019-01-29 20:37:19.958172: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0eba6700 of size 2359296
2019-01-29 20:37:19.958176: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0ede6700 of size 2359296
2019-01-29 20:37:19.958181: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0f026700 of size 2048
2019-01-29 20:37:19.958185: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0f026f00 of size 9437184
2019-01-29 20:37:19.958189: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0f926f00 of size 9437184
2019-01-29 20:37:19.958193: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb10226f00 of size 9437184
2019-01-29 20:37:19.958198: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb10b26f00 of size 9437184
2019-01-29 20:37:19.958202: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb11426f00 of size 4718592
2019-01-29 20:37:19.958207: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb118a6f00 of size 1179648
2019-01-29 20:37:19.958211: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb119c6f00 of size 3538944
2019-01-29 20:37:19.958216: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d26f00 of size 2048
2019-01-29 20:37:19.958220: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d27700 of size 16384
2019-01-29 20:37:19.958225: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d2b700 of size 2048
2019-01-29 20:37:19.958230: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d2bf00 of size 16384000
2019-01-29 20:37:19.958234: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ccbf00 of size 2048
2019-01-29 20:37:19.958239: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ccc700 of size 2048
2019-01-29 20:37:19.958243: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12cccf00 of size 4096
2019-01-29 20:37:19.958247: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ccdf00 of size 2048
2019-01-29 20:37:19.958252: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12cce700 of size 9437184
2019-01-29 20:37:19.958256: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb135ce700 of size 9437184
2019-01-29 20:37:19.958260: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13ece700 of size 9437184
2019-01-29 20:37:19.958265: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb147ce700 of size 16777216
2019-01-29 20:37:19.958269: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb157ce700 of size 16777216
2019-01-29 20:37:19.958274: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb167ce700 of size 16777216
2019-01-29 20:37:19.958278: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb177ce700 of size 26214400
2019-01-29 20:37:19.958283: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb190ce700 of size 16777216
2019-01-29 20:37:19.958288: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1a0ce700 of size 67108864
2019-01-29 20:37:19.958292: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb1e0ce700 of size 22229811
2
2019-01-29 20:37:19.958297: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ce700 of size 16384
2019-01-29 20:37:19.958302: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4d2700 of size 6912
2019-01-29 20:37:19.958306: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4d4200 of size 147456
2019-01-29 20:37:19.958311: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4f8200 of size 294912
2019-01-29 20:37:19.958315: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b540200 of size 589824
2019-01-29 20:37:19.958320: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b5d0200 of size 1179648
2019-01-29 20:37:19.958324: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b6f0200 of size 2359296
2019-01-29 20:37:19.958329: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b930200 of size 67108864
2019-01-29 20:37:19.958333: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2f930200 of size 41104179
2
2019-01-29 20:37:19.958338: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb48130200 of size 41104179
2
2019-01-29 20:37:19.958343: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb60930200 of size 41104179
2
2019-01-29 20:37:19.958348: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb79130200 of size 30828134
40
2019-01-29 20:37:19.958354: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xc30d30200 of size 30828134
40
2019-01-29 20:37:19.958359: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xce8930200 of size 29365824
00
2019-01-29 20:37:19.958363: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size
:
2019-01-29 20:37:19.958372: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 113 Chunks of size 256 totalling 28.2
KiB
2019-01-29 20:37:19.958377: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 12 Chunks of size 512 totalling 6.0Ki
B
2019-01-29 20:37:19.958383: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 13 Chunks of size 1024 totalling 13.0
KiB
2019-01-29 20:37:19.958388: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.2Ki
B
2019-01-29 20:37:19.958393: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 1792 totalling 7.0Ki
B
2019-01-29 20:37:19.958399: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 14 Chunks of size 2048 totalling 28.0
KiB
2019-01-29 20:37:19.958404: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 6 Chunks of size 4096 totalling 24.0K
iB
2019-01-29 20:37:19.958409: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 8 Chunks of size 6400 totalling 50.0K
iB
2019-01-29 20:37:19.958414: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 6656 totalling 6.5Ki
B
2019-01-29 20:37:19.958419: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 6912 totalling 6.8Ki
B
2019-01-29 20:37:19.958424: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 11264 totalling 11.0
KiB
2019-01-29 20:37:19.958430: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 12288 totalling 60.0
KiB
2019-01-29 20:37:19.958435: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 16384 totalling 80.0
KiB
2019-01-29 20:37:19.958440: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 20992 totalling 20.5
KiB
2019-01-29 20:37:19.958446: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 147456 totalling 432
.0KiB
2019-01-29 20:37:19.958451: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 294912 totalling 576
.0KiB
2019-01-29 20:37:19.958456: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 589824 totalling 1.1
2MiB
2019-01-29 20:37:19.958461: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 1179648 totalling 2.
25MiB
2019-01-29 20:37:19.958466: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 2359296 totalling 6.
75MiB
2019-01-29 20:37:19.958471: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3538944 totalling 3.
38MiB
2019-01-29 20:37:19.958477: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 4718592 totalling 9.
00MiB
2019-01-29 20:37:19.958482: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 8 Chunks of size 9437184 totalling 72
.00MiB
2019-01-29 20:37:19.958487: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 15073280 totalling 1
4.38MiB
2019-01-29 20:37:19.958493: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 16384000 totalling 3
1.25MiB
2019-01-29 20:37:19.958498: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 16777216 totalling 8
0.00MiB
2019-01-29 20:37:19.958503: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 67108864 totalling 1
28.00MiB
2019-01-29 20:37:19.958509: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 411041792 totalling
784.00MiB
2019-01-29 20:37:19.958514: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 3082813440 totalling
 5.74GiB
2019-01-29 20:37:19.958519: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 6.85GiB
2019-01-29 20:37:19.958526: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                 10960483124
InUse:                  7354128640
MaxInUse:               8799056896
NumAllocs:                     368
MaxAllocSize:           3082813440

2019-01-29 20:37:19.958543: W tensorflow/core/common_runtime/bfc_allocator.cc:279] ****_*****___************************
*************************************__________________________
2019-01-29 20:37:19.958558: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_ops.cc:672 : Resou
rce exhausted: OOM when allocating tensor with shape[240,64,224,224] and type float on /job:localhost/replica:0/task:0/d
evice:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_tf_
sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in __e
xit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[240,64,224,224] an
d type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdCNN_1/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], paddi
ng="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](tdCNN_1/
block1_conv1/Relu, block1_conv2_1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 75, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 124,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 85, i
n evaluateSubject
    predictions = full_model.predict_generator(test_gen, steps = int(len(test_labels)/batch_size), verbose = 1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1522, in predict_generator
    verbose=verbose)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py", line 453, in predict_gen
erator
    outs = model.predict_on_batch(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1274, in predict_on_batch
    outputs = self.predict_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2721, in __call__
    return self._legacy_call(inputs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2693, in _legacy_c
all
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[240,64,224,224] an
d type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdCNN_1/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], paddi
ng="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](tdCNN_1/
block1_conv1/Relu, block1_conv2_1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.


Caused by op 'tdCNN_1/block1_conv2/convolution', defined at:
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 44, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.py", line 52,
 in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py", line 419, in load_model
    model = _deserialize_model(f, custom_objects, compile)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py", line 225, in _deserialize_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py", line 458, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserialize
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 145, in deserialize_keras
_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py", line 301, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/sequential.py", line 165, in add
    layer(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py", line 457, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 248, in call
    y = self.layer.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/network.py", line 564, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/network.py", line 721, in run_internal_graph
    layer.call(computed_tensor, **kwargs))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py", line 171, in call
    dilation_rate=self.dilation_rate)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 3650, in conv2d
    data_format=tf_data_format)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 782, in convolution
    return op(input, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 870, in __call__
    return self.conv_op(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 522, in __call__
    return self.call(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 206, in __call__
    name=self.name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 953, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in
_apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, in create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[240,64,224,224] and type float o
n /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdCNN_1/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], paddi
ng="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](tdCNN_1/
block1_conv1/Relu, block1_conv2_1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.


mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_19-35-30_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 20:37:43.321714: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 20:37:43.402567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 20:37:43.402865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 20:37:43.402879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 20:37:43.557090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 20:37:43.557116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 20:37:43.557121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 20:37:43.557305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_19-35-30_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 12

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_19-35-30_part1_and_2019-01-29_20-37-45
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 20:37:47.999197
For the Subject 3 (F03):
59/59 [==============================] - 50s 852ms/step
(698, 1) (708, 1)
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 75, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 124,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 97, i
n evaluateSubject
    matrix = numpy.concatenate((test_labels[:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 20:41:27.767099: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 20:41:27.863730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 20:41:27.863985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 20:41:27.863997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 20:41:28.018299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 20:41:28.018326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 20:41:28.018330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 20:41:28.018469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_20-41-28 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 12

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_20-
41-28
All frames and annotations from 20 datasets have been read by 2019-01-29 20:41:33.136381
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 20:41:39.564016!
Epoch 1/1
 8/44 [====>.........................] - ETA: 1:01 - loss: 0.2572^C
Model Exp2019-01-29_20-41-28_part1 has been interrupted.
Exp2019-01-29_20-41-28_part1.h5 has been saved.
Model Exp2019-01-29_20-41-28_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_19-35-30_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 20:42:17.654319: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 20:42:17.751824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 20:42:17.752089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 20:42:17.752103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 20:42:17.905992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 20:42:17.906019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 20:42:17.906026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 20:42:17.906167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_19-35-30_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 12

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_19-35-30_part1_and_2019-01-29_20-42-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 20:42:22.238634
For the Subject 3 (F03):
59/59 [==============================] - 51s 864ms/step
(714, 1) (708, 1)
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 75, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 124,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 97, i
n evaluateSubject
    matrix = numpy.concatenate((test_labels[:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py

/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 20:44:42.473675: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 20:44:42.570250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 20:44:42.570506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 20:44:42.570517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 20:44:42.726626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 20:44:42.726652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 20:44:42.726657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 20:44:42.726796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_20-44-43 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 12

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize15_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-29_20-
44-43
All frames and annotations from 20 datasets have been read by 2019-01-29 20:44:47.976578
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 20:44:54.408795!
Epoch 1/1
 8/44 [====>.........................] - ETA: 1:03 - loss: 0.2540^C
Model Exp2019-01-29_20-44-43_part1 has been interrupted.
Exp2019-01-29_20-44-43_part1.h5 has been saved.
Model Exp2019-01-29_20-44-43_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_19-35-30_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 20:45:26.349140: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 20:45:26.447625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 20:45:26.447882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 20:45:26.447894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 20:45:26.602045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 20:45:26.602071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 20:45:26.602076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 20:45:26.602209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_19-35-30_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 12

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_19-35-30_part1_and_2019-01-29_20-45-28
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 20:45:30.884068
For the Subject 3 (F03):
60/60 [==============================] - 54s 892ms/step
(730, 1) (714, 1)
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 75, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 122,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 95, i
n evaluateSubject
    matrix = numpy.concatenate((test_labels[:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_19-35-30_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 21:40:45.603918: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 21:40:45.701008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 21:40:45.701268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 21:40:45.701280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 21:40:45.855893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 21:40:45.855919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 21:40:45.855924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 21:40:45.856063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_19-35-30_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_19-35-30_part1_and_2019-01-29_21-40-48
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 21:40:50.219770
For the Subject 3 (F03):
730/730 [==============================] - 66s 90ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 12.39 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 33.29 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 6.88 Degree
For the Subject 5 (F05):
946/946 [==============================] - 86s 90ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 23.06 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 29.85 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 3.94 Degree
For the Subject 9 (M03):
882/882 [==============================] - 81s 92ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 17.30 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 26.32 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 6.81 Degree
For the Subject 14 (M08):
797/797 [==============================] - 74s 93ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 19.83 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 36.52 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 14.43 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.14 Degree
        The absolute mean error on Yaw angle estimations: 31.50 Degree
        The absolute mean error on Roll angle estimations: 8.01 Degree
subject3_Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48.png has been saved by 2019-01-29 21:46:30.679811.
subject5_Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48.png has been saved by 2019-01-29 21:46:30.873301.
subject9_Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48.png has been saved by 2019-01-29 21:46:31.065556.
subject14_Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48.png has been saved by 2019-01-29 21:46:31.277702.
Model Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48 has been evaluated successfully.
Model Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_19-35-30_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 22:30:11.345239: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 22:30:11.442132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 22:30:11.442388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 22:30:11.442399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 22:30:11.596439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 22:30:11.596466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 22:30:11.596471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 22:30:11.596609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_19-35-30_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 3
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_19-35-30_part1_and_2019-01-29_22-30-13
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 22:30:15.977416
^CTraceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 85, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 82, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 75, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 119,
in evaluateCNN_LSTM
    batch_size = batch_size, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 57, i
n getTestBiwiForImageModel
    for inputMatrix, labels in testBiwi:
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 151, in <gen
expr>
    biwi = (labeledFrames(frames, biwiAnnos[subj]) for subj, frames in biwiFrames.items())
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 150, in <lam
bda>
    labeledFrames = lambda frames, labels: labelFramesForSubj(frames, labels, timesteps, overlapping, scaling, scalers)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 134, in labe
lFramesForSubj
    frames = {n: f for n, f in frames}
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 134, in <dic
tcomp>
    frames = {n: f for n, f in frames}
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 74, in <gene
xpr>
    frames = ((n, preprocess_input(framePath)) for n, framePath in frameNamesForSubj)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 49, in pngOb
jToNpArr
    img = image.load_img(imagePath, target_size = Target_Frame_Shape_VGG16)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image.py", line 520, in load_img
    img = img.resize(width_height_tuple, resample)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/PIL/Image.py", line 1763, in resize
    self.load()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py", line 235, in load
    n, err_code = decoder.decode(b)
KeyboardInterrupt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py

/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 22:30:50.671560: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 22:30:50.751399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 22:30:50.751655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 22:30:50.751668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 22:30:50.904881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 22:30:50.904907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 22:30:50.904912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 22:30:50.905047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_22-30-51 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 3)                    84
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    12
=================================================================
Total params: 138,459,043
Trainable params: 4,198,499
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 3
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm3_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01-
29_22-30-51
All frames and annotations from 20 datasets have been read by 2019-01-29 22:30:56.226933
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:31:02.647044!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.1406
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:31:24.629499!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.1567
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:31:42.463366!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.2064
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:32:02.700554!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.1888
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:32:22.061307!
Epoch 1/1
772/772 [==============================] - 18s 24ms/step - loss: 0.2772
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:32:46.004438!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.2460
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:33:05.646021!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.2494
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:33:28.596557!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.1965
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:33:58.072209!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.2857
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:34:22.622810!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.2429
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:34:46.518006!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.2368
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:35:08.322028!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.2863
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:35:26.205449!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.2531
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:35:45.955440!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.3110
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:36:05.634059!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.2916
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:36:28.840708!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.1746
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:36:45.968641!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.1395
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:37:00.170443!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.2340
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:37:17.352897!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.1912
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:37:34.436490!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.1726
Epoch 1 for Experiment 1 completed!
Exp2019-01-29_22-30-51_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm3_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-0
1-29_22-30-51
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 22:37:50.119024
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 73, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 70, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record, preprocess_input = preprocess_input)
  File "runFC_RNN_Experiment.py", line 40, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record = record, prep
rocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 122,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, output_begin, num_outpu
ts, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 84, i
n evaluateSubject
    if stateful:  model.reset_states()
NameError: name 'model' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python Exp2019-01-29_22-30-51_par
t1
python: can't open file 'Exp2019-01-29_22-30-51_part1': [Errno 2] No such file or directory
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_22-30-51_part1
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 22:45:22.966973: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 22:45:23.063962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 22:45:23.064231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 22:45:23.064247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 22:45:23.218253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 22:45:23.218279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 22:45:23.218284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 22:45:23.218430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_22-30-51_part1.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 3)                    84
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    12
=================================================================
Total params: 138,459,043
Trainable params: 4,198,499
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 3
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-29_22-30-51_part1_and_2019-01-29_22-45-25
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 22:45:27.635903
^[[AFor the Subject 3 (F03):
730/730 [==============================] - 12s 17ms/step
        The absolute mean error on Pitch angle estimation: 16.76 Degree
        The absolute mean error on Yaw angle estimation: 32.60 Degree
        The absolute mean error on Roll angle estimation: 8.88 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 22.09 Degree
        The absolute mean error on Yaw angle estimation: 29.78 Degree
        The absolute mean error on Roll angle estimation: 4.27 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 18.27 Degree
        The absolute mean error on Yaw angle estimation: 26.42 Degree
        The absolute mean error on Roll angle estimation: 8.43 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 23.11 Degree
        The absolute mean error on Yaw angle estimation: 34.82 Degree
        The absolute mean error on Roll angle estimation: 14.15 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 20.06 Degree
        The absolute mean error on Yaw angle estimations: 30.90 Degree
        The absolute mean error on Roll angle estimations: 8.93 Degree
subject3_Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25.png has been saved by 2019-01-29 22:46:55.747127.
subject5_Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25.png has been saved by 2019-01-29 22:46:55.933824.
subject9_Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25.png has been saved by 2019-01-29 22:46:56.120273.
subject14_Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25.png has been saved by 2019-01-29 22:46:56.372783.
Model Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25 has been evaluated successfully.
Model Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "
>
>
> "
On branch master
Your branch is up-to-date with 'origin/master'.
Changes not staged for commit:
        modified:   FC_RNN_Evaluater.py
        modified:   Stateful_FC_RNN_Configuration.py
        modified:   Stateless_FC_RNN_Configuration.py
        modified:   continueFC_RNN_Experiment.py
        modified:   results/Last_Model_/output_Last_Model.txt
        modified:   results/Last_Model__/output_Last_Model.txt
        modified:   results/Last_Model___/output_Last_Model.txt
        modified:   results/Last_Model____/output_Last_Model.txt
        modified:   results/Last_Model_____/output_Last_Model.txt
        modified:   runFC_RNN_Experiment.py

Untracked files:
        results/Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48/
        results/Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25/
        results/Exp2019-01-29_20-41-28_part1/
        results/Exp2019-01-29_20-44-43_part1/
        results/Last_Model______/
        results/Last_Model_______/
        results/Last_Model________/
        results/Last_Model_________/
        results/Last_Model__________/

no changes added to commit
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "Stay stateful"
[master 3a0f6f0] Stay stateful
 27 files changed, 1351 insertions(+), 154 deletions(-)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48/output_Exp1-2
9_19-35-30_part1_and_2019-01-29_21-40-48.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48/subject14_Exp
1-29_19-35-30_part1_and_2019-01-29_21-40-48.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48/subject3_Exp1
-29_19-35-30_part1_and_2019-01-29_21-40-48.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48/subject5_Exp1
-29_19-35-30_part1_and_2019-01-29_21-40-48.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_19-35-30_part1_and_2019-01-29_21-40-48/subject9_Exp1
-29_19-35-30_part1_and_2019-01-29_21-40-48.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25/output_Exp1-2
9_22-30-51_part1_and_2019-01-29_22-45-25.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25/subject14_Exp
1-29_22-30-51_part1_and_2019-01-29_22-45-25.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25/subject3_Exp1
-29_22-30-51_part1_and_2019-01-29_22-45-25.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25/subject5_Exp1
-29_22-30-51_part1_and_2019-01-29_22-45-25.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-29_22-30-51_part1_and_2019-01-29_22-45-25/subject9_Exp1
-29_22-30-51_part1_and_2019-01-29_22-45-25.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_20-41-28_part1/output_Exp2019-01-29_20-41-28_p
art1.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_20-44-43_part1/output_Exp2019-01-29_20-44-43_p
art1.txt
 rewrite DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_/output_Last_Model.txt (100%)
 rewrite DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model__/output_Last_Model.txt (77%)
 copy DeepRL_For_HPE/FC_RNN_Evaluater/results/{Last_Model_ => Last_Model______}/output_Last_Model.txt (100%)
 copy DeepRL_For_HPE/FC_RNN_Evaluater/results/{Last_Model__ => Last_Model_______}/output_Last_Model.txt (100%)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model________/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_________/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model__________/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 36, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (31/31), done.
Writing objects: 100% (36/36), 1017.34 KiB | 0 bytes/s, done.
Total 36 (delta 17), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   4fc633b..3a0f6f0  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 22:52:22.025421: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 22:52:22.122601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 22:52:22.122894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 22:52:22.122908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 22:52:22.277312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 22:52:22.277338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 22:52:22.277343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 22:52:22.277520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_22-52-23 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 1024)              138455872
_________________________________________________________________
lstm_1 (LSTM)                (1, 500)                  3050000
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    1503
=================================================================
Total params: 141,507,375
Trainable params: 7,246,831
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 500
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm500_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-0
1-29_22-52-23
All frames and annotations from 20 datasets have been read by 2019-01-29 22:52:27.583976
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:52:33.996296!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.2741
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:52:57.371308!
Epoch 1/1
492/492 [==============================] - 13s 27ms/step - loss: 0.1808
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:53:17.209817!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.1906
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:53:38.684184!
Epoch 1/1
502/502 [==============================] - 14s 27ms/step - loss: 0.1607
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:54:00.068726!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.1905
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:54:25.374201!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.2126
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:54:45.436599!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.2041
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:55:09.611871!
Epoch 1/1
914/914 [==============================] - 25s 28ms/step - loss: 0.1373
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:55:42.521947!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.1439
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:56:08.384604!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.1315
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:56:33.893907!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1405
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:56:57.438037!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1501
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:57:16.346617!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1881
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:57:36.936827!
Epoch 1/1
511/511 [==============================] - 14s 27ms/step - loss: 0.1488
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:57:58.068648!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1525
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:58:22.371469!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.1279
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:58:40.172492!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.1223
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:58:55.224254!
Epoch 1/1
542/542 [==============================] - 15s 27ms/step - loss: 0.1882
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:59:14.738731!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.1058
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-29 22:59:32.978473!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.1041
Epoch 1 for Experiment 1 completed!
Exp2019-01-29_22-52-23_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm500_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019
-01-29_22-52-23
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 22:59:49.420795
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 10.83 Degree
        The absolute mean error on Yaw angle estimation: 55.79 Degree
        The absolute mean error on Roll angle estimation: 20.12 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.46 Degree
        The absolute mean error on Yaw angle estimation: 29.36 Degree
        The absolute mean error on Roll angle estimation: 6.40 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 16.75 Degree
        The absolute mean error on Yaw angle estimation: 32.89 Degree
        The absolute mean error on Roll angle estimation: 9.24 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.59 Degree
        The absolute mean error on Yaw angle estimation: 25.72 Degree
        The absolute mean error on Roll angle estimation: 13.27 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 13.91 Degree
        The absolute mean error on Yaw angle estimations: 35.94 Degree
        The absolute mean error on Roll angle estimations: 12.26 Degree
Exp2019-01-29_22-52-23_part1 completed!
Exp2019-01-29_22-52-23.h5 has been saved.
subject3_Exp2019-01-29_22-52-23.png has been saved by 2019-01-29 23:01:16.454279.
subject5_Exp2019-01-29_22-52-23.png has been saved by 2019-01-29 23:01:16.655039.
subject9_Exp2019-01-29_22-52-23.png has been saved by 2019-01-29 23:01:16.854127.
subject14_Exp2019-01-29_22-52-23.png has been saved by 2019-01-29 23:01:17.073502.
Model Exp2019-01-29_22-52-23 has been evaluated successfully.
Model Exp2019-01-29_22-52-23 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 23:20:34.232797: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 23:20:34.330137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 23:20:34.330434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 23:20:34.330448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 23:20:34.484732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 23:20:34.484758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 23:20:34.484763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 23:20:34.484939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_23-20-35 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   164280
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,424,857
Trainable params: 164,313
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-29_23-20-35
All frames and annotations from 20 datasets have been read by 2019-01-29 23:20:39.523731
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:20:45.940543!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.1435
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:21:05.939633!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.1327
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:21:23.349157!
Epoch 1/1
654/654 [==============================] - 14s 21ms/step - loss: 0.1799
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:21:42.073060!
Epoch 1/1
502/502 [==============================] - 10s 21ms/step - loss: 0.1799
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:22:00.269307!
Epoch 1/1
772/772 [==============================] - 17s 23ms/step - loss: 0.2083
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:22:23.264385!
Epoch 1/1
569/569 [==============================] - 13s 22ms/step - loss: 0.2260
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:22:42.205666!
Epoch 1/1
634/634 [==============================] - 14s 21ms/step - loss: 0.2099
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:23:04.615852!
Epoch 1/1
914/914 [==============================] - 20s 22ms/step - loss: 0.1642
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:23:32.119094!
Epoch 1/1
745/745 [==============================] - 16s 22ms/step - loss: 0.2273
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:23:55.818125!
Epoch 1/1
732/732 [==============================] - 16s 21ms/step - loss: 0.1559
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:24:18.695647!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.1803
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:24:39.675947!
Epoch 1/1
498/498 [==============================] - 11s 21ms/step - loss: 0.2171
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:24:56.213778!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.2007
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:25:15.377496!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1592
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:25:33.890439!
Epoch 1/1
744/744 [==============================] - 16s 21ms/step - loss: 0.1904
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:25:55.348269!
Epoch 1/1
556/556 [==============================] - 12s 21ms/step - loss: 0.1424
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:26:11.137161!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.1202
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:26:24.936315!
Epoch 1/1
542/542 [==============================] - 11s 21ms/step - loss: 0.2053
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:26:40.917541!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.1260
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:26:57.448596!
Epoch 1/1
572/572 [==============================] - 12s 22ms/step - loss: 0.1533
Epoch 1 for Experiment 1 completed!
Exp2019-01-29_23-20-35_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-29_23-20-35
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 23:27:12.206351
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 20.74 Degree
        The absolute mean error on Yaw angle estimation: 34.85 Degree
        The absolute mean error on Roll angle estimation: 6.31 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.17 Degree
        The absolute mean error on Yaw angle estimation: 26.50 Degree
        The absolute mean error on Roll angle estimation: 4.85 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 13.96 Degree
        The absolute mean error on Yaw angle estimation: 26.46 Degree
        The absolute mean error on Roll angle estimation: 9.29 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 14.89 Degree
        The absolute mean error on Yaw angle estimation: 31.58 Degree
        The absolute mean error on Roll angle estimation: 15.66 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.44 Degree
        The absolute mean error on Yaw angle estimations: 29.85 Degree
        The absolute mean error on Roll angle estimations: 9.02 Degree
Exp2019-01-29_23-20-35_part1 completed!
Exp2019-01-29_23-20-35.h5 has been saved.
subject3_Exp2019-01-29_23-20-35.png has been saved by 2019-01-29 23:28:36.111815.
subject5_Exp2019-01-29_23-20-35.png has been saved by 2019-01-29 23:28:36.311584.
subject9_Exp2019-01-29_23-20-35.png has been saved by 2019-01-29 23:28:36.515125.
subject14_Exp2019-01-29_23-20-35.png has been saved by 2019-01-29 23:28:36.735844.
Model Exp2019-01-29_23-20-35 has been evaluated successfully.
Model Exp2019-01-29_23-20-35 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 23:30:17.461729: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 23:30:17.559548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 23:30:17.559816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 23:30:17.559831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 23:30:17.713412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 23:30:17.713441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 23:30:17.713446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 23:30:17.713586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_23-30-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
dropout3_025 (Dropout)       (None, 4096)              0
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-29_23-30-18
All frames and annotations from 20 datasets have been read by 2019-01-29 23:30:22.827242
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:30:29.246482!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.2769
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:30:51.287233!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.2548
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:31:08.952692!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.2667
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:31:29.003348!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.2493
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:31:47.990681!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.3176
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:32:10.807596!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.2815
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:32:30.487482!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.2824
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:32:53.476856!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.2300
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:33:21.813772!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.3074
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:33:46.675856!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.2614
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:34:11.422857!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.2522
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:34:33.130486!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.3156
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:34:50.124699!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.3277
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:35:08.834057!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.3468
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:35:28.215422!
Epoch 1/1
744/744 [==============================] - 16s 22ms/step - loss: 0.3175
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:35:49.724455!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.2371
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:36:06.870791!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.2048
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:36:21.145803!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.2734
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:36:38.732282!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.2177
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:36:55.649075!
Epoch 1/1
572/572 [==============================] - 13s 22ms/step - loss: 0.2081
Epoch 1 for Experiment 1 completed!
Exp2019-01-29_23-30-18_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-29_23-30-18
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 23:37:10.847950
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 16.95 Degree
        The absolute mean error on Yaw angle estimation: 31.59 Degree
        The absolute mean error on Roll angle estimation: 7.79 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 22.55 Degree
        The absolute mean error on Yaw angle estimation: 30.05 Degree
        The absolute mean error on Roll angle estimation: 4.32 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.31 Degree
        The absolute mean error on Yaw angle estimation: 26.42 Degree
        The absolute mean error on Roll angle estimation: 8.64 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 22.78 Degree
        The absolute mean error on Yaw angle estimation: 34.40 Degree
        The absolute mean error on Roll angle estimation: 14.36 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 20.15 Degree
        The absolute mean error on Yaw angle estimations: 30.61 Degree
        The absolute mean error on Roll angle estimations: 8.78 Degree
Exp2019-01-29_23-30-18_part1 completed!
Exp2019-01-29_23-30-18.h5 has been saved.
subject3_Exp2019-01-29_23-30-18.png has been saved by 2019-01-29 23:38:38.407153.
subject5_Exp2019-01-29_23-30-18.png has been saved by 2019-01-29 23:38:38.603726.
subject9_Exp2019-01-29_23-30-18.png has been saved by 2019-01-29 23:38:38.798119.
subject14_Exp2019-01-29_23-30-18.png has been saved by 2019-01-29 23:38:39.012915.
Model Exp2019-01-29_23-30-18 has been evaluated successfully.
Model Exp2019-01-29_23-30-18 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-29 23:41:00.314472: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-29 23:41:00.411144: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-29 23:41:00.411451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-29 23:41:00.411466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-29 23:41:00.565972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-29 23:41:00.565999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-29 23:41:00.566004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-29 23:41:00.566184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-29_23-41-01 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-29_23-41-01
All frames and annotations from 20 datasets have been read by 2019-01-29 23:41:05.798830
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:41:12.222016!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.2549
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:41:34.099226!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.1776
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:41:51.308982!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.2080
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:42:11.214557!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.1834
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:42:30.796210!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.2443
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:42:54.156033!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.2464
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:43:13.396053!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.2331
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:43:36.263402!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.1827
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:44:05.671077!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.2286
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:44:30.251227!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.1853
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:44:54.253308!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.1808
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:45:16.329240!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.2202
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:45:33.767192!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.2191
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:45:53.230099!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.2119
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:46:12.361790!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.2129
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:46:34.961973!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.1713
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:46:51.038570!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.1413
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:47:05.227745!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.2204
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:47:22.595077!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.1365
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-29 23:47:39.617829!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.1497
Epoch 1 for Experiment 1 completed!
Exp2019-01-29_23-41-01_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-29_23-41-01
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-29 23:47:55.089350
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.65 Degree
        The absolute mean error on Yaw angle estimation: 52.30 Degree
        The absolute mean error on Roll angle estimation: 10.94 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 9.45 Degree
        The absolute mean error on Yaw angle estimation: 30.46 Degree
        The absolute mean error on Roll angle estimation: 4.77 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 16.68 Degree
        The absolute mean error on Yaw angle estimation: 38.99 Degree
        The absolute mean error on Roll angle estimation: 7.27 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.62 Degree
        The absolute mean error on Yaw angle estimation: 31.47 Degree
        The absolute mean error on Roll angle estimation: 15.99 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.35 Degree
        The absolute mean error on Yaw angle estimations: 38.31 Degree
        The absolute mean error on Roll angle estimations: 9.74 Degree
Exp2019-01-29_23-41-01_part1 completed!
Exp2019-01-29_23-41-01.h5 has been saved.
subject3_Exp2019-01-29_23-41-01.png has been saved by 2019-01-29 23:49:23.116972.
subject5_Exp2019-01-29_23-41-01.png has been saved by 2019-01-29 23:49:23.317167.
subject9_Exp2019-01-29_23-41-01.png has been saved by 2019-01-29 23:49:23.514953.
subject14_Exp2019-01-29_23-41-01.png has been saved by 2019-01-29 23:49:23.733570.
Model Exp2019-01-29_23-41-01 has been evaluated successfully.
Model Exp2019-01-29_23-41-01 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-30 00:01:36.235562: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-30 00:01:36.331637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-30 00:01:36.331892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-30 00:01:36.331904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-30 00:01:36.486803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-30 00:01:36.486831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-30 00:01:36.486835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-30 00:01:36.486973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-30_00-01-37 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 6
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs6_outEpochs1_AdamOpt_lr-0.000100_2019-01
-30_00-01-37
All frames and annotations from 20 datasets have been read by 2019-01-30 00:01:41.579946
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:01:48.001294!
Epoch 1/6
665/665 [==============================] - 17s 25ms/step - loss: 0.2790
Epoch 2/6
665/665 [==============================] - 15s 23ms/step - loss: 0.2307
Epoch 3/6
665/665 [==============================] - 16s 24ms/step - loss: 0.2130
Epoch 4/6
665/665 [==============================] - 15s 22ms/step - loss: 0.1873
Epoch 5/6
665/665 [==============================] - 16s 23ms/step - loss: 0.1808
Epoch 6/6
665/665 [==============================] - 16s 23ms/step - loss: 0.1704
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:03:27.190613!
Epoch 1/6
492/492 [==============================] - 11s 23ms/step - loss: 0.2024
Epoch 2/6
492/492 [==============================] - 12s 23ms/step - loss: 0.1884
Epoch 3/6
492/492 [==============================] - 11s 22ms/step - loss: 0.1786
Epoch 4/6
492/492 [==============================] - 11s 23ms/step - loss: 0.1680
Epoch 5/6
492/492 [==============================] - 11s 23ms/step - loss: 0.1682
Epoch 6/6
492/492 [==============================] - 12s 24ms/step - loss: 0.1658
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:04:41.687273!
Epoch 1/6
654/654 [==============================] - 15s 23ms/step - loss: 0.2217
Epoch 2/6
654/654 [==============================] - 16s 24ms/step - loss: 0.2192
Epoch 3/6
654/654 [==============================] - 15s 23ms/step - loss: 0.2117
Epoch 4/6
654/654 [==============================] - 15s 23ms/step - loss: 0.2077
Epoch 5/6
654/654 [==============================] - 15s 23ms/step - loss: 0.2078
Epoch 6/6
654/654 [==============================] - 15s 23ms/step - loss: 0.2009
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:06:17.917809!
Epoch 1/6
502/502 [==============================] - 11s 22ms/step - loss: 0.2001
Epoch 2/6
502/502 [==============================] - 12s 23ms/step - loss: 0.1798
Epoch 3/6
502/502 [==============================] - 12s 24ms/step - loss: 0.1767
Epoch 4/6
502/502 [==============================] - 12s 23ms/step - loss: 0.1716
Epoch 5/6
502/502 [==============================] - 11s 23ms/step - loss: 0.1681
Epoch 6/6
502/502 [==============================] - 12s 24ms/step - loss: 0.1690
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:07:35.829933!
Epoch 1/6
772/772 [==============================] - 17s 23ms/step - loss: 0.2777
Epoch 2/6
772/772 [==============================] - 18s 23ms/step - loss: 0.2510
Epoch 3/6
772/772 [==============================] - 19s 24ms/step - loss: 0.2427
Epoch 4/6
772/772 [==============================] - 18s 23ms/step - loss: 0.2172
Epoch 5/6
772/772 [==============================] - 19s 24ms/step - loss: 0.2075
Epoch 6/6
772/772 [==============================] - 19s 24ms/step - loss: 0.2002
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:09:30.715928!
Epoch 1/6
569/569 [==============================] - 13s 22ms/step - loss: 0.2672
Epoch 2/6
569/569 [==============================] - 13s 22ms/step - loss: 0.2335
Epoch 3/6
569/569 [==============================] - 13s 24ms/step - loss: 0.2207
Epoch 4/6
569/569 [==============================] - 13s 23ms/step - loss: 0.2163
Epoch 5/6
569/569 [==============================] - 14s 24ms/step - loss: 0.2125
Epoch 6/6
569/569 [==============================] - 13s 23ms/step - loss: 0.2099
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:10:55.473051!
Epoch 1/6
634/634 [==============================] - 15s 23ms/step - loss: 0.2374
Epoch 2/6
634/634 [==============================] - 15s 23ms/step - loss: 0.2195
Epoch 3/6
634/634 [==============================] - 15s 24ms/step - loss: 0.2136
Epoch 4/6
634/634 [==============================] - 15s 24ms/step - loss: 0.2081
Epoch 5/6
634/634 [==============================] - 15s 24ms/step - loss: 0.2044
Epoch 6/6
634/634 [==============================] - 14s 22ms/step - loss: 0.2004
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:12:33.047793!
Epoch 1/6
914/914 [==============================] - 22s 24ms/step - loss: 0.1789
Epoch 2/6
914/914 [==============================] - 21s 23ms/step - loss: 0.1624
Epoch 3/6
914/914 [==============================] - 21s 23ms/step - loss: 0.1520
Epoch 4/6
914/914 [==============================] - 22s 24ms/step - loss: 0.1467
Epoch 5/6
914/914 [==============================] - 21s 23ms/step - loss: 0.1430
Epoch 6/6
914/914 [==============================] - 21s 23ms/step - loss: 0.1344
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:14:48.429877!
Epoch 1/6
745/745 [==============================] - 17s 23ms/step - loss: 0.1999
Epoch 2/6
745/745 [==============================] - 18s 24ms/step - loss: 0.1666
Epoch 3/6
745/745 [==============================] - 18s 24ms/step - loss: 0.1541
Epoch 4/6
745/745 [==============================] - 17s 23ms/step - loss: 0.1489
Epoch 5/6
745/745 [==============================] - 18s 24ms/step - loss: 0.1384
Epoch 6/6
745/745 [==============================] - 18s 24ms/step - loss: 0.1385
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:16:41.116660!
Epoch 1/6
732/732 [==============================] - 17s 23ms/step - loss: 0.1407
Epoch 2/6
732/732 [==============================] - 17s 23ms/step - loss: 0.1214
Epoch 3/6
732/732 [==============================] - 17s 23ms/step - loss: 0.1143
Epoch 4/6
732/732 [==============================] - 18s 24ms/step - loss: 0.1105
Epoch 5/6
732/732 [==============================] - 17s 24ms/step - loss: 0.1090
Epoch 6/6
732/732 [==============================] - 17s 23ms/step - loss: 0.1055
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:18:31.512199!
Epoch 1/6
726/726 [==============================] - 17s 24ms/step - loss: 0.1543
Epoch 2/6
726/726 [==============================] - 17s 23ms/step - loss: 0.1205
Epoch 3/6
726/726 [==============================] - 17s 24ms/step - loss: 0.1160
Epoch 4/6
726/726 [==============================] - 16s 23ms/step - loss: 0.1082
Epoch 5/6
726/726 [==============================] - 17s 24ms/step - loss: 0.1074
Epoch 6/6
726/726 [==============================] - 16s 23ms/step - loss: 0.1032
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:20:18.247033!
Epoch 1/6
498/498 [==============================] - 11s 22ms/step - loss: 0.1595
Epoch 2/6
498/498 [==============================] - 12s 24ms/step - loss: 0.1421
Epoch 3/6
498/498 [==============================] - 12s 24ms/step - loss: 0.1342
Epoch 4/6
498/498 [==============================] - 12s 24ms/step - loss: 0.1275
Epoch 5/6
498/498 [==============================] - 11s 23ms/step - loss: 0.1280
Epoch 6/6
498/498 [==============================] - 12s 24ms/step - loss: 0.1255
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:21:34.804396!
Epoch 1/6
614/614 [==============================] - 14s 23ms/step - loss: 0.2127
Epoch 2/6
614/614 [==============================] - 15s 24ms/step - loss: 0.1751
Epoch 3/6
614/614 [==============================] - 14s 23ms/step - loss: 0.1595
Epoch 4/6
614/614 [==============================] - 14s 23ms/step - loss: 0.1515
Epoch 5/6
614/614 [==============================] - 15s 24ms/step - loss: 0.1411
Epoch 6/6
614/614 [==============================] - 15s 24ms/step - loss: 0.1406
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:23:06.989104!
Epoch 1/6
511/511 [==============================] - 12s 24ms/step - loss: 0.1810
Epoch 2/6
511/511 [==============================] - 12s 24ms/step - loss: 0.1341
Epoch 3/6
511/511 [==============================] - 12s 23ms/step - loss: 0.1210
Epoch 4/6
511/511 [==============================] - 12s 24ms/step - loss: 0.1175
Epoch 5/6
511/511 [==============================] - 12s 24ms/step - loss: 0.1106
Epoch 6/6
511/511 [==============================] - 12s 23ms/step - loss: 0.1127
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:24:27.171374!
Epoch 1/6
744/744 [==============================] - 17s 23ms/step - loss: 0.1710
Epoch 2/6
744/744 [==============================] - 17s 23ms/step - loss: 0.1337
Epoch 3/6
744/744 [==============================] - 17s 23ms/step - loss: 0.1252
Epoch 4/6
744/744 [==============================] - 17s 23ms/step - loss: 0.1204
Epoch 5/6
744/744 [==============================] - 17s 23ms/step - loss: 0.1178
Epoch 6/6
744/744 [==============================] - 18s 24ms/step - loss: 0.1173
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:26:15.736580!
Epoch 1/6
556/556 [==============================] - 13s 24ms/step - loss: 0.1665
Epoch 2/6
556/556 [==============================] - 13s 23ms/step - loss: 0.1215
Epoch 3/6
556/556 [==============================] - 13s 24ms/step - loss: 0.1127
Epoch 4/6
556/556 [==============================] - 14s 25ms/step - loss: 0.1045
Epoch 5/6
556/556 [==============================] - 12s 22ms/step - loss: 0.1003
Epoch 6/6
556/556 [==============================] - 14s 24ms/step - loss: 0.0968
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:27:38.098251!
Epoch 1/6
395/395 [==============================] - 9s 23ms/step - loss: 0.1270
Epoch 2/6
395/395 [==============================] - 9s 23ms/step - loss: 0.1174
Epoch 3/6
395/395 [==============================] - 9s 22ms/step - loss: 0.1102
Epoch 4/6
395/395 [==============================] - 9s 23ms/step - loss: 0.1027
Epoch 5/6
395/395 [==============================] - 9s 23ms/step - loss: 0.1041
Epoch 6/6
395/395 [==============================] - 9s 24ms/step - loss: 0.0993
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:28:37.843681!
Epoch 1/6
542/542 [==============================] - 12s 23ms/step - loss: 0.1948
Epoch 2/6
542/542 [==============================] - 13s 23ms/step - loss: 0.1746
Epoch 3/6
542/542 [==============================] - 12s 23ms/step - loss: 0.1591
Epoch 4/6
542/542 [==============================] - 13s 24ms/step - loss: 0.1503
Epoch 5/6
542/542 [==============================] - 12s 23ms/step - loss: 0.1422
Epoch 6/6
542/542 [==============================] - 13s 23ms/step - loss: 0.1400
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:29:58.175194!
Epoch 1/6
485/485 [==============================] - 11s 23ms/step - loss: 0.1326
Epoch 2/6
485/485 [==============================] - 11s 22ms/step - loss: 0.1091
Epoch 3/6
485/485 [==============================] - 11s 22ms/step - loss: 0.1071
Epoch 4/6
485/485 [==============================] - 11s 23ms/step - loss: 0.1040
Epoch 5/6
485/485 [==============================] - 11s 22ms/step - loss: 0.1065
Epoch 6/6
485/485 [==============================] - 11s 23ms/step - loss: 0.0983
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-30 00:31:10.098573!
Epoch 1/6
572/572 [==============================] - 13s 22ms/step - loss: 0.1222
Epoch 2/6
572/572 [==============================] - 14s 24ms/step - loss: 0.0929
Epoch 3/6
572/572 [==============================] - 13s 23ms/step - loss: 0.0864
Epoch 4/6
572/572 [==============================] - 13s 23ms/step - loss: 0.0898
Epoch 5/6
572/572 [==============================] - 14s 24ms/step - loss: 0.0822
Epoch 6/6
572/572 [==============================] - 13s 23ms/step - loss: 0.0814
Epoch 1 for Experiment 1 completed!
Exp2019-01-30_00-01-37_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs6_outEpochs1_AdamOpt_lr-0.000100_2019-
01-30_00-01-37
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-30 00:32:32.164000
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 18.78 Degree
        The absolute mean error on Yaw angle estimation: 53.31 Degree
        The absolute mean error on Roll angle estimation: 18.76 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.80 Degree
        The absolute mean error on Yaw angle estimation: 28.46 Degree
        The absolute mean error on Roll angle estimation: 5.76 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.06 Degree
        The absolute mean error on Yaw angle estimation: 25.43 Degree
        The absolute mean error on Roll angle estimation: 9.29 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 11.71 Degree
        The absolute mean error on Yaw angle estimation: 31.14 Degree
        The absolute mean error on Roll angle estimation: 12.68 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 13.59 Degree
        The absolute mean error on Yaw angle estimations: 34.58 Degree
        The absolute mean error on Roll angle estimations: 11.62 Degree
Exp2019-01-30_00-01-37_part1 completed!
Exp2019-01-30_00-01-37.h5 has been saved.
subject3_Exp2019-01-30_00-01-37.png has been saved by 2019-01-30 00:34:00.602343.
subject5_Exp2019-01-30_00-01-37.png has been saved by 2019-01-30 00:34:00.802423.
subject9_Exp2019-01-30_00-01-37.png has been saved by 2019-01-30 00:34:01.003260.
subject14_Exp2019-01-30_00-01-37.png has been saved by 2019-01-30 00:34:01.224149.
Model Exp2019-01-30_00-01-37 has been evaluated successfully.
Model Exp2019-01-30_00-01-37 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_13-03-34_part6 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-30 03:22:54.673536: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-30 03:22:54.770647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-30 03:22:54.770914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-30 03:22:54.770926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-30 03:22:54.925874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-30 03:22:54.925902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-30 03:22:54.925907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-30 03:22:54.926044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10453 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_13-03-34_part6.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 6
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-22-57
All frames and annotations from 20 datasets have been read by 2019-01-30 03:23:01.773193
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:23:08.179235!
Epoch 1/6
665/665 [==============================] - 17s 25ms/step - loss: 0.0607
Epoch 2/6
665/665 [==============================] - 15s 23ms/step - loss: 0.0588
Epoch 3/6
665/665 [==============================] - 16s 24ms/step - loss: 0.0595
Epoch 4/6
665/665 [==============================] - 16s 24ms/step - loss: 0.0592
Epoch 5/6
 11/665 [..............................] - ETA: 20s - loss: 0.0454^C
Model Exp2019-01-30_03-22-57_part1 has been interrupted.
Exp2019-01-30_03-22-57_part1.h5 has been saved.
Model Exp2019-01-30_03-22-57_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-29_13-03-34_part6 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-30 03:25:13.812625: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-30 03:25:13.911331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-30 03:25:13.911596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-30 03:25:13.911613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-30 03:25:14.065559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-30 03:25:14.065585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-30 03:25:14.065590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-30 03:25:14.065731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-29_13-03-34_part6.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 3)                 138458947
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 40
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
All frames and annotations from 20 datasets have been read by 2019-01-30 03:25:20.881576
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:25:27.297522!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0644
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:25:48.787567!
Epoch 1/1
492/492 [==============================] - 12s 23ms/step - loss: 0.0648
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:26:06.793396!
Epoch 1/1
654/654 [==============================] - 14s 22ms/step - loss: 0.0862
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:26:26.159512!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0772
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:26:45.309012!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0917
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:27:08.937487!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1194
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:27:28.345712!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1293
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:27:51.935396!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0814
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:28:21.228887!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0984
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:28:45.542300!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0830
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:29:09.884576!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0849
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:29:32.335925!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1142
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:29:49.455737!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1103
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:30:09.277116!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1027
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:30:28.458339!
Epoch 1/1
744/744 [==============================] - 17s 22ms/step - loss: 0.1017
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:30:50.529105!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0795
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:31:06.766017!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0824
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:31:21.321466!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1102
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:31:39.067080!
Epoch 1/1
485/485 [==============================] - 11s 24ms/step - loss: 0.0701
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-30 03:31:56.308992!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0678
Epoch 1 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 03:32:13.653027
1. set (Dataset 6) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:32:18.862733!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1067
2. set (Dataset 11) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:32:37.206917!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0600
3. set (Dataset 10) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:32:58.160554!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0862
4. set (Dataset 4) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:33:22.492058!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1028
5. set (Dataset 23) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:33:45.830595!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1230
6. set (Dataset 13) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:34:04.318890!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0656
7. set (Dataset 17) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:34:18.497428!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0816
8. set (Dataset 1) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:34:32.624693!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1088
9. set (Dataset 8) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:34:51.923910!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0923
10. set (Dataset 7) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:35:17.129188!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0921
11. set (Dataset 21) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:35:40.563917!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1421
12. set (Dataset 22) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:36:02.155254!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0603
13. set (Dataset 2) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:36:22.515201!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1043
14. set (Dataset 24) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:36:39.487633!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0692
15. set (Dataset 15) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:36:57.389629!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0921
16. set (Dataset 20) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:37:18.194018!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0717
17. set (Dataset 18) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:37:37.374240!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1020
18. set (Dataset 19) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:37:56.795962!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0785
19. set (Dataset 12) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:38:15.720841!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0852
20. set (Dataset 16) being trained for epoch 2 in Experiment 1 by 2019-01-30 03:38:41.711456!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0829
Epoch 2 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 03:39:07.560826
1. set (Dataset 19) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:39:12.459247!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0755
2. set (Dataset 16) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:39:32.798383!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0828
3. set (Dataset 21) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:40:00.070707!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1349
4. set (Dataset 15) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:40:21.019919!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0880
5. set (Dataset 13) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:40:41.552611!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0713
6. set (Dataset 12) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:40:59.985384!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0854
7. set (Dataset 18) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:41:23.463793!
Epoch 1/1
614/614 [==============================] - 14s 24ms/step - loss: 0.1042
8. set (Dataset 22) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:41:44.414024!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0624
9. set (Dataset 23) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:42:05.791673!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1143
10. set (Dataset 8) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:42:27.000280!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0953
11. set (Dataset 17) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:42:48.476382!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0810
12. set (Dataset 6) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:43:03.214331!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1085
13. set (Dataset 24) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:43:20.154434!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0655
14. set (Dataset 11) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:43:37.249687!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0651
15. set (Dataset 10) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:43:57.838604!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0925
16. set (Dataset 20) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:44:20.493832!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0763
17. set (Dataset 2) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:44:38.503525!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1096
18. set (Dataset 4) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:44:57.691083!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1026
19. set (Dataset 7) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:45:22.350684!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0969
20. set (Dataset 1) being trained for epoch 3 in Experiment 1 by 2019-01-30 03:45:45.311546!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1185
Epoch 3 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 03:46:01.608205
1. set (Dataset 4) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:46:09.003700!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1006
2. set (Dataset 1) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:46:31.407051!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1096
3. set (Dataset 17) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:46:47.162145!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0889
4. set (Dataset 10) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:47:03.587839!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0865
5. set (Dataset 12) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:47:28.328267!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0797
6. set (Dataset 7) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:47:52.746640!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0948
7. set (Dataset 2) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:48:15.038265!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1064
8. set (Dataset 6) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:48:32.013661!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1169
9. set (Dataset 13) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:48:49.524442!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0686
10. set (Dataset 23) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:49:06.615757!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1198
11. set (Dataset 18) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:49:26.175626!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1067
12. set (Dataset 19) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:49:44.924691!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0778
13. set (Dataset 11) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:50:02.161849!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0677
14. set (Dataset 16) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:50:24.394331!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0824
15. set (Dataset 21) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:50:51.390566!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1318
16. set (Dataset 20) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:51:12.019490!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0705
17. set (Dataset 24) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:51:29.371334!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0625
18. set (Dataset 15) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:51:46.694226!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0910
19. set (Dataset 8) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:52:10.099246!
Epoch 1/1
772/772 [==============================] - 18s 24ms/step - loss: 0.0938
20. set (Dataset 22) being trained for epoch 4 in Experiment 1 by 2019-01-30 03:52:35.005436!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0601
Epoch 4 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 03:52:55.392343
1. set (Dataset 15) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:53:01.764157!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0870
2. set (Dataset 22) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:53:23.855906!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0619
3. set (Dataset 18) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:53:45.050572!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1049
4. set (Dataset 21) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:54:05.044703!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1306
5. set (Dataset 7) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:54:27.875358!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.1002
6. set (Dataset 8) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:54:53.281890!
Epoch 1/1
772/772 [==============================] - 17s 23ms/step - loss: 0.0924
7. set (Dataset 24) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:55:15.564121!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0642
8. set (Dataset 19) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:55:32.265640!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0764
9. set (Dataset 12) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:55:51.171062!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0838
10. set (Dataset 13) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:56:13.030160!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0667
11. set (Dataset 2) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:56:29.482071!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1059
12. set (Dataset 4) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:56:48.773090!
Epoch 1/1
744/744 [==============================] - 16s 22ms/step - loss: 0.1013
13. set (Dataset 16) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:57:13.866326!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0874
14. set (Dataset 1) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:57:39.863257!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1143
15. set (Dataset 17) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:57:55.284613!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0835
16. set (Dataset 20) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:58:09.380374!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0765
17. set (Dataset 11) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:58:27.826876!
Epoch 1/1
572/572 [==============================] - 13s 22ms/step - loss: 0.0599
18. set (Dataset 10) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:58:47.994621!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0872
19. set (Dataset 23) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:59:10.871313!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1192
20. set (Dataset 6) being trained for epoch 5 in Experiment 1 by 2019-01-30 03:59:29.763078!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1103
Epoch 5 for Experiment 1 completed!
Exp2019-01-30_03-25-16_part1.h5 has been saved.
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24, 'M14'), (19,
 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'), (20, 'M12'), (11, 'M05'
), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-30 03:59:44.718282
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.45 Degree
        The absolute mean error on Yaw angle estimation: 22.18 Degree
        The absolute mean error on Roll angle estimation: 10.97 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 7.96 Degree
        The absolute mean error on Yaw angle estimation: 20.69 Degree
        The absolute mean error on Roll angle estimation: 3.78 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 31.12 Degree
        The absolute mean error on Yaw angle estimation: 19.23 Degree
        The absolute mean error on Roll angle estimation: 8.60 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 13.79 Degree
        The absolute mean error on Yaw angle estimation: 30.30 Degree
        The absolute mean error on Roll angle estimation: 13.07 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.33 Degree
        The absolute mean error on Yaw angle estimations: 23.10 Degree
        The absolute mean error on Roll angle estimations: 9.11 Degree
Exp2019-01-30_03-25-16_part1 completed!
Training model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
All frames and annotations from 20 datasets have been read by 2019-01-30 04:01:17.197202
1. set (Dataset 10) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:01:24.435575!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0844
2. set (Dataset 6) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:01:45.946458!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1076
3. set (Dataset 2) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:02:04.074698!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1041
4. set (Dataset 17) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:02:19.710137!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0805
5. set (Dataset 8) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:02:36.839086!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0889
6. set (Dataset 23) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:03:00.541238!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1180
7. set (Dataset 11) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:03:19.304646!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0656
8. set (Dataset 4) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:03:39.869222!
Epoch 1/1
744/744 [==============================] - 17s 22ms/step - loss: 0.1039
9. set (Dataset 7) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:04:04.117141!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0921
10. set (Dataset 12) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:04:28.652197!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0771
11. set (Dataset 24) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:04:50.505761!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0677
12. set (Dataset 15) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:05:08.455980!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0886
13. set (Dataset 1) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:05:28.384168!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1151
14. set (Dataset 22) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:05:45.833958!
Epoch 1/1
665/665 [==============================] - 16s 23ms/step - loss: 0.0646
15. set (Dataset 18) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:06:07.375053!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1082
16. set (Dataset 20) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:06:27.004793!
Epoch 1/1
556/556 [==============================] - 12s 21ms/step - loss: 0.0772
17. set (Dataset 16) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:06:47.750803!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0839
18. set (Dataset 21) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:07:14.900831!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1327
19. set (Dataset 13) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:07:34.693333!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0743
20. set (Dataset 19) being trained for epoch 1 in Experiment 2 by 2019-01-30 04:07:50.731480!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0805
Epoch 1 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 04:08:06.608757
1. set (Dataset 21) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:08:12.640844!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1329
2. set (Dataset 19) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:08:32.611434!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0776
3. set (Dataset 24) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:08:49.030177!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0628
4. set (Dataset 18) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:09:05.809044!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1009
5. set (Dataset 23) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:09:25.636213!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.1130
6. set (Dataset 13) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:09:42.537356!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0740
7. set (Dataset 16) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:10:02.103952!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0819
8. set (Dataset 15) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:10:29.992691!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0849
9. set (Dataset 8) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:10:52.916826!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0971
10. set (Dataset 7) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:11:18.201597!
Epoch 1/1
745/745 [==============================] - 17s 22ms/step - loss: 0.0935
11. set (Dataset 11) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:11:40.678846!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0653
12. set (Dataset 10) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:12:01.336222!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0859
13. set (Dataset 22) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:12:24.504713!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0660
14. set (Dataset 6) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:12:44.940636!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1156
15. set (Dataset 2) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:13:03.087484!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1049
16. set (Dataset 20) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:13:20.380895!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0752
17. set (Dataset 1) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:13:38.731670!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1246
18. set (Dataset 17) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:13:54.199697!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0814
19. set (Dataset 12) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:14:10.879012!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0791
20. set (Dataset 4) being trained for epoch 2 in Experiment 2 by 2019-01-30 04:14:35.074024!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1029
Epoch 2 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 04:14:56.710466
1. set (Dataset 17) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:15:00.469873!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0812
2. set (Dataset 4) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:15:17.384085!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.0995
3. set (Dataset 11) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:15:40.350177!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0692
4. set (Dataset 2) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:15:58.702998!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1083
5. set (Dataset 13) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:16:15.197692!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0687
6. set (Dataset 12) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:16:33.649233!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0786
7. set (Dataset 1) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:16:56.237684!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1108
8. set (Dataset 10) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:17:15.054425!
Epoch 1/1
726/726 [==============================] - 16s 23ms/step - loss: 0.0857
9. set (Dataset 23) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:17:37.035093!
Epoch 1/1
569/569 [==============================] - 12s 22ms/step - loss: 0.1240
10. set (Dataset 8) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:17:57.289431!
Epoch 1/1
772/772 [==============================] - 18s 24ms/step - loss: 0.0870
11. set (Dataset 16) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:18:24.608339!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0898
12. set (Dataset 21) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:18:52.217932!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1331
13. set (Dataset 6) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:19:12.593255!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1125
14. set (Dataset 19) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:19:30.526116!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0785
15. set (Dataset 24) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:19:46.660840!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0635
16. set (Dataset 20) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:20:03.598975!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0758
17. set (Dataset 22) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:20:22.333761!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0602
18. set (Dataset 18) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:20:44.013742!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1045
19. set (Dataset 7) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:21:06.351285!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.1003
20. set (Dataset 15) being trained for epoch 3 in Experiment 2 by 2019-01-30 04:21:30.598808!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0848
Epoch 3 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 04:21:50.668284
1. set (Dataset 18) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:21:56.585088!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1052
2. set (Dataset 15) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:22:17.718174!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0820
3. set (Dataset 16) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:22:41.669482!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0828
4. set (Dataset 24) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:23:08.338013!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0642
5. set (Dataset 12) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:23:27.468056!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0875
6. set (Dataset 7) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:23:51.193734!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0975
7. set (Dataset 22) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:24:14.727615!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0623
8. set (Dataset 21) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:24:36.763687!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1340
9. set (Dataset 13) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:24:55.373063!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0720
10. set (Dataset 23) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:25:12.075306!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1144
11. set (Dataset 1) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:25:30.586125!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1236
12. set (Dataset 17) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:25:46.315212!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0780
13. set (Dataset 19) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:26:00.629216!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0780
14. set (Dataset 4) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:26:19.819040!
Epoch 1/1
744/744 [==============================] - 17s 22ms/step - loss: 0.1107
15. set (Dataset 11) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:26:42.177897!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0656
16. set (Dataset 20) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:27:00.807149!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0767
17. set (Dataset 6) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:27:19.354296!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1124
18. set (Dataset 2) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:27:37.133892!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1154
19. set (Dataset 8) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:27:57.029664!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0992
20. set (Dataset 10) being trained for epoch 4 in Experiment 2 by 2019-01-30 04:28:22.131505!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0886
Epoch 4 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 04:28:42.373261
1. set (Dataset 2) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:28:47.472845!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1052
2. set (Dataset 10) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:29:07.000971!
Epoch 1/1
726/726 [==============================] - 16s 23ms/step - loss: 0.0845
3. set (Dataset 1) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:29:28.639582!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1071
4. set (Dataset 11) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:29:45.708393!
Epoch 1/1
572/572 [==============================] - 13s 22ms/step - loss: 0.0633
5. set (Dataset 7) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:30:06.080101!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0976
6. set (Dataset 8) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:30:31.619775!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0907
7. set (Dataset 6) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:30:54.443642!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1172
8. set (Dataset 17) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:31:10.587009!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0852
9. set (Dataset 12) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:31:26.815254!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0830
10. set (Dataset 13) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:31:48.681032!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0687
11. set (Dataset 22) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:32:06.201332!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0658
12. set (Dataset 18) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:32:27.094999!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1060
13. set (Dataset 4) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:32:49.250721!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1046
14. set (Dataset 15) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:33:12.978524!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0851
15. set (Dataset 16) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:33:37.492956!
Epoch 1/1
914/914 [==============================] - 21s 22ms/step - loss: 0.0814
16. set (Dataset 20) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:34:03.502361!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0712
17. set (Dataset 19) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:34:20.924894!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0791
18. set (Dataset 24) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:34:37.159593!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0626
19. set (Dataset 23) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:34:53.709500!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1155
20. set (Dataset 21) being trained for epoch 5 in Experiment 2 by 2019-01-30 04:35:12.871388!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1305
Epoch 5 for Experiment 2 completed!
Exp2019-01-30_03-25-16_part2.h5 has been saved.
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F06'), (17, 'M
10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'), (20, 'M12'), (19, 'M11')
, (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-30 04:35:30.012932
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.74 Degree
        The absolute mean error on Yaw angle estimation: 23.04 Degree
        The absolute mean error on Roll angle estimation: 17.77 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 6.33 Degree
        The absolute mean error on Yaw angle estimation: 20.66 Degree
        The absolute mean error on Roll angle estimation: 5.76 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 33.27 Degree
        The absolute mean error on Yaw angle estimation: 17.09 Degree
        The absolute mean error on Roll angle estimation: 8.60 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 13.83 Degree
        The absolute mean error on Yaw angle estimation: 30.34 Degree
        The absolute mean error on Roll angle estimation: 13.61 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.54 Degree
        The absolute mean error on Yaw angle estimations: 22.78 Degree
        The absolute mean error on Roll angle estimations: 11.43 Degree
Exp2019-01-30_03-25-16_part2 completed!
Training model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
All frames and annotations from 20 datasets have been read by 2019-01-30 04:37:01.393460
1. set (Dataset 24) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:37:06.082286!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0615
2. set (Dataset 21) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:37:23.932305!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1280
3. set (Dataset 22) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:37:45.169100!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0616
4. set (Dataset 16) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:38:09.385156!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0798
5. set (Dataset 8) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:38:39.262381!
Epoch 1/1
772/772 [==============================] - 18s 24ms/step - loss: 0.0954
6. set (Dataset 23) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:39:03.296243!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1159
7. set (Dataset 19) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:39:21.882219!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0717
8. set (Dataset 18) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:39:39.298452!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1020
9. set (Dataset 7) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:40:01.066576!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0978
10. set (Dataset 12) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:40:25.961325!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0854
11. set (Dataset 6) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:40:48.325903!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1086
12. set (Dataset 2) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:41:06.469652!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1099
13. set (Dataset 15) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:41:24.788861!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0864
14. set (Dataset 10) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:41:47.249429!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0914
15. set (Dataset 1) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:42:09.653476!
Epoch 1/1
498/498 [==============================] - 11s 21ms/step - loss: 0.1151
16. set (Dataset 20) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:42:25.845963!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0771
17. set (Dataset 4) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:42:46.422825!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1043
18. set (Dataset 11) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:43:09.146576!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0629
19. set (Dataset 13) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:43:27.375751!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0709
20. set (Dataset 17) being trained for epoch 1 in Experiment 3 by 2019-01-30 04:43:42.881622!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0810
Epoch 1 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 04:43:56.742798
1. set (Dataset 11) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:44:02.498984!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0640
2. set (Dataset 17) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:44:19.874843!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0803
3. set (Dataset 6) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:44:34.306789!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1089
4. set (Dataset 1) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:44:51.315784!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1151
5. set (Dataset 23) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:45:08.224694!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1173
6. set (Dataset 13) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:45:26.834777!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0696
7. set (Dataset 4) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:45:45.088531!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1023
8. set (Dataset 2) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:46:07.187139!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1070
9. set (Dataset 8) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:46:27.141878!
Epoch 1/1
772/772 [==============================] - 18s 24ms/step - loss: 0.0952
10. set (Dataset 7) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:46:53.085565!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0969
11. set (Dataset 19) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:47:15.505688!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0836
12. set (Dataset 24) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:47:31.690434!
Epoch 1/1
492/492 [==============================] - 12s 23ms/step - loss: 0.0652
13. set (Dataset 10) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:47:50.594508!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0902
14. set (Dataset 21) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:48:13.736139!
Epoch 1/1
634/634 [==============================] - 14s 21ms/step - loss: 0.1339
15. set (Dataset 22) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:48:33.791408!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0641
16. set (Dataset 20) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:48:54.652371!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0722
17. set (Dataset 15) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:49:13.967042!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0825
18. set (Dataset 16) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:49:37.877048!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0838
19. set (Dataset 12) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:50:06.832124!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.0835
20. set (Dataset 18) being trained for epoch 2 in Experiment 3 by 2019-01-30 04:50:30.331250!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1063
Epoch 2 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 04:50:48.674767
1. set (Dataset 16) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:50:57.444715!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0816
2. set (Dataset 18) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:51:24.698087!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1028
3. set (Dataset 19) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:51:43.774364!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0769
4. set (Dataset 22) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:52:01.806554!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0619
5. set (Dataset 13) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:52:22.632402!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0726
6. set (Dataset 12) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:52:40.659200!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0878
7. set (Dataset 15) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:53:03.973902!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0860
8. set (Dataset 24) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:53:24.407054!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0655
9. set (Dataset 23) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:53:41.738463!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1160
10. set (Dataset 8) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:54:02.926655!
Epoch 1/1
772/772 [==============================] - 18s 24ms/step - loss: 0.0938
11. set (Dataset 4) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:54:28.790508!
Epoch 1/1
744/744 [==============================] - 17s 22ms/step - loss: 0.1035
12. set (Dataset 11) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:54:51.235187!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0663
13. set (Dataset 21) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:55:10.275937!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1351
14. set (Dataset 17) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:55:29.268919!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0825
15. set (Dataset 6) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:55:43.987851!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1123
16. set (Dataset 20) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:56:02.318126!
Epoch 1/1
556/556 [==============================] - 12s 21ms/step - loss: 0.0718
17. set (Dataset 10) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:56:21.573974!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0922
18. set (Dataset 1) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:56:43.577789!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1139
19. set (Dataset 7) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:57:02.844014!
Epoch 1/1
745/745 [==============================] - 16s 22ms/step - loss: 0.0945
20. set (Dataset 2) being trained for epoch 3 in Experiment 3 by 2019-01-30 04:57:24.460247!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1076
Epoch 3 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 04:57:40.116817
1. set (Dataset 1) being trained for epoch 4 in Experiment 3 by 2019-01-30 04:57:45.183074!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1127
2. set (Dataset 2) being trained for epoch 4 in Experiment 3 by 2019-01-30 04:58:01.778766!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1046
3. set (Dataset 4) being trained for epoch 4 in Experiment 3 by 2019-01-30 04:58:20.415690!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1010
4. set (Dataset 6) being trained for epoch 4 in Experiment 3 by 2019-01-30 04:58:43.009993!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1154
5. set (Dataset 12) being trained for epoch 4 in Experiment 3 by 2019-01-30 04:59:02.825693!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0801
6. set (Dataset 7) being trained for epoch 4 in Experiment 3 by 2019-01-30 04:59:27.932598!
Epoch 1/1
745/745 [==============================] - 17s 22ms/step - loss: 0.0988
7. set (Dataset 10) being trained for epoch 4 in Experiment 3 by 2019-01-30 04:59:51.938679!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0869
8. set (Dataset 11) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:00:14.547960!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0640
9. set (Dataset 13) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:00:33.178334!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0691
10. set (Dataset 23) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:00:49.646060!
Epoch 1/1
569/569 [==============================] - 13s 24ms/step - loss: 0.1206
11. set (Dataset 15) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:01:09.527633!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0854
12. set (Dataset 16) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:01:33.511525!
Epoch 1/1
914/914 [==============================] - 22s 24ms/step - loss: 0.0842
13. set (Dataset 17) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:01:59.167734!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0829
14. set (Dataset 18) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:02:14.618422!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1015
15. set (Dataset 19) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:02:33.720242!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0750
16. set (Dataset 20) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:02:50.422996!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0705
17. set (Dataset 21) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:03:09.572575!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1314
18. set (Dataset 22) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:03:30.518217!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0629
19. set (Dataset 8) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:03:53.612166!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0928
20. set (Dataset 24) being trained for epoch 4 in Experiment 3 by 2019-01-30 05:04:16.289251!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0639
Epoch 4 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 05:04:31.970570
1. set (Dataset 22) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:04:38.391340!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0613
2. set (Dataset 24) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:04:58.283055!
Epoch 1/1
492/492 [==============================] - 12s 23ms/step - loss: 0.0608
3. set (Dataset 15) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:05:16.315297!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0868
4. set (Dataset 19) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:05:36.636372!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0751
5. set (Dataset 7) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:05:55.555782!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.1003
6. set (Dataset 8) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:06:21.235282!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0898
7. set (Dataset 21) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:06:45.214639!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1345
8. set (Dataset 16) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:07:08.112910!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0838
9. set (Dataset 12) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:07:36.451307!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0790
10. set (Dataset 13) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:07:58.223985!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0672
11. set (Dataset 10) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:08:16.571650!
Epoch 1/1
726/726 [==============================] - 16s 23ms/step - loss: 0.0896
12. set (Dataset 1) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:08:38.109091!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1090
13. set (Dataset 18) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:08:55.984557!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1055
14. set (Dataset 2) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:09:15.290539!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1086
15. set (Dataset 4) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:09:34.871446!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.0997
16. set (Dataset 20) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:09:57.716987!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0773
17. set (Dataset 17) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:10:14.681477!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0824
18. set (Dataset 6) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:10:29.391172!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1082
19. set (Dataset 23) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:10:47.129770!
Epoch 1/1
569/569 [==============================] - 13s 24ms/step - loss: 0.1154
20. set (Dataset 11) being trained for epoch 5 in Experiment 3 by 2019-01-30 05:11:06.329162!
Epoch 1/1
572/572 [==============================] - 13s 22ms/step - loss: 0.0657
Epoch 5 for Experiment 3 completed!
Exp2019-01-30_03-25-16_part3.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21, 'F02'), (16,
 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-30 05:11:21.471491
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.36 Degree
        The absolute mean error on Yaw angle estimation: 23.21 Degree
        The absolute mean error on Roll angle estimation: 13.32 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 7.53 Degree
        The absolute mean error on Yaw angle estimation: 19.63 Degree
        The absolute mean error on Roll angle estimation: 5.00 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 31.44 Degree
        The absolute mean error on Yaw angle estimation: 19.20 Degree
        The absolute mean error on Roll angle estimation: 9.38 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 14.16 Degree
        The absolute mean error on Yaw angle estimation: 29.86 Degree
        The absolute mean error on Roll angle estimation: 13.18 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.37 Degree
        The absolute mean error on Yaw angle estimations: 22.97 Degree
        The absolute mean error on Roll angle estimations: 10.22 Degree
Exp2019-01-30_03-25-16_part3 completed!
Training model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
All frames and annotations from 20 datasets have been read by 2019-01-30 05:12:54.232039
1. set (Dataset 6) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:12:59.436466!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1061
2. set (Dataset 11) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:13:17.719155!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0632
3. set (Dataset 10) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:13:37.923781!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0895
4. set (Dataset 4) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:14:02.536218!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1031
5. set (Dataset 8) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:14:27.594125!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0924
6. set (Dataset 23) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:14:51.029791!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1235
7. set (Dataset 17) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:15:08.508619!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0803
8. set (Dataset 1) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:15:22.458948!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1096
9. set (Dataset 7) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:15:41.281683!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0947
10. set (Dataset 12) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:16:06.459947!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.0821
11. set (Dataset 21) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:16:30.069430!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1363
12. set (Dataset 22) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:16:51.451936!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0632
13. set (Dataset 2) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:17:12.397636!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1035
14. set (Dataset 24) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:17:28.310374!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0652
15. set (Dataset 15) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:17:46.208477!
Epoch 1/1
654/654 [==============================] - 15s 22ms/step - loss: 0.0906
16. set (Dataset 20) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:18:06.258690!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0728
17. set (Dataset 18) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:18:24.963750!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1025
18. set (Dataset 19) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:18:44.096840!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0777
19. set (Dataset 13) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:19:00.631975!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0722
20. set (Dataset 16) being trained for epoch 1 in Experiment 4 by 2019-01-30 05:19:20.003477!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0849
Epoch 1 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 05:19:45.466636
1. set (Dataset 19) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:19:50.361139!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0789
2. set (Dataset 16) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:20:10.878967!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0831
3. set (Dataset 21) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:20:37.906586!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1316
4. set (Dataset 15) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:20:59.371642!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0876
5. set (Dataset 23) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:21:19.810249!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1155
6. set (Dataset 13) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:21:38.318230!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0714
7. set (Dataset 18) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:21:55.600345!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1017
8. set (Dataset 22) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:22:15.752766!
Epoch 1/1
665/665 [==============================] - 16s 23ms/step - loss: 0.0630
9. set (Dataset 8) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:22:39.182372!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0945
10. set (Dataset 7) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:23:04.111926!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0950
11. set (Dataset 17) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:23:25.534912!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0804
12. set (Dataset 6) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:23:39.616318!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1057
13. set (Dataset 24) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:23:56.728138!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0664
14. set (Dataset 11) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:24:13.935660!
Epoch 1/1
572/572 [==============================] - 13s 24ms/step - loss: 0.0648
15. set (Dataset 10) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:24:34.771198!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0895
16. set (Dataset 20) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:24:56.900255!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0783
17. set (Dataset 2) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:25:14.963100!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1144
18. set (Dataset 4) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:25:34.675458!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1018
19. set (Dataset 12) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:25:59.828978!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0838
20. set (Dataset 1) being trained for epoch 2 in Experiment 4 by 2019-01-30 05:26:21.704445!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1096
Epoch 2 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 05:26:37.411779
1. set (Dataset 4) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:26:44.803581!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.0976
2. set (Dataset 1) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:27:07.364919!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1065
3. set (Dataset 17) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:27:22.232962!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0867
4. set (Dataset 10) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:27:38.952548!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0858
5. set (Dataset 13) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:28:00.214009!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0707
6. set (Dataset 12) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:28:18.272794!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0773
7. set (Dataset 2) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:28:40.796296!
Epoch 1/1
511/511 [==============================] - 11s 21ms/step - loss: 0.1008
8. set (Dataset 6) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:28:57.023856!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1125
9. set (Dataset 23) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:29:15.319795!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1175
10. set (Dataset 8) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:29:36.442071!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0917
11. set (Dataset 18) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:30:00.008110!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1102
12. set (Dataset 19) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:30:19.382565!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0787
13. set (Dataset 11) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:30:36.674237!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0644
14. set (Dataset 16) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:30:59.254225!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0830
15. set (Dataset 21) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:31:26.648906!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1323
16. set (Dataset 20) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:31:47.294059!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0691
17. set (Dataset 24) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:32:04.992939!
Epoch 1/1
492/492 [==============================] - 12s 23ms/step - loss: 0.0626
18. set (Dataset 15) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:32:23.014276!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0892
19. set (Dataset 7) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:32:45.831792!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0979
20. set (Dataset 22) being trained for epoch 3 in Experiment 4 by 2019-01-30 05:33:09.655917!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0635
Epoch 3 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 05:33:29.140897
1. set (Dataset 15) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:33:35.515229!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0823
2. set (Dataset 22) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:33:57.304084!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0632
3. set (Dataset 18) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:34:18.512199!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1052
4. set (Dataset 21) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:34:39.251852!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1348
5. set (Dataset 12) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:35:01.804150!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0832
6. set (Dataset 7) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:35:26.706959!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0962
7. set (Dataset 24) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:35:48.419845!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0628
8. set (Dataset 19) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:36:04.702946!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0762
9. set (Dataset 13) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:36:21.626094!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0728
10. set (Dataset 23) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:36:37.566517!
Epoch 1/1
569/569 [==============================] - 13s 22ms/step - loss: 0.1160
11. set (Dataset 2) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:36:55.312119!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1081
12. set (Dataset 4) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:37:14.886277!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1074
13. set (Dataset 16) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:37:41.558132!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0844
14. set (Dataset 1) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:38:08.025828!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1212
15. set (Dataset 17) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:38:23.747301!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0819
16. set (Dataset 20) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:38:38.289364!
Epoch 1/1
556/556 [==============================] - 12s 21ms/step - loss: 0.0762
17. set (Dataset 11) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:38:56.008339!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0672
18. set (Dataset 10) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:39:16.926857!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0864
19. set (Dataset 8) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:39:41.056532!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0894
20. set (Dataset 6) being trained for epoch 4 in Experiment 4 by 2019-01-30 05:40:04.035668!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1086
Epoch 4 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 05:40:21.009361
1. set (Dataset 10) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:40:28.246163!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0853
2. set (Dataset 6) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:40:50.320202!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1121
3. set (Dataset 2) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:41:08.447539!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1051
4. set (Dataset 17) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:41:24.522829!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0802
5. set (Dataset 7) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:41:41.620990!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0951
6. set (Dataset 8) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:42:06.559761!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0908
7. set (Dataset 11) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:42:30.324694!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0612
8. set (Dataset 4) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:42:51.132759!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1020
9. set (Dataset 12) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:43:15.537984!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0815
10. set (Dataset 13) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:43:37.729861!
Epoch 1/1
485/485 [==============================] - 11s 24ms/step - loss: 0.0669
11. set (Dataset 24) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:43:53.984128!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0712
12. set (Dataset 15) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:44:11.033004!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0884
13. set (Dataset 1) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:44:31.534291!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1185
14. set (Dataset 22) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:44:49.010339!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0650
15. set (Dataset 18) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:45:09.627949!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1066
16. set (Dataset 20) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:45:29.027460!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0772
17. set (Dataset 16) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:45:50.707557!
Epoch 1/1
914/914 [==============================] - 20s 22ms/step - loss: 0.0821
18. set (Dataset 21) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:46:17.330420!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1254
19. set (Dataset 23) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:46:37.605815!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1129
20. set (Dataset 19) being trained for epoch 5 in Experiment 4 by 2019-01-30 05:46:55.558515!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0750
Epoch 5 for Experiment 4 completed!
Exp2019-01-30_03-25-16_part4.h5 has been saved.
The subjects are trained: [(10, 'M04'), (6, 'F06'), (2, 'F02'), (17, 'M10'), (7, 'M01'), (8, 'M02'), (11, 'M05'), (4, 'F
04'), (12, 'M06'), (13, 'M07'), (24, 'M14'), (15, 'F03'), (1, 'F01'), (22, 'M01'), (18, 'F05'), (20, 'M12'), (16, 'M09')
, (21, 'F02'), (23, 'M13'), (19, 'M11')]
Evaluating model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-30 05:47:09.356531
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.77 Degree
        The absolute mean error on Yaw angle estimation: 23.30 Degree
        The absolute mean error on Roll angle estimation: 13.11 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 9.02 Degree
        The absolute mean error on Yaw angle estimation: 23.59 Degree
        The absolute mean error on Roll angle estimation: 4.74 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 28.76 Degree
        The absolute mean error on Yaw angle estimation: 19.73 Degree
        The absolute mean error on Roll angle estimation: 7.46 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 13.60 Degree
        The absolute mean error on Yaw angle estimation: 30.65 Degree
        The absolute mean error on Roll angle estimation: 12.80 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.04 Degree
        The absolute mean error on Yaw angle estimations: 24.32 Degree
        The absolute mean error on Roll angle estimations: 9.53 Degree
Exp2019-01-30_03-25-16_part4 completed!
Training model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
All frames and annotations from 20 datasets have been read by 2019-01-30 05:48:41.484711
1. set (Dataset 21) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:48:47.505677!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1301
2. set (Dataset 19) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:49:07.426991!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0741
3. set (Dataset 24) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:49:23.347133!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0622
4. set (Dataset 18) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:49:40.210107!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1001
5. set (Dataset 8) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:50:02.732903!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0957
6. set (Dataset 23) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:50:26.478832!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1151
7. set (Dataset 16) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:50:48.171093!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0786
8. set (Dataset 15) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:51:15.755866!
Epoch 1/1
654/654 [==============================] - 15s 24ms/step - loss: 0.0827
9. set (Dataset 7) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:51:38.857663!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0979
10. set (Dataset 12) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:52:03.981001!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0828
11. set (Dataset 11) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:52:26.950068!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0644
12. set (Dataset 10) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:52:47.198299!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0860
13. set (Dataset 22) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:53:10.492913!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0624
14. set (Dataset 6) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:53:31.042944!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1126
15. set (Dataset 2) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:53:48.667374!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1135
16. set (Dataset 20) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:54:06.381877!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0761
17. set (Dataset 1) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:54:24.219002!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1209
18. set (Dataset 17) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:54:39.524255!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0822
19. set (Dataset 13) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:54:53.199881!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0649
20. set (Dataset 4) being trained for epoch 1 in Experiment 5 by 2019-01-30 05:55:11.576986!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1066
Epoch 1 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 05:55:33.161876
1. set (Dataset 17) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:55:36.923583!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0801
2. set (Dataset 4) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:55:53.440773!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.0995
3. set (Dataset 11) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:56:17.049222!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0677
4. set (Dataset 2) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:56:35.914105!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1083
5. set (Dataset 23) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:56:53.612923!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1186
6. set (Dataset 13) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:57:11.536141!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0649
7. set (Dataset 1) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:57:27.670987!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1168
8. set (Dataset 10) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:57:46.629857!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0864
9. set (Dataset 8) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:58:11.402114!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0883
10. set (Dataset 7) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:58:36.859246!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0966
11. set (Dataset 16) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:59:03.314434!
Epoch 1/1
914/914 [==============================] - 21s 22ms/step - loss: 0.0883
12. set (Dataset 21) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:59:30.001965!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1399
13. set (Dataset 6) being trained for epoch 2 in Experiment 5 by 2019-01-30 05:59:50.278501!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1104
14. set (Dataset 19) being trained for epoch 2 in Experiment 5 by 2019-01-30 06:00:07.655861!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0772
15. set (Dataset 24) being trained for epoch 2 in Experiment 5 by 2019-01-30 06:00:23.490326!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0618
16. set (Dataset 20) being trained for epoch 2 in Experiment 5 by 2019-01-30 06:00:40.406426!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0707
17. set (Dataset 22) being trained for epoch 2 in Experiment 5 by 2019-01-30 06:00:59.997841!
Epoch 1/1
665/665 [==============================] - 16s 23ms/step - loss: 0.0608
18. set (Dataset 18) being trained for epoch 2 in Experiment 5 by 2019-01-30 06:01:21.551547!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1019
19. set (Dataset 12) being trained for epoch 2 in Experiment 5 by 2019-01-30 06:01:42.728534!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0841
20. set (Dataset 15) being trained for epoch 2 in Experiment 5 by 2019-01-30 06:02:06.668866!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0848
Epoch 2 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 06:02:26.252799
1. set (Dataset 18) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:02:32.146585!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1041
2. set (Dataset 15) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:02:52.779692!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0841
3. set (Dataset 16) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:03:16.765431!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0812
4. set (Dataset 24) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:03:42.644385!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0643
5. set (Dataset 13) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:03:58.862904!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0752
6. set (Dataset 12) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:04:17.036186!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0856
7. set (Dataset 22) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:04:40.478750!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0602
8. set (Dataset 21) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:05:02.269506!
Epoch 1/1
634/634 [==============================] - 14s 21ms/step - loss: 0.1329
9. set (Dataset 23) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:05:21.386969!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1141
10. set (Dataset 8) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:05:42.293318!
Epoch 1/1
772/772 [==============================] - 18s 24ms/step - loss: 0.1002
11. set (Dataset 1) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:06:05.697897!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1150
12. set (Dataset 17) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:06:21.033262!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0818
13. set (Dataset 19) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:06:35.408330!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0784
14. set (Dataset 4) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:06:54.285951!
Epoch 1/1
744/744 [==============================] - 17s 22ms/step - loss: 0.1081
15. set (Dataset 11) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:07:16.685074!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0663
16. set (Dataset 20) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:07:35.342734!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0770
17. set (Dataset 6) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:07:53.921167!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1087
18. set (Dataset 2) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:08:11.835385!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1097
19. set (Dataset 7) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:08:31.723363!
Epoch 1/1
745/745 [==============================] - 17s 22ms/step - loss: 0.0959
20. set (Dataset 10) being trained for epoch 3 in Experiment 5 by 2019-01-30 06:08:55.806528!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0866
Epoch 3 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 06:09:16.186361
1. set (Dataset 2) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:09:21.277386!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1058
2. set (Dataset 10) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:09:40.097408!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0877
3. set (Dataset 1) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:10:01.550455!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1168
4. set (Dataset 11) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:10:19.254642!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0637
5. set (Dataset 12) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:10:39.676489!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0819
6. set (Dataset 7) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:11:03.221034!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0926
7. set (Dataset 6) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:11:25.886097!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1126
8. set (Dataset 17) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:11:42.064176!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0803
9. set (Dataset 13) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:11:56.381397!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0643
10. set (Dataset 23) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:12:12.941105!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1151
11. set (Dataset 22) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:12:32.777409!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0592
12. set (Dataset 18) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:12:53.708955!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1024
13. set (Dataset 4) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:13:14.917552!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1072
14. set (Dataset 15) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:13:38.438774!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0879
15. set (Dataset 16) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:14:02.652453!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0822
16. set (Dataset 20) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:14:29.127627!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0718
17. set (Dataset 19) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:14:46.738928!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0768
18. set (Dataset 24) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:15:03.281197!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0606
19. set (Dataset 8) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:15:22.161888!
Epoch 1/1
772/772 [==============================] - 18s 24ms/step - loss: 0.0976
20. set (Dataset 21) being trained for epoch 4 in Experiment 5 by 2019-01-30 06:15:46.728120!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1319
Epoch 4 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 06:16:05.719480
1. set (Dataset 24) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:16:10.409585!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0596
2. set (Dataset 21) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:16:28.022398!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1272
3. set (Dataset 22) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:16:49.260321!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.0612
4. set (Dataset 16) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:17:10.721315!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0813
5. set (Dataset 7) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:17:39.527816!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.1027
6. set (Dataset 8) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:18:04.553949!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0926
7. set (Dataset 19) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:18:27.252885!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0741
8. set (Dataset 18) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:18:44.449383!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1076
9. set (Dataset 12) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:19:05.738941!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0830
10. set (Dataset 13) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:19:27.228721!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0681
11. set (Dataset 6) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:19:43.354683!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1088
12. set (Dataset 2) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:20:01.514040!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1041
13. set (Dataset 15) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:20:20.166696!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0879
14. set (Dataset 10) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:20:42.764950!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0938
15. set (Dataset 1) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:21:04.788356!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1157
16. set (Dataset 20) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:21:21.370211!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0771
17. set (Dataset 4) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:21:42.154128!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1012
18. set (Dataset 11) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:22:05.567304!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0656
19. set (Dataset 23) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:22:24.260648!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1224
20. set (Dataset 17) being trained for epoch 5 in Experiment 5 by 2019-01-30 06:22:41.169597!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0802
Epoch 5 for Experiment 5 completed!
Exp2019-01-30_03-25-16_part5.h5 has been saved.
The subjects are trained: [(24, 'M14'), (21, 'F02'), (22, 'M01'), (16, 'M09'), (7, 'M01'), (8, 'M02'), (19, 'M11'), (18,
 'F05'), (12, 'M06'), (13, 'M07'), (6, 'F06'), (2, 'F02'), (15, 'F03'), (10, 'M04'), (1, 'F01'), (20, 'M12'), (4, 'F04')
, (11, 'M05'), (23, 'M13'), (17, 'M10')]
Evaluating model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-30 06:22:52.864333
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.64 Degree
        The absolute mean error on Yaw angle estimation: 23.18 Degree
        The absolute mean error on Roll angle estimation: 10.85 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.04 Degree
        The absolute mean error on Yaw angle estimation: 19.57 Degree
        The absolute mean error on Roll angle estimation: 3.86 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 29.83 Degree
        The absolute mean error on Yaw angle estimation: 15.89 Degree
        The absolute mean error on Roll angle estimation: 8.00 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 13.42 Degree
        The absolute mean error on Yaw angle estimation: 32.09 Degree
        The absolute mean error on Roll angle estimation: 12.78 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.98 Degree
        The absolute mean error on Yaw angle estimations: 22.68 Degree
        The absolute mean error on Roll angle estimations: 8.87 Degree
Exp2019-01-30_03-25-16_part5 completed!
Training model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
All frames and annotations from 20 datasets have been read by 2019-01-30 06:24:24.908092
1. set (Dataset 11) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:24:30.641457!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0626
2. set (Dataset 17) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:24:47.572520!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0799
3. set (Dataset 6) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:25:02.215559!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1034
4. set (Dataset 1) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:25:19.925613!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1175
5. set (Dataset 8) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:25:39.675796!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0936
6. set (Dataset 23) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:26:03.037816!
Epoch 1/1
569/569 [==============================] - 13s 24ms/step - loss: 0.1174
7. set (Dataset 4) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:26:23.948871!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1040
8. set (Dataset 2) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:26:46.526980!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1089
9. set (Dataset 7) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:27:05.888831!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0934
10. set (Dataset 12) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:27:30.292754!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0823
11. set (Dataset 19) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:27:51.623960!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0810
12. set (Dataset 24) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:28:08.186902!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0667
13. set (Dataset 10) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:28:27.234911!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0879
14. set (Dataset 21) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:28:50.725314!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1358
15. set (Dataset 22) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:29:11.984990!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0627
16. set (Dataset 20) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:29:33.339856!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0725
17. set (Dataset 15) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:29:52.448360!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0868
18. set (Dataset 16) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:30:16.299196!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0840
19. set (Dataset 13) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:30:42.603101!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0689
20. set (Dataset 18) being trained for epoch 1 in Experiment 6 by 2019-01-30 06:30:59.260850!
Epoch 1/1
614/614 [==============================] - 13s 21ms/step - loss: 0.1039
Epoch 1 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 06:31:16.847125
1. set (Dataset 16) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:31:25.602164!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0821
2. set (Dataset 18) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:31:52.926260!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1040
3. set (Dataset 19) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:32:12.636686!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0739
4. set (Dataset 22) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:32:30.409792!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0633
5. set (Dataset 23) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:32:51.047528!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1149
6. set (Dataset 13) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:33:09.289135!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0715
7. set (Dataset 15) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:33:26.715875!
Epoch 1/1
654/654 [==============================] - 14s 22ms/step - loss: 0.0844
8. set (Dataset 24) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:33:45.928559!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0633
9. set (Dataset 8) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:34:05.400013!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0966
10. set (Dataset 7) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:34:30.968483!
Epoch 1/1
745/745 [==============================] - 17s 22ms/step - loss: 0.0970
11. set (Dataset 4) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:34:55.123963!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1001
12. set (Dataset 11) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:35:17.729499!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0671
13. set (Dataset 21) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:35:37.075460!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1348
14. set (Dataset 17) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:35:56.120476!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0830
15. set (Dataset 6) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:36:10.508044!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1083
16. set (Dataset 20) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:36:28.244807!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0721
17. set (Dataset 10) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:36:48.394782!
Epoch 1/1
726/726 [==============================] - 16s 22ms/step - loss: 0.0942
18. set (Dataset 1) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:37:09.735237!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1127
19. set (Dataset 12) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:37:28.733405!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0816
20. set (Dataset 2) being trained for epoch 2 in Experiment 6 by 2019-01-30 06:37:51.347325!
Epoch 1/1
511/511 [==============================] - 10s 19ms/step - loss: 0.1017
Epoch 2 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 06:38:05.454374
1. set (Dataset 1) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:38:10.538054!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1131
2. set (Dataset 2) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:38:26.971450!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1021
3. set (Dataset 4) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:38:46.350106!
Epoch 1/1
744/744 [==============================] - 16s 22ms/step - loss: 0.0963
4. set (Dataset 6) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:39:07.887237!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1136
5. set (Dataset 13) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:39:25.521733!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0675
6. set (Dataset 12) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:39:43.961700!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0795
7. set (Dataset 10) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:40:07.941473!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0873
8. set (Dataset 11) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:40:30.287470!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0616
9. set (Dataset 23) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:40:48.832595!
Epoch 1/1
569/569 [==============================] - 12s 22ms/step - loss: 0.1192
10. set (Dataset 8) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:41:09.156017!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0913
11. set (Dataset 15) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:41:33.198113!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0900
12. set (Dataset 16) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:41:57.021442!
Epoch 1/1
914/914 [==============================] - 20s 22ms/step - loss: 0.0852
13. set (Dataset 17) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:42:21.280159!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0807
14. set (Dataset 18) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:42:36.427479!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1020
15. set (Dataset 19) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:42:56.091642!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0742
16. set (Dataset 20) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:43:13.222020!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0700
17. set (Dataset 21) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:43:32.615024!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1319
18. set (Dataset 22) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:43:53.879056!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0626
19. set (Dataset 7) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:44:16.582257!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0985
20. set (Dataset 24) being trained for epoch 3 in Experiment 6 by 2019-01-30 06:44:38.514621!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0650
Epoch 3 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 06:44:54.045608
1. set (Dataset 22) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:45:00.448880!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0577
2. set (Dataset 24) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:45:20.590192!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0607
3. set (Dataset 15) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:45:38.204278!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0859
4. set (Dataset 19) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:45:58.063505!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0749
5. set (Dataset 12) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:46:17.077892!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0860
6. set (Dataset 7) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:46:41.063959!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0967
7. set (Dataset 21) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:47:04.488157!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1384
8. set (Dataset 16) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:47:27.702179!
Epoch 1/1
914/914 [==============================] - 21s 22ms/step - loss: 0.0808
9. set (Dataset 13) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:47:53.149566!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0683
10. set (Dataset 23) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:48:09.467314!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1166
11. set (Dataset 10) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:48:29.913952!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0946
12. set (Dataset 1) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:48:52.272790!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1115
13. set (Dataset 18) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:49:09.063164!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1060
14. set (Dataset 2) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:49:27.987974!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1049
15. set (Dataset 4) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:49:46.819759!
Epoch 1/1
744/744 [==============================] - 17s 22ms/step - loss: 0.1005
16. set (Dataset 20) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:50:08.942626!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0759
17. set (Dataset 17) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:50:25.507674!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0805
18. set (Dataset 6) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:50:39.807549!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1067
19. set (Dataset 8) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:50:59.352860!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0926
20. set (Dataset 11) being trained for epoch 4 in Experiment 6 by 2019-01-30 06:51:23.063815!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0615
Epoch 4 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 06:51:40.760017
1. set (Dataset 6) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:51:45.963281!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1055
2. set (Dataset 11) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:52:03.805346!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0643
3. set (Dataset 10) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:52:24.069741!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0897
4. set (Dataset 4) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:52:48.642298!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1028
5. set (Dataset 7) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:53:13.644459!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0917
6. set (Dataset 8) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:53:38.877696!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0927
7. set (Dataset 17) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:54:00.624313!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0831
8. set (Dataset 1) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:54:14.759232!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1122
9. set (Dataset 12) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:54:33.319871!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0807
10. set (Dataset 13) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:54:54.379747!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0682
11. set (Dataset 21) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:55:11.687146!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1438
12. set (Dataset 22) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:55:33.233993!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0596
13. set (Dataset 2) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:55:53.924062!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1026
14. set (Dataset 24) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:56:10.935211!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0689
15. set (Dataset 15) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:56:29.070540!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0917
16. set (Dataset 20) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:56:50.154279!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0732
17. set (Dataset 18) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:57:08.791813!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1068
18. set (Dataset 19) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:57:27.774769!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0740
19. set (Dataset 23) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:57:44.652634!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1116
20. set (Dataset 16) being trained for epoch 5 in Experiment 6 by 2019-01-30 06:58:06.691031!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0783
Epoch 5 for Experiment 6 completed!
Exp2019-01-30_03-25-16_part6.h5 has been saved.
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (7, 'M01'), (8, 'M02'), (17, 'M10'), (1, 'F
01'), (12, 'M06'), (13, 'M07'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'), (20, 'M12'), (18, 'F05')
, (19, 'M11'), (23, 'M13'), (16, 'M09')]
Evaluating model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-30 06:58:29.716123
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.84 Degree
        The absolute mean error on Yaw angle estimation: 22.90 Degree
        The absolute mean error on Roll angle estimation: 13.49 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.00 Degree
        The absolute mean error on Yaw angle estimation: 21.40 Degree
        The absolute mean error on Roll angle estimation: 4.73 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 30.47 Degree
        The absolute mean error on Yaw angle estimation: 19.48 Degree
        The absolute mean error on Roll angle estimation: 7.46 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 12.79 Degree
        The absolute mean error on Yaw angle estimation: 30.16 Degree
        The absolute mean error on Roll angle estimation: 12.59 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.02 Degree
        The absolute mean error on Yaw angle estimations: 23.48 Degree
        The absolute mean error on Roll angle estimations: 9.57 Degree
Exp2019-01-30_03-25-16_part6 completed!
Training model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
All frames and annotations from 20 datasets have been read by 2019-01-30 07:00:01.083695
1. set (Dataset 19) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:00:06.017533!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0722
2. set (Dataset 16) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:00:26.197086!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0812
3. set (Dataset 21) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:00:53.288959!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1304
4. set (Dataset 15) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:01:14.460583!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0904
5. set (Dataset 8) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:01:37.201387!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0954
6. set (Dataset 23) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:02:00.568639!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1151
7. set (Dataset 18) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:02:20.178197!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1027
8. set (Dataset 22) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:02:41.288537!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0590
9. set (Dataset 7) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:03:04.342133!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.1010
10. set (Dataset 12) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:03:29.050741!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0844
11. set (Dataset 17) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:03:49.794629!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0793
12. set (Dataset 6) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:04:03.663332!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1121
13. set (Dataset 24) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:04:20.687812!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0603
14. set (Dataset 11) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:04:37.815138!
Epoch 1/1
572/572 [==============================] - 12s 22ms/step - loss: 0.0641
15. set (Dataset 10) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:04:57.497981!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0852
16. set (Dataset 20) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:05:20.229812!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0768
17. set (Dataset 2) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:05:38.600230!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1086
18. set (Dataset 4) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:05:57.798945!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1024
19. set (Dataset 13) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:06:19.675815!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0663
20. set (Dataset 1) being trained for epoch 1 in Experiment 7 by 2019-01-30 07:06:35.551603!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1089
Epoch 1 for Experiment 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 07:06:51.884385
1. set (Dataset 4) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:06:59.296480!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1003
2. set (Dataset 1) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:07:22.216766!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1112
3. set (Dataset 17) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:07:37.285767!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0825
4. set (Dataset 10) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:07:53.970458!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0860
5. set (Dataset 23) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:08:16.661485!
Epoch 1/1
569/569 [==============================] - 13s 22ms/step - loss: 0.1209
6. set (Dataset 13) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:08:34.194618!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0648
7. set (Dataset 2) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:08:50.740716!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1055
8. set (Dataset 6) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:09:08.152090!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1106
9. set (Dataset 8) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:09:28.906438!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0933
10. set (Dataset 7) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:09:53.439647!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0954
11. set (Dataset 18) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:10:16.795882!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1083
12. set (Dataset 19) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:10:36.086478!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0773
13. set (Dataset 11) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:10:53.783130!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0652
14. set (Dataset 16) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:11:15.878021!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0814
15. set (Dataset 21) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:11:42.996115!
Epoch 1/1
634/634 [==============================] - 14s 21ms/step - loss: 0.1291
16. set (Dataset 20) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:12:02.086195!
Epoch 1/1
556/556 [==============================] - 12s 21ms/step - loss: 0.0721
17. set (Dataset 24) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:12:18.739502!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0600
18. set (Dataset 15) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:12:36.166950!
Epoch 1/1
654/654 [==============================] - 15s 22ms/step - loss: 0.0858
19. set (Dataset 12) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:12:58.274144!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0863
20. set (Dataset 22) being trained for epoch 2 in Experiment 7 by 2019-01-30 07:13:22.145038!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0625
Epoch 2 for Experiment 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 07:13:41.709706
1. set (Dataset 15) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:13:48.094480!
Epoch 1/1
654/654 [==============================] - 15s 22ms/step - loss: 0.0799
2. set (Dataset 22) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:14:09.251627!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0598
3. set (Dataset 18) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:14:30.272526!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1026
4. set (Dataset 21) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:14:50.189940!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1301
5. set (Dataset 13) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:15:10.220531!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0776
6. set (Dataset 12) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:15:29.149362!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0866
7. set (Dataset 24) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:15:50.669974!
Epoch 1/1
492/492 [==============================] - 12s 23ms/step - loss: 0.0622
8. set (Dataset 19) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:16:07.227952!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0740
9. set (Dataset 23) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:16:23.975438!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1149
10. set (Dataset 8) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:16:45.392261!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0930
11. set (Dataset 2) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:17:07.523948!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1120
12. set (Dataset 4) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:17:26.983838!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1040
13. set (Dataset 16) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:17:52.976814!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0851
14. set (Dataset 1) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:18:19.514230!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1170
15. set (Dataset 17) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:18:34.727463!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0805
16. set (Dataset 20) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:18:49.632642!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0735
17. set (Dataset 11) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:19:08.348340!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0648
18. set (Dataset 10) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:19:29.064343!
Epoch 1/1
726/726 [==============================] - 16s 23ms/step - loss: 0.0864
19. set (Dataset 7) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:19:53.265207!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0955
20. set (Dataset 6) being trained for epoch 3 in Experiment 7 by 2019-01-30 07:20:16.307385!
Epoch 1/1
542/542 [==============================] - 12s 23ms/step - loss: 0.1120
Epoch 3 for Experiment 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 07:20:33.224106
1. set (Dataset 10) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:20:40.467694!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0900
2. set (Dataset 6) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:21:02.334455!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1087
3. set (Dataset 2) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:21:19.513525!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1113
4. set (Dataset 17) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:21:35.102509!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0809
5. set (Dataset 12) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:21:51.967468!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0784
6. set (Dataset 7) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:22:16.208053!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0890
7. set (Dataset 11) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:22:39.336231!
Epoch 1/1
572/572 [==============================] - 13s 24ms/step - loss: 0.0661
8. set (Dataset 4) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:23:00.354814!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1049
9. set (Dataset 13) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:23:22.058842!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0634
10. set (Dataset 23) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:23:37.993590!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1208
11. set (Dataset 24) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:23:55.924086!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0667
12. set (Dataset 15) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:24:13.723753!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0858
13. set (Dataset 1) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:24:34.447969!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1157
14. set (Dataset 22) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:24:52.499620!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0625
15. set (Dataset 18) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:25:14.416967!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1028
16. set (Dataset 20) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:25:33.577663!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0711
17. set (Dataset 16) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:25:55.115315!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0830
18. set (Dataset 21) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:26:22.371779!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1277
19. set (Dataset 8) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:26:44.292288!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0980
20. set (Dataset 19) being trained for epoch 4 in Experiment 7 by 2019-01-30 07:27:06.945684!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0753
Epoch 4 for Experiment 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 07:27:22.685731
1. set (Dataset 21) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:27:28.717816!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1267
2. set (Dataset 19) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:27:48.533507!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0744
3. set (Dataset 24) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:28:04.820308!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0630
4. set (Dataset 18) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:28:22.582947!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1005
5. set (Dataset 7) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:28:44.320509!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.1016
6. set (Dataset 8) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:29:09.366815!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0952
7. set (Dataset 16) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:29:35.844629!
Epoch 1/1
914/914 [==============================] - 19s 21ms/step - loss: 0.0794
8. set (Dataset 15) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:30:01.387978!
Epoch 1/1
654/654 [==============================] - 15s 24ms/step - loss: 0.0821
9. set (Dataset 12) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:30:24.245402!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0859
10. set (Dataset 13) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:30:46.382611!
Epoch 1/1
485/485 [==============================] - 10s 22ms/step - loss: 0.0684
11. set (Dataset 11) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:31:02.696875!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0672
12. set (Dataset 10) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:31:23.251327!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0868
13. set (Dataset 22) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:31:46.768389!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0640
14. set (Dataset 6) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:32:07.394616!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1074
15. set (Dataset 2) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:32:25.450957!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1074
16. set (Dataset 20) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:32:42.255434!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0738
17. set (Dataset 1) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:33:00.076981!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1128
18. set (Dataset 17) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:33:15.674442!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0781
19. set (Dataset 23) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:33:30.662845!
Epoch 1/1
569/569 [==============================] - 13s 22ms/step - loss: 0.1143
20. set (Dataset 4) being trained for epoch 5 in Experiment 7 by 2019-01-30 07:33:50.968922!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1032
Epoch 5 for Experiment 7 completed!
Exp2019-01-30_03-25-16_part7.h5 has been saved.
The subjects are trained: [(21, 'F02'), (19, 'M11'), (24, 'M14'), (18, 'F05'), (7, 'M01'), (8, 'M02'), (16, 'M09'), (15,
 'F03'), (12, 'M06'), (13, 'M07'), (11, 'M05'), (10, 'M04'), (22, 'M01'), (6, 'F06'), (2, 'F02'), (20, 'M12'), (1, 'F01'
), (17, 'M10'), (23, 'M13'), (4, 'F04')]
Evaluating model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-30 07:34:11.119747
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.76 Degree
        The absolute mean error on Yaw angle estimation: 21.75 Degree
        The absolute mean error on Roll angle estimation: 8.16 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 6.72 Degree
        The absolute mean error on Yaw angle estimation: 19.27 Degree
        The absolute mean error on Roll angle estimation: 3.47 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 33.21 Degree
        The absolute mean error on Yaw angle estimation: 11.42 Degree
        The absolute mean error on Roll angle estimation: 7.80 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 13.86 Degree
        The absolute mean error on Yaw angle estimation: 30.07 Degree
        The absolute mean error on Roll angle estimation: 13.53 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.64 Degree
        The absolute mean error on Yaw angle estimations: 20.63 Degree
        The absolute mean error on Roll angle estimations: 8.24 Degree
Exp2019-01-30_03-25-16_part7 completed!
Training model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
All frames and annotations from 20 datasets have been read by 2019-01-30 07:35:41.443999
1. set (Dataset 17) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:35:45.211341!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0813
2. set (Dataset 4) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:36:01.072844!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.0988
3. set (Dataset 11) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:36:24.642166!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0669
4. set (Dataset 2) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:36:43.226691!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1059
5. set (Dataset 8) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:37:02.626526!
Epoch 1/1
772/772 [==============================] - 17s 23ms/step - loss: 0.0871
6. set (Dataset 23) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:37:25.651913!
Epoch 1/1
569/569 [==============================] - 13s 22ms/step - loss: 0.1177
7. set (Dataset 1) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:37:43.413641!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1095
8. set (Dataset 10) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:38:02.588717!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0871
9. set (Dataset 7) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:38:27.030356!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0938
10. set (Dataset 12) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:38:51.540644!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0743
11. set (Dataset 16) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:39:17.726646!
Epoch 1/1
914/914 [==============================] - 20s 22ms/step - loss: 0.0821
12. set (Dataset 21) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:39:44.285319!
Epoch 1/1
634/634 [==============================] - 14s 23ms/step - loss: 0.1320
13. set (Dataset 6) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:40:03.962115!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1042
14. set (Dataset 19) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:40:21.547370!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0743
15. set (Dataset 24) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:40:37.486535!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0624
16. set (Dataset 20) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:40:54.392910!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0678
17. set (Dataset 22) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:41:12.921229!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0607
18. set (Dataset 18) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:41:33.896143!
Epoch 1/1
614/614 [==============================] - 13s 22ms/step - loss: 0.1039
19. set (Dataset 13) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:41:52.224294!
Epoch 1/1
485/485 [==============================] - 11s 24ms/step - loss: 0.0696
20. set (Dataset 15) being trained for epoch 1 in Experiment 8 by 2019-01-30 07:42:10.183198!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0844
Epoch 1 for Experiment 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 07:42:29.619805
1. set (Dataset 18) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:42:35.520413!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.1001
2. set (Dataset 15) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:42:56.792724!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0808
3. set (Dataset 16) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:43:20.586074!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0795
4. set (Dataset 24) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:43:45.983834!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0603
5. set (Dataset 23) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:44:03.273622!
Epoch 1/1
569/569 [==============================] - 13s 24ms/step - loss: 0.1116
6. set (Dataset 13) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:44:21.649824!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0759
7. set (Dataset 22) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:44:39.451552!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0595
8. set (Dataset 21) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:45:00.983325!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1275
9. set (Dataset 8) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:45:23.996573!
Epoch 1/1
772/772 [==============================] - 18s 24ms/step - loss: 0.0943
10. set (Dataset 7) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:45:50.093069!
Epoch 1/1
745/745 [==============================] - 17s 22ms/step - loss: 0.0952
11. set (Dataset 1) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:46:11.832861!
Epoch 1/1
498/498 [==============================] - 12s 23ms/step - loss: 0.1175
12. set (Dataset 17) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:46:27.269068!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0790
13. set (Dataset 19) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:46:41.704927!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0763
14. set (Dataset 4) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:47:00.776417!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1034
15. set (Dataset 11) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:47:23.743312!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0660
16. set (Dataset 20) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:47:42.340870!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0769
17. set (Dataset 6) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:48:00.946770!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1092
18. set (Dataset 2) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:48:18.853623!
Epoch 1/1
511/511 [==============================] - 11s 22ms/step - loss: 0.1054
19. set (Dataset 12) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:48:37.690330!
Epoch 1/1
732/732 [==============================] - 17s 24ms/step - loss: 0.0826
20. set (Dataset 10) being trained for epoch 2 in Experiment 8 by 2019-01-30 07:49:02.277328!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0883
Epoch 2 for Experiment 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 07:49:24.189496
1. set (Dataset 2) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:49:29.288525!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1073
2. set (Dataset 10) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:49:48.344138!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0856
3. set (Dataset 1) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:50:10.258192!
Epoch 1/1
498/498 [==============================] - 11s 21ms/step - loss: 0.1070
4. set (Dataset 11) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:50:26.712466!
Epoch 1/1
572/572 [==============================] - 12s 21ms/step - loss: 0.0631
5. set (Dataset 13) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:50:43.923083!
Epoch 1/1
485/485 [==============================] - 11s 23ms/step - loss: 0.0648
6. set (Dataset 12) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:51:02.441899!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0791
7. set (Dataset 6) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:51:24.002679!
Epoch 1/1
542/542 [==============================] - 12s 22ms/step - loss: 0.1127
8. set (Dataset 17) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:51:40.003391!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0808
9. set (Dataset 23) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:51:54.363740!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1143
10. set (Dataset 8) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:52:15.417094!
Epoch 1/1
772/772 [==============================] - 17s 22ms/step - loss: 0.0947
11. set (Dataset 22) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:52:39.277079!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0616
12. set (Dataset 18) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:53:00.558877!
Epoch 1/1
614/614 [==============================] - 14s 23ms/step - loss: 0.1080
13. set (Dataset 4) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:53:22.359867!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1058
14. set (Dataset 15) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:53:45.827339!
Epoch 1/1
654/654 [==============================] - 14s 22ms/step - loss: 0.0826
15. set (Dataset 16) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:54:09.167653!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0826
16. set (Dataset 20) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:54:35.726766!
Epoch 1/1
556/556 [==============================] - 13s 23ms/step - loss: 0.0722
17. set (Dataset 19) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:54:53.687499!
Epoch 1/1
502/502 [==============================] - 12s 23ms/step - loss: 0.0764
18. set (Dataset 24) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:55:10.092595!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0606
19. set (Dataset 7) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:55:29.012650!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0986
20. set (Dataset 21) being trained for epoch 3 in Experiment 8 by 2019-01-30 07:55:52.915009!
Epoch 1/1
634/634 [==============================] - 14s 22ms/step - loss: 0.1316
Epoch 3 for Experiment 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 07:56:11.577795
1. set (Dataset 24) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:56:16.269795!
Epoch 1/1
492/492 [==============================] - 11s 23ms/step - loss: 0.0604
2. set (Dataset 21) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:56:33.501993!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1279
3. set (Dataset 22) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:56:55.123937!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0608
4. set (Dataset 16) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:57:19.904177!
Epoch 1/1
914/914 [==============================] - 21s 23ms/step - loss: 0.0777
5. set (Dataset 12) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:57:47.909180!
Epoch 1/1
732/732 [==============================] - 16s 22ms/step - loss: 0.0883
6. set (Dataset 7) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:58:11.399096!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0977
7. set (Dataset 19) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:58:33.268856!
Epoch 1/1
502/502 [==============================] - 11s 23ms/step - loss: 0.0770
8. set (Dataset 18) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:58:50.625563!
Epoch 1/1
614/614 [==============================] - 14s 22ms/step - loss: 0.1059
9. set (Dataset 13) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:59:09.154903!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0645
10. set (Dataset 23) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:59:26.251144!
Epoch 1/1
569/569 [==============================] - 13s 23ms/step - loss: 0.1144
11. set (Dataset 6) being trained for epoch 4 in Experiment 8 by 2019-01-30 07:59:44.634376!
Epoch 1/1
542/542 [==============================] - 13s 23ms/step - loss: 0.1047
12. set (Dataset 2) being trained for epoch 4 in Experiment 8 by 2019-01-30 08:00:02.514346!
Epoch 1/1
511/511 [==============================] - 12s 23ms/step - loss: 0.1124
13. set (Dataset 15) being trained for epoch 4 in Experiment 8 by 2019-01-30 08:00:20.931625!
Epoch 1/1
654/654 [==============================] - 15s 23ms/step - loss: 0.0859
14. set (Dataset 10) being trained for epoch 4 in Experiment 8 by 2019-01-30 08:00:43.286708!
Epoch 1/1
726/726 [==============================] - 17s 23ms/step - loss: 0.0947
15. set (Dataset 1) being trained for epoch 4 in Experiment 8 by 2019-01-30 08:01:05.279303!
Epoch 1/1
498/498 [==============================] - 11s 22ms/step - loss: 0.1126
16. set (Dataset 20) being trained for epoch 4 in Experiment 8 by 2019-01-30 08:01:21.555604!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0761
17. set (Dataset 4) being trained for epoch 4 in Experiment 8 by 2019-01-30 08:01:41.194031!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.1066
18. set (Dataset 11) being trained for epoch 4 in Experiment 8 by 2019-01-30 08:02:04.639164!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0667
19. set (Dataset 8) being trained for epoch 4 in Experiment 8 by 2019-01-30 08:02:25.846804!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0907
20. set (Dataset 17) being trained for epoch 4 in Experiment 8 by 2019-01-30 08:02:47.798258!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0856
Epoch 4 for Experiment 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-30 08:03:01.161143
1. set (Dataset 11) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:03:06.879053!
Epoch 1/1
572/572 [==============================] - 13s 23ms/step - loss: 0.0603
2. set (Dataset 17) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:03:23.838673!
Epoch 1/1
395/395 [==============================] - 9s 22ms/step - loss: 0.0800
3. set (Dataset 6) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:03:37.677318!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1084
4. set (Dataset 1) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:03:55.678026!
Epoch 1/1
498/498 [==============================] - 11s 23ms/step - loss: 0.1151
5. set (Dataset 7) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:04:14.828926!
Epoch 1/1
745/745 [==============================] - 17s 23ms/step - loss: 0.0913
6. set (Dataset 8) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:04:39.594230!
Epoch 1/1
772/772 [==============================] - 18s 23ms/step - loss: 0.0877
7. set (Dataset 4) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:05:04.955005!
Epoch 1/1
744/744 [==============================] - 17s 23ms/step - loss: 0.1056
8. set (Dataset 2) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:05:27.581774!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1050
9. set (Dataset 12) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:05:47.020347!
Epoch 1/1
732/732 [==============================] - 17s 23ms/step - loss: 0.0789
10. set (Dataset 13) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:06:08.910188!
Epoch 1/1
485/485 [==============================] - 11s 22ms/step - loss: 0.0638
11. set (Dataset 19) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:06:24.782824!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0834
12. set (Dataset 24) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:06:41.408428!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0628
13. set (Dataset 10) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:07:00.339558!
Epoch 1/1
726/726 [==============================] - 17s 24ms/step - loss: 0.0906
14. set (Dataset 21) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:07:23.774321!
Epoch 1/1
634/634 [==============================] - 15s 23ms/step - loss: 0.1337
15. set (Dataset 22) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:07:45.000796!
Epoch 1/1
665/665 [==============================] - 15s 23ms/step - loss: 0.0612
16. set (Dataset 20) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:08:05.771661!
Epoch 1/1
556/556 [==============================] - 12s 22ms/step - loss: 0.0704
17. set (Dataset 15) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:08:24.366460!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0848
18. set (Dataset 16) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:08:48.801214!
Epoch 1/1
914/914 [==============================] - 20s 22ms/step - loss: 0.0809
19. set (Dataset 23) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:09:14.683721!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1154
20. set (Dataset 18) being trained for epoch 5 in Experiment 8 by 2019-01-30 08:09:34.251722!
Epoch 1/1
614/614 [==============================] - 13s 22ms/step - loss: 0.0996
Epoch 5 for Experiment 8 completed!
Exp2019-01-30_03-25-16_part8.h5 has been saved.
The subjects are trained: [(11, 'M05'), (17, 'M10'), (6, 'F06'), (1, 'F01'), (7, 'M01'), (8, 'M02'), (4, 'F04'), (2, 'F0
2'), (12, 'M06'), (13, 'M07'), (19, 'M11'), (24, 'M14'), (10, 'M04'), (21, 'F02'), (22, 'M01'), (20, 'M12'), (15, 'F03')
, (16, 'M09'), (23, 'M13'), (18, 'F05')]
Evaluating model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-30 08:09:49.994637
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.13 Degree
        The absolute mean error on Yaw angle estimation: 23.28 Degree
        The absolute mean error on Roll angle estimation: 13.14 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.47 Degree
        The absolute mean error on Yaw angle estimation: 24.86 Degree
        The absolute mean error on Roll angle estimation: 5.02 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 29.58 Degree
        The absolute mean error on Yaw angle estimation: 15.11 Degree
        The absolute mean error on Roll angle estimation: 6.36 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 14.41 Degree
        The absolute mean error on Yaw angle estimation: 29.84 Degree
        The absolute mean error on Roll angle estimation: 11.93 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.40 Degree
        The absolute mean error on Yaw angle estimations: 23.27 Degree
        The absolute mean error on Roll angle estimations: 9.11 Degree
Exp2019-01-30_03-25-16_part8 completed!
Exp2019-01-30_03-25-16.h5 has been saved.
The subjects are trained: [(11, 'M05'), (17, 'M10'), (6, 'F06'), (1, 'F01'), (7, 'M01'), (8, 'M02'), (4, 'F04'), (2, 'F0
2'), (12, 'M06'), (13, 'M07'), (19, 'M11'), (24, 'M14'), (10, 'M04'), (21, 'F02'), (22, 'M01'), (20, 'M12'), (15, 'F03')
, (16, 'M09'), (23, 'M13'), (18, 'F05')]
Evaluating model Exp2019-01-29_13-03-34_part6_and_2019-01-30_03-25-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-30 08:11:19.367720
For the Subject 3 (F03):
730/730 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 9.13 Degree
        The absolute mean error on Yaw angle estimation: 23.28 Degree
        The absolute mean error on Roll angle estimation: 13.14 Degree
For the Subject 5 (F05):
946/946 [==============================] - 15s 16ms/step
        The absolute mean error on Pitch angle estimation: 8.47 Degree
        The absolute mean error on Yaw angle estimation: 24.86 Degree
        The absolute mean error on Roll angle estimation: 5.02 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 16ms/step
        The absolute mean error on Pitch angle estimation: 29.58 Degree
        The absolute mean error on Yaw angle estimation: 15.11 Degree
        The absolute mean error on Roll angle estimation: 6.36 Degree
For the Subject 14 (M08):
797/797 [==============================] - 13s 16ms/step
        The absolute mean error on Pitch angle estimation: 14.41 Degree
        The absolute mean error on Yaw angle estimation: 29.84 Degree
        The absolute mean error on Roll angle estimation: 11.93 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.40 Degree
        The absolute mean error on Yaw angle estimations: 23.27 Degree
        The absolute mean error on Roll angle estimations: 9.11 Degree
subject3_Exp2019-01-30_03-25-16.png has been saved by 2019-01-30 08:12:47.225708.
subject5_Exp2019-01-30_03-25-16.png has been saved by 2019-01-30 08:12:47.423035.
subject9_Exp2019-01-30_03-25-16.png has been saved by 2019-01-30 08:12:47.620516.
subject14_Exp2019-01-30_03-25-16.png has been saved by 2019-01-30 08:12:47.843804.
Model Exp2019-01-30_03-25-16 has been evaluated successfully.
Model Exp2019-01-30_03-25-16 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 15:05:50.966333: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 15:05:51.063841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 15:05:51.064100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 15:05:51.064111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 15:05:51.218448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 15:05:51.218473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 15:05:51.218478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 15:05:51.218621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10453 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_15-05-51 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
_________________________________________________________________
fc3 (Dense)                  (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (None, 16, 3)             138458947
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                1920
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 6
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15-0
5-51
All frames and annotations from 20 datasets have been read by 2019-01-31 15:05:56.204630
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:06:02.792447!
Epoch 1/1
649/649 [==============================] - 77s 119ms/step - loss: 0.3628
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:07:25.390131!
Epoch 1/1
476/476 [==============================] - 57s 119ms/step - loss: 0.2141
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:08:28.335102!
Epoch 1/1
638/638 [==============================] - 75s 118ms/step - loss: 0.2442
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:09:48.639869!
Epoch 1/1
486/486 [==============================] - 58s 119ms/step - loss: 0.2081
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:10:54.477076!
Epoch 1/1
756/756 [==============================] - 89s 118ms/step - loss: 0.3007
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:12:29.342484!
Epoch 1/1
553/553 [==============================] - 66s 120ms/step - loss: 0.2855
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:13:41.550609!
Epoch 1/1
618/618 [==============================] - 73s 119ms/step - loss: 0.2653
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:15:03.598875!
Epoch 1/1
898/898 [==============================] - 108s 120ms/step - loss: 0.2105
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:16:58.820810!
Epoch 1/1
729/729 [==============================] - 87s 120ms/step - loss: 0.3225
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:18:33.359181!
Epoch 1/1
716/716 [==============================] - 86s 119ms/step - loss: 0.2595
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:20:06.157392!
Epoch 1/1
710/710 [==============================] - 84s 118ms/step - loss: 0.2504
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:21:35.260864!
Epoch 1/1
482/482 [==============================] - 58s 120ms/step - loss: 0.2802
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:22:38.865195!
Epoch 1/1
598/598 [==============================] - 71s 119ms/step - loss: 0.2761
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:23:55.282000!
Epoch 1/1
495/495 [==============================] - 59s 120ms/step - loss: 0.3002
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:25:02.003557!
Epoch 1/1
728/728 [==============================] - 86s 118ms/step - loss: 0.2821
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:26:33.583010!
Epoch 1/1
540/540 [==============================] - 65s 120ms/step - loss: 0.2044
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:27:41.905580!
Epoch 1/1
379/379 [==============================] - 45s 119ms/step - loss: 0.1713
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:28:32.395619!
Epoch 1/1
526/526 [==============================] - 63s 119ms/step - loss: 0.2506
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:29:39.876710!
Epoch 1/1
469/469 [==============================] - 56s 118ms/step - loss: 0.1972
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-31 15:30:41.170675!
Epoch 1/1
556/556 [==============================] - 67s 121ms/step - loss: 0.1856
Epoch 1 for Experiment 1 completed!
Exp2019-01-31_15-05-51_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15
-05-51
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 15:31:50.504414
For the Subject 3 (F03):
730/730 [==============================] - 67s 92ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 18.85 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 30.61 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 9.30 Degree
For the Subject 5 (F05):
946/946 [==============================] - 88s 93ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 23.20 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 29.76 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 5.49 Degree
For the Subject 9 (M03):
882/882 [==============================] - 83s 94ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 16.92 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 25.46 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 9.44 Degree
For the Subject 14 (M08):
797/797 [==============================] - 74s 93ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 19.93 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 34.56 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 14.50 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.72 Degree
        The absolute mean error on Yaw angle estimations: 30.10 Degree
        The absolute mean error on Roll angle estimations: 9.68 Degree
Exp2019-01-31_15-05-51_part1 completed!
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15-0
5-51
All frames and annotations from 20 datasets have been read by 2019-01-31 15:37:41.563328
1. set (Dataset 6) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:37:46.768427!
Epoch 1/1
526/526 [==============================] - 63s 119ms/step - loss: 0.2497
2. set (Dataset 11) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:38:55.210181!
Epoch 1/1
556/556 [==============================] - 66s 119ms/step - loss: 0.1709
3. set (Dataset 10) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:40:08.688368!
Epoch 1/1
710/710 [==============================] - 85s 120ms/step - loss: 0.2122
4. set (Dataset 4) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:41:41.351346!
Epoch 1/1
728/728 [==============================] - 86s 119ms/step - loss: 0.2599
5. set (Dataset 23) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:43:13.264133!
Epoch 1/1
553/553 [==============================] - 66s 120ms/step - loss: 0.2581
6. set (Dataset 13) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:44:24.551746!
Epoch 1/1
469/469 [==============================] - 56s 119ms/step - loss: 0.1755
7. set (Dataset 17) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:45:24.336505!
Epoch 1/1
379/379 [==============================] - 46s 120ms/step - loss: 0.1610
8. set (Dataset 1) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:46:14.982019!
Epoch 1/1
482/482 [==============================] - 58s 120ms/step - loss: 0.2514
9. set (Dataset 8) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:47:20.660765!
Epoch 1/1
756/756 [==============================] - 91s 120ms/step - loss: 0.2220
10. set (Dataset 7) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:48:58.903778!
Epoch 1/1
729/729 [==============================] - 87s 119ms/step - loss: 0.2302
11. set (Dataset 21) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:50:31.522533!
Epoch 1/1
618/618 [==============================] - 74s 120ms/step - loss: 0.2434
12. set (Dataset 22) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:51:52.251813!
Epoch 1/1
649/649 [==============================] - 77s 119ms/step - loss: 0.1486
13. set (Dataset 2) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:53:14.424713!
Epoch 1/1
495/495 [==============================] - 59s 120ms/step - loss: 0.2440
14. set (Dataset 24) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:54:18.455686!
Epoch 1/1
476/476 [==============================] - 57s 120ms/step - loss: 0.1509
15. set (Dataset 15) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:55:22.129504!
Epoch 1/1
638/638 [==============================] - 76s 120ms/step - loss: 0.1903
16. set (Dataset 20) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:56:43.878140!
Epoch 1/1
540/540 [==============================] - 65s 120ms/step - loss: 0.1523
17. set (Dataset 18) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:57:54.598968!
Epoch 1/1
598/598 [==============================] - 72s 120ms/step - loss: 0.2071
18. set (Dataset 19) being trained for epoch 1 in Experiment 2 by 2019-01-31 15:59:11.279742!
Epoch 1/1
486/486 [==============================] - 57s 118ms/step - loss: 0.1511
19. set (Dataset 12) being trained for epoch 1 in Experiment 2 by 2019-01-31 16:00:15.988396!
Epoch 1/1
716/716 [==============================] - 86s 120ms/step - loss: 0.1658
20. set (Dataset 16) being trained for epoch 1 in Experiment 2 by 2019-01-31 16:01:50.887204!
Epoch 1/1
898/898 [==============================] - 107s 119ms/step - loss: 0.1543
Epoch 1 for Experiment 2 completed!
Exp2019-01-31_15-05-51_part2.h5 has been saved.
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (23, 'M13'), (13, 'M07'), (17, 'M10'), (1,
'F01'), (8, 'M02'), (7, 'M01'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'), (20, 'M12'), (18, 'F05')
, (19, 'M11'), (12, 'M06'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15
-05-51
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 16:03:40.013221
For the Subject 3 (F03):
730/730 [==============================] - 67s 92ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 16.58 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 31.16 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 14.95 Degree
For the Subject 5 (F05):
946/946 [==============================] - 89s 94ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 13.81 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 28.89 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 3.82 Degree
For the Subject 9 (M03):
882/882 [==============================] - 82s 93ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 16.36 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 27.09 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 7.11 Degree
For the Subject 14 (M08):
797/797 [==============================] - 75s 94ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 15.42 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 33.21 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 13.90 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.54 Degree
        The absolute mean error on Yaw angle estimations: 30.09 Degree
        The absolute mean error on Roll angle estimations: 9.95 Degree
Exp2019-01-31_15-05-51_part2 completed!
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15-0
5-51
All frames and annotations from 20 datasets have been read by 2019-01-31 16:09:31.004987
1. set (Dataset 19) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:09:35.911009!
Epoch 1/1
486/486 [==============================] - 58s 120ms/step - loss: 0.1304
2. set (Dataset 16) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:10:42.789025!
Epoch 1/1
898/898 [==============================] - 108s 120ms/step - loss: 0.1329
3. set (Dataset 21) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:12:36.972678!
Epoch 1/1
618/618 [==============================] - 74s 120ms/step - loss: 0.2074
4. set (Dataset 15) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:13:57.751330!
Epoch 1/1
638/638 [==============================] - 77s 120ms/step - loss: 0.1460
5. set (Dataset 13) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:15:19.315666!
Epoch 1/1
469/469 [==============================] - 56s 120ms/step - loss: 0.1429
6. set (Dataset 12) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:16:22.797813!
Epoch 1/1
716/716 [==============================] - 86s 120ms/step - loss: 0.1534
7. set (Dataset 18) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:17:54.708681!
Epoch 1/1
598/598 [==============================] - 72s 120ms/step - loss: 0.1809
8. set (Dataset 22) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:19:12.946730!
Epoch 1/1
649/649 [==============================] - 77s 118ms/step - loss: 0.1172
9. set (Dataset 23) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:20:34.972287!
Epoch 1/1
553/553 [==============================] - 67s 120ms/step - loss: 0.1852
10. set (Dataset 8) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:21:49.345322!
Epoch 1/1
756/756 [==============================] - 90s 119ms/step - loss: 0.1852
11. set (Dataset 17) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:23:22.894190!
Epoch 1/1
379/379 [==============================] - 45s 120ms/step - loss: 0.1379
12. set (Dataset 6) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:24:13.485352!
Epoch 1/1
526/526 [==============================] - 62s 118ms/step - loss: 0.2020
13. set (Dataset 24) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:25:20.449976!
Epoch 1/1
476/476 [==============================] - 57s 120ms/step - loss: 0.1087
14. set (Dataset 11) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:26:23.242354!
Epoch 1/1
556/556 [==============================] - 67s 120ms/step - loss: 0.1420
15. set (Dataset 10) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:27:37.080752!
Epoch 1/1
710/710 [==============================] - 85s 120ms/step - loss: 0.1539
16. set (Dataset 20) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:29:07.655139!
Epoch 1/1
540/540 [==============================] - 64s 119ms/step - loss: 0.1213
17. set (Dataset 2) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:30:16.790574!
Epoch 1/1
495/495 [==============================] - 60s 120ms/step - loss: 0.1843
18. set (Dataset 4) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:31:23.786816!
Epoch 1/1
728/728 [==============================] - 87s 119ms/step - loss: 0.1689
19. set (Dataset 7) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:32:58.046886!
Epoch 1/1
729/729 [==============================] - 88s 120ms/step - loss: 0.1551
20. set (Dataset 1) being trained for epoch 1 in Experiment 3 by 2019-01-31 16:34:30.728429!
Epoch 1/1
482/482 [==============================] - 58s 119ms/step - loss: 0.1994
Epoch 1 for Experiment 3 completed!
Exp2019-01-31_15-05-51_part3.h5 has been saved.
The subjects are trained: [(19, 'M11'), (16, 'M09'), (21, 'F02'), (15, 'F03'), (13, 'M07'), (12, 'M06'), (18, 'F05'), (2
2, 'M01'), (23, 'M13'), (8, 'M02'), (17, 'M10'), (6, 'F06'), (24, 'M14'), (11, 'M05'), (10, 'M04'), (20, 'M12'), (2, 'F0
2'), (4, 'F04'), (7, 'M01'), (1, 'F01')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15
-05-51
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 16:35:30.548975
For the Subject 3 (F03):
730/730 [==============================] - 67s 91ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 14.71 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 49.91 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 11.11 Degree
For the Subject 5 (F05):
946/946 [==============================] - 88s 93ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 9.47 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 25.24 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 4.12 Degree
For the Subject 9 (M03):
882/882 [==============================] - 81s 92ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 18.67 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 32.56 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 7.35 Degree
For the Subject 14 (M08):
797/797 [==============================] - 75s 94ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 13.99 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 44.61 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 13.57 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.21 Degree
        The absolute mean error on Yaw angle estimations: 38.08 Degree
        The absolute mean error on Roll angle estimations: 9.04 Degree
Exp2019-01-31_15-05-51_part3 completed!
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15-0
5-51
All frames and annotations from 20 datasets have been read by 2019-01-31 16:41:19.280443
1. set (Dataset 4) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:41:26.700171!
Epoch 1/1
728/728 [==============================] - 86s 118ms/step - loss: 0.1400
2. set (Dataset 1) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:42:58.002243!
Epoch 1/1
482/482 [==============================] - 58s 120ms/step - loss: 0.1802
3. set (Dataset 17) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:43:59.615905!
Epoch 1/1
379/379 [==============================] - 45s 118ms/step - loss: 0.1362
4. set (Dataset 10) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:44:51.664222!
Epoch 1/1
710/710 [==============================] - 84s 118ms/step - loss: 0.1287
5. set (Dataset 12) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:46:23.051931!
Epoch 1/1
716/716 [==============================] - 86s 120ms/step - loss: 0.1211
6. set (Dataset 7) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:47:56.596440!
Epoch 1/1
729/729 [==============================] - 88s 120ms/step - loss: 0.1242
7. set (Dataset 2) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:49:29.271421!
Epoch 1/1
495/495 [==============================] - 59s 119ms/step - loss: 0.1428
8. set (Dataset 6) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:50:33.535727!
Epoch 1/1
526/526 [==============================] - 62s 119ms/step - loss: 0.1996
9. set (Dataset 13) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:51:40.737802!
Epoch 1/1
469/469 [==============================] - 55s 118ms/step - loss: 0.1100
10. set (Dataset 23) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:52:41.696121!
Epoch 1/1
553/553 [==============================] - 66s 119ms/step - loss: 0.1791
11. set (Dataset 18) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:53:53.220600!
Epoch 1/1
598/598 [==============================] - 71s 119ms/step - loss: 0.1717
12. set (Dataset 19) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:55:09.464059!
Epoch 1/1
486/486 [==============================] - 58s 119ms/step - loss: 0.1226
13. set (Dataset 11) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:56:12.845582!
Epoch 1/1
556/556 [==============================] - 67s 120ms/step - loss: 0.0985
14. set (Dataset 16) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:57:28.259800!
Epoch 1/1
898/898 [==============================] - 107s 119ms/step - loss: 0.1272
15. set (Dataset 21) being trained for epoch 1 in Experiment 4 by 2019-01-31 16:59:20.894353!
Epoch 1/1
618/618 [==============================] - 74s 120ms/step - loss: 0.1865
16. set (Dataset 20) being trained for epoch 1 in Experiment 4 by 2019-01-31 17:00:40.228298!
Epoch 1/1
540/540 [==============================] - 65s 120ms/step - loss: 0.1093
17. set (Dataset 24) being trained for epoch 1 in Experiment 4 by 2019-01-31 17:01:49.891447!
Epoch 1/1
476/476 [==============================] - 56s 119ms/step - loss: 0.0995
18. set (Dataset 15) being trained for epoch 1 in Experiment 4 by 2019-01-31 17:02:52.734879!
Epoch 1/1
638/638 [==============================] - 76s 119ms/step - loss: 0.1254
19. set (Dataset 8) being trained for epoch 1 in Experiment 4 by 2019-01-31 17:04:16.687143!
Epoch 1/1
756/756 [==============================] - 90s 119ms/step - loss: 0.1476
20. set (Dataset 22) being trained for epoch 1 in Experiment 4 by 2019-01-31 17:05:52.728725!
Epoch 1/1
649/649 [==============================] - 78s 120ms/step - loss: 0.0968
Epoch 1 for Experiment 4 completed!
Exp2019-01-31_15-05-51_part4.h5 has been saved.
The subjects are trained: [(4, 'F04'), (1, 'F01'), (17, 'M10'), (10, 'M04'), (12, 'M06'), (7, 'M01'), (2, 'F02'), (6, 'F
06'), (13, 'M07'), (23, 'M13'), (18, 'F05'), (19, 'M11'), (11, 'M05'), (16, 'M09'), (21, 'F02'), (20, 'M12'), (24, 'M14'
), (15, 'F03'), (8, 'M02'), (22, 'M01')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15
-05-51
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 17:07:13.081337
For the Subject 3 (F03):
730/730 [==============================] - 67s 91ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 9.52 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 40.33 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 17.39 Degree
For the Subject 5 (F05):
946/946 [==============================] - 88s 93ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 10.03 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 25.22 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 4.99 Degree
For the Subject 9 (M03):
882/882 [==============================] - 83s 94ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 26.69 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 29.38 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 7.54 Degree
For the Subject 14 (M08):
797/797 [==============================] - 74s 93ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 20.93 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 42.74 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 14.02 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.79 Degree
        The absolute mean error on Yaw angle estimations: 34.41 Degree
        The absolute mean error on Roll angle estimations: 10.99 Degree
Exp2019-01-31_15-05-51_part4 completed!
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15-0
5-51
All frames and annotations from 20 datasets have been read by 2019-01-31 17:13:02.877212
1. set (Dataset 15) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:13:09.257328!
Epoch 1/1
638/638 [==============================] - 77s 120ms/step - loss: 0.1170
2. set (Dataset 22) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:14:32.230747!
Epoch 1/1
649/649 [==============================] - 78s 120ms/step - loss: 0.0936
3. set (Dataset 18) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:15:56.014683!
Epoch 1/1
598/598 [==============================] - 72s 120ms/step - loss: 0.1458
4. set (Dataset 21) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:17:14.056664!
Epoch 1/1
618/618 [==============================] - 74s 120ms/step - loss: 0.1710
5. set (Dataset 7) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:18:35.877877!
Epoch 1/1
729/729 [==============================] - 86s 118ms/step - loss: 0.1269
6. set (Dataset 8) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:20:09.844072!
Epoch 1/1
756/756 [==============================] - 90s 120ms/step - loss: 0.1247
7. set (Dataset 24) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:21:44.975438!
Epoch 1/1
476/476 [==============================] - 57s 121ms/step - loss: 0.1104
8. set (Dataset 19) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:22:47.259691!
Epoch 1/1
486/486 [==============================] - 58s 119ms/step - loss: 0.1203
9. set (Dataset 12) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:23:52.235249!
Epoch 1/1
716/716 [==============================] - 84s 118ms/step - loss: 0.1126
10. set (Dataset 13) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:25:21.416841!
Epoch 1/1
469/469 [==============================] - 56s 119ms/step - loss: 0.0980
11. set (Dataset 2) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:26:22.491301!
Epoch 1/1
495/495 [==============================] - 59s 120ms/step - loss: 0.1252
12. set (Dataset 4) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:27:29.406787!
Epoch 1/1
728/728 [==============================] - 87s 120ms/step - loss: 0.1246
13. set (Dataset 16) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:29:05.647280!
Epoch 1/1
898/898 [==============================] - 108s 120ms/step - loss: 0.1195
14. set (Dataset 1) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:30:58.747174!
Epoch 1/1
482/482 [==============================] - 58s 120ms/step - loss: 0.1633
15. set (Dataset 17) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:32:00.408784!
Epoch 1/1
379/379 [==============================] - 45s 120ms/step - loss: 0.1279
16. set (Dataset 20) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:32:51.213708!
Epoch 1/1
540/540 [==============================] - 65s 120ms/step - loss: 0.1140
17. set (Dataset 11) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:34:01.832228!
Epoch 1/1
556/556 [==============================] - 66s 119ms/step - loss: 0.0870
18. set (Dataset 10) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:35:15.503673!
Epoch 1/1
710/710 [==============================] - 85s 120ms/step - loss: 0.1158
19. set (Dataset 23) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:36:46.358682!
Epoch 1/1
553/553 [==============================] - 66s 120ms/step - loss: 0.1608
20. set (Dataset 6) being trained for epoch 1 in Experiment 5 by 2019-01-31 17:37:57.880497!
Epoch 1/1
526/526 [==============================] - 63s 120ms/step - loss: 0.1705
Epoch 1 for Experiment 5 completed!
Exp2019-01-31_15-05-51_part5.h5 has been saved.
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24, 'M14'), (19,
 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'), (20, 'M12'), (11, 'M05'
), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15
-05-51
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 17:39:03.383396
For the Subject 3 (F03):
730/730 [==============================] - 67s 92ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 12.43 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 38.02 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 16.68 Degree
For the Subject 5 (F05):
946/946 [==============================] - 89s 94ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 13.35 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 28.63 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 3.87 Degree
For the Subject 9 (M03):
882/882 [==============================] - 83s 94ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 19.55 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 43.72 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 7.31 Degree
For the Subject 14 (M08):
797/797 [==============================] - 75s 94ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 15.95 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 46.14 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 13.92 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.32 Degree
        The absolute mean error on Yaw angle estimations: 39.13 Degree
        The absolute mean error on Roll angle estimations: 10.44 Degree
Exp2019-01-31_15-05-51_part5 completed!
Training model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15-0
5-51
All frames and annotations from 20 datasets have been read by 2019-01-31 17:44:55.392496
1. set (Dataset 10) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:45:02.680232!
Epoch 1/1
710/710 [==============================] - 85s 120ms/step - loss: 0.1104
2. set (Dataset 6) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:46:33.196931!
Epoch 1/1
526/526 [==============================] - 63s 120ms/step - loss: 0.1467
3. set (Dataset 2) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:47:41.657379!
Epoch 1/1
495/495 [==============================] - 59s 118ms/step - loss: 0.1119
4. set (Dataset 17) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:48:43.998605!
Epoch 1/1
379/379 [==============================] - 45s 119ms/step - loss: 0.1234
5. set (Dataset 8) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:49:36.686183!
Epoch 1/1
756/756 [==============================] - 90s 119ms/step - loss: 0.1102
6. set (Dataset 23) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:51:11.798793!
Epoch 1/1
553/553 [==============================] - 66s 120ms/step - loss: 0.1511
7. set (Dataset 11) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:52:23.957468!
Epoch 1/1
556/556 [==============================] - 67s 120ms/step - loss: 0.0894
8. set (Dataset 4) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:53:38.273381!
Epoch 1/1
728/728 [==============================] - 86s 118ms/step - loss: 0.1173
9. set (Dataset 7) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:55:12.041841!
Epoch 1/1
729/729 [==============================] - 88s 120ms/step - loss: 0.1021
10. set (Dataset 12) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:56:46.984963!
Epoch 1/1
716/716 [==============================] - 85s 119ms/step - loss: 0.0981
11. set (Dataset 24) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:58:16.713253!
Epoch 1/1
476/476 [==============================] - 57s 120ms/step - loss: 0.1035
12. set (Dataset 15) being trained for epoch 1 in Experiment 6 by 2019-01-31 17:59:20.451498!
Epoch 1/1
638/638 [==============================] - 77s 120ms/step - loss: 0.1161
13. set (Dataset 1) being trained for epoch 1 in Experiment 6 by 2019-01-31 18:00:42.212575!
Epoch 1/1
482/482 [==============================] - 58s 120ms/step - loss: 0.1469
14. set (Dataset 22) being trained for epoch 1 in Experiment 6 by 2019-01-31 18:01:46.296533!
Epoch 1/1
649/649 [==============================] - 78s 120ms/step - loss: 0.0900
15. set (Dataset 18) being trained for epoch 1 in Experiment 6 by 2019-01-31 18:03:10.183593!
Epoch 1/1
598/598 [==============================] - 72s 120ms/step - loss: 0.1385
16. set (Dataset 20) being trained for epoch 1 in Experiment 6 by 2019-01-31 18:04:27.130570!
Epoch 1/1
540/540 [==============================] - 65s 120ms/step - loss: 0.1047
17. set (Dataset 16) being trained for epoch 1 in Experiment 6 by 2019-01-31 18:05:40.603007!
Epoch 1/1
898/898 [==============================] - 107s 119ms/step - loss: 0.1039
18. set (Dataset 21) being trained for epoch 1 in Experiment 6 by 2019-01-31 18:07:33.353768!
Epoch 1/1
618/618 [==============================] - 74s 119ms/step - loss: 0.1644
19. set (Dataset 13) being trained for epoch 1 in Experiment 6 by 2019-01-31 18:08:52.060560!
Epoch 1/1
469/469 [==============================] - 56s 120ms/step - loss: 0.0925
20. set (Dataset 19) being trained for epoch 1 in Experiment 6 by 2019-01-31 18:09:53.358976!
Epoch 1/1
486/486 [==============================] - 58s 119ms/step - loss: 0.1094
Epoch 1 for Experiment 6 completed!
Exp2019-01-31_15-05-51_part6.h5 has been saved.
The subjects are trained: [(10, 'M04'), (6, 'F06'), (2, 'F02'), (17, 'M10'), (8, 'M02'), (23, 'M13'), (11, 'M05'), (4, '
F04'), (7, 'M01'), (12, 'M06'), (24, 'M14'), (15, 'F03'), (1, 'F01'), (22, 'M01'), (18, 'F05'), (20, 'M12'), (16, 'M09')
, (21, 'F02'), (13, 'M07'), (19, 'M11')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_BatchSize1_inEpochs1_outEpochs6_AdamOpt_lr-0.000100_2019-01-31_15
-05-51
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 18:10:53.685747
For the Subject 3 (F03):
730/730 [==============================] - 67s 92ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 13.34 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 33.14 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 18.12 Degree
For the Subject 5 (F05):
946/946 [==============================] - 88s 93ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 14.35 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 27.69 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 4.01 Degree
For the Subject 9 (M03):
882/882 [==============================] - 83s 94ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 23.11 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 29.79 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 7.14 Degree
For the Subject 14 (M08):
797/797 [==============================] - 75s 94ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 16.26 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 39.81 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 13.77 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.76 Degree
        The absolute mean error on Yaw angle estimations: 32.61 Degree
        The absolute mean error on Roll angle estimations: 10.76 Degree
Exp2019-01-31_15-05-51_part6 completed!
Exp2019-01-31_15-05-51.h5 has been saved.
subject3_Exp2019-01-31_15-05-51.png has been saved by 2019-01-31 18:16:41.294945.
subject5_Exp2019-01-31_15-05-51.png has been saved by 2019-01-31 18:16:41.493584.
subject9_Exp2019-01-31_15-05-51.png has been saved by 2019-01-31 18:16:41.692791.
subject14_Exp2019-01-31_15-05-51.png has been saved by 2019-01-31 18:16:41.910650.
Model Exp2019-01-31_15-05-51 has been evaluated successfully.
Model Exp2019-01-31_15-05-51 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "trying tanh at fc1
024 in Exp2019-01-31_15-05-51"
[master 04bff95] trying tanh at fc1024 in Exp2019-01-31_15-05-51
 49 files changed, 5141 insertions(+), 992 deletions(-)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_22-52-23/output_Exp2019-01-29_22-52-23.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_22-52-23/subject14_Exp2019-01-29_22-52-23.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_22-52-23/subject3_Exp2019-01-29_22-52-23.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_22-52-23/subject5_Exp2019-01-29_22-52-23.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_22-52-23/subject9_Exp2019-01-29_22-52-23.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-20-35/output_Exp2019-01-29_23-20-35.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-20-35/subject14_Exp2019-01-29_23-20-35.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-20-35/subject3_Exp2019-01-29_23-20-35.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-20-35/subject5_Exp2019-01-29_23-20-35.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-20-35/subject9_Exp2019-01-29_23-20-35.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-30-18/output_Exp2019-01-29_23-30-18.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-30-18/subject14_Exp2019-01-29_23-30-18.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-30-18/subject3_Exp2019-01-29_23-30-18.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-30-18/subject5_Exp2019-01-29_23-30-18.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-30-18/subject9_Exp2019-01-29_23-30-18.png
 rename DeepRL_For_HPE/FC_RNN_Evaluater/results/{Last_Model_____/output_Last_Model.txt => Exp2019-01-29_23-41-01/output_
Exp2019-01-29_23-41-01.txt} (70%)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-41-01/subject14_Exp2019-01-29_23-41-01.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-41-01/subject3_Exp2019-01-29_23-41-01.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-41-01/subject5_Exp2019-01-29_23-41-01.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-29_23-41-01/subject9_Exp2019-01-29_23-41-01.png
 rename DeepRL_For_HPE/FC_RNN_Evaluater/results/{Last_Model____/output_Last_Model.txt => Exp2019-01-30_00-01-37/output_E
xp2019-01-30_00-01-37.txt} (69%)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-30_00-01-37/subject14_Exp2019-01-30_00-01-37.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-30_00-01-37/subject3_Exp2019-01-30_00-01-37.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-30_00-01-37/subject5_Exp2019-01-30_00-01-37.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-30_00-01-37/subject9_Exp2019-01-30_00-01-37.png
 rename DeepRL_For_HPE/FC_RNN_Evaluater/results/{Last_Model_/output_Last_Model.txt => Exp2019-01-30_03-22-57_part1/outpu
t_Exp2019-01-30_03-22-57_part1.txt} (62%)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-30_03-25-16/output_Exp2019-01-30_03-25-16.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-30_03-25-16/subject14_Exp2019-01-30_03-25-16.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-30_03-25-16/subject3_Exp2019-01-30_03-25-16.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-30_03-25-16/subject5_Exp2019-01-30_03-25-16.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-30_03-25-16/subject9_Exp2019-01-30_03-25-16.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-31_15-05-51/output_Exp2019-01-31_15-05-51.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-31_15-05-51/scrollback_Exp2019-01-31_15-05-51.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-31_15-05-51/subject14_Exp2019-01-31_15-05-51.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-31_15-05-51/subject3_Exp2019-01-31_15-05-51.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-31_15-05-51/subject5_Exp2019-01-31_15-05-51.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-31_15-05-51/subject9_Exp2019-01-31_15-05-51.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model__/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model___/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model______/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_______/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model__________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/untitled
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git pull
remote: Counting objects: 21, done.
remote: Compressing objects: 100% (21/21), done.
remote: Total 21 (delta 14), reused 0 (delta 0)
Unpacking objects: 100% (21/21), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   3a0f6f0..72a238b  master     -> origin/master
Auto-merging DeepRL_For_HPE/FC_RNN_Evaluater/runFC_RNN_Experiment.py
CONFLICT (content): Merge conflict in DeepRL_For_HPE/FC_RNN_Evaluater/runFC_RNN_Experiment.py
Auto-merging DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py
Automatic merge failed; fix conflicts and then commit the result.
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git pull -s recursive -X theirs
error: Pull is not possible because you have unmerged files.
hint: Fix them up in the work tree, and then use 'git add/rm <file>'
hint: as appropriate to mark resolution and make a commit.
fatal: Exiting because of an unresolved conflict.
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git pull -s recursive -X theirs
error: Pull is not possible because you have unmerged files.
hint: Fix them up in the work tree, and then use 'git add/rm <file>'
hint: as appropriate to mark resolution and make a commit.
fatal: Exiting because of an unresolved conflict.
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "fixing conflicts"
[master a12a505] fixing conflicts
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git pull -s recursive -X theirs
Already up-to-date.
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git pull -s recursive -X theirs
remote: Counting objects: 5, done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 5 (delta 3), reused 0 (delta 0)
Unpacking objects: 100% (5/5), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   72a238b..a3475bb  master     -> origin/master
Merge made by the 'recursive' strategy.
 DeepRL_For_HPE/Note_Files/commands.txt | 11 +----------
 1 file changed, 1 insertion(+), 10 deletions(-)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
warning: CRLF will be replaced by LF in DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py.
The file will have its original line endings in your working directory.
warning: CRLF will be replaced by LF in DeepRL_For_HPE/FC_RNN_Evaluater/runFC_RNN_Experiment.py.
The file will have its original line endings in your working directory.
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "take theirs"
[master b183111] take theirs
warning: CRLF will be replaced by LF in DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py.
The file will have its original line endings in your working directory.
warning: CRLF will be replaced by LF in DeepRL_For_HPE/FC_RNN_Evaluater/runFC_RNN_Experiment.py.
The file will have its original line endings in your working directory.
 2 files changed, 204 insertions(+)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/runFC_RNN_Experiment.py
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 65, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (65/65), done.
Writing objects: 100% (65/65), 4.43 MiB | 963.00 KiB/s, done.
Total 65 (delta 22), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   a3475bb..b183111  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ cd ..
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE$ cd CNN_Evaluater/
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File "runCNN_Experiment.py", line 8, in <module>
    from CNN_Evaluater import *
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater/CNN_Evaluater.py", line 4, in <modu
le>
    from CNN_Configuration import *
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater/CNN_Configuration.py", line 22
    out_epochs  1
                ^
SyntaxError: invalid syntax
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 21:59:11.136599: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 21:59:11.234289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 21:59:11.234597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 21:59:11.234612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 21:59:11.388909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 21:59:11.388935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 21:59:11.388940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 21:59:11.389117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_21-59-11 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
dropout3_025 (Dropout)       (None, 4096)              0
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 10
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #  [9, 1] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 500
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_output3_BatchSize10_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_21-59-11
Epoch 1/1
1232/1232 [==============================] - 198s 160ms/step - loss: 0.6310
The subjects are trained: [(2, 'F02'), (24, 'M14'), (19, 'M11'), (21, 'F02'), (11, 'M05'), (1, 'F01'), (20, 'M12'), (6,
'F06'), (15, 'F03'), (12, 'M06'), (4, 'F04'), (17, 'M10'), (10, 'M04'), (8, 'M02'), (23, 'M13'), (16, 'M09'), (22, 'M01'
), (13, 'M07'), (18, 'F05'), (7, 'M01')]
Evaluating model VGG16_inc_top_output3_BatchSize10_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_21-59-11
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 22:04:36.307914
For the Subject 3 (F03):
730/730 [==============================] - 9s 12ms/step
        The absolute mean error on Pitch angle estimation: 44.23 Degree
        The absolute mean error on Yaw angle estimation: 41.02 Degree
        The absolute mean error on Roll angle estimation: 6.38 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 12ms/step
        The absolute mean error on Pitch angle estimation: 21.33 Degree
        The absolute mean error on Yaw angle estimation: 30.16 Degree
        The absolute mean error on Roll angle estimation: 7.36 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 19.22 Degree
        The absolute mean error on Yaw angle estimation: 42.30 Degree
        The absolute mean error on Roll angle estimation: 12.91 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 12ms/step
        The absolute mean error on Pitch angle estimation: 57.38 Degree
        The absolute mean error on Yaw angle estimation: 37.33 Degree
        The absolute mean error on Roll angle estimation: 29.07 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 35.54 Degree
        The absolute mean error on Yaw angle estimations: 37.70 Degree
        The absolute mean error on Roll angle estimations: 13.93 Degree
Exp2019-01-31_21-59-11 completed!
Experiment 1 completed!
Exp2019-01-31_21-59-11.h5 has been saved.
subject3_Exp2019-01-31_21-59-11.png has been saved by 2019-01-31 22:05:49.603276.
subject5_Exp2019-01-31_21-59-11.png has been saved by 2019-01-31 22:05:49.812013.
subject9_Exp2019-01-31_21-59-11.png has been saved by 2019-01-31 22:05:50.055792.
subject14_Exp2019-01-31_21-59-11.png has been saved by 2019-01-31 22:05:50.285589.
Model Exp2019-01-31_21-59-11 has been evaluated successfully.
Model Exp2019-01-31_21-59-11 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 22:06:23.158205: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 22:06:23.241190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 22:06:23.241458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 22:06:23.241475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 22:06:23.395988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 22:06:23.396015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 22:06:23.396020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 22:06:23.396157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_22-06-23 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
dropout3_025 (Dropout)       (None, 4096)              0
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
train_batch_size = 10
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #  [9, 1] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_output3_BatchSize10_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01-31_22-06-23
Epoch 1/1
1232/1232 [==============================] - 198s 161ms/step - loss: 0.6257
The subjects are trained: [(2, 'F02'), (24, 'M14'), (19, 'M11'), (21, 'F02'), (11, 'M05'), (1, 'F01'), (20, 'M12'), (6,
'F06'), (15, 'F03'), (12, 'M06'), (4, 'F04'), (17, 'M10'), (10, 'M04'), (8, 'M02'), (23, 'M13'), (16, 'M09'), (22, 'M01'
), (13, 'M07'), (18, 'F05'), (7, 'M01')]
Evaluating model VGG16_inc_top_output3_BatchSize10_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01-31_22-06-23
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 22:11:49.826527
For the Subject 3 (F03):
730/730 [==============================] - 9s 12ms/step
        The absolute mean error on Pitch angle estimation: 17.01 Degree
        The absolute mean error on Yaw angle estimation: 40.98 Degree
        The absolute mean error on Roll angle estimation: 82.52 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 12ms/step
        The absolute mean error on Pitch angle estimation: 20.66 Degree
        The absolute mean error on Yaw angle estimation: 61.48 Degree
        The absolute mean error on Roll angle estimation: 76.07 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 44.29 Degree
        The absolute mean error on Yaw angle estimation: 32.69 Degree
        The absolute mean error on Roll angle estimation: 17.22 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 41.92 Degree
        The absolute mean error on Yaw angle estimation: 57.15 Degree
        The absolute mean error on Roll angle estimation: 79.47 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 30.97 Degree
        The absolute mean error on Yaw angle estimations: 48.07 Degree
        The absolute mean error on Roll angle estimations: 63.82 Degree
Exp2019-01-31_22-06-23 completed!
Experiment 1 completed!
Training model VGG16_inc_top_output3_BatchSize10_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01-31_22-06-23
Epoch 1/1
1232/1232 [==============================] - 198s 160ms/step - loss: 0.5497
The subjects are trained: [(15, 'F03'), (2, 'F02'), (16, 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (18, 'F05'), (4,
 'F04'), (11, 'M05'), (6, 'F06'), (21, 'F02'), (22, 'M01'), (20, 'M12'), (1, 'F01'), (17, 'M10'), (19, 'M11'), (8, 'M02'
), (7, 'M01'), (24, 'M14'), (23, 'M13')]
Evaluating model VGG16_inc_top_output3_BatchSize10_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01-31_22-06-23
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 22:18:27.179398
For the Subject 3 (F03):
730/730 [==============================] - 8s 12ms/step
        The absolute mean error on Pitch angle estimation: 26.21 Degree
        The absolute mean error on Yaw angle estimation: 41.08 Degree
        The absolute mean error on Roll angle estimation: 5.41 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 12ms/step
        The absolute mean error on Pitch angle estimation: 20.98 Degree
        The absolute mean error on Yaw angle estimation: 61.56 Degree
        The absolute mean error on Roll angle estimation: 39.43 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 25.37 Degree
        The absolute mean error on Yaw angle estimation: 54.85 Degree
        The absolute mean error on Roll angle estimation: 35.92 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 13.48 Degree
        The absolute mean error on Yaw angle estimation: 48.70 Degree
        The absolute mean error on Roll angle estimation: 40.64 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 21.51 Degree
        The absolute mean error on Yaw angle estimations: 51.55 Degree
        The absolute mean error on Roll angle estimations: 30.35 Degree
Exp2019-01-31_22-06-23 completed!
Experiment 2 completed!
Exp2019-01-31_22-06-23.h5 has been saved.
subject3_Exp2019-01-31_22-06-23.png has been saved by 2019-01-31 22:19:39.958390.
subject5_Exp2019-01-31_22-06-23.png has been saved by 2019-01-31 22:19:40.168725.
subject9_Exp2019-01-31_22-06-23.png has been saved by 2019-01-31 22:19:40.377257.
subject14_Exp2019-01-31_22-06-23.png has been saved by 2019-01-31 22:19:40.604942.
Model Exp2019-01-31_22-06-23 has been evaluated successfully.
Model Exp2019-01-31_22-06-23 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 22:22:57.665888: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 22:22:57.765431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 22:22:57.765705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 22:22:57.765718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 22:22:57.919395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 22:22:57.919422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 22:22:57.919430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 22:22:57.919570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_22-22-58 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
dropout3_025 (Dropout)       (None, 4096)              0
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
train_batch_size = 10
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #  [9, 1] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01-31_22-22-58
^CTraceback (most recent call last):
  File "runCNN_Experiment.py", line 53, in <module>
    main()
  File "runCNN_Experiment.py", line 50, in main
    runCNN_Evaluater(record = RECORD)
  File "runCNN_Experiment.py", line 40, in runCNN_Evaluater
    full_model, means, results = runCNN_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, record = record, p
reprocess_input = preprocess_input)
  File "runCNN_Experiment.py", line 19, in runCNN_ExperimentWithModel
    batch_size = train_batch_size, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater/CNN_Evaluater.py", line 9, in train
CNN
    samples_per_epoch, biwiGenerators = getGeneratorsForBIWIDataset(in_epochs, subjectList = subjectList, preprocess_inp
ut = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 208, in getG
eneratorsForBIWIDataset
    samples_per_epoch, gen = next(biwiGenerators)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 207, in <gen
expr>
    biwiGenerators = (generate() for e in range(epochs+1))
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 206, in gene
rate
    def generate(): return generatorForBIWIDataset(dataFolder, labelsTarFile, subjectList, timesteps, overlapping, scali
ng, preprocess_input, shuffle)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 163, in gene
ratorForBIWIDataset
    for inputMatrix, labels in biwi:
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 153, in <gen
expr>
    biwi = (labeledFrames(frames, biwiAnnos[subj]) for subj, frames in biwiFrames.items())
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 152, in <lam
bda>
    labeledFrames = lambda frames, labels: labelFramesForSubj(frames, labels, timesteps, overlapping, scaling, scalers)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 136, in labe
lFramesForSubj
    frames = {n: f for n, f in frames}
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 136, in <dic
tcomp>
    frames = {n: f for n, f in frames}
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py", line 76, in <gene
xpr>
    frames = ((n, preprocess_input(framePath)) for n, framePath in frameNamesForSubj)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater/CNN_Configuration.py", line 81, in
preprocess_input
    def preprocess_input(imagePath): return preprocess_input_for_model(imagePath, Target_Frame_Shape, margins, modelPack
age)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater/CNN_Configuration.py", line 39, in
preprocess_input_for_model
    img = image.load_img(imagePath, target_size = Target_Frame_Shape)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras_preprocessing/image.py", line 520, in load_img
    img = img.resize(width_height_tuple, resample)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/PIL/Image.py", line 1763, in resize
    self.load()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py", line 235, in load
    n, err_code = decoder.decode(b)
KeyboardInterrupt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 22:23:26.380275: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 22:23:26.478807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 22:23:26.479064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 22:23:26.479076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 22:23:26.633547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 22:23:26.633571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 22:23:26.633576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 22:23:26.633717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_22-23-27 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
dropout3_025 (Dropout)       (None, 4096)              0
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 2
out_epochs = 1
train_batch_size = 10
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #  [9, 1] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs2_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_22-23-27
Epoch 1/1
1232/1232 [==============================] - 198s 160ms/step - loss: 0.6181
The subjects are trained: [(2, 'F02'), (24, 'M14'), (19, 'M11'), (21, 'F02'), (11, 'M05'), (1, 'F01'), (20, 'M12'), (6,
'F06'), (15, 'F03'), (12, 'M06'), (4, 'F04'), (17, 'M10'), (10, 'M04'), (8, 'M02'), (23, 'M13'), (16, 'M09'), (22, 'M01'
), (13, 'M07'), (18, 'F05'), (7, 'M01')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs2_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_22-23-27
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 22:28:52.633293
For the Subject 3 (F03):
730/730 [==============================] - 9s 12ms/step
        The absolute mean error on Pitch angle estimation: 35.81 Degree
        The absolute mean error on Yaw angle estimation: 32.04 Degree
        The absolute mean error on Roll angle estimation: 48.81 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 12ms/step
        The absolute mean error on Pitch angle estimation: 16.41 Degree
        The absolute mean error on Yaw angle estimation: 31.20 Degree
        The absolute mean error on Roll angle estimation: 45.29 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 53.53 Degree
        The absolute mean error on Yaw angle estimation: 54.34 Degree
        The absolute mean error on Roll angle estimation: 19.26 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 12ms/step
        The absolute mean error on Pitch angle estimation: 25.34 Degree
        The absolute mean error on Yaw angle estimation: 53.40 Degree
        The absolute mean error on Roll angle estimation: 25.03 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 32.77 Degree
        The absolute mean error on Yaw angle estimations: 42.74 Degree
        The absolute mean error on Roll angle estimations: 34.59 Degree
Exp2019-01-31_22-23-27 completed!
Experiment 1 completed!
Exp2019-01-31_22-23-27.h5 has been saved.
subject3_Exp2019-01-31_22-23-27.png has been saved by 2019-01-31 22:30:05.862257.
subject5_Exp2019-01-31_22-23-27.png has been saved by 2019-01-31 22:30:06.071185.
subject9_Exp2019-01-31_22-23-27.png has been saved by 2019-01-31 22:30:06.315976.
subject14_Exp2019-01-31_22-23-27.png has been saved by 2019-01-31 22:30:06.548547.
Model Exp2019-01-31_22-23-27 has been evaluated successfully.
Model Exp2019-01-31_22-23-27 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 22:33:43.403718: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 22:33:43.500920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 22:33:43.501211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 22:33:43.501226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 22:33:43.655378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 22:33:43.655404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 22:33:43.655409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 22:33:43.655552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10453 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_22-33-44 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
dropout3_025 (Dropout)       (None, 4096)              0
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 10
out_epochs = 1
train_batch_size = 10
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #  [9, 1] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_22-33-44
Traceback (most recent call last):
  File "runCNN_Experiment.py", line 53, in <module>
    main()
  File "runCNN_Experiment.py", line 50, in main
    runCNN_Evaluater(record = RECORD)
  File "runCNN_Experiment.py", line 40, in runCNN_Evaluater
    full_model, means, results = runCNN_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, record = record, p
reprocess_input = preprocess_input)
  File "runCNN_Experiment.py", line 19, in runCNN_ExperimentWithModel
    batch_size = train_batch_size, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater/CNN_Evaluater.py", line 10, in trai
nCNN
    gen = batchGeneratorFromBIWIGenerators(biwiGenerators, train_batch_size, output_begin, num_outputs)
NameError: name 'train_batch_size' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 22:37:32.927072: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 22:37:33.024047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 22:37:33.024343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 22:37:33.024358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 22:37:33.178846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 22:37:33.178872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 22:37:33.178877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 22:37:33.179055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_22-37-33 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
dropout3_025 (Dropout)       (None, 4096)              0
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
_________________________________________________________________
dropout_025 (Dropout)        (None, 1024)              0
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 10
out_epochs = 1
train_batch_size = 10
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #  [9, 1] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_22-37-33
Epoch 1/10
1232/1232 [==============================] - 197s 160ms/step - loss: 0.6176
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.5428
Epoch 3/10
1232/1232 [==============================] - 76s 62ms/step - loss: 0.4754
Epoch 4/10
1232/1232 [==============================] - 76s 62ms/step - loss: 0.4109
Epoch 5/10
1232/1232 [==============================] - 76s 62ms/step - loss: 0.3461
Epoch 6/10
1232/1232 [==============================] - 76s 62ms/step - loss: 0.2792
Epoch 7/10
1232/1232 [==============================] - 76s 62ms/step - loss: 0.2175
Epoch 8/10
1232/1232 [==============================] - 76s 62ms/step - loss: 0.1556
Epoch 9/10
1232/1232 [==============================] - 76s 62ms/step - loss: 0.0991
Epoch 10/10
1224/1232 [============================>.] - ETA: 0s - loss: 0.0550
1232/1232 [==============================] - 201s 163ms/step - loss: 0.0560
The subjects are trained: [(16, 'M09'), (11, 'M05'), (13, 'M07'), (12, 'M06'), (10, 'M04'), (6, 'F06'), (2, 'F02'), (17,
 'M10'), (15, 'F03'), (20, 'M12'), (4, 'F04'), (8, 'M02'), (19, 'M11'), (21, 'F02'), (23, 'M13'), (1, 'F01'), (7, 'M01')
, (24, 'M14'), (22, 'M01'), (18, 'F05')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_22-37-33
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-31 22:56:27.584751
For the Subject 3 (F03):
730/730 [==============================] - 9s 12ms/step
        The absolute mean error on Pitch angle estimation: 17.50 Degree
        The absolute mean error on Yaw angle estimation: 35.01 Degree
        The absolute mean error on Roll angle estimation: 4.44 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.30 Degree
        The absolute mean error on Yaw angle estimation: 27.71 Degree
        The absolute mean error on Roll angle estimation: 3.72 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 31.40 Degree
        The absolute mean error on Yaw angle estimation: 29.87 Degree
        The absolute mean error on Roll angle estimation: 7.72 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 12ms/step
        The absolute mean error on Pitch angle estimation: 26.02 Degree
        The absolute mean error on Yaw angle estimation: 36.59 Degree
        The absolute mean error on Roll angle estimation: 14.31 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 23.05 Degree
        The absolute mean error on Yaw angle estimations: 32.29 Degree
        The absolute mean error on Roll angle estimations: 7.55 Degree
Exp2019-01-31_22-37-33 completed!
Experiment 1 completed!
Exp2019-01-31_22-37-33.h5 has been saved.
subject3_Exp2019-01-31_22-37-33.png has been saved by 2019-01-31 22:57:40.894146.
subject5_Exp2019-01-31_22-37-33.png has been saved by 2019-01-31 22:57:41.100511.
subject9_Exp2019-01-31_22-37-33.png has been saved by 2019-01-31 22:57:41.304508.
subject14_Exp2019-01-31_22-37-33.png has been saved by 2019-01-31 22:57:41.530556.
Model Exp2019-01-31_22-37-33 has been evaluated successfully.
Model Exp2019-01-31_22-37-33 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 23:03:54.682823: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 23:03:54.780845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 23:03:54.781102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 23:03:54.781115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 23:03:54.936577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 23:03:54.936602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 23:03:54.936606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 23:03:54.936743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_23-03-55 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 5
out_epochs = 1
train_batch_size = 10
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs5_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_23-03-55
Epoch 1/5
88/88 [==============================] - 16s 179ms/step - loss: 0.2328
Epoch 2/5
88/88 [==============================] - 5s 57ms/step - loss: 0.1561
Epoch 3/5
88/88 [==============================] - 5s 57ms/step - loss: 0.0991
Epoch 4/5
88/88 [==============================] - 5s 57ms/step - loss: 0.0889
Epoch 5/5
88/88 [==============================] - 5s 57ms/step - loss: 0.0684
The subjects are trained: [(9, 'M03')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs5_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_23-03-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-31 23:04:42.038766
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 6.85 Degree
        The absolute mean error on Yaw angle estimation: 13.76 Degree
        The absolute mean error on Roll angle estimation: 4.83 Degree
Exp2019-01-31_23-03-55 completed!
Experiment 1 completed!
subject9_Exp2019-01-31_23-03-55.png has been saved by 2019-01-31 23:05:01.430358.
Model Exp2019-01-31_23-03-55 has been evaluated successfully.
Model Exp2019-01-31_23-03-55 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 23:06:33.928559: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 23:06:34.026372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 23:06:34.026628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 23:06:34.026640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 23:06:34.180847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 23:06:34.180872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 23:06:34.180877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 23:06:34.181014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_23-06-34 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 10
out_epochs = 1
train_batch_size = 10
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_23-06-34
Epoch 1/10
88/88 [==============================] - 16s 180ms/step - loss: 0.2148
Epoch 2/10
88/88 [==============================] - 5s 57ms/step - loss: 0.1279
Epoch 3/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0940
Epoch 4/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0979
Epoch 5/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0637
Epoch 6/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0560
Epoch 7/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0689
Epoch 8/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0362
Epoch 9/10
88/88 [==============================] - 5s 59ms/step - loss: 0.0480
Epoch 10/10
88/88 [==============================] - 14s 164ms/step - loss: 0.0595
The subjects are trained: [(9, 'M03')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_23-06-34
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-31 23:07:56.573086
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 6.03 Degree
        The absolute mean error on Yaw angle estimation: 9.91 Degree
        The absolute mean error on Roll angle estimation: 5.05 Degree
Exp2019-01-31_23-06-34 completed!
Experiment 1 completed!
Exp2019-01-31_23-06-34.h5 has been saved.
subject9_Exp2019-01-31_23-06-34.png has been saved by 2019-01-31 23:08:16.293259.
Model Exp2019-01-31_23-06-34 has been evaluated successfully.
Model Exp2019-01-31_23-06-34 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 23:15:04.343718: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 23:15:04.441532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 23:15:04.441799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 23:15:04.441813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 23:15:04.596856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 23:15:04.596882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 23:15:04.596887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 23:15:04.597024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_23-15-05 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 10
out_epochs = 1
train_batch_size = 10
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.000010_2019-01-31_23-15-05
Epoch 1/10
88/88 [==============================] - 16s 179ms/step - loss: 0.1654
Epoch 2/10
88/88 [==============================] - 5s 57ms/step - loss: 0.0326
Epoch 3/10
88/88 [==============================] - 5s 57ms/step - loss: 0.0264
Epoch 4/10
88/88 [==============================] - 5s 57ms/step - loss: 0.0172
Epoch 5/10
88/88 [==============================] - 5s 57ms/step - loss: 0.0199
Epoch 6/10
88/88 [==============================] - 5s 57ms/step - loss: 0.0192
Epoch 7/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0168
Epoch 8/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0185
Epoch 9/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0186
Epoch 10/10
88/88 [==============================] - 14s 162ms/step - loss: 0.0319
The subjects are trained: [(9, 'M03')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.000010_2019-01-31_23-15-05
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-31 23:16:26.283807
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 11.31 Degree
        The absolute mean error on Yaw angle estimation: 20.63 Degree
        The absolute mean error on Roll angle estimation: 8.62 Degree
Exp2019-01-31_23-15-05 completed!
Experiment 1 completed!
Exp2019-01-31_23-15-05.h5 has been saved.
subject9_Exp2019-01-31_23-15-05.png has been saved by 2019-01-31 23:16:45.981280.
Model Exp2019-01-31_23-15-05 has been evaluated successfully.
Model Exp2019-01-31_23-15-05 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 23:18:31.256074: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 23:18:31.353675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 23:18:31.353939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 23:18:31.353951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 23:18:31.508121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 23:18:31.508149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 23:18:31.508154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 23:18:31.508289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_23-18-31 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 10
out_epochs = 1
train_batch_size = 10
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.001000_2019-01-31_23-18-31
Epoch 1/10
88/88 [==============================] - 16s 180ms/step - loss: 0.7615
Epoch 2/10
88/88 [==============================] - 5s 57ms/step - loss: 0.1058
Epoch 3/10
88/88 [==============================] - 5s 57ms/step - loss: 0.1003
Epoch 4/10
88/88 [==============================] - 5s 58ms/step - loss: 0.1143
Epoch 5/10
88/88 [==============================] - 5s 58ms/step - loss: 0.1179
Epoch 6/10
88/88 [==============================] - 5s 58ms/step - loss: 0.1381
Epoch 7/10
88/88 [==============================] - 5s 58ms/step - loss: 0.1344
Epoch 8/10
88/88 [==============================] - 5s 58ms/step - loss: 0.1322
Epoch 9/10
88/88 [==============================] - 5s 58ms/step - loss: 0.1306
Epoch 10/10
88/88 [==============================] - 15s 165ms/step - loss: 0.1323
The subjects are trained: [(9, 'M03')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.001000_2019-01-31_23-18-31
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-31 23:19:53.841558
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 5.64 Degree
        The absolute mean error on Yaw angle estimation: 12.30 Degree
        The absolute mean error on Roll angle estimation: 4.26 Degree
Exp2019-01-31_23-18-31 completed!
Experiment 1 completed!
Exp2019-01-31_23-18-31.h5 has been saved.
subject9_Exp2019-01-31_23-18-31.png has been saved by 2019-01-31 23:20:13.548168.
Model Exp2019-01-31_23-18-31 has been evaluated successfully.
Model Exp2019-01-31_23-18-31 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 23:21:09.336497: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 23:21:09.434217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 23:21:09.434477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 23:21:09.434490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 23:21:09.589186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 23:21:09.589212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 23:21:09.589218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 23:21:09.589355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_23-21-10 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 10
out_epochs = 1
train_batch_size = 10
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_23-21-10
Epoch 1/10
88/88 [==============================] - 16s 180ms/step - loss: 0.2620
Epoch 2/10
88/88 [==============================] - 5s 57ms/step - loss: 0.1209
Epoch 3/10
88/88 [==============================] - 5s 58ms/step - loss: 0.1020
Epoch 4/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0930
Epoch 5/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0849
Epoch 6/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0553
Epoch 7/10
88/88 [==============================] - 5s 58ms/step - loss: 0.0371
Epoch 8/10
88/88 [==============================] - 5s 61ms/step - loss: 0.0520
Epoch 9/10
88/88 [==============================] - 5s 61ms/step - loss: 0.0362
Epoch 10/10
88/88 [==============================] - 14s 165ms/step - loss: 0.0429
The subjects are trained: [(9, 'M03')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_23-21-10
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-31 23:22:32.412220
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 5.30 Degree
        The absolute mean error on Yaw angle estimation: 13.21 Degree
        The absolute mean error on Roll angle estimation: 4.11 Degree
Exp2019-01-31_23-21-10 completed!
Experiment 1 completed!
Exp2019-01-31_23-21-10.h5 has been saved.
subject9_Exp2019-01-31_23-21-10.png has been saved by 2019-01-31 23:22:52.195358.
Model Exp2019-01-31_23-21-10 has been evaluated successfully.
Model Exp2019-01-31_23-21-10 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 23:25:02.399379: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 23:25:02.496248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 23:25:02.496560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 23:25:02.496575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 23:25:02.650990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 23:25:02.651015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 23:25:02.651019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 23:25:02.651197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_23-25-03 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 10
out_epochs = 1
train_batch_size = 100
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_23-25-03
Epoch 1/10
8/8 [==============================] - 16s 2s/step - loss: 0.7676
Epoch 2/10
8/8 [==============================] - 3s 416ms/step - loss: 0.3584
Epoch 3/10
8/8 [==============================] - 3s 418ms/step - loss: 0.2546
Epoch 4/10
8/8 [==============================] - 3s 415ms/step - loss: 0.1495
Epoch 5/10
8/8 [==============================] - 3s 428ms/step - loss: 0.1268
Epoch 6/10
8/8 [==============================] - 3s 427ms/step - loss: 0.1364
Epoch 7/10
8/8 [==============================] - 3s 431ms/step - loss: 0.1281
Epoch 8/10
8/8 [==============================] - 3s 414ms/step - loss: 0.1210
Epoch 9/10
8/8 [==============================] - 3s 424ms/step - loss: 0.1233
Epoch 10/10
8/8 [==============================] - 3s 421ms/step - loss: 0.0922
The subjects are trained: [(9, 'M03')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_23-25-03
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-31 23:26:00.119794
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 4.48 Degree
        The absolute mean error on Yaw angle estimation: 4.65 Degree
        The absolute mean error on Roll angle estimation: 13.71 Degree
Exp2019-01-31_23-25-03 completed!
Experiment 1 completed!
Exp2019-01-31_23-25-03.h5 has been saved.
subject9_Exp2019-01-31_23-25-03.png has been saved by 2019-01-31 23:26:19.867012.
Model Exp2019-01-31_23-25-03 has been evaluated successfully.
Model Exp2019-01-31_23-25-03 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 23:27:14.852031: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 23:27:14.951348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 23:27:14.951606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 23:27:14.951619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 23:27:15.106820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 23:27:15.106850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 23:27:15.106855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 23:27:15.106994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_23-27-15 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 10
out_epochs = 1
train_batch_size = 100
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_23-27-15
Epoch 1/10
8/8 [==============================] - 16s 2s/step - loss: 0.6392
Epoch 2/10
8/8 [==============================] - 3s 421ms/step - loss: 0.2644
Epoch 3/10
8/8 [==============================] - 3s 414ms/step - loss: 0.2114
Epoch 4/10
8/8 [==============================] - 3s 422ms/step - loss: 0.1502
Epoch 5/10
8/8 [==============================] - 3s 421ms/step - loss: 0.1136
Epoch 6/10
8/8 [==============================] - 3s 420ms/step - loss: 0.1218
Epoch 7/10
8/8 [==============================] - 3s 422ms/step - loss: 0.1732
Epoch 8/10
8/8 [==============================] - 3s 423ms/step - loss: 0.1373
Epoch 9/10
8/8 [==============================] - 3s 428ms/step - loss: 0.1351
Epoch 10/10
8/8 [==============================] - 3s 431ms/step - loss: 0.1429
The subjects are trained: [(9, 'M03')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-01-31_23-27-15
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-31 23:28:12.685941
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 11.33 Degree
        The absolute mean error on Yaw angle estimation: 7.06 Degree
        The absolute mean error on Roll angle estimation: 15.01 Degree
Exp2019-01-31_23-27-15 completed!
Experiment 1 completed!
Exp2019-01-31_23-27-15.h5 has been saved.
subject9_Exp2019-01-31_23-27-15.png has been saved by 2019-01-31 23:28:32.415631.
Model Exp2019-01-31_23-27-15 has been evaluated successfully.
Model Exp2019-01-31_23-27-15 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 23:29:13.997903: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 23:29:14.096385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 23:29:14.096650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 23:29:14.096664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 23:29:14.250972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 23:29:14.250998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 23:29:14.251006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 23:29:14.251141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_23-29-14 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 10
out_epochs = 3
train_batch_size = 100
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-29-14
Epoch 1/10
8/8 [==============================] - 16s 2s/step - loss: 0.6586
Epoch 2/10
8/8 [==============================] - 3s 422ms/step - loss: 0.3136
Epoch 3/10
8/8 [==============================] - 3s 420ms/step - loss: 0.1859
Epoch 4/10
8/8 [==============================] - 3s 421ms/step - loss: 0.1142
Epoch 5/10
8/8 [==============================] - 3s 425ms/step - loss: 0.0875
Epoch 6/10
8/8 [==============================] - 3s 424ms/step - loss: 0.0725
Epoch 7/10
8/8 [==============================] - 3s 430ms/step - loss: 0.0633
Epoch 8/10
8/8 [==============================] - 3s 429ms/step - loss: 0.0705
Epoch 9/10
8/8 [==============================] - 3s 426ms/step - loss: 0.0884
Epoch 10/10
8/8 [==============================] - 3s 426ms/step - loss: 0.0613
The subjects are trained: [(9, 'M03')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-29-14
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-31 23:30:11.991518
For the Subject 9 (M03):
882/882 [==============================] - 10s 12ms/step
        The absolute mean error on Pitch angle estimation: 4.80 Degree
        The absolute mean error on Yaw angle estimation: 4.74 Degree
        The absolute mean error on Roll angle estimation: 3.40 Degree
Exp2019-01-31_23-29-14 completed!
Experiment 1 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-29-14
Epoch 1/10
8/8 [==============================] - 13s 2s/step - loss: 0.0623
Epoch 2/10
8/8 [==============================] - 3s 422ms/step - loss: 0.0561
Epoch 3/10
8/8 [==============================] - 3s 424ms/step - loss: 0.0504
Epoch 4/10
8/8 [==============================] - 3s 425ms/step - loss: 0.0726
Epoch 5/10
8/8 [==============================] - 3s 423ms/step - loss: 0.0779
Epoch 6/10
8/8 [==============================] - 3s 426ms/step - loss: 0.0725
Epoch 7/10
8/8 [==============================] - 3s 430ms/step - loss: 0.1054
Epoch 8/10
8/8 [==============================] - 3s 435ms/step - loss: 0.1527
Epoch 9/10
8/8 [==============================] - 4s 439ms/step - loss: 0.1329
Epoch 10/10
8/8 [==============================] - 4s 443ms/step - loss: 0.1265
The subjects are trained: [(9, 'M03')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-29-14
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-31 23:31:25.950441
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 9.31 Degree
        The absolute mean error on Yaw angle estimation: 19.45 Degree
        The absolute mean error on Roll angle estimation: 8.45 Degree
Exp2019-01-31_23-29-14 completed!
Experiment 2 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-29-14
Epoch 1/10
8/8 [==============================] - 13s 2s/step - loss: 0.1088
Epoch 2/10
8/8 [==============================] - 3s 421ms/step - loss: 0.1045
Epoch 3/10
8/8 [==============================] - 3s 420ms/step - loss: 0.1011
Epoch 4/10
8/8 [==============================] - 3s 428ms/step - loss: 0.0723
Epoch 5/10
8/8 [==============================] - 3s 421ms/step - loss: 0.0600
Epoch 6/10
8/8 [==============================] - 3s 424ms/step - loss: 0.0990
Epoch 7/10
8/8 [==============================] - 3s 422ms/step - loss: 0.1847
Epoch 8/10
8/8 [==============================] - 4s 442ms/step - loss: 0.1208
Epoch 9/10
8/8 [==============================] - 4s 448ms/step - loss: 0.1046
Epoch 10/10
8/8 [==============================] - 3s 436ms/step - loss: 0.0888
The subjects are trained: [(9, 'M03')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-29-14
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-31 23:32:39.637565
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 9.58 Degree
        The absolute mean error on Yaw angle estimation: 3.34 Degree
        The absolute mean error on Roll angle estimation: 6.09 Degree
Exp2019-01-31_23-29-14 completed!
Experiment 3 completed!
Exp2019-01-31_23-29-14.h5 has been saved.
subject9_Exp2019-01-31_23-29-14.png has been saved by 2019-01-31 23:32:59.134539.
Model Exp2019-01-31_23-29-14 has been evaluated successfully.
Model Exp2019-01-31_23-29-14 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-31 23:54:13.195330: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-31 23:54:13.292940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-31 23:54:13.293206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-01-31 23:54:13.293220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-31 23:54:13.448188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-31 23:54:13.448212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-31 23:54:13.448220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-31 23:54:13.448360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10453 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-31_23-54-13 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 10
out_epochs = 3
train_batch_size = 100
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-54-13
Epoch 1/10
123/123 [==============================] - 179s 1s/step - loss: 0.1734
Epoch 2/10
123/123 [==============================] - 53s 435ms/step - loss: 0.1178
Epoch 3/10
123/123 [==============================] - 54s 440ms/step - loss: 0.1004
Epoch 4/10
123/123 [==============================] - 54s 439ms/step - loss: 0.0750
Epoch 5/10
123/123 [==============================] - 54s 441ms/step - loss: 0.0486
Epoch 6/10
123/123 [==============================] - 54s 441ms/step - loss: 0.0446
Epoch 7/10
123/123 [==============================] - 54s 441ms/step - loss: 0.0363
Epoch 8/10
123/123 [==============================] - 54s 442ms/step - loss: 0.0421
Epoch 9/10
123/123 [==============================] - 54s 443ms/step - loss: 0.0307
Epoch 10/10
123/123 [==============================] - 54s 442ms/step - loss: 0.0325
The subjects are trained: [(2, 'F02'), (24, 'M14'), (19, 'M11'), (21, 'F02'), (11, 'M05'), (1, 'F01'), (20, 'M12'), (6,
'F06'), (15, 'F03'), (12, 'M06'), (4, 'F04'), (17, 'M10'), (10, 'M04'), (8, 'M02'), (23, 'M13'), (16, 'M09'), (22, 'M01'
), (13, 'M07'), (18, 'F05'), (7, 'M01')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-54-13
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 00:07:28.721337
For the Subject 3 (F03):
730/730 [==============================] - 9s 12ms/step
        The absolute mean error on Pitch angle estimation: 88.97 Degree
        The absolute mean error on Yaw angle estimation: 54.41 Degree
        The absolute mean error on Roll angle estimation: 51.62 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 84.43 Degree
        The absolute mean error on Yaw angle estimation: 70.17 Degree
        The absolute mean error on Roll angle estimation: 27.57 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 83.99 Degree
        The absolute mean error on Yaw angle estimation: 36.59 Degree
        The absolute mean error on Roll angle estimation: 17.98 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 109.12 Degree
        The absolute mean error on Yaw angle estimation: 101.62 Degree
        The absolute mean error on Roll angle estimation: 80.21 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 91.63 Degree
        The absolute mean error on Yaw angle estimations: 65.69 Degree
        The absolute mean error on Roll angle estimations: 44.34 Degree
Exp2019-01-31_23-54-13 completed!
Experiment 1 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-54-13
Epoch 1/10
123/123 [==============================] - 174s 1s/step - loss: 0.1165
Epoch 2/10
123/123 [==============================] - 131s 1s/step - loss: 0.0391
Epoch 3/10
123/123 [==============================] - 52s 424ms/step - loss: 0.0284
Epoch 4/10
123/123 [==============================] - 52s 424ms/step - loss: 0.0293
Epoch 5/10
123/123 [==============================] - 52s 424ms/step - loss: 0.0205
Epoch 6/10
123/123 [==============================] - 52s 423ms/step - loss: 0.0233
Epoch 7/10
123/123 [==============================] - 52s 423ms/step - loss: 0.0237
Epoch 8/10
123/123 [==============================] - 52s 423ms/step - loss: 0.0237
Epoch 9/10
123/123 [==============================] - 52s 423ms/step - loss: 0.0207
Epoch 10/10
123/123 [==============================] - 52s 423ms/step - loss: 0.0256
The subjects are trained: [(15, 'F03'), (2, 'F02'), (16, 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (18, 'F05'), (4,
 'F04'), (11, 'M05'), (6, 'F06'), (21, 'F02'), (22, 'M01'), (20, 'M12'), (1, 'F01'), (17, 'M10'), (19, 'M11'), (8, 'M02'
), (7, 'M01'), (24, 'M14'), (23, 'M13')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-54-13
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 00:22:49.809484
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 38.21 Degree
        The absolute mean error on Yaw angle estimation: 32.59 Degree
        The absolute mean error on Roll angle estimation: 16.58 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 49.19 Degree
        The absolute mean error on Yaw angle estimation: 35.03 Degree
        The absolute mean error on Roll angle estimation: 10.91 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 72.58 Degree
        The absolute mean error on Yaw angle estimation: 34.98 Degree
        The absolute mean error on Roll angle estimation: 21.71 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 79.39 Degree
        The absolute mean error on Yaw angle estimation: 38.26 Degree
        The absolute mean error on Roll angle estimation: 27.59 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 59.84 Degree
        The absolute mean error on Yaw angle estimations: 35.21 Degree
        The absolute mean error on Roll angle estimations: 19.20 Degree
Exp2019-01-31_23-54-13 completed!
Experiment 2 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-54-13
Epoch 1/10
123/123 [==============================] - 174s 1s/step - loss: 0.0653
Epoch 2/10
123/123 [==============================] - 52s 422ms/step - loss: 0.0252
Epoch 3/10
123/123 [==============================] - 52s 423ms/step - loss: 0.0213
Epoch 4/10
123/123 [==============================] - 52s 424ms/step - loss: 0.0210
Epoch 5/10
123/123 [==============================] - 52s 424ms/step - loss: 0.0173
Epoch 6/10
123/123 [==============================] - 52s 423ms/step - loss: 0.0184
Epoch 7/10
123/123 [==============================] - 52s 424ms/step - loss: 0.0150
Epoch 8/10
123/123 [==============================] - 52s 423ms/step - loss: 0.0143
Epoch 9/10
123/123 [==============================] - 52s 423ms/step - loss: 0.0133
Epoch 10/10
123/123 [==============================] - 52s 424ms/step - loss: 0.0132
The subjects are trained: [(13, 'M07'), (19, 'M11'), (8, 'M02'), (2, 'F02'), (12, 'M06'), (22, 'M01'), (10, 'M04'), (11,
 'M05'), (6, 'F06'), (7, 'M01'), (21, 'F02'), (4, 'F04'), (17, 'M10'), (24, 'M14'), (18, 'F05'), (20, 'M12'), (15, 'F03'
), (1, 'F01'), (23, 'M13'), (16, 'M09')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize100_inEpochs10_outEpochs3_AdamOpt_lr-0.000100_2019-01-31_23-54-13
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 00:36:50.783790
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 18.05 Degree
        The absolute mean error on Yaw angle estimation: 28.72 Degree
        The absolute mean error on Roll angle estimation: 26.55 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 10.62 Degree
        The absolute mean error on Yaw angle estimation: 35.35 Degree
        The absolute mean error on Roll angle estimation: 12.13 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 46.39 Degree
        The absolute mean error on Yaw angle estimation: 26.00 Degree
        The absolute mean error on Roll angle estimation: 27.01 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 28.54 Degree
        The absolute mean error on Yaw angle estimation: 32.08 Degree
        The absolute mean error on Roll angle estimation: 13.01 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 25.90 Degree
        The absolute mean error on Yaw angle estimations: 30.54 Degree
        The absolute mean error on Roll angle estimations: 19.67 Degree
Exp2019-01-31_23-54-13 completed!
Experiment 3 completed!
Exp2019-01-31_23-54-13.h5 has been saved.
subject3_Exp2019-01-31_23-54-13.png has been saved by 2019-02-01 00:38:02.906946.
subject5_Exp2019-01-31_23-54-13.png has been saved by 2019-02-01 00:38:03.115849.
subject9_Exp2019-01-31_23-54-13.png has been saved by 2019-02-01 00:38:03.325443.
subject14_Exp2019-01-31_23-54-13.png has been saved by 2019-02-01 00:38:03.549948.
Model Exp2019-01-31_23-54-13 has been evaluated successfully.
Model Exp2019-01-31_23-54-13 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-02-01 00:46:19.194832: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-02-01 00:46:19.293194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-01 00:46:19.293488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-02-01 00:46:19.293500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-02-01 00:46:19.449081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-02-01 00:46:19.449107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-02-01 00:46:19.449112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-02-01 00:46:19.449288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
^CTraceback (most recent call last):
  File "runCNN_Experiment.py", line 53, in <module>
    main()
  File "runCNN_Experiment.py", line 50, in main
    runCNN_Evaluater(record = RECORD)
  File "runCNN_Experiment.py", line 32, in runCNN_Evaluater
    vgg_model, full_model, modelID, preprocess_input = getFinalModel(num_outputs = num_outputs, lr = learning_rate, incl
ude_vgg_top = include_vgg_top, use_vgg16 = use_vgg16)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater/CNN_Configuration.py", line 65, in
getFinalModel
    cnn_model = vgg16.VGG16(weights='imagenet', input_shape = inp, include_top=include_vgg_top)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/applications/__init__.py", line 28, in wrapper
    return base_fun(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/applications/vgg16.py", line 11, in VGG16
    return vgg16.VGG16(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras_applications/vgg16.py", line 210, in VGG16
    model.load_weights(weights_path)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/network.py", line 1166, in load_weights
    f, self.layers, reshape=reshape)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py", line 1058, in load_weights_from_hdf5
_group
    K.batch_set_value(weight_value_tuples)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2470, in batch_set
_value
    get_session().run(assign_ops, feed_dict=feed_dict)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_tf_
sessionrun
    status, run_metadata)
KeyboardInterrupt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-02-01 00:46:44.367838: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-02-01 00:46:44.465299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-01 00:46:44.465565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-02-01 00:46:44.465586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-02-01 00:46:44.621145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-02-01 00:46:44.621173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-02-01 00:46:44.621178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-02-01 00:46:44.621314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-02-01_00-46-45 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 10
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize1_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-02-01_00-46-45
Epoch 1/10
12321/12321 [==============================] - 340s 28ms/step - loss: 0.0174
Epoch 2/10
12321/12321 [==============================] - 341s 28ms/step - loss: 0.0103
Epoch 3/10
12321/12321 [==============================] - 341s 28ms/step - loss: 0.0114
Epoch 4/10
12321/12321 [==============================] - 341s 28ms/step - loss: 0.0172
Epoch 5/10
12321/12321 [==============================] - 341s 28ms/step - loss: 0.0080
Epoch 6/10
12321/12321 [==============================] - 341s 28ms/step - loss: 0.0195
Epoch 7/10
12321/12321 [==============================] - 341s 28ms/step - loss: 0.0142
Epoch 8/10
12321/12321 [==============================] - 342s 28ms/step - loss: 0.0112
Epoch 9/10
12321/12321 [==============================] - 341s 28ms/step - loss: 0.0162
Epoch 10/10
12321/12321 [==============================] - 341s 28ms/step - loss: 0.0146
The subjects are trained: [(22, 'M01'), (17, 'M10'), (7, 'M01'), (21, 'F02'), (10, 'M04'), (6, 'F06'), (4, 'F04'), (12,
'M06'), (2, 'F02'), (16, 'M09'), (20, 'M12'), (1, 'F01'), (13, 'M07'), (24, 'M14'), (11, 'M05'), (18, 'F05'), (15, 'F03'
), (8, 'M02'), (19, 'M11'), (23, 'M13')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize1_inEpochs10_outEpochs1_AdamOpt_lr-0.000100_2019-02-01_00-46-45
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 01:45:43.643874
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 16.50 Degree
        The absolute mean error on Yaw angle estimation: 31.65 Degree
        The absolute mean error on Roll angle estimation: 11.85 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 20.49 Degree
        The absolute mean error on Yaw angle estimation: 29.74 Degree
        The absolute mean error on Roll angle estimation: 6.57 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 18.42 Degree
        The absolute mean error on Yaw angle estimation: 25.78 Degree
        The absolute mean error on Roll angle estimation: 10.29 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 24.17 Degree
        The absolute mean error on Yaw angle estimation: 34.52 Degree
        The absolute mean error on Roll angle estimation: 14.31 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.89 Degree
        The absolute mean error on Yaw angle estimations: 30.42 Degree
        The absolute mean error on Roll angle estimations: 10.76 Degree
Exp2019-02-01_00-46-45 completed!
Experiment 1 completed!
Exp2019-02-01_00-46-45.h5 has been saved.
subject3_Exp2019-02-01_00-46-45.png has been saved by 2019-02-01 01:46:55.822344.
subject5_Exp2019-02-01_00-46-45.png has been saved by 2019-02-01 01:46:56.017236.
subject9_Exp2019-02-01_00-46-45.png has been saved by 2019-02-01 01:46:56.211850.
subject14_Exp2019-02-01_00-46-45.png has been saved by 2019-02-01 01:46:56.425536.
Model Exp2019-02-01_00-46-45 has been evaluated successfully.
Model Exp2019-02-01_00-46-45 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-02-01 01:49:27.609064: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-02-01 01:49:27.706006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-01 01:49:27.706260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-02-01 01:49:27.706272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-02-01 01:49:27.862658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-02-01 01:49:27.862684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-02-01 01:49:27.862689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-02-01 01:49:27.862827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-02-01_01-49-28 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 3
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize3_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-02-01_01-49-28
Epoch 1/1
4107/4107 [==============================] - 241s 59ms/step - loss: 0.0351
The subjects are trained: [(2, 'F02'), (24, 'M14'), (19, 'M11'), (21, 'F02'), (11, 'M05'), (1, 'F01'), (20, 'M12'), (6,
'F06'), (15, 'F03'), (12, 'M06'), (4, 'F04'), (17, 'M10'), (10, 'M04'), (8, 'M02'), (23, 'M13'), (16, 'M09'), (22, 'M01'
), (13, 'M07'), (18, 'F05'), (7, 'M01')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize3_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-02-01_01-49-28
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 01:55:36.426786
For the Subject 3 (F03):
730/730 [==============================] - 9s 12ms/step
        The absolute mean error on Pitch angle estimation: 17.80 Degree
        The absolute mean error on Yaw angle estimation: 72.13 Degree
        The absolute mean error on Roll angle estimation: 19.72 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 16.79 Degree
        The absolute mean error on Yaw angle estimation: 49.30 Degree
        The absolute mean error on Roll angle estimation: 28.34 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.30 Degree
        The absolute mean error on Yaw angle estimation: 89.32 Degree
        The absolute mean error on Roll angle estimation: 8.92 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 22.48 Degree
        The absolute mean error on Yaw angle estimation: 61.82 Degree
        The absolute mean error on Roll angle estimation: 43.08 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.59 Degree
        The absolute mean error on Yaw angle estimations: 68.14 Degree
        The absolute mean error on Roll angle estimations: 25.02 Degree
Exp2019-02-01_01-49-28 completed!
Experiment 1 completed!
Exp2019-02-01_01-49-28.h5 has been saved.
subject3_Exp2019-02-01_01-49-28.png has been saved by 2019-02-01 01:56:49.087942.
subject5_Exp2019-02-01_01-49-28.png has been saved by 2019-02-01 01:56:49.297226.
subject9_Exp2019-02-01_01-49-28.png has been saved by 2019-02-01 01:56:49.500669.
subject14_Exp2019-02-01_01-49-28.png has been saved by 2019-02-01 01:56:49.732309.
Model Exp2019-02-01_01-49-28 has been evaluated successfully.
Model Exp2019-02-01_01-49-28 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-02-01 02:05:41.589226: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-02-01 02:05:41.688184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-01 02:05:41.688441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-02-01 02:05:41.688453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-02-01 02:05:41.844318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-02-01 02:05:41.844344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-02-01 02:05:41.844349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-02-01 02:05:41.844487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-02-01_02-05-42 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 10
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-02-01_02-05-42
Epoch 1/1
1232/1232 [==============================] - 197s 160ms/step - loss: 0.0740
The subjects are trained: [(2, 'F02'), (24, 'M14'), (19, 'M11'), (21, 'F02'), (11, 'M05'), (1, 'F01'), (20, 'M12'), (6,
'F06'), (15, 'F03'), (12, 'M06'), (4, 'F04'), (17, 'M10'), (10, 'M04'), (8, 'M02'), (23, 'M13'), (16, 'M09'), (22, 'M01'
), (13, 'M07'), (18, 'F05'), (7, 'M01')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-02-01_02-05-42
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 02:11:07.108727
For the Subject 3 (F03):
730/730 [==============================] - 9s 12ms/step
        The absolute mean error on Pitch angle estimation: 27.05 Degree
        The absolute mean error on Yaw angle estimation: 32.96 Degree
        The absolute mean error on Roll angle estimation: 43.86 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 14.24 Degree
        The absolute mean error on Yaw angle estimation: 38.14 Degree
        The absolute mean error on Roll angle estimation: 37.18 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 20.46 Degree
        The absolute mean error on Yaw angle estimation: 33.29 Degree
        The absolute mean error on Roll angle estimation: 19.38 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 18.01 Degree
        The absolute mean error on Yaw angle estimation: 45.76 Degree
        The absolute mean error on Roll angle estimation: 113.15 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.94 Degree
        The absolute mean error on Yaw angle estimations: 37.54 Degree
        The absolute mean error on Roll angle estimations: 53.39 Degree
Exp2019-02-01_02-05-42 completed!
Experiment 1 completed!
Exp2019-02-01_02-05-42.h5 has been saved.
subject3_Exp2019-02-01_02-05-42.png has been saved by 2019-02-01 02:12:19.832121.
subject5_Exp2019-02-01_02-05-42.png has been saved by 2019-02-01 02:12:20.045440.
subject9_Exp2019-02-01_02-05-42.png has been saved by 2019-02-01 02:12:20.257784.
subject14_Exp2019-02-01_02-05-42.png has been saved by 2019-02-01 02:12:20.486069.
Model Exp2019-02-01_02-05-42 has been evaluated successfully.
Model Exp2019-02-01_02-05-42 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$ python runCNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-02-01 03:20:19.319667: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-02-01 03:20:19.417494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-01 03:20:19.417756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.80GiB
2019-02-01 03:20:19.417768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-02-01 03:20:19.572805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-02-01 03:20:19.572832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-02-01 03:20:19.572837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-02-01 03:20:19.572971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10452 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-02-01_03-20-20 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
_________________________________________________________________
fc1024 (Dense)               (None, 1024)              4195328
=================================================================
Total params: 138,455,872
Trainable params: 4,195,328
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
model_1 (Model)              (None, 1024)              138455872
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3075
=================================================================
Total params: 138,458,947
Trainable params: 4,198,403
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 10
out_epochs = 30
train_batch_size = 10
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 197s 160ms/step - loss: 0.0727
Epoch 2/10
1232/1232 [==============================] - 75s 61ms/step - loss: 0.0214
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0146
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0129
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0111
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0105
Epoch 10/10
1232/1232 [==============================] - 201s 163ms/step - loss: 0.0109
The subjects are trained: [(16, 'M09'), (11, 'M05'), (13, 'M07'), (12, 'M06'), (10, 'M04'), (6, 'F06'), (2, 'F02'), (17,
 'M10'), (15, 'F03'), (20, 'M12'), (4, 'F04'), (8, 'M02'), (19, 'M11'), (21, 'F02'), (23, 'M13'), (1, 'F01'), (7, 'M01')
, (24, 'M14'), (22, 'M01'), (18, 'F05')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 03:39:09.593519
For the Subject 3 (F03):
730/730 [==============================] - 8s 12ms/step
        The absolute mean error on Pitch angle estimation: 16.52 Degree
        The absolute mean error on Yaw angle estimation: 34.98 Degree
        The absolute mean error on Roll angle estimation: 5.74 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.19 Degree
        The absolute mean error on Yaw angle estimation: 25.99 Degree
        The absolute mean error on Roll angle estimation: 7.10 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 33.76 Degree
        The absolute mean error on Yaw angle estimation: 26.55 Degree
        The absolute mean error on Roll angle estimation: 15.34 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 32.90 Degree
        The absolute mean error on Yaw angle estimation: 36.70 Degree
        The absolute mean error on Roll angle estimation: 18.72 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 25.09 Degree
        The absolute mean error on Yaw angle estimations: 31.06 Degree
        The absolute mean error on Roll angle estimations: 11.73 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 1 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 160ms/step - loss: 0.0192
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0122
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0111
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0108
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0117
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0115
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0115
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0134
The subjects are trained: [(13, 'M07'), (19, 'M11'), (8, 'M02'), (2, 'F02'), (12, 'M06'), (22, 'M01'), (10, 'M04'), (11,
 'M05'), (6, 'F06'), (7, 'M01'), (21, 'F02'), (4, 'F04'), (17, 'M10'), (24, 'M14'), (18, 'F05'), (20, 'M12'), (15, 'F03'
), (1, 'F01'), (23, 'M13'), (16, 'M09')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 03:59:12.260799
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 15.32 Degree
        The absolute mean error on Yaw angle estimation: 30.21 Degree
        The absolute mean error on Roll angle estimation: 6.64 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 26.31 Degree
        The absolute mean error on Yaw angle estimation: 25.16 Degree
        The absolute mean error on Roll angle estimation: 6.26 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 19.47 Degree
        The absolute mean error on Yaw angle estimation: 30.10 Degree
        The absolute mean error on Roll angle estimation: 8.22 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 16.88 Degree
        The absolute mean error on Yaw angle estimation: 34.70 Degree
        The absolute mean error on Roll angle estimation: 14.29 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.50 Degree
        The absolute mean error on Yaw angle estimations: 30.04 Degree
        The absolute mean error on Roll angle estimations: 8.85 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 2 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 197s 160ms/step - loss: 0.0130
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0084
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0083
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0079
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0099
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0116
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0126
The subjects are trained: [(7, 'M01'), (20, 'M12'), (6, 'F06'), (8, 'M02'), (18, 'F05'), (10, 'M04'), (16, 'M09'), (22,
'M01'), (23, 'M13'), (1, 'F01'), (15, 'F03'), (24, 'M14'), (19, 'M11'), (17, 'M10'), (13, 'M07'), (4, 'F04'), (2, 'F02')
, (11, 'M05'), (21, 'F02'), (12, 'M06')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 04:19:13.209504
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 14.57 Degree
        The absolute mean error on Yaw angle estimation: 28.86 Degree
        The absolute mean error on Roll angle estimation: 8.52 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 16.86 Degree
        The absolute mean error on Yaw angle estimation: 24.50 Degree
        The absolute mean error on Roll angle estimation: 5.08 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 23.17 Degree
        The absolute mean error on Yaw angle estimation: 31.26 Degree
        The absolute mean error on Roll angle estimation: 8.20 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 24.56 Degree
        The absolute mean error on Yaw angle estimation: 33.44 Degree
        The absolute mean error on Roll angle estimation: 15.46 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.79 Degree
        The absolute mean error on Yaw angle estimations: 29.51 Degree
        The absolute mean error on Roll angle estimations: 9.32 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 3 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 197s 160ms/step - loss: 0.0108
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0081
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0084
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0092
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0096
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0089
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0100
Epoch 10/10
1232/1232 [==============================] - 199s 161ms/step - loss: 0.0124
The subjects are trained: [(10, 'M04'), (18, 'F05'), (2, 'F02'), (19, 'M11'), (21, 'F02'), (1, 'F01'), (7, 'M01'), (4, '
F04'), (17, 'M10'), (20, 'M12'), (6, 'F06'), (13, 'M07'), (23, 'M13'), (15, 'F03'), (24, 'M14'), (22, 'M01'), (8, 'M02')
, (12, 'M06'), (16, 'M09'), (11, 'M05')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 04:39:13.007039
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.99 Degree
        The absolute mean error on Yaw angle estimation: 29.72 Degree
        The absolute mean error on Roll angle estimation: 4.09 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 18.47 Degree
        The absolute mean error on Yaw angle estimation: 24.79 Degree
        The absolute mean error on Roll angle estimation: 3.85 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 24.52 Degree
        The absolute mean error on Yaw angle estimation: 33.93 Degree
        The absolute mean error on Roll angle estimation: 9.33 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 27.85 Degree
        The absolute mean error on Yaw angle estimation: 33.48 Degree
        The absolute mean error on Roll angle estimation: 15.94 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 22.21 Degree
        The absolute mean error on Yaw angle estimations: 30.48 Degree
        The absolute mean error on Roll angle estimations: 8.30 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 4 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 197s 160ms/step - loss: 0.0161
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 5/10
1232/1232 [==============================] - 75s 61ms/step - loss: 0.0103
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0089
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0096
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0092
Epoch 10/10
1232/1232 [==============================] - 201s 163ms/step - loss: 0.0114
The subjects are trained: [(15, 'F03'), (18, 'F05'), (10, 'M04'), (20, 'M12'), (23, 'M13'), (12, 'M06'), (19, 'M11'), (1
1, 'M05'), (2, 'F02'), (7, 'M01'), (17, 'M10'), (21, 'F02'), (1, 'F01'), (8, 'M02'), (24, 'M14'), (16, 'M09'), (4, 'F04'
), (13, 'M07'), (22, 'M01'), (6, 'F06')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 04:59:14.718975
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 9.01 Degree
        The absolute mean error on Yaw angle estimation: 34.00 Degree
        The absolute mean error on Roll angle estimation: 13.28 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 35.00 Degree
        The absolute mean error on Yaw angle estimation: 33.13 Degree
        The absolute mean error on Roll angle estimation: 6.88 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 20.96 Degree
        The absolute mean error on Yaw angle estimation: 24.95 Degree
        The absolute mean error on Roll angle estimation: 8.28 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 18.69 Degree
        The absolute mean error on Yaw angle estimation: 42.81 Degree
        The absolute mean error on Roll angle estimation: 13.46 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 20.92 Degree
        The absolute mean error on Yaw angle estimations: 33.72 Degree
        The absolute mean error on Roll angle estimations: 10.47 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 5 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0117
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0091
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0093
Epoch 4/10
1232/1232 [==============================] - 75s 61ms/step - loss: 0.0087
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0096
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0100
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0117
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0109
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0096
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0123
The subjects are trained: [(6, 'F06'), (21, 'F02'), (15, 'F03'), (19, 'M11'), (11, 'M05'), (4, 'F04'), (17, 'M10'), (1,
'F01'), (13, 'M07'), (10, 'M04'), (2, 'F02'), (24, 'M14'), (7, 'M01'), (23, 'M13'), (8, 'M02'), (22, 'M01'), (20, 'M12')
, (18, 'F05'), (16, 'M09'), (12, 'M06')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 05:19:16.378371
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 10.92 Degree
        The absolute mean error on Yaw angle estimation: 38.41 Degree
        The absolute mean error on Roll angle estimation: 20.93 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 18.49 Degree
        The absolute mean error on Yaw angle estimation: 34.12 Degree
        The absolute mean error on Roll angle estimation: 11.96 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 22.92 Degree
        The absolute mean error on Yaw angle estimation: 26.21 Degree
        The absolute mean error on Roll angle estimation: 9.27 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.89 Degree
        The absolute mean error on Yaw angle estimation: 45.26 Degree
        The absolute mean error on Roll angle estimation: 15.11 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.55 Degree
        The absolute mean error on Yaw angle estimations: 36.00 Degree
        The absolute mean error on Roll angle estimations: 14.32 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 6 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 197s 160ms/step - loss: 0.0203
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0119
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0100
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0100
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0112
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0113
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0109
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0102
The subjects are trained: [(2, 'F02'), (8, 'M02'), (16, 'M09'), (22, 'M01'), (1, 'F01'), (15, 'F03'), (12, 'M06'), (24,
'M14'), (21, 'F02'), (13, 'M07'), (4, 'F04'), (7, 'M01'), (18, 'F05'), (23, 'M13'), (20, 'M12'), (19, 'M11'), (11, 'M05'
), (17, 'M10'), (6, 'F06'), (10, 'M04')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 05:39:15.984665
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 29.54 Degree
        The absolute mean error on Yaw angle estimation: 34.45 Degree
        The absolute mean error on Roll angle estimation: 8.12 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 15.14 Degree
        The absolute mean error on Yaw angle estimation: 22.90 Degree
        The absolute mean error on Roll angle estimation: 4.16 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 20.43 Degree
        The absolute mean error on Yaw angle estimation: 30.30 Degree
        The absolute mean error on Roll angle estimation: 8.22 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 25.12 Degree
        The absolute mean error on Yaw angle estimation: 28.35 Degree
        The absolute mean error on Roll angle estimation: 18.22 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 22.56 Degree
        The absolute mean error on Yaw angle estimations: 29.00 Degree
        The absolute mean error on Roll angle estimations: 9.68 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 7 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 160ms/step - loss: 0.0141
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0103
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0092
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0112
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0083
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0091
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0099
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0126
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0107
The subjects are trained: [(7, 'M01'), (23, 'M13'), (13, 'M07'), (22, 'M01'), (21, 'F02'), (19, 'M11'), (15, 'F03'), (12
, 'M06'), (17, 'M10'), (4, 'F04'), (11, 'M05'), (24, 'M14'), (16, 'M09'), (20, 'M12'), (2, 'F02'), (1, 'F01'), (6, 'F06'
), (18, 'F05'), (8, 'M02'), (10, 'M04')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 05:59:18.088914
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 9.93 Degree
        The absolute mean error on Yaw angle estimation: 30.58 Degree
        The absolute mean error on Roll angle estimation: 13.41 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 16.38 Degree
        The absolute mean error on Yaw angle estimation: 30.40 Degree
        The absolute mean error on Roll angle estimation: 5.97 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 16.75 Degree
        The absolute mean error on Yaw angle estimation: 57.09 Degree
        The absolute mean error on Roll angle estimation: 6.83 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 16.33 Degree
        The absolute mean error on Yaw angle estimation: 28.53 Degree
        The absolute mean error on Roll angle estimation: 14.37 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.85 Degree
        The absolute mean error on Yaw angle estimations: 36.65 Degree
        The absolute mean error on Roll angle estimations: 10.14 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 8 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 197s 160ms/step - loss: 0.0156
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0105
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0113
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0119
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0132
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0105
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0106
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0126
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0112
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0125
The subjects are trained: [(16, 'M09'), (21, 'F02'), (11, 'M05'), (7, 'M01'), (20, 'M12'), (23, 'M13'), (10, 'M04'), (12
, 'M06'), (22, 'M01'), (1, 'F01'), (15, 'F03'), (19, 'M11'), (24, 'M14'), (4, 'F04'), (8, 'M02'), (13, 'M07'), (2, 'F02'
), (6, 'F06'), (18, 'F05'), (17, 'M10')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 06:19:19.702945
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 13.04 Degree
        The absolute mean error on Yaw angle estimation: 29.24 Degree
        The absolute mean error on Roll angle estimation: 24.03 Degree
For the Subject 5 (F05):
946/946 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.16 Degree
        The absolute mean error on Yaw angle estimation: 28.91 Degree
        The absolute mean error on Roll angle estimation: 16.82 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.95 Degree
        The absolute mean error on Yaw angle estimation: 51.44 Degree
        The absolute mean error on Roll angle estimation: 10.80 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 12.81 Degree
        The absolute mean error on Yaw angle estimation: 27.83 Degree
        The absolute mean error on Roll angle estimation: 18.15 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.24 Degree
        The absolute mean error on Yaw angle estimations: 34.35 Degree
        The absolute mean error on Roll angle estimations: 17.45 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 9 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0112
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0107
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0109
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0122
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0103
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0126
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0129
The subjects are trained: [(22, 'M01'), (18, 'F05'), (17, 'M10'), (8, 'M02'), (10, 'M04'), (21, 'F02'), (23, 'M13'), (7,
 'M01'), (16, 'M09'), (11, 'M05'), (19, 'M11'), (12, 'M06'), (15, 'F03'), (1, 'F01'), (2, 'F02'), (24, 'M14'), (6, 'F06'
), (4, 'F04'), (20, 'M12'), (13, 'M07')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 06:39:21.063930
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 13.96 Degree
        The absolute mean error on Yaw angle estimation: 28.73 Degree
        The absolute mean error on Roll angle estimation: 17.44 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 14.20 Degree
        The absolute mean error on Yaw angle estimation: 30.95 Degree
        The absolute mean error on Roll angle estimation: 4.89 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 27.32 Degree
        The absolute mean error on Yaw angle estimation: 43.02 Degree
        The absolute mean error on Roll angle estimation: 7.23 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 23.55 Degree
        The absolute mean error on Yaw angle estimation: 28.96 Degree
        The absolute mean error on Roll angle estimation: 15.59 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.75 Degree
        The absolute mean error on Yaw angle estimations: 32.91 Degree
        The absolute mean error on Roll angle estimations: 11.29 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 10 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0146
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0099
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0105
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0123
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0116
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0115
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0102
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0128
The subjects are trained: [(13, 'M07'), (17, 'M10'), (6, 'F06'), (12, 'M06'), (8, 'M02'), (21, 'F02'), (2, 'F02'), (20,
'M12'), (11, 'M05'), (1, 'F01'), (16, 'M09'), (23, 'M13'), (4, 'F04'), (24, 'M14'), (10, 'M04'), (18, 'F05'), (22, 'M01'
), (7, 'M01'), (15, 'F03'), (19, 'M11')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 06:59:21.558295
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 9.61 Degree
        The absolute mean error on Yaw angle estimation: 31.50 Degree
        The absolute mean error on Roll angle estimation: 5.67 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 12.81 Degree
        The absolute mean error on Yaw angle estimation: 27.79 Degree
        The absolute mean error on Roll angle estimation: 7.90 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 24.27 Degree
        The absolute mean error on Yaw angle estimation: 41.18 Degree
        The absolute mean error on Roll angle estimation: 14.33 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 14.98 Degree
        The absolute mean error on Yaw angle estimation: 33.43 Degree
        The absolute mean error on Roll angle estimation: 16.87 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.42 Degree
        The absolute mean error on Yaw angle estimations: 33.47 Degree
        The absolute mean error on Roll angle estimations: 11.19 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 11 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0125
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0103
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0092
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0105
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0115
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0108
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0123
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0126
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0109
Epoch 10/10
1232/1232 [==============================] - 200s 163ms/step - loss: 0.0122
The subjects are trained: [(20, 'M12'), (19, 'M11'), (16, 'M09'), (13, 'M07'), (6, 'F06'), (23, 'M13'), (22, 'M01'), (15
, 'F03'), (4, 'F04'), (11, 'M05'), (12, 'M06'), (7, 'M01'), (8, 'M02'), (18, 'F05'), (24, 'M14'), (2, 'F02'), (1, 'F01')
, (21, 'F02'), (17, 'M10'), (10, 'M04')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 07:19:23.569762
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 15.33 Degree
        The absolute mean error on Yaw angle estimation: 30.77 Degree
        The absolute mean error on Roll angle estimation: 14.97 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 18.65 Degree
        The absolute mean error on Yaw angle estimation: 30.05 Degree
        The absolute mean error on Roll angle estimation: 4.75 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.38 Degree
        The absolute mean error on Yaw angle estimation: 47.58 Degree
        The absolute mean error on Roll angle estimation: 6.72 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.66 Degree
        The absolute mean error on Yaw angle estimation: 40.58 Degree
        The absolute mean error on Roll angle estimation: 14.06 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.25 Degree
        The absolute mean error on Yaw angle estimations: 37.24 Degree
        The absolute mean error on Roll angle estimations: 10.12 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 12 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 196s 159ms/step - loss: 0.0120
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0129
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0131
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0108
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0092
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0093
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0107
Epoch 9/10
1232/1232 [==============================] - 75s 61ms/step - loss: 0.0097
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0118
The subjects are trained: [(8, 'M02'), (22, 'M01'), (18, 'F05'), (2, 'F02'), (21, 'F02'), (6, 'F06'), (20, 'M12'), (15,
'F03'), (4, 'F04'), (11, 'M05'), (12, 'M06'), (16, 'M09'), (7, 'M01'), (24, 'M14'), (10, 'M04'), (23, 'M13'), (19, 'M11'
), (13, 'M07'), (1, 'F01'), (17, 'M10')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 07:39:23.991208
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 31.58 Degree
        The absolute mean error on Yaw angle estimation: 38.82 Degree
        The absolute mean error on Roll angle estimation: 16.97 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 22.15 Degree
        The absolute mean error on Yaw angle estimation: 37.45 Degree
        The absolute mean error on Roll angle estimation: 7.39 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 40.50 Degree
        The absolute mean error on Yaw angle estimation: 26.30 Degree
        The absolute mean error on Roll angle estimation: 10.17 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 33.17 Degree
        The absolute mean error on Yaw angle estimation: 39.15 Degree
        The absolute mean error on Roll angle estimation: 15.17 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 31.85 Degree
        The absolute mean error on Yaw angle estimations: 35.43 Degree
        The absolute mean error on Roll angle estimations: 12.42 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 13 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 196s 159ms/step - loss: 0.0153
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0105
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0113
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0096
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0136
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0116
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0099
Epoch 10/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0117
The subjects are trained: [(6, 'F06'), (2, 'F02'), (19, 'M11'), (15, 'F03'), (17, 'M10'), (23, 'M13'), (18, 'F05'), (1,
'F01'), (20, 'M12'), (21, 'F02'), (8, 'M02'), (10, 'M04'), (24, 'M14'), (12, 'M06'), (13, 'M07'), (16, 'M09'), (11, 'M05
'), (7, 'M01'), (4, 'F04'), (22, 'M01')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 07:59:23.295381
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 15.16 Degree
        The absolute mean error on Yaw angle estimation: 27.98 Degree
        The absolute mean error on Roll angle estimation: 13.83 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 9.17 Degree
        The absolute mean error on Yaw angle estimation: 28.28 Degree
        The absolute mean error on Roll angle estimation: 5.09 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 27.65 Degree
        The absolute mean error on Yaw angle estimation: 40.36 Degree
        The absolute mean error on Roll angle estimation: 7.49 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 16.12 Degree
        The absolute mean error on Yaw angle estimation: 31.61 Degree
        The absolute mean error on Roll angle estimation: 14.62 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.03 Degree
        The absolute mean error on Yaw angle estimations: 32.06 Degree
        The absolute mean error on Roll angle estimations: 10.26 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 14 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 160ms/step - loss: 0.0154
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0090
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0099
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0100
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0099
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0087
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0093
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0107
The subjects are trained: [(16, 'M09'), (17, 'M10'), (13, 'M07'), (19, 'M11'), (20, 'M12'), (6, 'F06'), (7, 'M01'), (21,
 'F02'), (11, 'M05'), (12, 'M06'), (4, 'F04'), (22, 'M01'), (15, 'F03'), (18, 'F05'), (10, 'M04'), (24, 'M14'), (1, 'F01
'), (2, 'F02'), (23, 'M13'), (8, 'M02')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 08:19:25.951737
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 10.65 Degree
        The absolute mean error on Yaw angle estimation: 34.64 Degree
        The absolute mean error on Roll angle estimation: 19.66 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 10.76 Degree
        The absolute mean error on Yaw angle estimation: 27.87 Degree
        The absolute mean error on Roll angle estimation: 9.33 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 16.03 Degree
        The absolute mean error on Yaw angle estimation: 43.55 Degree
        The absolute mean error on Roll angle estimation: 8.43 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.03 Degree
        The absolute mean error on Yaw angle estimation: 31.70 Degree
        The absolute mean error on Roll angle estimation: 14.19 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 13.62 Degree
        The absolute mean error on Yaw angle estimations: 34.44 Degree
        The absolute mean error on Roll angle estimations: 12.90 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 15 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 160ms/step - loss: 0.0109
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0102
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0107
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0120
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0097
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0105
The subjects are trained: [(17, 'M10'), (23, 'M13'), (4, 'F04'), (8, 'M02'), (13, 'M07'), (2, 'F02'), (20, 'M12'), (12,
'M06'), (15, 'F03'), (7, 'M01'), (11, 'M05'), (19, 'M11'), (24, 'M14'), (16, 'M09'), (21, 'F02'), (10, 'M04'), (6, 'F06'
), (1, 'F01'), (22, 'M01'), (18, 'F05')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 08:39:28.394304
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 29.03 Degree
        The absolute mean error on Yaw angle estimation: 26.07 Degree
        The absolute mean error on Roll angle estimation: 9.35 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 11.62 Degree
        The absolute mean error on Yaw angle estimation: 31.18 Degree
        The absolute mean error on Roll angle estimation: 6.27 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 24.91 Degree
        The absolute mean error on Yaw angle estimation: 43.14 Degree
        The absolute mean error on Roll angle estimation: 10.22 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 11.75 Degree
        The absolute mean error on Yaw angle estimation: 30.31 Degree
        The absolute mean error on Roll angle estimation: 15.58 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.33 Degree
        The absolute mean error on Yaw angle estimations: 32.67 Degree
        The absolute mean error on Roll angle estimations: 10.36 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 16 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0116
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0121
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0116
Epoch 4/10
1232/1232 [==============================] - 75s 61ms/step - loss: 0.0118
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0117
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0093
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0120
Epoch 10/10
1232/1232 [==============================] - 200s 163ms/step - loss: 0.0126
The subjects are trained: [(16, 'M09'), (21, 'F02'), (23, 'M13'), (1, 'F01'), (24, 'M14'), (18, 'F05'), (13, 'M07'), (20
, 'M12'), (15, 'F03'), (7, 'M01'), (19, 'M11'), (8, 'M02'), (12, 'M06'), (4, 'F04'), (22, 'M01'), (17, 'M10'), (10, 'M04
'), (2, 'F02'), (6, 'F06'), (11, 'M05')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 08:59:30.429236
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 14.95 Degree
        The absolute mean error on Yaw angle estimation: 26.32 Degree
        The absolute mean error on Roll angle estimation: 29.46 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 10.13 Degree
        The absolute mean error on Yaw angle estimation: 30.42 Degree
        The absolute mean error on Roll angle estimation: 14.77 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 19.76 Degree
        The absolute mean error on Yaw angle estimation: 32.22 Degree
        The absolute mean error on Roll angle estimation: 12.58 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 13.03 Degree
        The absolute mean error on Yaw angle estimation: 30.19 Degree
        The absolute mean error on Roll angle estimation: 16.98 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.47 Degree
        The absolute mean error on Yaw angle estimations: 29.78 Degree
        The absolute mean error on Roll angle estimations: 18.45 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 17 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 199s 161ms/step - loss: 0.0119
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0107
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0095
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0080
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0089
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0107
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0085
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0111
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0104
The subjects are trained: [(2, 'F02'), (1, 'F01'), (23, 'M13'), (16, 'M09'), (24, 'M14'), (22, 'M01'), (13, 'M07'), (19,
 'M11'), (4, 'F04'), (10, 'M04'), (7, 'M01'), (17, 'M10'), (20, 'M12'), (12, 'M06'), (15, 'F03'), (8, 'M02'), (6, 'F06')
, (11, 'M05'), (18, 'F05'), (21, 'F02')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 09:19:33.077232
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 20.71 Degree
        The absolute mean error on Yaw angle estimation: 25.26 Degree
        The absolute mean error on Roll angle estimation: 17.59 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 7.72 Degree
        The absolute mean error on Yaw angle estimation: 29.92 Degree
        The absolute mean error on Roll angle estimation: 8.25 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 21.73 Degree
        The absolute mean error on Yaw angle estimation: 37.43 Degree
        The absolute mean error on Roll angle estimation: 7.18 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 10.41 Degree
        The absolute mean error on Yaw angle estimation: 32.25 Degree
        The absolute mean error on Roll angle estimation: 14.67 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.14 Degree
        The absolute mean error on Yaw angle estimations: 31.21 Degree
        The absolute mean error on Roll angle estimations: 11.92 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 18 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0250
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0107
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0105
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0120
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0121
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0122
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0126
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0105
The subjects are trained: [(21, 'F02'), (12, 'M06'), (16, 'M09'), (24, 'M14'), (6, 'F06'), (8, 'M02'), (11, 'M05'), (18,
 'F05'), (15, 'F03'), (4, 'F04'), (17, 'M10'), (20, 'M12'), (2, 'F02'), (1, 'F01'), (13, 'M07'), (19, 'M11'), (23, 'M13'
), (7, 'M01'), (10, 'M04'), (22, 'M01')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 09:39:34.773008
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 13.96 Degree
        The absolute mean error on Yaw angle estimation: 35.97 Degree
        The absolute mean error on Roll angle estimation: 30.35 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 5.68 Degree
        The absolute mean error on Yaw angle estimation: 31.34 Degree
        The absolute mean error on Roll angle estimation: 14.43 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.23 Degree
        The absolute mean error on Yaw angle estimation: 29.49 Degree
        The absolute mean error on Roll angle estimation: 17.38 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 12.28 Degree
        The absolute mean error on Yaw angle estimation: 29.35 Degree
        The absolute mean error on Roll angle estimation: 19.66 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 12.29 Degree
        The absolute mean error on Yaw angle estimations: 31.54 Degree
        The absolute mean error on Roll angle estimations: 20.45 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 19 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 197s 160ms/step - loss: 0.0120
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0109
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0112
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0102
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0103
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0099
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0119
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0092
The subjects are trained: [(15, 'F03'), (13, 'M07'), (19, 'M11'), (21, 'F02'), (8, 'M02'), (2, 'F02'), (1, 'F01'), (10,
'M04'), (12, 'M06'), (17, 'M10'), (23, 'M13'), (7, 'M01'), (18, 'F05'), (22, 'M01'), (16, 'M09'), (4, 'F04'), (20, 'M12'
), (24, 'M14'), (11, 'M05'), (6, 'F06')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 09:59:35.901449
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 13.65 Degree
        The absolute mean error on Yaw angle estimation: 47.88 Degree
        The absolute mean error on Roll angle estimation: 21.35 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 8.93 Degree
        The absolute mean error on Yaw angle estimation: 28.90 Degree
        The absolute mean error on Roll angle estimation: 5.28 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 22.03 Degree
        The absolute mean error on Yaw angle estimation: 34.16 Degree
        The absolute mean error on Roll angle estimation: 8.37 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 12.10 Degree
        The absolute mean error on Yaw angle estimation: 35.05 Degree
        The absolute mean error on Roll angle estimation: 15.56 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.18 Degree
        The absolute mean error on Yaw angle estimations: 36.50 Degree
        The absolute mean error on Roll angle estimations: 12.64 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 20 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 197s 160ms/step - loss: 0.0135
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0094
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0115
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0113
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0089
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0088
Epoch 10/10
1232/1232 [==============================] - 199s 162ms/step - loss: 0.0118
The subjects are trained: [(24, 'M14'), (12, 'M06'), (10, 'M04'), (22, 'M01'), (15, 'F03'), (18, 'F05'), (2, 'F02'), (19
, 'M11'), (23, 'M13'), (13, 'M07'), (17, 'M10'), (11, 'M05'), (8, 'M02'), (21, 'F02'), (20, 'M12'), (7, 'M01'), (16, 'M0
9'), (4, 'F04'), (1, 'F01'), (6, 'F06')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 10:19:35.450826
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 10.50 Degree
        The absolute mean error on Yaw angle estimation: 46.65 Degree
        The absolute mean error on Roll angle estimation: 12.05 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 12.51 Degree
        The absolute mean error on Yaw angle estimation: 33.84 Degree
        The absolute mean error on Roll angle estimation: 9.83 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 15.17 Degree
        The absolute mean error on Yaw angle estimation: 46.34 Degree
        The absolute mean error on Roll angle estimation: 12.99 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 10.67 Degree
        The absolute mean error on Yaw angle estimation: 27.15 Degree
        The absolute mean error on Roll angle estimation: 15.93 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 12.21 Degree
        The absolute mean error on Yaw angle estimations: 38.49 Degree
        The absolute mean error on Roll angle estimations: 12.70 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 21 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0151
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0102
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0107
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0114
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0095
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0106
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 10/10
1232/1232 [==============================] - 200s 163ms/step - loss: 0.0099
The subjects are trained: [(13, 'M07'), (8, 'M02'), (6, 'F06'), (15, 'F03'), (23, 'M13'), (19, 'M11'), (12, 'M06'), (20,
 'M12'), (21, 'F02'), (17, 'M10'), (10, 'M04'), (7, 'M01'), (22, 'M01'), (24, 'M14'), (18, 'F05'), (1, 'F01'), (16, 'M09
'), (2, 'F02'), (11, 'M05'), (4, 'F04')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 10:39:37.177961
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 7.14 Degree
        The absolute mean error on Yaw angle estimation: 50.22 Degree
        The absolute mean error on Roll angle estimation: 12.21 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 22.03 Degree
        The absolute mean error on Yaw angle estimation: 33.63 Degree
        The absolute mean error on Roll angle estimation: 3.53 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 18.25 Degree
        The absolute mean error on Yaw angle estimation: 43.50 Degree
        The absolute mean error on Roll angle estimation: 6.83 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 13.71 Degree
        The absolute mean error on Yaw angle estimation: 26.39 Degree
        The absolute mean error on Roll angle estimation: 14.05 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.28 Degree
        The absolute mean error on Yaw angle estimations: 38.44 Degree
        The absolute mean error on Roll angle estimations: 9.15 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 22 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0126
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0098
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0120
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0113
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0093
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0112
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0107
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0117
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0105
Epoch 10/10
1232/1232 [==============================] - 200s 163ms/step - loss: 0.0134
The subjects are trained: [(19, 'M11'), (1, 'F01'), (2, 'F02'), (17, 'M10'), (11, 'M05'), (15, 'F03'), (13, 'M07'), (24,
 'M14'), (8, 'M02'), (6, 'F06'), (16, 'M09'), (18, 'F05'), (23, 'M13'), (12, 'M06'), (21, 'F02'), (4, 'F04'), (7, 'M01')
, (10, 'M04'), (20, 'M12'), (22, 'M01')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 10:59:39.583946
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 20.03 Degree
        The absolute mean error on Yaw angle estimation: 50.49 Degree
        The absolute mean error on Roll angle estimation: 12.39 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 8.25 Degree
        The absolute mean error on Yaw angle estimation: 24.17 Degree
        The absolute mean error on Roll angle estimation: 3.86 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 12.81 Degree
        The absolute mean error on Yaw angle estimation: 35.51 Degree
        The absolute mean error on Roll angle estimation: 7.72 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 12.32 Degree
        The absolute mean error on Yaw angle estimation: 31.09 Degree
        The absolute mean error on Roll angle estimation: 13.44 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 13.35 Degree
        The absolute mean error on Yaw angle estimations: 35.31 Degree
        The absolute mean error on Roll angle estimations: 9.35 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 23 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0104
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0117
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0119
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0108
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0096
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0106
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0117
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0114
Epoch 10/10
1232/1232 [==============================] - 200s 163ms/step - loss: 0.0118
The subjects are trained: [(10, 'M04'), (21, 'F02'), (17, 'M10'), (22, 'M01'), (1, 'F01'), (23, 'M13'), (18, 'F05'), (20
, 'M12'), (19, 'M11'), (15, 'F03'), (8, 'M02'), (16, 'M09'), (24, 'M14'), (4, 'F04'), (12, 'M06'), (7, 'M01'), (2, 'F02'
), (11, 'M05'), (6, 'F06'), (13, 'M07')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 11:19:40.886742
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 13.02 Degree
        The absolute mean error on Yaw angle estimation: 29.85 Degree
        The absolute mean error on Roll angle estimation: 16.40 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 22.28 Degree
        The absolute mean error on Yaw angle estimation: 40.36 Degree
        The absolute mean error on Roll angle estimation: 5.52 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.72 Degree
        The absolute mean error on Yaw angle estimation: 59.95 Degree
        The absolute mean error on Roll angle estimation: 6.19 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 15.41 Degree
        The absolute mean error on Yaw angle estimation: 40.60 Degree
        The absolute mean error on Roll angle estimation: 14.94 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.11 Degree
        The absolute mean error on Yaw angle estimations: 42.69 Degree
        The absolute mean error on Roll angle estimations: 10.76 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 24 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 160ms/step - loss: 0.0149
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0102
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0112
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0114
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0113
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0138
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0135
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0120
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0089
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0111
The subjects are trained: [(19, 'M11'), (8, 'M02'), (23, 'M13'), (12, 'M06'), (15, 'F03'), (4, 'F04'), (20, 'M12'), (2,
'F02'), (11, 'M05'), (21, 'F02'), (6, 'F06'), (10, 'M04'), (1, 'F01'), (17, 'M10'), (22, 'M01'), (16, 'M09'), (13, 'M07'
), (24, 'M14'), (18, 'F05'), (7, 'M01')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 11:39:42.814391
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 9.43 Degree
        The absolute mean error on Yaw angle estimation: 41.30 Degree
        The absolute mean error on Roll angle estimation: 14.42 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 11.67 Degree
        The absolute mean error on Yaw angle estimation: 29.24 Degree
        The absolute mean error on Roll angle estimation: 5.95 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 22.06 Degree
        The absolute mean error on Yaw angle estimation: 39.28 Degree
        The absolute mean error on Roll angle estimation: 9.33 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 15.29 Degree
        The absolute mean error on Yaw angle estimation: 34.02 Degree
        The absolute mean error on Roll angle estimation: 13.84 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.61 Degree
        The absolute mean error on Yaw angle estimations: 35.96 Degree
        The absolute mean error on Roll angle estimations: 10.88 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 25 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 160ms/step - loss: 0.0112
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0088
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0095
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0106
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0100
Epoch 8/10
1232/1232 [==============================] - 75s 61ms/step - loss: 0.0087
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 10/10
1232/1232 [==============================] - 200s 163ms/step - loss: 0.0127
The subjects are trained: [(23, 'M13'), (19, 'M11'), (12, 'M06'), (4, 'F04'), (7, 'M01'), (24, 'M14'), (17, 'M10'), (10,
 'M04'), (8, 'M02'), (21, 'F02'), (2, 'F02'), (15, 'F03'), (22, 'M01'), (11, 'M05'), (13, 'M07'), (6, 'F06'), (20, 'M12'
), (1, 'F01'), (16, 'M09'), (18, 'F05')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 11:59:43.961665
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 7.03 Degree
        The absolute mean error on Yaw angle estimation: 38.12 Degree
        The absolute mean error on Roll angle estimation: 4.54 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 13.99 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 18.87 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.45 Degree
        The absolute mean error on Yaw angle estimation: 36.74 Degree
        The absolute mean error on Roll angle estimation: 18.98 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 13.61 Degree
        The absolute mean error on Yaw angle estimation: 32.16 Degree
        The absolute mean error on Roll angle estimation: 19.73 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 13.02 Degree
        The absolute mean error on Yaw angle estimations: 33.43 Degree
        The absolute mean error on Roll angle estimations: 15.53 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 26 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 197s 160ms/step - loss: 0.0138
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0119
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0119
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0100
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0114
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0097
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0111
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0114
Epoch 10/10
1232/1232 [==============================] - 199s 161ms/step - loss: 0.0114
The subjects are trained: [(18, 'F05'), (13, 'M07'), (24, 'M14'), (16, 'M09'), (20, 'M12'), (15, 'F03'), (17, 'M10'), (2
2, 'M01'), (7, 'M01'), (19, 'M11'), (21, 'F02'), (2, 'F02'), (6, 'F06'), (1, 'F01'), (4, 'F04'), (10, 'M04'), (12, 'M06'
), (11, 'M05'), (8, 'M02'), (23, 'M13')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 12:19:44.348001
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 23.25 Degree
        The absolute mean error on Yaw angle estimation: 31.06 Degree
        The absolute mean error on Roll angle estimation: 19.32 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 11.85 Degree
        The absolute mean error on Yaw angle estimation: 28.46 Degree
        The absolute mean error on Roll angle estimation: 6.22 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 25.91 Degree
        The absolute mean error on Yaw angle estimation: 46.81 Degree
        The absolute mean error on Roll angle estimation: 7.07 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 15.89 Degree
        The absolute mean error on Yaw angle estimation: 33.52 Degree
        The absolute mean error on Roll angle estimation: 13.15 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.22 Degree
        The absolute mean error on Yaw angle estimations: 34.96 Degree
        The absolute mean error on Roll angle estimations: 11.44 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 27 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 196s 159ms/step - loss: 0.0146
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0104
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0100
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0112
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0102
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0103
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0103
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0097
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 10/10
1232/1232 [==============================] - 200s 163ms/step - loss: 0.0123
The subjects are trained: [(19, 'M11'), (12, 'M06'), (2, 'F02'), (6, 'F06'), (15, 'F03'), (16, 'M09'), (7, 'M01'), (10,
'M04'), (17, 'M10'), (20, 'M12'), (21, 'F02'), (22, 'M01'), (8, 'M02'), (23, 'M13'), (11, 'M05'), (4, 'F04'), (24, 'M14'
), (13, 'M07'), (1, 'F01'), (18, 'F05')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 12:39:45.222328
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 19.38 Degree
        The absolute mean error on Yaw angle estimation: 35.55 Degree
        The absolute mean error on Roll angle estimation: 15.86 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 10.25 Degree
        The absolute mean error on Yaw angle estimation: 31.75 Degree
        The absolute mean error on Roll angle estimation: 7.83 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 26.46 Degree
        The absolute mean error on Yaw angle estimation: 39.44 Degree
        The absolute mean error on Roll angle estimation: 9.26 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 9.89 Degree
        The absolute mean error on Yaw angle estimation: 35.25 Degree
        The absolute mean error on Roll angle estimation: 13.11 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.49 Degree
        The absolute mean error on Yaw angle estimations: 35.50 Degree
        The absolute mean error on Roll angle estimations: 11.51 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 28 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 161ms/step - loss: 0.0135
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0103
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0093
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0090
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0102
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0112
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0111
The subjects are trained: [(10, 'M04'), (12, 'M06'), (15, 'F03'), (1, 'F01'), (17, 'M10'), (20, 'M12'), (7, 'M01'), (23,
 'M13'), (24, 'M14'), (13, 'M07'), (18, 'F05'), (22, 'M01'), (21, 'F02'), (4, 'F04'), (11, 'M05'), (6, 'F06'), (19, 'M11
'), (16, 'M09'), (2, 'F02'), (8, 'M02')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 12:59:47.772998
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 11.44 Degree
        The absolute mean error on Yaw angle estimation: 37.38 Degree
        The absolute mean error on Roll angle estimation: 12.12 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 11.20 Degree
        The absolute mean error on Yaw angle estimation: 28.75 Degree
        The absolute mean error on Roll angle estimation: 4.33 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 15.42 Degree
        The absolute mean error on Yaw angle estimation: 32.72 Degree
        The absolute mean error on Roll angle estimation: 7.20 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 7.96 Degree
        The absolute mean error on Yaw angle estimation: 33.92 Degree
        The absolute mean error on Roll angle estimation: 13.72 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 11.50 Degree
        The absolute mean error on Yaw angle estimations: 33.19 Degree
        The absolute mean error on Roll angle estimations: 9.34 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 29 completed!
Training model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
Epoch 1/10
1232/1232 [==============================] - 198s 160ms/step - loss: 0.0105
Epoch 2/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0108
Epoch 3/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0088
Epoch 4/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0107
Epoch 5/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0103
Epoch 6/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0102
Epoch 7/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0101
Epoch 8/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0107
Epoch 9/10
1232/1232 [==============================] - 76s 61ms/step - loss: 0.0110
Epoch 10/10
1232/1232 [==============================] - 200s 162ms/step - loss: 0.0119
The subjects are trained: [(21, 'F02'), (18, 'F05'), (10, 'M04'), (15, 'F03'), (24, 'M14'), (13, 'M07'), (17, 'M10'), (8
, 'M02'), (20, 'M12'), (6, 'F06'), (19, 'M11'), (4, 'F04'), (11, 'M05'), (16, 'M09'), (12, 'M06'), (2, 'F02'), (1, 'F01'
), (22, 'M01'), (23, 'M13'), (7, 'M01')]
Evaluating model CNN_VGG16_inc_top_output3_BatchSize10_inEpochs10_outEpochs30_AdamOpt_lr-0.000100_2019-02-01_03-20-20
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-02-01 13:19:50.149069
For the Subject 3 (F03):
730/730 [==============================] - 8s 11ms/step
        The absolute mean error on Pitch angle estimation: 17.32 Degree
        The absolute mean error on Yaw angle estimation: 35.22 Degree
        The absolute mean error on Roll angle estimation: 14.87 Degree
For the Subject 5 (F05):
946/946 [==============================] - 11s 11ms/step
        The absolute mean error on Pitch angle estimation: 12.45 Degree
        The absolute mean error on Yaw angle estimation: 28.38 Degree
        The absolute mean error on Roll angle estimation: 4.01 Degree
For the Subject 9 (M03):
882/882 [==============================] - 10s 11ms/step
        The absolute mean error on Pitch angle estimation: 22.12 Degree
        The absolute mean error on Yaw angle estimation: 31.42 Degree
        The absolute mean error on Roll angle estimation: 6.77 Degree
For the Subject 14 (M08):
797/797 [==============================] - 9s 11ms/step
        The absolute mean error on Pitch angle estimation: 9.71 Degree
        The absolute mean error on Yaw angle estimation: 29.77 Degree
        The absolute mean error on Roll angle estimation: 13.16 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.40 Degree
        The absolute mean error on Yaw angle estimations: 31.20 Degree
        The absolute mean error on Roll angle estimations: 9.70 Degree
Exp2019-02-01_03-20-20 completed!
Experiment 30 completed!
Exp2019-02-01_03-20-20.h5 has been saved.
subject3_Exp2019-02-01_03-20-20.png has been saved by 2019-02-01 13:21:02.437081.
subject5_Exp2019-02-01_03-20-20.png has been saved by 2019-02-01 13:21:02.641874.
subject9_Exp2019-02-01_03-20-20.png has been saved by 2019-02-01 13:21:02.844978.
subject14_Exp2019-02-01_03-20-20.png has been saved by 2019-02-01 13:21:03.070198.
Model Exp2019-02-01_03-20-20 has been evaluated successfully.
Model Exp2019-02-01_03-20-20 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/CNN_Evaluater$
