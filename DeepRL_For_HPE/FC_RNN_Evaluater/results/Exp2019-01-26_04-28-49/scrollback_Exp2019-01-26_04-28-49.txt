2019-01-25 18:17:15.532336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 18:17:15.532594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 18:17:15.532612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 18:17:15.687658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 18:17:15.687684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 18:17:15.687689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 18:17:15.687822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_03-13-09.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 10)                164280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 33
=================================================================
Total params: 134,424,857
Trainable params: 164,313
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate = 0.0001
in_epochs = 1
out_epochs = 30
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04
'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'),
(20, 'M12'), (21, 'F02'), (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-25_03-13-09_and_2019-01-25_18-17-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 18:17:18.623197
For the Subject 3 (F03):
730/730 [==============================] - 58s 80ms/step
(714, 1) (730, 1)
Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 64, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 91,
in evaluateCNN_LSTM
    outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, ba
tch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 64,
in evaluateSubject
    matrix = numpy.concatenate((test_labels[timesteps:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_03-13-09
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 18:19:48.786476: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 18:19:48.882389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 18:19:48.882692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 18:19:48.882707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 18:19:49.038317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 18:19:49.038344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 18:19:49.038349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 18:19:49.038525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_03-13-09.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 10)                164280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 33
=================================================================
Total params: 134,424,857
Trainable params: 164,313
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate = 0.0001
in_epochs = 1
out_epochs = 30
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04
'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'),
(20, 'M12'), (21, 'F02'), (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 18:19:51.984968
For the Subject 3 (F03):
730/730 [==============================] - 59s 81ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 8.64 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 27.16 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 5.80 Degree
For the Subject 5 (F05):
946/946 [==============================] - 78s 82ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 5.74 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 19.72 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 5.28 Degree
For the Subject 9 (M03):
882/882 [==============================] - 73s 83ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 27.54 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 36.32 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 9.94 Degree
For the Subject 14 (M08):
797/797 [==============================] - 66s 83ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 18.87 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 42.55 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 34.30 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.20 Degree
        The absolute mean error on Yaw angle estimations: 31.44 Degree
        The absolute mean error on Roll angle estimations: 13.83 Degree
subject3_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png has been saved by 2019-01-25 18:25:01.643703.
subject5_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png has been saved by 2019-01-25 18:25:01.840532.
subject9_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png has been saved by 2019-01-25 18:25:02.040890.
subject14_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png has been saved by 2019-01-25 18:25:02.257325.
Model Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49 has been evaluated successfully.
Model Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_03-13-09
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 18:50:03.057010: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 18:50:03.152630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 18:50:03.152886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 18:50:03.152900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 18:50:03.308227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 18:50:03.308254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 18:50:03.308259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 18:50:03.308396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_03-13-09.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 10)                164280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 33
=================================================================
Total params: 134,424,857
Trainable params: 164,313
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate = 0.0001
in_epochs = 1
out_epochs = 30
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04
'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'),
(20, 'M12'), (21, 'F02'), (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-25_03-13-09_and_2019-01-25_18-50-04
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 18:50:06.283436
For the Subject 3 (F03):
Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 64, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 92,
in evaluateCNN_LSTM
    outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, ba
tch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 52,
in evaluateSubject
    model.reset_states()
NameError: name 'model' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_03-13-09
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 18:51:24.032334: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 18:51:24.129107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 18:51:24.129369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 18:51:24.129384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 18:51:24.285845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 18:51:24.285872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 18:51:24.285879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 18:51:24.286020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_03-13-09.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 10)                164280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 33
=================================================================
Total params: 134,424,857
Trainable params: 164,313
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate = 0.0001
in_epochs = 1
out_epochs = 30
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04
'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'),
(20, 'M12'), (21, 'F02'), (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 18:51:27.299800
For the Subject 3 (F03):
730/730 [==============================] - 58s 80ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 8.64 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 27.16 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 5.80 Degree
For the Subject 5 (F05):
946/946 [==============================] - 77s 82ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 5.74 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 19.72 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 5.28 Degree
For the Subject 9 (M03):
882/882 [==============================] - 72s 82ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 27.54 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 36.32 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 9.94 Degree
For the Subject 14 (M08):
797/797 [==============================] - 65s 82ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 18.87 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 42.55 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 34.30 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.20 Degree
        The absolute mean error on Yaw angle estimations: 31.44 Degree
        The absolute mean error on Roll angle estimations: 13.83 Degree
subject3_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png has been saved by 2019-01-25 18:56:34.782819.
subject5_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png has been saved by 2019-01-25 18:56:34.978974.
subject9_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png has been saved by 2019-01-25 18:56:35.178593.
subject14_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png has been saved by 2019-01-25 18:56:35.394346.
Model Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25 has been evaluated successfully.
Model Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit -m "stateless 30
 epochs"
[master 03447ea] stateless 30 epochs
 43 files changed, 27888 insertions(+), 18 deletions(-)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26_and_2019-01-25_01-00-50/output_
Exp2019-01-24_23-58-26_and_2019-01-25_01-00-50.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/output_
Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
14_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
3_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
5_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
9_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-12-13/output_Exp2019-01-25_02-12-13.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-12-13/subject9_Exp2019-01-25_02-12-13
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-13-30/output_Exp2019-01-25_02-13-30.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-13-30/subject1_Exp2019-01-25_02-13-30
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-17-13/output_Exp2019-01-25_02-17-13.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-17-13/subject1_Exp2019-01-25_02-17-13
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/output_Exp2019-01-25_02-20-23.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject14_Exp2019-01-25_02-20-2
3.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject3_Exp2019-01-25_02-20-23
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject5_Exp2019-01-25_02-20-23
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject9_Exp2019-01-25_02-20-23
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/output_Exp2019-01-25_02-37-37.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject14_Exp2019-01-25_02-37-3
7.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject3_Exp2019-01-25_02-37-37
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject5_Exp2019-01-25_02-37-37
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject9_Exp2019-01-25_02-37-37
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/output_Exp2019-01-25_03-00-06.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject14_Exp2019-01-25_03-00-0
6.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject3_Exp2019-01-25_03-00-06
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject5_Exp2019-01-25_03-00-06
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject9_Exp2019-01-25_03-00-06
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/output_
Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
14_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
3_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
5_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
9_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/output_
Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/scrollb
ack_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
14_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
3_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
5_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
9_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 59, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (58/58), done.
Writing objects: 100% (59/59), 3.38 MiB | 945.00 KiB/s, done.
Total 59 (delta 17), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   c8143d6..03447ea  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 19:22:55.671462: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 19:22:55.769163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 19:22:55.769423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 19:22:55.769435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 19:22:55.924424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 19:22:55.924451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 19:22:55.924456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 19:22:55.924596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_19-22-56 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 320)                  5653760
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    963
=================================================================
Total params: 139,915,267
Trainable params: 5,654,723
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 320
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.0
00010_2019-01-25_19-22-56
All frames and annotations from 20 datasets have been read by 2019-01-25 19:23:00.967199
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 19:23:07.377758!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0298 - mean_absolute_error: 0.1315
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 19:23:27.335422!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.0203 - mean_absolute_error: 0.1100
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 19:23:43.862988!
Epoch 1/1
654/654 [==============================] - 14s 21ms/step - loss: 0.0242 - mean_absolute_error: 0.1121
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 19:24:02.392879!
Epoch 1/1
502/502 [==============================] - 10s 21ms/step - loss: 0.0241 - mean_absolute_error: 0.1135
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 19:24:20.547182!
Epoch 1/1
772/772 [==============================] - 16s 21ms/step - loss: 0.0269 - mean_absolute_error: 0.1068
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 19:24:42.020915!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.0343 - mean_absolute_error: 0.1387
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 19:24:59.921230!
Epoch 1/1
634/634 [==============================] - 13s 20ms/step - loss: 0.0282 - mean_absolute_error: 0.1261
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 19:25:21.750818!
Epoch 1/1
914/914 [==============================] - 19s 21ms/step - loss: 0.0146 - mean_absolute_error: 0.0874
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 19:25:48.493508!
Epoch 1/1
745/745 [==============================] - 15s 21ms/step - loss: 0.0138 - mean_absolute_error: 0.0817
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 19:26:11.240707!
Epoch 1/1
732/732 [==============================] - 15s 21ms/step - loss: 0.0129 - mean_absolute_error: 0.0776
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 19:26:33.642862!
Epoch 1/1
726/726 [==============================] - 15s 21ms/step - loss: 0.0190 - mean_absolute_error: 0.0900
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 19:26:53.730193!
Epoch 1/1
498/498 [==============================] - 10s 21ms/step - loss: 0.0220 - mean_absolute_error: 0.1016
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 19:27:09.951435!
Epoch 1/1
614/614 [==============================] - 13s 21ms/step - loss: 0.0219 - mean_absolute_error: 0.1093
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 19:27:27.956547!
Epoch 1/1
511/511 [==============================] - 11s 21ms/step - loss: 0.0200 - mean_absolute_error: 0.0967
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 19:27:46.016054!
Epoch 1/1
744/744 [==============================] - 15s 21ms/step - loss: 0.0163 - mean_absolute_error: 0.0888
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 19:28:06.923049!
Epoch 1/1
556/556 [==============================] - 11s 21ms/step - loss: 0.0232 - mean_absolute_error: 0.1052
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 19:28:22.119816!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0113 - mean_absolute_error: 0.0784
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 19:28:35.524195!
Epoch 1/1
542/542 [==============================] - 11s 21ms/step - loss: 0.0388 - mean_absolute_error: 0.1397
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 19:28:51.830547!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0102 - mean_absolute_error: 0.0733
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 19:29:07.692549!
Epoch 1/1
572/572 [==============================] - 12s 21ms/step - loss: 0.0122 - mean_absolute_error: 0.0713
Epoch 1 completed!
Exp2019-01-25_19-22-56.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0
.000010_2019-01-25_19-22-56
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 19:29:21.851676
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 12.84 Degree
        The absolute mean error on Yaw angle estimation: 103.06 Degree
        The absolute mean error on Roll angle estimation: 16.31 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 10.49 Degree
        The absolute mean error on Yaw angle estimation: 31.10 Degree
        The absolute mean error on Roll angle estimation: 5.07 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 12.84 Degree
        The absolute mean error on Yaw angle estimation: 29.57 Degree
        The absolute mean error on Roll angle estimation: 35.91 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 17.41 Degree
        The absolute mean error on Yaw angle estimation: 54.23 Degree
        The absolute mean error on Roll angle estimation: 27.75 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 13.39 Degree
        The absolute mean error on Yaw angle estimations: 54.49 Degree
        The absolute mean error on Roll angle estimations: 21.26 Degree
subject3_Exp2019-01-25_19-22-56.png has been saved by 2019-01-25 19:30:28.871980.
subject5_Exp2019-01-25_19-22-56.png has been saved by 2019-01-25 19:30:29.076140.
subject9_Exp2019-01-25_19-22-56.png has been saved by 2019-01-25 19:30:29.277677.
subject14_Exp2019-01-25_19-22-56.png has been saved by 2019-01-25 19:30:29.499349.
Model Exp2019-01-25_19-22-56 has been evaluated successfully.
Model Exp2019-01-25_19-22-56 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 19:31:27.238735: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 19:31:27.335759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 19:31:27.336064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 19:31:27.336078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 19:31:27.492029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 19:31:27.492054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 19:31:27.492059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 19:31:27.492236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_19-31-28 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 320)                  5653760
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    963
=================================================================
Total params: 139,915,267
Trainable params: 5,654,723
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 320
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.0
00100_2019-01-25_19-31-28
All frames and annotations from 20 datasets have been read by 2019-01-25 19:31:32.605126
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 19:31:39.011387!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0293 - mean_absolute_error: 0.1096
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 19:31:58.887011!
Epoch 1/1
492/492 [==============================] - 10s 21ms/step - loss: 0.0114 - mean_absolute_error: 0.0746
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 19:32:15.623966!
Epoch 1/1
654/654 [==============================] - 14s 21ms/step - loss: 0.0140 - mean_absolute_error: 0.0862
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 19:32:34.229226!
Epoch 1/1
502/502 [==============================] - 10s 21ms/step - loss: 0.0159 - mean_absolute_error: 0.0911
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 19:32:52.511540!
Epoch 1/1
772/772 [==============================] - 16s 21ms/step - loss: 0.0129 - mean_absolute_error: 0.0718
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 19:33:13.995964!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.0228 - mean_absolute_error: 0.1116
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 19:33:31.807473!
Epoch 1/1
634/634 [==============================] - 13s 21ms/step - loss: 0.0169 - mean_absolute_error: 0.0977
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 19:33:53.883514!
Epoch 1/1
914/914 [==============================] - 19s 21ms/step - loss: 0.0092 - mean_absolute_error: 0.0676
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 19:34:20.524197!
Epoch 1/1
745/745 [==============================] - 15s 21ms/step - loss: 0.0076 - mean_absolute_error: 0.0594
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 19:34:43.224167!
Epoch 1/1
732/732 [==============================] - 15s 21ms/step - loss: 0.0101 - mean_absolute_error: 0.0675
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 19:35:05.614513!
Epoch 1/1
726/726 [==============================] - 15s 21ms/step - loss: 0.0146 - mean_absolute_error: 0.0800
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 19:35:25.854676!
Epoch 1/1
498/498 [==============================] - 10s 21ms/step - loss: 0.0173 - mean_absolute_error: 0.0832
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 19:35:42.076243!
Epoch 1/1
614/614 [==============================] - 13s 21ms/step - loss: 0.0179 - mean_absolute_error: 0.1016
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 19:35:59.864252!
Epoch 1/1
511/511 [==============================] - 11s 21ms/step - loss: 0.0142 - mean_absolute_error: 0.0746
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 19:36:18.044680!
Epoch 1/1
744/744 [==============================] - 15s 20ms/step - loss: 0.0137 - mean_absolute_error: 0.0768
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 19:36:38.375221!
Epoch 1/1
556/556 [==============================] - 12s 21ms/step - loss: 0.0127 - mean_absolute_error: 0.0723
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 19:36:53.839500!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0115 - mean_absolute_error: 0.0742
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 19:37:07.330325!
Epoch 1/1
542/542 [==============================] - 11s 21ms/step - loss: 0.0368 - mean_absolute_error: 0.1306
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 19:37:23.458181!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0084 - mean_absolute_error: 0.0639
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 19:37:39.385376!
Epoch 1/1
572/572 [==============================] - 12s 20ms/step - loss: 0.0103 - mean_absolute_error: 0.0635
Epoch 1 completed!
Exp2019-01-25_19-31-28.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0
.000100_2019-01-25_19-31-28
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 19:37:53.493730
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 26.15 Degree
        The absolute mean error on Yaw angle estimation: 90.21 Degree
        The absolute mean error on Roll angle estimation: 29.46 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 7.16 Degree
        The absolute mean error on Yaw angle estimation: 25.04 Degree
        The absolute mean error on Roll angle estimation: 6.84 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 26.52 Degree
        The absolute mean error on Yaw angle estimation: 26.45 Degree
        The absolute mean error on Roll angle estimation: 10.82 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 13.55 Degree
        The absolute mean error on Yaw angle estimation: 45.58 Degree
        The absolute mean error on Roll angle estimation: 17.28 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.34 Degree
        The absolute mean error on Yaw angle estimations: 46.82 Degree
        The absolute mean error on Roll angle estimations: 16.10 Degree
subject3_Exp2019-01-25_19-31-28.png has been saved by 2019-01-25 19:39:00.520167.
subject5_Exp2019-01-25_19-31-28.png has been saved by 2019-01-25 19:39:00.721563.
subject9_Exp2019-01-25_19-31-28.png has been saved by 2019-01-25 19:39:00.920002.
subject14_Exp2019-01-25_19-31-28.png has been saved by 2019-01-25 19:39:01.138635.
Model Exp2019-01-25_19-31-28 has been evaluated successfully.
Model Exp2019-01-25_19-31-28 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_19-31-28 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 19:41:00.790660: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 19:41:00.887793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 19:41:00.888051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 19:41:00.888063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 19:41:01.043755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 19:41:01.043781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 19:41:01.043788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 19:41:01.043929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_19-31-28.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 320)                  5653760
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    963
=================================================================
Total params: 139,915,267
Trainable params: 5,654,723
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 320
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02
All frames and annotations from 20 datasets have been read by 2019-01-25 19:41:06.566689
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 19:41:12.967112!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0076 - mean_absolute_error: 0.0576
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 19:41:32.436436!
Epoch 1/1
492/492 [==============================] - 10s 21ms/step - loss: 0.0067 - mean_absolute_error: 0.0571
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 19:41:49.138914!
Epoch 1/1
654/654 [==============================] - 14s 21ms/step - loss: 0.0091 - mean_absolute_error: 0.0671
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 19:42:07.732692!
Epoch 1/1
502/502 [==============================] - 10s 20ms/step - loss: 0.0126 - mean_absolute_error: 0.0791
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 19:42:25.872676!
Epoch 1/1
772/772 [==============================] - 16s 21ms/step - loss: 0.0070 - mean_absolute_error: 0.0571
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 19:42:47.557152!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.0160 - mean_absolute_error: 0.0924
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 19:43:05.470606!
Epoch 1/1
634/634 [==============================] - 13s 21ms/step - loss: 0.0164 - mean_absolute_error: 0.0968
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 19:43:27.326531!
Epoch 1/1
914/914 [==============================] - 19s 21ms/step - loss: 0.0063 - mean_absolute_error: 0.0547
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 19:43:53.945284!
Epoch 1/1
745/745 [==============================] - 15s 21ms/step - loss: 0.0035 - mean_absolute_error: 0.0432
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 19:44:16.789719!
Epoch 1/1
732/732 [==============================] - 15s 21ms/step - loss: 0.0035 - mean_absolute_error: 0.0419
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 19:44:39.283436!
Epoch 1/1
726/726 [==============================] - 15s 21ms/step - loss: 0.0052 - mean_absolute_error: 0.0501
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 19:44:59.429129!
Epoch 1/1
498/498 [==============================] - 10s 21ms/step - loss: 0.0079 - mean_absolute_error: 0.0575
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 19:45:15.815033!
Epoch 1/1
614/614 [==============================] - 13s 21ms/step - loss: 0.0109 - mean_absolute_error: 0.0790
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 19:45:33.709925!
Epoch 1/1
511/511 [==============================] - 11s 21ms/step - loss: 0.0043 - mean_absolute_error: 0.0465
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 19:45:51.647740!
Epoch 1/1
744/744 [==============================] - 16s 21ms/step - loss: 0.0067 - mean_absolute_error: 0.0574
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 19:46:12.636868!
Epoch 1/1
556/556 [==============================] - 11s 20ms/step - loss: 0.0075 - mean_absolute_error: 0.0618
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 19:46:27.785545!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0083 - mean_absolute_error: 0.0662
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 19:46:41.269235!
Epoch 1/1
542/542 [==============================] - 11s 21ms/step - loss: 0.0172 - mean_absolute_error: 0.0908
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 19:46:57.544770!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0059 - mean_absolute_error: 0.0529
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 19:47:13.419765!
Epoch 1/1
572/572 [==============================] - 12s 21ms/step - loss: 0.0037 - mean_absolute_error: 0.0422
Epoch 1 completed!
Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 19:47:27.710249
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 23.32 Degree
        The absolute mean error on Yaw angle estimation: 71.51 Degree
        The absolute mean error on Roll angle estimation: 38.54 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 5.54 Degree
        The absolute mean error on Yaw angle estimation: 26.87 Degree
        The absolute mean error on Roll angle estimation: 6.51 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 24.33 Degree
        The absolute mean error on Yaw angle estimation: 20.80 Degree
        The absolute mean error on Roll angle estimation: 14.47 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 13.23 Degree
        The absolute mean error on Yaw angle estimation: 41.72 Degree
        The absolute mean error on Roll angle estimation: 24.13 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.61 Degree
        The absolute mean error on Yaw angle estimations: 40.23 Degree
        The absolute mean error on Roll angle estimations: 20.91 Degree
subject3_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png has been saved by 2019-01-25 19:48:34.708354.
subject5_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png has been saved by 2019-01-25 19:48:34.905422.
subject9_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png has been saved by 2019-01-25 19:48:35.103156.
subject14_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png has been saved by 2019-01-25 19:48:35.319176.
Model Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02 has been evaluated successfully.
Model Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py

/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 19:51:43.217483: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 19:51:43.314361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 19:51:43.314624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 19:51:43.314638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 19:51:43.471013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 19:51:43.471040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 19:51:43.471050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 19:51:43.471190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_19-51-44 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_19-51-44
All frames and annotations from 20 datasets have been read by 2019-01-25 19:51:48.441954
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 19:51:54.843805!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.0189 - mean_absolute_error: 0.0961
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 19:52:12.880277!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0125 - mean_absolute_error: 0.0784
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 19:52:28.335169!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0162 - mean_absolute_error: 0.0883
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 19:52:44.847695!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0154 - mean_absolute_error: 0.0869
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 19:53:01.668156!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0181 - mean_absolute_error: 0.0876
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 19:53:21.109962!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0239 - mean_absolute_error: 0.1149
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 19:53:37.539858!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0307 - mean_absolute_error: 0.1283
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 19:53:58.002635!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0165 - mean_absolute_error: 0.0902
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 19:54:22.350679!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0098 - mean_absolute_error: 0.0694
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 19:54:42.679066!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0144 - mean_absolute_error: 0.0819
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 19:55:03.352900!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0183 - mean_absolute_error: 0.0877
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 19:55:21.675636!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0206 - mean_absolute_error: 0.0967
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 19:55:36.322902!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0234 - mean_absolute_error: 0.1141
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 19:55:52.604706!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0190 - mean_absolute_error: 0.0932
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 19:56:09.301802!
Epoch 1/1
744/744 [==============================] - 14s 19ms/step - loss: 0.0164 - mean_absolute_error: 0.0841
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 19:56:28.516558!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0197 - mean_absolute_error: 0.0964
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 19:56:42.317798!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0141 - mean_absolute_error: 0.0923
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 19:56:54.773881!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0415 - mean_absolute_error: 0.1457
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 19:57:09.257387!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0087 - mean_absolute_error: 0.0669
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 19:57:23.603258!
Epoch 1/1
572/572 [==============================] - 11s 19ms/step - loss: 0.0134 - mean_absolute_error: 0.0735
Epoch 1 completed!
Exp2019-01-25_19-51-44.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_19-51-44
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 19:57:36.559735
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 10.15 Degree
        The absolute mean error on Yaw angle estimation: 71.16 Degree
        The absolute mean error on Roll angle estimation: 34.03 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.64 Degree
        The absolute mean error on Yaw angle estimation: 25.72 Degree
        The absolute mean error on Roll angle estimation: 4.15 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 27.77 Degree
        The absolute mean error on Yaw angle estimation: 49.33 Degree
        The absolute mean error on Roll angle estimation: 8.36 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 17.46 Degree
        The absolute mean error on Yaw angle estimation: 59.36 Degree
        The absolute mean error on Roll angle estimation: 25.39 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.00 Degree
        The absolute mean error on Yaw angle estimations: 51.39 Degree
        The absolute mean error on Roll angle estimations: 17.98 Degree
subject3_Exp2019-01-25_19-51-44.png has been saved by 2019-01-25 19:58:42.447388.
subject5_Exp2019-01-25_19-51-44.png has been saved by 2019-01-25 19:58:42.649336.
subject9_Exp2019-01-25_19-51-44.png has been saved by 2019-01-25 19:58:42.849330.
subject14_Exp2019-01-25_19-51-44.png has been saved by 2019-01-25 19:58:43.071652.
Model Exp2019-01-25_19-51-44 has been evaluated successfully.
Model Exp2019-01-25_19-51-44 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 20:09:27.874083: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 20:09:27.971935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 20:09:27.972236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 20:09:27.972250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 20:09:28.128474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 20:09:28.128502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 20:09:28.128507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 20:09:28.128686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_20-09-28 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_20-09-28
All frames and annotations from 20 datasets have been read by 2019-01-25 20:09:33.213658
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 20:09:39.627556!
Epoch 1/1
665/665 [==============================] - 14s 22ms/step - loss: 0.0242 - mean_absolute_error: 0.1071
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 20:09:59.294695!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.0188 - mean_absolute_error: 0.0994
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 20:10:15.661798!
Epoch 1/1
654/654 [==============================] - 13s 20ms/step - loss: 0.0274 - mean_absolute_error: 0.1217
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 20:10:33.648571!
Epoch 1/1
502/502 [==============================] - 10s 20ms/step - loss: 0.0173 - mean_absolute_error: 0.0958
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 20:10:51.483142!
Epoch 1/1
772/772 [==============================] - 16s 20ms/step - loss: 0.0424 - mean_absolute_error: 0.1472
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 20:11:12.621354!
Epoch 1/1
569/569 [==============================] - 11s 20ms/step - loss: 0.0406 - mean_absolute_error: 0.1498
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 20:11:30.196915!
Epoch 1/1
634/634 [==============================] - 13s 20ms/step - loss: 0.0342 - mean_absolute_error: 0.1388
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 20:11:51.545036!
Epoch 1/1
914/914 [==============================] - 19s 20ms/step - loss: 0.0157 - mean_absolute_error: 0.0891
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 20:12:17.794842!
Epoch 1/1
745/745 [==============================] - 16s 21ms/step - loss: 0.0358 - mean_absolute_error: 0.1344
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 20:12:40.655678!
Epoch 1/1
732/732 [==============================] - 15s 20ms/step - loss: 0.0171 - mean_absolute_error: 0.0934
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 20:13:02.838804!
Epoch 1/1
726/726 [==============================] - 15s 20ms/step - loss: 0.0189 - mean_absolute_error: 0.0943
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 20:13:22.506862!
Epoch 1/1
498/498 [==============================] - 10s 20ms/step - loss: 0.0207 - mean_absolute_error: 0.0970
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 20:13:38.463144!
Epoch 1/1
614/614 [==============================] - 12s 20ms/step - loss: 0.0264 - mean_absolute_error: 0.1233
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 20:13:55.990582!
Epoch 1/1
511/511 [==============================] - 10s 20ms/step - loss: 0.0162 - mean_absolute_error: 0.0881
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 20:14:13.689405!
Epoch 1/1
744/744 [==============================] - 15s 20ms/step - loss: 0.0136 - mean_absolute_error: 0.0839
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 20:14:34.347482!
Epoch 1/1
556/556 [==============================] - 11s 20ms/step - loss: 0.0204 - mean_absolute_error: 0.0967
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 20:14:49.180138!
Epoch 1/1
395/395 [==============================] - 8s 20ms/step - loss: 0.0143 - mean_absolute_error: 0.0890
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 20:15:02.239528!
Epoch 1/1
542/542 [==============================] - 11s 21ms/step - loss: 0.0353 - mean_absolute_error: 0.1295
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 20:15:18.234422!
Epoch 1/1
485/485 [==============================] - 10s 20ms/step - loss: 0.0083 - mean_absolute_error: 0.0674
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 20:15:33.833666!
Epoch 1/1
572/572 [==============================] - 12s 20ms/step - loss: 0.0095 - mean_absolute_error: 0.0634
Epoch 1 completed!
Exp2019-01-25_20-09-28.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_20-09-28
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 20:15:47.777946
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 16.11 Degree
        The absolute mean error on Yaw angle estimation: 68.87 Degree
        The absolute mean error on Roll angle estimation: 25.63 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.95 Degree
        The absolute mean error on Yaw angle estimation: 25.49 Degree
        The absolute mean error on Roll angle estimation: 4.62 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 19.19 Degree
        The absolute mean error on Yaw angle estimation: 27.35 Degree
        The absolute mean error on Roll angle estimation: 9.17 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 13.25 Degree
        The absolute mean error on Yaw angle estimation: 39.89 Degree
        The absolute mean error on Roll angle estimation: 22.82 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.87 Degree
        The absolute mean error on Yaw angle estimations: 40.40 Degree
        The absolute mean error on Roll angle estimations: 15.56 Degree
subject3_Exp2019-01-25_20-09-28.png has been saved by 2019-01-25 20:16:54.329623.
subject5_Exp2019-01-25_20-09-28.png has been saved by 2019-01-25 20:16:54.529163.
subject9_Exp2019-01-25_20-09-28.png has been saved by 2019-01-25 20:16:54.725852.
subject14_Exp2019-01-25_20-09-28.png has been saved by 2019-01-25 20:16:54.949242.
Model Exp2019-01-25_20-09-28 has been evaluated successfully.
Model Exp2019-01-25_20-09-28 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 20:19:33.758066: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 20:19:33.856273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 20:19:33.856537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 20:19:33.856551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 20:19:34.013544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 20:19:34.013572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 20:19:34.013580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 20:19:34.013723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_20-19-34 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_20-19-34
All frames and annotations from 20 datasets have been read by 2019-01-25 20:19:39.274677
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 20:19:45.684010!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.0305 - mean_absolute_error: 0.1266
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 20:20:04.147402!
Epoch 1/1
492/492 [==============================] - 9s 19ms/step - loss: 0.0210 - mean_absolute_error: 0.1041
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 20:20:19.808759!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0321 - mean_absolute_error: 0.1254
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 20:20:36.848703!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0217 - mean_absolute_error: 0.1056
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 20:20:53.791070!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0459 - mean_absolute_error: 0.1417
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 20:21:13.307989!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0415 - mean_absolute_error: 0.1530
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 20:21:29.801170!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0415 - mean_absolute_error: 0.1543
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 20:21:50.069800!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0193 - mean_absolute_error: 0.1002
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 20:22:14.299166!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0223 - mean_absolute_error: 0.1059
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 20:22:34.910522!
Epoch 1/1
732/732 [==============================] - 14s 19ms/step - loss: 0.0169 - mean_absolute_error: 0.0926
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 20:22:55.861404!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0243 - mean_absolute_error: 0.1038
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 20:23:14.127551!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0301 - mean_absolute_error: 0.1138
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 20:23:28.923407!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0306 - mean_absolute_error: 0.1343
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 20:23:45.259308!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0351 - mean_absolute_error: 0.1228
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 20:24:02.063833!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0219 - mean_absolute_error: 0.1071
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 20:24:21.062521!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0300 - mean_absolute_error: 0.1231
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 20:24:35.192200!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0163 - mean_absolute_error: 0.0991
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 20:24:47.598678!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0531 - mean_absolute_error: 0.1582
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 20:25:02.260663!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0112 - mean_absolute_error: 0.0780
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 20:25:16.924093!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0142 - mean_absolute_error: 0.0800
Epoch 1 completed!
Exp2019-01-25_20-19-34.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_20-19-34
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 20:25:29.536691
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 9.63 Degree
        The absolute mean error on Yaw angle estimation: 98.50 Degree
        The absolute mean error on Roll angle estimation: 38.93 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.51 Degree
        The absolute mean error on Yaw angle estimation: 28.40 Degree
        The absolute mean error on Roll angle estimation: 8.99 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 18.39 Degree
        The absolute mean error on Yaw angle estimation: 38.39 Degree
        The absolute mean error on Roll angle estimation: 8.05 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 8.87 Degree
        The absolute mean error on Yaw angle estimation: 56.54 Degree
        The absolute mean error on Roll angle estimation: 22.12 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 13.10 Degree
        The absolute mean error on Yaw angle estimations: 55.46 Degree
        The absolute mean error on Roll angle estimations: 19.52 Degree
subject3_Exp2019-01-25_20-19-34.png has been saved by 2019-01-25 20:26:36.924086.
subject5_Exp2019-01-25_20-19-34.png has been saved by 2019-01-25 20:26:37.126513.
subject9_Exp2019-01-25_20-19-34.png has been saved by 2019-01-25 20:26:37.366218.
subject14_Exp2019-01-25_20-19-34.png has been saved by 2019-01-25 20:26:37.588083.
Model Exp2019-01-25_20-19-34 has been evaluated successfully.
Model Exp2019-01-25_20-19-34 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 20:27:44.121939: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 20:27:44.219804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 20:27:44.220067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 20:27:44.220084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 20:27:44.374676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 20:27:44.374703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 20:27:44.374708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 20:27:44.374843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_20-27-45 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_20-27-45
All frames and annotations from 20 datasets have been read by 2019-01-25 20:27:49.586105
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 20:27:55.998416!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.0676 - mean_absolute_error: 0.1948
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 20:28:19.459493!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0409 - mean_absolute_error: 0.1533
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 20:28:37.945893!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0594 - mean_absolute_error: 0.1769
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 20:28:58.985975!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0373 - mean_absolute_error: 0.1458
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 20:29:19.311381!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0719 - mean_absolute_error: 0.2041
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 20:29:43.796026!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0668 - mean_absolute_error: 0.2000
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 20:30:04.111513!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0610 - mean_absolute_error: 0.1866
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 20:30:28.890812!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0339 - mean_absolute_error: 0.1381
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 20:30:59.964428!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0499 - mean_absolute_error: 0.1702
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 20:31:26.046263!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0364 - mean_absolute_error: 0.1445
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 20:31:51.723456!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0413 - mean_absolute_error: 0.1455
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 20:32:15.109685!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0606 - mean_absolute_error: 0.1784
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 20:32:33.498093!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0482 - mean_absolute_error: 0.1705
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 20:32:53.953105!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0461 - mean_absolute_error: 0.1624
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 20:33:14.503022!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0433 - mean_absolute_error: 0.1511
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 20:33:38.451712!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0324 - mean_absolute_error: 0.1318
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 20:33:56.742021!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0213 - mean_absolute_error: 0.1108
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 20:34:11.869948!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0621 - mean_absolute_error: 0.1761
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 20:34:30.132069!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0213 - mean_absolute_error: 0.1062
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 20:34:47.594655!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0231 - mean_absolute_error: 0.1044
Epoch 1 completed!
Exp2019-01-25_20-27-45.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_20-27-45
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 20:35:04.718573
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.14 Degree
        The absolute mean error on Yaw angle estimation: 53.58 Degree
        The absolute mean error on Roll angle estimation: 17.27 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.75 Degree
        The absolute mean error on Yaw angle estimation: 28.73 Degree
        The absolute mean error on Roll angle estimation: 4.68 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.36 Degree
        The absolute mean error on Yaw angle estimation: 32.03 Degree
        The absolute mean error on Roll angle estimation: 11.26 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.80 Degree
        The absolute mean error on Yaw angle estimation: 37.89 Degree
        The absolute mean error on Roll angle estimation: 14.19 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.51 Degree
        The absolute mean error on Yaw angle estimations: 38.06 Degree
        The absolute mean error on Roll angle estimations: 11.85 Degree
subject3_Exp2019-01-25_20-27-45.png has been saved by 2019-01-25 20:36:29.256036.
subject5_Exp2019-01-25_20-27-45.png has been saved by 2019-01-25 20:36:29.453853.
subject9_Exp2019-01-25_20-27-45.png has been saved by 2019-01-25 20:36:29.649442.
subject14_Exp2019-01-25_20-27-45.png has been saved by 2019-01-25 20:36:29.867309.
Model Exp2019-01-25_20-27-45 has been evaluated successfully.
Model Exp2019-01-25_20-27-45 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 20:37:41.375782: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 20:37:41.472472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 20:37:41.472731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 20:37:41.472750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 20:37:41.628167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 20:37:41.628192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 20:37:41.628197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 20:37:41.628334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_20-37-42 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_20-37-42
All frames and annotations from 20 datasets have been read by 2019-01-25 20:37:46.800402
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 20:37:53.203522!
Epoch 1/1
665/665 [==============================] - 18s 28ms/step - loss: 0.0973 - mean_absolute_error: 0.2330
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 20:38:16.923838!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.1001 - mean_absolute_error: 0.2253
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 20:38:35.182750!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.1154 - mean_absolute_error: 0.2538
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 20:38:56.353424!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0889 - mean_absolute_error: 0.2309
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 20:39:16.889442!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1669 - mean_absolute_error: 0.3127
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 20:39:41.592178!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1041 - mean_absolute_error: 0.2435
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 20:40:01.521372!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1048 - mean_absolute_error: 0.2440
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 20:40:26.071813!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0738 - mean_absolute_error: 0.1993
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 20:40:56.567390!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1167 - mean_absolute_error: 0.2563
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 20:41:22.775703!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.1156 - mean_absolute_error: 0.2607
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 20:41:48.018957!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0916 - mean_absolute_error: 0.2260
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 20:42:10.767807!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1026 - mean_absolute_error: 0.2359
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 20:42:29.131067!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0881 - mean_absolute_error: 0.2294
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 20:42:49.466898!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0951 - mean_absolute_error: 0.2355
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 20:43:09.757143!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0884 - mean_absolute_error: 0.2269
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 20:43:34.135177!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0614 - mean_absolute_error: 0.1786
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 20:43:51.627728!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0369 - mean_absolute_error: 0.1372
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 20:44:06.635602!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0902 - mean_absolute_error: 0.2156
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 20:44:25.078467!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0485 - mean_absolute_error: 0.1662
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 20:44:42.697012!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0498 - mean_absolute_error: 0.1533
Epoch 1 completed!
Exp2019-01-25_20-37-42.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_20-37-42
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 20:44:59.405719
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 36.38 Degree
        The absolute mean error on Yaw angle estimation: 33.15 Degree
        The absolute mean error on Roll angle estimation: 7.97 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 29.30 Degree
        The absolute mean error on Yaw angle estimation: 35.05 Degree
        The absolute mean error on Roll angle estimation: 15.87 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 35.26 Degree
        The absolute mean error on Yaw angle estimation: 31.52 Degree
        The absolute mean error on Roll angle estimation: 15.96 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 53.41 Degree
        The absolute mean error on Yaw angle estimation: 38.89 Degree
        The absolute mean error on Roll angle estimation: 17.76 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 38.59 Degree
        The absolute mean error on Yaw angle estimations: 34.65 Degree
        The absolute mean error on Roll angle estimations: 14.39 Degree
subject3_Exp2019-01-25_20-37-42.png has been saved by 2019-01-25 20:46:24.157936.
subject5_Exp2019-01-25_20-37-42.png has been saved by 2019-01-25 20:46:24.349229.
subject9_Exp2019-01-25_20-37-42.png has been saved by 2019-01-25 20:46:24.540379.
subject14_Exp2019-01-25_20-37-42.png has been saved by 2019-01-25 20:46:24.755958.
Model Exp2019-01-25_20-37-42 has been evaluated successfully.
Model Exp2019-01-25_20-37-42 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 20:48:21.256389: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 20:48:21.354729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 20:48:21.354998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 20:48:21.355012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 20:48:21.511423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 20:48:21.511449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 20:48:21.511458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 20:48:21.511600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_20-48-22 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_20-48-22
All frames and annotations from 20 datasets have been read by 2019-01-25 20:48:26.700343
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 20:48:33.090651!
Epoch 1/1
665/665 [==============================] - 19s 28ms/step - loss: 0.0343 - mean_absolute_error: 0.1232
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 20:48:57.192287!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0307 - mean_absolute_error: 0.1180
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 20:49:15.344403!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0562 - mean_absolute_error: 0.1538
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 20:49:36.941772!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0339 - mean_absolute_error: 0.1350
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 20:49:56.674498!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0700 - mean_absolute_error: 0.1930
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 20:50:20.694320!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0745 - mean_absolute_error: 0.2002
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 20:50:41.014758!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0703 - mean_absolute_error: 0.1879
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 20:51:05.743434!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0496 - mean_absolute_error: 0.1503
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 20:51:36.390077!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0839 - mean_absolute_error: 0.2141
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 20:52:01.783879!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0507 - mean_absolute_error: 0.1675
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 20:52:27.642991!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0595 - mean_absolute_error: 0.1708
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 20:52:51.617848!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0906 - mean_absolute_error: 0.2167
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 20:53:10.589965!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0675 - mean_absolute_error: 0.1962
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 20:53:31.253237!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0880 - mean_absolute_error: 0.2218
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 20:53:52.053837!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0858 - mean_absolute_error: 0.2174
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 20:54:16.169458!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0436 - mean_absolute_error: 0.1448
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 20:54:33.926989!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0254 - mean_absolute_error: 0.1028
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 20:54:48.858286!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0728 - mean_absolute_error: 0.1816
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 20:55:07.477788!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0415 - mean_absolute_error: 0.1464
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 20:55:25.158345!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0449 - mean_absolute_error: 0.1391
Epoch 1 completed!
Exp2019-01-25_20-48-22.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_20-48-22
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 20:55:42.159894
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.65 Degree
        The absolute mean error on Yaw angle estimation: 32.30 Degree
        The absolute mean error on Roll angle estimation: 9.62 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 21.83 Degree
        The absolute mean error on Yaw angle estimation: 29.76 Degree
        The absolute mean error on Roll angle estimation: 4.77 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.23 Degree
        The absolute mean error on Yaw angle estimation: 26.27 Degree
        The absolute mean error on Roll angle estimation: 8.83 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.14 Degree
        The absolute mean error on Yaw angle estimation: 34.71 Degree
        The absolute mean error on Roll angle estimation: 14.13 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.96 Degree
        The absolute mean error on Yaw angle estimations: 30.76 Degree
        The absolute mean error on Roll angle estimations: 9.34 Degree
subject3_Exp2019-01-25_20-48-22.png has been saved by 2019-01-25 20:57:06.854349.
subject5_Exp2019-01-25_20-48-22.png has been saved by 2019-01-25 20:57:07.044592.
subject9_Exp2019-01-25_20-48-22.png has been saved by 2019-01-25 20:57:07.233937.
subject14_Exp2019-01-25_20-48-22.png has been saved by 2019-01-25 20:57:07.443411.
Model Exp2019-01-25_20-48-22 has been evaluated successfully.
Model Exp2019-01-25_20-48-22 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_20-48-22 trainLSTM_VGG16.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Incorrect argument for method. Try again...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_20-48-22 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:01:27.730948: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:01:27.828494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:01:27.828797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:01:27.828811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:01:27.985223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:01:27.985251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:01:27.985256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:01:27.985442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_20-48-22.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model Exp2019-01-25_20-48-22_and_2019-01-25_21-01-29
All frames and annotations from 20 datasets have been read by 2019-01-25 21:01:33.741987
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:01:40.149632!
Epoch 1/1
2019-01-25 21:01:41.407892: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_arra
y_ops.cc:121 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does no
t exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327,
in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312,
in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420,
in _call_tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line
 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 44,
in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 56, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 46, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on
_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482,
in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, i
n run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140,
in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321,
in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340,
in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGra
dV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_tra
in_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519,
in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 48
8, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 37
9, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line
 104, in _TensorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line
 6221, in tensor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", l
ine 787, in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, i
n create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, i
n __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserializ
e
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deser
ialize_keras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771,
in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754,
in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Tenso
rArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_20-48-22 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:02:47.698628: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:02:47.796456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:02:47.796719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:02:47.796737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:02:47.952566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:02:47.952587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:02:47.952591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:02:47.952727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_20-48-22.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model Exp2019-01-25_20-48-22_and_2019-01-25_21-02-49
All frames and annotations from 20 datasets have been read by 2019-01-25 21:02:53.716250
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:03:00.118834!
Epoch 1/1
2019-01-25 21:03:01.352415: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_arra
y_ops.cc:121 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does no
t exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327,
in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312,
in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420,
in _call_tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line
 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 44,
in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 56, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 46, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on
_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482,
in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, i
n run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140,
in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321,
in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340,
in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGra
dV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_tra
in_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519,
in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 48
8, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 37
9, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line
 104, in _TensorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line
 6221, in tensor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", l
ine 787, in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, i
n create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, i
n __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserializ
e
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deser
ialize_keras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771,
in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754,
in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Tenso
rArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py

/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:04:24.358133: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:04:24.454825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:04:24.455080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:04:24.455092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:04:24.611086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:04:24.611111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:04:24.611116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:04:24.611251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_21-04-25 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 220)                  197120
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    663
=================================================================
Total params: 138,656,730
Trainable params: 4,396,186
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 220
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm220_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.0
00100_2019-01-25_21-04-25
All frames and annotations from 20 datasets have been read by 2019-01-25 21:04:29.818114
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:04:36.228989!
Epoch 1/1
665/665 [==============================] - 18s 26ms/step - loss: 0.0317 - mean_absolute_error: 0.1183
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 21:04:59.175922!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0297 - mean_absolute_error: 0.1155
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 21:05:17.686704!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0561 - mean_absolute_error: 0.1528
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 21:05:38.901843!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0321 - mean_absolute_error: 0.1302
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 21:05:58.963960!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0668 - mean_absolute_error: 0.1849
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 21:06:23.763213!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0695 - mean_absolute_error: 0.1892
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 21:06:43.983929!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0697 - mean_absolute_error: 0.1872
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 21:07:08.665075!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0494 - mean_absolute_error: 0.1496
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 21:07:39.304678!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0823 - mean_absolute_error: 0.2134
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 21:08:05.353688!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0496 - mean_absolute_error: 0.1663
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 21:08:31.059163!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0597 - mean_absolute_error: 0.1715
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 21:08:54.264154!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0898 - mean_absolute_error: 0.2168
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 21:09:12.544788!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0626 - mean_absolute_error: 0.1845
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 21:09:32.908943!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0760 - mean_absolute_error: 0.2055
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 21:09:52.748137!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0794 - mean_absolute_error: 0.2082
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 21:10:16.639813!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0417 - mean_absolute_error: 0.1364
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 21:10:34.054182!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0249 - mean_absolute_error: 0.1051
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 21:10:49.135734!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0727 - mean_absolute_error: 0.1828
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 21:11:07.080386!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0389 - mean_absolute_error: 0.1425
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 21:11:25.067093!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0440 - mean_absolute_error: 0.1366
Epoch 1 completed!
Exp2019-01-25_21-04-25.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm220_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0
.000100_2019-01-25_21-04-25
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 21:11:41.802172
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.63 Degree
        The absolute mean error on Yaw angle estimation: 33.09 Degree
        The absolute mean error on Roll angle estimation: 10.98 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 21.47 Degree
        The absolute mean error on Yaw angle estimation: 29.85 Degree
        The absolute mean error on Roll angle estimation: 5.83 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.26 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 9.63 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.37 Degree
        The absolute mean error on Yaw angle estimation: 35.02 Degree
        The absolute mean error on Roll angle estimation: 14.20 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.93 Degree
        The absolute mean error on Yaw angle estimations: 31.16 Degree
        The absolute mean error on Roll angle estimations: 10.16 Degree
subject3_Exp2019-01-25_21-04-25.png has been saved by 2019-01-25 21:13:06.164778.
subject5_Exp2019-01-25_21-04-25.png has been saved by 2019-01-25 21:13:06.355577.
subject9_Exp2019-01-25_21-04-25.png has been saved by 2019-01-25 21:13:06.546277.
subject14_Exp2019-01-25_21-04-25.png has been saved by 2019-01-25 21:13:06.755282.
Model Exp2019-01-25_21-04-25 has been evaluated successfully.
Model Exp2019-01-25_21-04-25 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_21-04-25 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:14:07.900861: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:14:07.999181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:14:07.999439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:14:07.999451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:14:08.155522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:14:08.155544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:14:08.155549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:14:08.155685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_21-04-25.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 220)                  197120
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    663
=================================================================
Total params: 138,656,730
Trainable params: 4,396,186
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 220
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model Exp2019-01-25_21-04-25_and_2019-01-25_21-14-09
All frames and annotations from 20 datasets have been read by 2019-01-25 21:14:13.956025
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:14:20.351969!
Epoch 1/1
2019-01-25 21:14:21.612943: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_arra
y_ops.cc:121 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does no
t exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327,
in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312,
in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420,
in _call_tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line
 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 44,
in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 56, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 46, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on
_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482,
in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, i
n run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140,
in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321,
in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340,
in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGra
dV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_tra
in_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519,
in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 48
8, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 37
9, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line
 104, in _TensorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line
 6221, in tensor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", l
ine 787, in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, i
n create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, i
n __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserializ
e
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deser
ialize_keras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771,
in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754,
in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Tenso
rArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_21-04-25 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:14:50.039352: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:14:50.135763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:14:50.136021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:14:50.136033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:14:50.291171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:14:50.291198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:14:50.291203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:14:50.291341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_21-04-25.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 220)                  197120
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    663
=================================================================
Total params: 138,656,730
Trainable params: 4,396,186
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model Exp2019-01-25_21-04-25_and_2019-01-25_21-14-51
All frames and annotations from 20 datasets have been read by 2019-01-25 21:14:56.103022
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:15:02.541440!
Epoch 1/1
2019-01-25 21:15:03.808742: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_arra
y_ops.cc:121 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does no
t exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327,
in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312,
in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420,
in _call_tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line
 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 44,
in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 56, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 46, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on
_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482,
in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, i
n run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140,
in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321,
in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340,
in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGra
dV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_tra
in_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519,
in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 48
8, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 37
9, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line
 104, in _TensorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line
 6221, in tensor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", l
ine 787, in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, i
n create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, i
n __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserializ
e
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deser
ialize_keras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771,
in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754,
in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Tenso
rArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py

/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:15:23.280117: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:15:23.363371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:15:23.363630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:15:23.363642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:15:23.519061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:15:23.519085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:15:23.519090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:15:23.519226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_21-15-24 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0010_2019-01-25_21-15-24
All frames and annotations from 20 datasets have been read by 2019-01-25 21:15:28.725173
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:15:35.121011!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.0307 - mean_absolute_error: 0.1202
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 21:15:58.484969!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0327 - mean_absolute_error: 0.1286
^C
Model Exp2019-01-25_21-15-24 has been interrupted.
Exp2019-01-25_21-15-24.h5 has been saved.
Model Exp2019-01-25_21-15-24 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:16:35.809219: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:16:35.907161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:16:35.907422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:16:35.907434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:16:36.063488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:16:36.063512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:16:36.063517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:16:36.063653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_21-16-36 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0010_2019-01-25_21-16-36
All frames and annotations from 20 datasets have been read by 2019-01-25 21:16:41.203046
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:16:47.604799!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.0432 - mean_absolute_error: 0.1479
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 21:17:10.789603!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0527 - mean_absolute_error: 0.1658
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 21:17:29.207964!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0786 - mean_absolute_error: 0.2051
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 21:17:50.533495!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0638 - mean_absolute_error: 0.1986
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 21:18:10.963529!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0759 - mean_absolute_error: 0.1950
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 21:18:35.733518!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0707 - mean_absolute_error: 0.1944
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 21:18:56.109214!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0808 - mean_absolute_error: 0.2077
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 21:19:20.477033!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0619 - mean_absolute_error: 0.1729
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 21:19:51.150242!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0839 - mean_absolute_error: 0.2161
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 21:20:17.277450!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0573 - mean_absolute_error: 0.1755
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 21:20:43.045608!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0618 - mean_absolute_error: 0.1781
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 21:21:06.316051!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0943 - mean_absolute_error: 0.2188
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 21:21:24.713538!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0704 - mean_absolute_error: 0.2033
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 21:21:45.400497!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0955 - mean_absolute_error: 0.2320
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 21:22:05.549894!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0959 - mean_absolute_error: 0.2308
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 21:22:29.605328!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0442 - mean_absolute_error: 0.1439
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 21:22:47.750536!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0341 - mean_absolute_error: 0.1221
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 21:23:02.904529!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0837 - mean_absolute_error: 0.1900
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 21:23:21.384257!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0410 - mean_absolute_error: 0.1455
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 21:23:39.488155!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0438 - mean_absolute_error: 0.1387
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-25 21:23:57.951911
1. set (Dataset 6) being trained for epoch 2 by 2019-01-25 21:24:03.130997!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0819 - mean_absolute_error: 0.1872
2. set (Dataset 11) being trained for epoch 2 by 2019-01-25 21:24:22.665649!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0431 - mean_absolute_error: 0.1348
3. set (Dataset 10) being trained for epoch 2 by 2019-01-25 21:24:44.297650!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0632 - mean_absolute_error: 0.1804
4. set (Dataset 4) being trained for epoch 2 by 2019-01-25 21:25:10.482302!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0931 - mean_absolute_error: 0.2309
5. set (Dataset 23) being trained for epoch 2 by 2019-01-25 21:25:34.804892!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0690 - mean_absolute_error: 0.1899
6. set (Dataset 13) being trained for epoch 2 by 2019-01-25 21:25:53.776504!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0406 - mean_absolute_error: 0.1435
7. set (Dataset 17) being trained for epoch 2 by 2019-01-25 21:26:10.332711!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0309 - mean_absolute_error: 0.1136
8. set (Dataset 1) being trained for epoch 2 by 2019-01-25 21:26:25.098666!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0981 - mean_absolute_error: 0.2207
9. set (Dataset 8) being trained for epoch 2 by 2019-01-25 21:26:45.617723!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0741 - mean_absolute_error: 0.1982
10. set (Dataset 7) being trained for epoch 2 by 2019-01-25 21:27:12.652208!
Epoch 1/1
745/745 [==============================] - 287s 385ms/step - loss: 0.0814 - mean_absolute_error: 0.2108
11. set (Dataset 21) being trained for epoch 2 by 2019-01-25 21:32:05.538803!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0800 - mean_absolute_error: 0.2080
12. set (Dataset 22) being trained for epoch 2 by 2019-01-25 21:32:27.815278!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0313 - mean_absolute_error: 0.1098
13. set (Dataset 2) being trained for epoch 2 by 2019-01-25 21:32:49.340874!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0988 - mean_absolute_error: 0.2351
14. set (Dataset 24) being trained for epoch 2 by 2019-01-25 21:33:07.149618!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0411 - mean_absolute_error: 0.1414
15. set (Dataset 15) being trained for epoch 2 by 2019-01-25 21:33:26.324381!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0676 - mean_absolute_error: 0.1827
16. set (Dataset 20) being trained for epoch 2 by 2019-01-25 21:33:48.015391!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0406 - mean_absolute_error: 0.1324
17. set (Dataset 18) being trained for epoch 2 by 2019-01-25 21:34:08.304271!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.0660 - mean_absolute_error: 0.1990
18. set (Dataset 19) being trained for epoch 2 by 2019-01-25 21:34:27.976188!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0459 - mean_absolute_error: 0.1615
19. set (Dataset 12) being trained for epoch 2 by 2019-01-25 21:34:47.731945!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0616 - mean_absolute_error: 0.1806
20. set (Dataset 16) being trained for epoch 2 by 2019-01-25 21:35:14.863557!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0571 - mean_absolute_error: 0.1665
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-25 21:35:42.203045
1. set (Dataset 19) being trained for epoch 3 by 2019-01-25 21:35:47.081226!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0443 - mean_absolute_error: 0.1581
2. set (Dataset 16) being trained for epoch 3 by 2019-01-25 21:36:08.492940!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0545 - mean_absolute_error: 0.1614
3. set (Dataset 21) being trained for epoch 3 by 2019-01-25 21:36:37.252151!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0723 - mean_absolute_error: 0.1925
4. set (Dataset 15) being trained for epoch 3 by 2019-01-25 21:36:59.577983!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0595 - mean_absolute_error: 0.1649
5. set (Dataset 13) being trained for epoch 3 by 2019-01-25 21:37:21.047623!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0475 - mean_absolute_error: 0.1615
6. set (Dataset 12) being trained for epoch 3 by 2019-01-25 21:37:40.922420!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0661 - mean_absolute_error: 0.1860
7. set (Dataset 18) being trained for epoch 3 by 2019-01-25 21:38:05.315966!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0635 - mean_absolute_error: 0.1937
8. set (Dataset 22) being trained for epoch 3 by 2019-01-25 21:38:27.350732!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0296 - mean_absolute_error: 0.1048
9. set (Dataset 23) being trained for epoch 3 by 2019-01-25 21:38:49.306848!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0683 - mean_absolute_error: 0.1863
10. set (Dataset 8) being trained for epoch 3 by 2019-01-25 21:39:11.388628!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0787 - mean_absolute_error: 0.2070
11. set (Dataset 17) being trained for epoch 3 by 2019-01-25 21:39:35.061663!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0277 - mean_absolute_error: 0.1047
12. set (Dataset 6) being trained for epoch 3 by 2019-01-25 21:39:50.505633!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0774 - mean_absolute_error: 0.1805
13. set (Dataset 24) being trained for epoch 3 by 2019-01-25 21:40:08.536982!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0326 - mean_absolute_error: 0.1227
14. set (Dataset 11) being trained for epoch 3 by 2019-01-25 21:40:27.274785!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0433 - mean_absolute_error: 0.1340
15. set (Dataset 10) being trained for epoch 3 by 2019-01-25 21:40:49.139868!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0670 - mean_absolute_error: 0.1860
16. set (Dataset 20) being trained for epoch 3 by 2019-01-25 21:41:12.575648!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0395 - mean_absolute_error: 0.1276
17. set (Dataset 2) being trained for epoch 3 by 2019-01-25 21:41:31.985709!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.1081 - mean_absolute_error: 0.2426
18. set (Dataset 4) being trained for epoch 3 by 2019-01-25 21:41:52.610471!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0901 - mean_absolute_error: 0.2275
19. set (Dataset 7) being trained for epoch 3 by 2019-01-25 21:42:18.663241!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0823 - mean_absolute_error: 0.2121
20. set (Dataset 1) being trained for epoch 3 by 2019-01-25 21:42:42.760509!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0992 - mean_absolute_error: 0.2222
Epoch 3 completed!
Exp2019-01-25_21-16-36.h5 has been saved.
The subjects are trained: [(19, 'M11'), (16, 'M09'), (21, 'F02'), (15, 'F03'), (13, 'M07'), (12, 'M06'), (18
, 'F05'), (22, 'M01'), (23, 'M13'), (8, 'M02'), (17, 'M10'), (6, 'F06'), (24, 'M14'), (11, 'M05'), (10, 'M04
'), (20, 'M12'), (2, 'F02'), (4, 'F04'), (7, 'M01'), (1, 'F01')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000010_2019-01-25_21-16-36
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 21:42:57.325885
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.47 Degree
        The absolute mean error on Yaw angle estimation: 32.07 Degree
        The absolute mean error on Roll angle estimation: 6.68 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 19.86 Degree
        The absolute mean error on Yaw angle estimation: 29.75 Degree
        The absolute mean error on Roll angle estimation: 3.34 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.48 Degree
        The absolute mean error on Yaw angle estimation: 26.16 Degree
        The absolute mean error on Roll angle estimation: 7.43 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.46 Degree
        The absolute mean error on Yaw angle estimation: 34.62 Degree
        The absolute mean error on Roll angle estimation: 14.31 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.82 Degree
        The absolute mean error on Yaw angle estimations: 30.65 Degree
        The absolute mean error on Roll angle estimations: 7.94 Degree
subject3_Exp2019-01-25_21-16-36.png has been saved by 2019-01-25 21:44:21.848924.
subject5_Exp2019-01-25_21-16-36.png has been saved by 2019-01-25 21:44:22.038883.
subject9_Exp2019-01-25_21-16-36.png has been saved by 2019-01-25 21:44:22.228643.
subject14_Exp2019-01-25_21-16-36.png has been saved by 2019-01-25 21:44:22.437772.
Model Exp2019-01-25_21-16-36 has been evaluated successfully.
Model Exp2019-01-25_21-16-36 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:47:30.345755: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:47:30.442350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:47:30.442613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:47:30.442627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:47:30.597299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:47:30.597326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:47:30.597334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:47:30.597495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_21-47-31 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-25_21-47-31
All frames and annotations from 20 datasets have been read by 2019-01-25 21:47:35.716582
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:47:42.127845!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.0338 - mean_absolute_error: 0.1227
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 21:48:05.672260!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0325 - mean_absolute_error: 0.1241
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 21:48:24.851210!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0567 - mean_absolute_error: 0.1554
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 21:48:46.512061!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0351 - mean_absolute_error: 0.1373
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 21:49:06.891789!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0711 - mean_absolute_error: 0.1965
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 21:49:31.711141!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0741 - mean_absolute_error: 0.2008
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 21:49:52.129148!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0705 - mean_absolute_error: 0.1881
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 21:50:17.031585!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0498 - mean_absolute_error: 0.1522
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 21:50:47.753576!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0847 - mean_absolute_error: 0.2154
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 21:51:14.169974!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0516 - mean_absolute_error: 0.1700
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 21:51:40.355781!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0594 - mean_absolute_error: 0.1707
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 21:52:04.022981!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0907 - mean_absolute_error: 0.2163
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 21:52:22.369723!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0688 - mean_absolute_error: 0.1998
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 21:52:43.091718!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0882 - mean_absolute_error: 0.2231
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 21:53:03.348247!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0880 - mean_absolute_error: 0.2202
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 21:53:27.682484!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0440 - mean_absolute_error: 0.1479
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 21:53:45.131146!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0260 - mean_absolute_error: 0.1050
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 21:54:00.020712!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0728 - mean_absolute_error: 0.1818
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 21:54:17.726008!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0427 - mean_absolute_error: 0.1462
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 21:54:35.986823!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0446 - mean_absolute_error: 0.1399
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-25 21:54:55.114967
1. set (Dataset 6) being trained for epoch 2 by 2019-01-25 21:55:00.336519!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0738 - mean_absolute_error: 0.1796
2. set (Dataset 11) being trained for epoch 2 by 2019-01-25 21:55:19.517829!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0441 - mean_absolute_error: 0.1361
3. set (Dataset 10) being trained for epoch 2 by 2019-01-25 21:55:40.935609!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0613 - mean_absolute_error: 0.1759
4. set (Dataset 4) being trained for epoch 2 by 2019-01-25 21:56:07.043105!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0820 - mean_absolute_error: 0.2127
5. set (Dataset 23) being trained for epoch 2 by 2019-01-25 21:56:31.035354!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0739 - mean_absolute_error: 0.1982
6. set (Dataset 13) being trained for epoch 2 by 2019-01-25 21:56:50.488569!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0386 - mean_absolute_error: 0.1399
7. set (Dataset 17) being trained for epoch 2 by 2019-01-25 21:57:06.237974!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0297 - mean_absolute_error: 0.1145
8. set (Dataset 1) being trained for epoch 2 by 2019-01-25 21:57:21.145009!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0985 - mean_absolute_error: 0.2227
9. set (Dataset 8) being trained for epoch 2 by 2019-01-25 21:57:41.616639!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0665 - mean_absolute_error: 0.1854
10. set (Dataset 7) being trained for epoch 2 by 2019-01-25 21:58:09.077665!
Epoch 1/1
745/745 [==============================] - 20s 27ms/step - loss: 0.0804 - mean_absolute_error: 0.2125
11. set (Dataset 21) being trained for epoch 2 by 2019-01-25 21:58:35.027346!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.0747 - mean_absolute_error: 0.1960
12. set (Dataset 22) being trained for epoch 2 by 2019-01-25 21:58:58.012074!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0295 - mean_absolute_error: 0.1110
13. set (Dataset 2) being trained for epoch 2 by 2019-01-25 21:59:19.396022!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0830 - mean_absolute_error: 0.2154
14. set (Dataset 24) being trained for epoch 2 by 2019-01-25 21:59:36.850312!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0497 - mean_absolute_error: 0.1558
15. set (Dataset 15) being trained for epoch 2 by 2019-01-25 21:59:55.735098!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0577 - mean_absolute_error: 0.1577
16. set (Dataset 20) being trained for epoch 2 by 2019-01-25 22:00:16.926903!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0398 - mean_absolute_error: 0.1308
17. set (Dataset 18) being trained for epoch 2 by 2019-01-25 22:00:36.719189!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0581 - mean_absolute_error: 0.1789
18. set (Dataset 19) being trained for epoch 2 by 2019-01-25 22:00:56.908603!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0383 - mean_absolute_error: 0.1431
19. set (Dataset 12) being trained for epoch 2 by 2019-01-25 22:01:16.758834!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0573 - mean_absolute_error: 0.1743
20. set (Dataset 16) being trained for epoch 2 by 2019-01-25 22:01:44.383620!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0551 - mean_absolute_error: 0.1612
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-25 22:02:11.547986
1. set (Dataset 19) being trained for epoch 3 by 2019-01-25 22:02:16.425644!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0368 - mean_absolute_error: 0.1389
2. set (Dataset 16) being trained for epoch 3 by 2019-01-25 22:02:37.497128!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0509 - mean_absolute_error: 0.1572
3. set (Dataset 21) being trained for epoch 3 by 2019-01-25 22:03:06.680904!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0699 - mean_absolute_error: 0.1867
4. set (Dataset 15) being trained for epoch 3 by 2019-01-25 22:03:28.811506!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0567 - mean_absolute_error: 0.1556
5. set (Dataset 13) being trained for epoch 3 by 2019-01-25 22:03:50.112380!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0418 - mean_absolute_error: 0.1456
6. set (Dataset 12) being trained for epoch 3 by 2019-01-25 22:04:09.586942!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0509 - mean_absolute_error: 0.1682
7. set (Dataset 18) being trained for epoch 3 by 2019-01-25 22:04:33.888146!
Epoch 1/1
614/614 [==============================] - 14s 24ms/step - loss: 0.0661 - mean_absolute_error: 0.1946
8. set (Dataset 22) being trained for epoch 3 by 2019-01-25 22:04:54.862167!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0295 - mean_absolute_error: 0.1102
9. set (Dataset 23) being trained for epoch 3 by 2019-01-25 22:05:17.409987!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0665 - mean_absolute_error: 0.1810
10. set (Dataset 8) being trained for epoch 3 by 2019-01-25 22:05:39.363474!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0702 - mean_absolute_error: 0.1920
11. set (Dataset 17) being trained for epoch 3 by 2019-01-25 22:06:03.228587!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0334 - mean_absolute_error: 0.1246
12. set (Dataset 6) being trained for epoch 3 by 2019-01-25 22:06:18.566099!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0733 - mean_absolute_error: 0.1805
13. set (Dataset 24) being trained for epoch 3 by 2019-01-25 22:06:36.997875!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0282 - mean_absolute_error: 0.1100
14. set (Dataset 11) being trained for epoch 3 by 2019-01-25 22:06:55.263270!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0447 - mean_absolute_error: 0.1384
15. set (Dataset 10) being trained for epoch 3 by 2019-01-25 22:07:17.480090!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0610 - mean_absolute_error: 0.1749
16. set (Dataset 20) being trained for epoch 3 by 2019-01-25 22:07:41.197798!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0413 - mean_absolute_error: 0.1345
17. set (Dataset 2) being trained for epoch 3 by 2019-01-25 22:08:00.202170!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0789 - mean_absolute_error: 0.2122
18. set (Dataset 4) being trained for epoch 3 by 2019-01-25 22:08:20.514494!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0861 - mean_absolute_error: 0.2172
19. set (Dataset 7) being trained for epoch 3 by 2019-01-25 22:08:46.941678!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0812 - mean_absolute_error: 0.2114
20. set (Dataset 1) being trained for epoch 3 by 2019-01-25 22:09:11.292430!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0945 - mean_absolute_error: 0.2180
Epoch 3 completed!
Exp2019-01-25_21-47-31.h5 has been saved.
The subjects are trained: [(19, 'M11'), (16, 'M09'), (21, 'F02'), (15, 'F03'), (13, 'M07'), (12, 'M06'), (18
, 'F05'), (22, 'M01'), (23, 'M13'), (8, 'M02'), (17, 'M10'), (6, 'F06'), (24, 'M14'), (11, 'M05'), (10, 'M04
'), (20, 'M12'), (2, 'F02'), (4, 'F04'), (7, 'M01'), (1, 'F01')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-25_21-47-31
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 22:09:26.458440
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.59 Degree
        The absolute mean error on Yaw angle estimation: 31.54 Degree
        The absolute mean error on Roll angle estimation: 8.00 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.98 Degree
        The absolute mean error on Yaw angle estimation: 29.77 Degree
        The absolute mean error on Roll angle estimation: 3.78 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 20.78 Degree
        The absolute mean error on Yaw angle estimation: 25.91 Degree
        The absolute mean error on Roll angle estimation: 8.00 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 29.81 Degree
        The absolute mean error on Yaw angle estimation: 34.52 Degree
        The absolute mean error on Roll angle estimation: 14.20 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 22.04 Degree
        The absolute mean error on Yaw angle estimations: 30.44 Degree
        The absolute mean error on Roll angle estimations: 8.49 Degree
subject3_Exp2019-01-25_21-47-31.png has been saved by 2019-01-25 22:10:51.023742.
subject5_Exp2019-01-25_21-47-31.png has been saved by 2019-01-25 22:10:51.212333.
subject9_Exp2019-01-25_21-47-31.png has been saved by 2019-01-25 22:10:51.400980.
subject14_Exp2019-01-25_21-47-31.png has been saved by 2019-01-25 22:10:51.609004.
Model Exp2019-01-25_21-47-31 has been evaluated successfully.
Model Exp2019-01-25_21-47-31 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit -m "still 30 deg
ree on yaw"
[master 70b8c9e] still 30 degree on yaw
 66 files changed, 2133 insertions(+), 2316 deletions(-)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/output_Exp2019-01-25_19-22-56.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject14_Exp2019-01-25_19-22-5
6.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject3_Exp2019-01-25_19-22-56
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject5_Exp2019-01-25_19-22-56
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject9_Exp2019-01-25_19-22-56
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/output_Exp2019-01-25_19-31-28.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject14_Exp2019-01-25_19-31-2
8.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject3_Exp2019-01-25_19-31-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject5_Exp2019-01-25_19-31-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject9_Exp2019-01-25_19-31-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/output_
Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
14_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
3_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
5_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
9_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/output_Exp2019-01-25_19-51-44.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject14_Exp2019-01-25_19-51-4
4.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject3_Exp2019-01-25_19-51-44
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject5_Exp2019-01-25_19-51-44
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject9_Exp2019-01-25_19-51-44
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/output_Exp2019-01-25_20-09-28.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject14_Exp2019-01-25_20-09-2
8.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject3_Exp2019-01-25_20-09-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject5_Exp2019-01-25_20-09-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject9_Exp2019-01-25_20-09-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-19-34/output_Exp2019-01-25_20-19-34.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-19-34/subject14_Exp2019-01-25_20-19-3
4.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-19-34/subject3_Exp2019-01-25_20-19-34
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-19-34/subject5_Exp2019-01-25_20-19-34
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-19-34/subject9_Exp2019-01-25_20-19-34
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/output_Exp2019-01-25_20-27-45.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject14_Exp2019-01-25_20-27-4
5.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject3_Exp2019-01-25_20-27-45
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject5_Exp2019-01-25_20-27-45
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject9_Exp2019-01-25_20-27-45
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/output_Exp2019-01-25_20-37-42.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject14_Exp2019-01-25_20-37-4
2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject3_Exp2019-01-25_20-37-42
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject5_Exp2019-01-25_20-37-42
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject9_Exp2019-01-25_20-37-42
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/output_Exp2019-01-25_20-48-22.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject14_Exp2019-01-25_20-48-2
2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject3_Exp2019-01-25_20-48-22
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject5_Exp2019-01-25_20-48-22
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject9_Exp2019-01-25_20-48-22
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/output_Exp2019-01-25_21-04-25.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject14_Exp2019-01-25_21-04-2
5.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject3_Exp2019-01-25_21-04-25
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject5_Exp2019-01-25_21-04-25
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject9_Exp2019-01-25_21-04-25
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-15-24/output_Exp2019-01-25_21-15-24.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/output_Exp2019-01-25_21-16-36.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject14_Exp2019-01-25_21-16-3
6.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject3_Exp2019-01-25_21-16-36
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject5_Exp2019-01-25_21-16-36
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject9_Exp2019-01-25_21-16-36
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/output_Exp2019-01-25_21-47-31.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject14_Exp2019-01-25_21-47-3
1.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject3_Exp2019-01-25_21-47-31
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject5_Exp2019-01-25_21-47-31
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject9_Exp2019-01-25_21-47-31
.png
 rewrite DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_/output_Last_Model.txt (100%)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model___/output_Last_Model.txt
 copy DeepRL_For_HPE/LSTM_VGG16/results/{Last_Model_ => Last_Model____}/output_Last_Model.txt (100%)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 86, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (82/82), done.
Writing objects: 100% (86/86), 6.94 MiB | 978.00 KiB/s, done.
Total 86 (delta 17), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   03447ea..70b8c9e  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
remote: Counting objects: 16, done.
remote: Compressing objects: 100% (16/16), done.
remote: Total 16 (delta 11), reused 0 (delta 0)
Unpacking objects: 100% (16/16), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   70b8c9e..dc6659e  master     -> origin/master
Updating 70b8c9e..dc6659e
Fast-forward
 DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py                      |  43 +++++++++++++
 DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16.pyproj                         |  15 +++--
 DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py                       | 126 ++++++++++++++++++---------------
---
 DeepRL_For_HPE/LSTM_VGG16/Quick_Scripts/LSTM_VGG16Helper.py         | 142 +++++++++++++++++++++++++++++++++
++++++++
 DeepRL_For_HPE/LSTM_VGG16/{ => Quick_Scripts}/tf_trainLSTM_VGG16.py |   0
 DeepRL_For_HPE/LSTM_VGG16/{ => Quick_Scripts}/trainLSTM_VGG16.py    |   0
 DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py                            | 137 ++++++++-------------------------
------
 DeepRL_For_HPE/Note_Files/commands.txt                              |   2 +-
 8 files changed, 287 insertions(+), 178 deletions(-)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/Quick_Scripts/LSTM_VGG16Helper.py
 rename DeepRL_For_HPE/LSTM_VGG16/{ => Quick_Scripts}/tf_trainLSTM_VGG16.py (100%)
 rename DeepRL_For_HPE/LSTM_VGG16/{ => Quick_Scripts}/trainLSTM_VGG16.py (100%)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 22:43:58.294630: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 22:43:58.391558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 22:43:58.391820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 22:43:58.391837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 22:43:58.547272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 22:43:58.547299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 22:43:58.547304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 22:43:58.547446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_22-43-59 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_22-43-59
All frames and annotations from 1 datasets have been read by 2019-01-25 22:44:00.155786
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 22:44:09.060969!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.0663 - mean_absolute_error: 0.1985
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 22:44:34.646643
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 22:44:43.529028!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0631 - mean_absolute_error: 0.1925
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_22-43-59
The subjects will be tested: [(1, 'F01')]
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 75, in <module>
    main()
  File "runCNN_LSTM.py", line 72, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 62, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runCNN_LSTM.py", line 45, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, record = record)
TypeError: evaluateCNN_LSTM() missing 1 required positional argument: 'angles'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 22:47:49.568788: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 22:47:49.664608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 22:47:49.664870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 22:47:49.664884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 22:47:49.820352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 22:47:49.820378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 22:47:49.820386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 22:47:49.820529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_22-47-50 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
1000_2019-01-25_22-47-50
All frames and annotations from 1 datasets have been read by 2019-01-25 22:47:51.435681
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 22:48:00.345485!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.0636 - mean_absolute_error: 0.1922
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 22:48:26.090188
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 22:48:34.981978!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0688 - mean_absolute_error: 0.1982
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
001000_2019-01-25_22-47-50
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 22:48:58.002920
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.33 Degree
        The absolute mean error on Yaw angle estimation: 31.67 Degree
        The absolute mean error on Roll angle estimation: 24.33 Degree
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 75, in <module>
    main()
  File "runCNN_LSTM.py", line 72, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 62, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runCNN_LSTM.py", line 45, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record =
record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 142, in evaluateCNN_LSTM
    means = evaluateAverage(results, angles, num_outputs, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 125, in evaluateAverage
    for an, (matrix, absolute_mean_error) in enumerate(outputs):
TypeError: 'Sequential' object is not iterable
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 22:52:43.242250: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 22:52:43.339347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 22:52:43.339645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 22:52:43.339660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 22:52:43.494786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 22:52:43.494813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 22:52:43.494818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 22:52:43.495000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_22-52-44 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
1000_2019-01-25_22-52-44
All frames and annotations from 1 datasets have been read by 2019-01-25 22:52:45.109564
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 22:52:53.994251!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1062 - mean_absolute_error: 0.2506
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 22:53:18.521622
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 22:53:27.286714!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0629 - mean_absolute_error: 0.1905
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
001000_2019-01-25_22-52-44
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 22:53:50.723118
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.70 Degree
        The absolute mean error on Yaw angle estimation: 31.72 Degree
        The absolute mean error on Roll angle estimation: 5.10 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.70 Degree
        The absolute mean error on Yaw angle estimations: 31.72 Degree
        The absolute mean error on Roll angle estimations: 5.10 Degree
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
1000_2019-01-25_22-52-44
All frames and annotations from 1 datasets have been read by 2019-01-25 22:54:04.208775
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 22:54:12.969114!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0633 - mean_absolute_error: 0.1910
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 22:54:35.730690
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 22:54:44.504656!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0633 - mean_absolute_error: 0.1912
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
001000_2019-01-25_22-52-44
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 22:55:07.704292
For the Subject 1 (F01):
498/498 [==============================] - 7s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.80 Degree
        The absolute mean error on Yaw angle estimation: 32.12 Degree
        The absolute mean error on Roll angle estimation: 6.22 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.80 Degree
        The absolute mean error on Yaw angle estimations: 32.12 Degree
        The absolute mean error on Roll angle estimations: 6.22 Degree
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 75, in <module>
    main()
  File "runCNN_LSTM.py", line 72, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 63, in runCNN_LSTM
    if out_epochs % eveva_epoch > 0:
NameError: name 'eveva_epoch' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 22:59:56.942864: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 22:59:57.040713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 22:59:57.040970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 22:59:57.040982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 22:59:57.197018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 22:59:57.197044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 22:59:57.197048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 22:59:57.197185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_22-59-57 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
1000_2019-01-25_22-59-57
All frames and annotations from 1 datasets have been read by 2019-01-25 22:59:58.854500
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:00:07.744985!
Epoch 1/1
596/882 [===================>..........] - ETA: 7s - loss: 0.1282 - mean_absolute_error: 0.2862^C
Model Exp2019-01-25_22-59-57_part1 has been interrupted.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:00:58.014848: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:00:58.110500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:00:58.110755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:00:58.110767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:00:58.265932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:00:58.265958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:00:58.265963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:00:58.266100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-00-59 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0010_2019-01-25_23-00-59
All frames and annotations from 1 datasets have been read by 2019-01-25 23:00:59.914507
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:01:08.814662!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 0.0718 - mean_absolute_error: 0.2086
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:01:33.858489
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:01:42.739566!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0705 - mean_absolute_error: 0.2120
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000010_2019-01-25_23-00-59
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:02:05.736254
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.29 Degree
        The absolute mean error on Yaw angle estimation: 31.85 Degree
        The absolute mean error on Roll angle estimation: 4.81 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.29 Degree
        The absolute mean error on Yaw angle estimations: 31.85 Degree
        The absolute mean error on Roll angle estimations: 4.81 Degree
Epoch 1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0010_2019-01-25_23-00-59
All frames and annotations from 1 datasets have been read by 2019-01-25 23:02:19.241711
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:02:28.113870!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0683 - mean_absolute_error: 0.2099
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:02:51.571476
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:03:00.459940!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0678 - mean_absolute_error: 0.2101
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000010_2019-01-25_23-00-59
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:03:23.838961
For the Subject 1 (F01):
498/498 [==============================] - 7s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.35 Degree
        The absolute mean error on Yaw angle estimation: 31.85 Degree
        The absolute mean error on Roll angle estimation: 6.07 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.35 Degree
        The absolute mean error on Yaw angle estimations: 31.85 Degree
        The absolute mean error on Roll angle estimations: 6.07 Degree
Epoch 2 completed!
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 67, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, out_epochs %
 eveva_epoch, record = False)
NameError: name 'eveva_epoch' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:06:19.237234: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:06:19.333499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:06:19.333768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:06:19.333781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:06:19.491005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:06:19.491033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:06:19.491041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:06:19.491185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-06-20 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-06-20
All frames and annotations from 1 datasets have been read by 2019-01-25 23:06:21.111882
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:06:30.014348!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 0.0667 - mean_absolute_error: 0.1991
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:06:55.158412
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:07:04.027475!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0630 - mean_absolute_error: 0.1920
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-06-20
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:07:26.705600
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.07 Degree
        The absolute mean error on Yaw angle estimation: 31.70 Degree
        The absolute mean error on Roll angle estimation: 5.59 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.07 Degree
        The absolute mean error on Yaw angle estimations: 31.70 Degree
        The absolute mean error on Roll angle estimations: 5.59 Degree
Epoch 1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-06-20
All frames and annotations from 1 datasets have been read by 2019-01-25 23:07:40.269742
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:07:49.140360!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0626 - mean_absolute_error: 0.1900
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:08:12.902864
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:08:21.776641!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0626 - mean_absolute_error: 0.1900
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-06-20
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:08:45.235755
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.17 Degree
        The absolute mean error on Yaw angle estimation: 31.84 Degree
        The absolute mean error on Roll angle estimation: 5.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.17 Degree
        The absolute mean error on Yaw angle estimations: 31.84 Degree
        The absolute mean error on Roll angle estimations: 5.30 Degree
Epoch 2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-06-20
All frames and annotations from 1 datasets have been read by 2019-01-25 23:08:58.721743
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:09:07.606753!
Epoch 1/1
882/882 [==============================] - 22s 24ms/step - loss: 0.0626 - mean_absolute_error: 0.1900
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-06-20
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:09:30.025457
For the Subject 1 (F01):
498/498 [==============================] - 7s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.16 Degree
        The absolute mean error on Yaw angle estimation: 31.82 Degree
        The absolute mean error on Roll angle estimation: 5.44 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.16 Degree
        The absolute mean error on Yaw angle estimations: 31.82 Degree
        The absolute mean error on Roll angle estimations: 5.44 Degree
Epoch 2 completed!
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 70, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py", lin
e 36, in drawResults
    f = drawPlotsForSubj(outputs, subject, BIWI_Subject_IDs[subject], modelStr, angles = angles)
TypeError: drawPlotsForSubj() missing 1 required positional argument: 'num_outputs'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 17, in <module>
    from EstimationPlotter import *
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py", lin
e 36
    f = drawPlotsForSubj(outputs, subject, BIWI_Subject_IDs[subject], modelStr num_outputs, angles = angles)
                                                                                         ^
SyntaxError: invalid syntax
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:12:13.543751: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:12:13.640583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:12:13.640838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:12:13.640850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:12:13.796263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:12:13.796288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:12:13.796292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:12:13.796429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-12-14 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   17408
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    195
=================================================================
Total params: 138,476,550
Trainable params: 4,216,006
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-12-14
All frames and annotations from 1 datasets have been read by 2019-01-25 23:12:15.423206
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:12:24.324229!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.0650 - mean_absolute_error: 0.1958
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:12:49.845610
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:12:58.708879!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0631 - mean_absolute_error: 0.1903
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-12-14
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:13:22.308549
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.25 Degree
        The absolute mean error on Yaw angle estimation: 31.83 Degree
        The absolute mean error on Roll angle estimation: 5.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.25 Degree
        The absolute mean error on Yaw angle estimations: 31.83 Degree
        The absolute mean error on Roll angle estimations: 5.37 Degree
Epoch 1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-12-14
All frames and annotations from 1 datasets have been read by 2019-01-25 23:13:35.872580
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:13:44.759136!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0628 - mean_absolute_error: 0.1902
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:14:07.558166
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:14:16.432293!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0628 - mean_absolute_error: 0.1900
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-12-14
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:14:39.476042
For the Subject 1 (F01):
498/498 [==============================] - 7s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.16 Degree
        The absolute mean error on Yaw angle estimation: 31.81 Degree
        The absolute mean error on Roll angle estimation: 5.35 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.16 Degree
        The absolute mean error on Yaw angle estimations: 31.81 Degree
        The absolute mean error on Roll angle estimations: 5.35 Degree
Epoch 2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-12-14
All frames and annotations from 1 datasets have been read by 2019-01-25 23:14:52.903867
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:15:01.796306!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0627 - mean_absolute_error: 0.1902
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-12-14
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:15:24.569932
For the Subject 1 (F01):
498/498 [==============================] - 7s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.45 Degree
        The absolute mean error on Yaw angle estimation: 31.93 Degree
        The absolute mean error on Roll angle estimation: 5.24 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.45 Degree
        The absolute mean error on Yaw angle estimations: 31.93 Degree
        The absolute mean error on Roll angle estimations: 5.24 Degree
Epoch 2 completed!
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 70, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py", lin
e 36, in drawResults
    f = drawPlotsForSubj(outputs, subject, BIWI_Subject_IDs[subject], modelStr, num_outputs, angles = angles
)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py", lin
e 18, in drawPlotsForSubj
    f, rows = plt.subplots(num_outputs, 1, sharey=True, sharex=True, figsize=(16, 3*num_outputs))
NameError: name 'plt' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:18:17.536312: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:18:17.634453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:18:17.634751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:18:17.634765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:18:17.790338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:18:17.790366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:18:17.790373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:18:17.790555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-18-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   17408
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    195
=================================================================
Total params: 138,476,550
Trainable params: 4,216,006
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-18-18
All frames and annotations from 1 datasets have been read by 2019-01-25 23:18:19.375663
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:18:28.291877!
Epoch 1/1
882/882 [==============================] - 19s 22ms/step - loss: 0.0642 - mean_absolute_error: 0.1937
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:18:48.972863
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:18:57.875567!
Epoch 1/1
882/882 [==============================] - 18s 20ms/step - loss: 0.0629 - mean_absolute_error: 0.1905
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-18-18
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:19:16.507094
For the Subject 1 (F01):
498/498 [==============================] - 5s 10ms/step
        The absolute mean error on Pitch angle estimation: 23.98 Degree
        The absolute mean error on Yaw angle estimation: 31.75 Degree
        The absolute mean error on Roll angle estimation: 5.10 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.98 Degree
        The absolute mean error on Yaw angle estimations: 31.75 Degree
        The absolute mean error on Roll angle estimations: 5.10 Degree
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 63, in runCNN_LSTM
    printLog('%S completed!' % (modelID), record = record)
ValueError: unsupported format character 'S' (0x53) at index 1
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:20:21.120619: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:20:21.218282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:20:21.218540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:20:21.218552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:20:21.375862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:20:21.375888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:20:21.375893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:20:21.376030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-20-22 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   17408
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    195
=================================================================
Total params: 138,476,550
Trainable params: 4,216,006
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-20-22
All frames and annotations from 1 datasets have been read by 2019-01-25 23:20:22.935868
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:20:31.845298!
Epoch 1/1
882/882 [==============================] - 19s 22ms/step - loss: 0.0645 - mean_absolute_error: 0.1942
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:20:52.761842
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:21:01.644105!
Epoch 1/1
882/882 [==============================] - 18s 20ms/step - loss: 0.0629 - mean_absolute_error: 0.1907
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-20-22
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:21:20.553619
For the Subject 1 (F01):
498/498 [==============================] - 5s 10ms/step
        The absolute mean error on Pitch angle estimation: 24.19 Degree
        The absolute mean error on Yaw angle estimation: 31.60 Degree
        The absolute mean error on Roll angle estimation: 5.39 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.19 Degree
        The absolute mean error on Yaw angle estimations: 31.60 Degree
        The absolute mean error on Roll angle estimations: 5.39 Degree
Exp2019-01-25_23-20-22_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-20-22
All frames and annotations from 1 datasets have been read by 2019-01-25 23:21:31.254096
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:21:40.155659!
Epoch 1/1
882/882 [==============================] - 18s 20ms/step - loss: 0.0628 - mean_absolute_error: 0.1901
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:21:58.987009
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:22:07.894580!
Epoch 1/1
882/882 [==============================] - 18s 21ms/step - loss: 0.0629 - mean_absolute_error: 0.1903
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-20-22
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:22:27.014083
For the Subject 1 (F01):
498/498 [==============================] - 5s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.12 Degree
        The absolute mean error on Yaw angle estimation: 31.75 Degree
        The absolute mean error on Roll angle estimation: 5.67 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.12 Degree
        The absolute mean error on Yaw angle estimations: 31.75 Degree
        The absolute mean error on Roll angle estimations: 5.67 Degree
Exp2019-01-25_23-20-22_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-20-22
All frames and annotations from 1 datasets have been read by 2019-01-25 23:22:37.658062
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:22:46.575975!
Epoch 1/1
882/882 [==============================] - 18s 21ms/step - loss: 0.0628 - mean_absolute_error: 0.1903
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-20-22
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:23:05.615915
For the Subject 1 (F01):
498/498 [==============================] - 5s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.08 Degree
        The absolute mean error on Yaw angle estimation: 31.73 Degree
        The absolute mean error on Roll angle estimation: 5.43 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.08 Degree
        The absolute mean error on Yaw angle estimations: 31.73 Degree
        The absolute mean error on Roll angle estimations: 5.43 Degree
Exp2019-01-25_23-20-22_part2 completed!
subject1_Exp2019-01-25_23-20-22_part2.png has been saved by 2019-01-25 23:23:15.593821.
Model Exp2019-01-25_23-20-22 has been evaluated successfully.
Model Exp2019-01-25_23-20-22 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:26:52.699524: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:26:52.795824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:26:52.796089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:26:52.796106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:26:52.951421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:26:52.951447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:26:52.951452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:26:52.951590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-26-53 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   17408
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    195
=================================================================
Total params: 138,476,550
Trainable params: 4,216,006
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-25_23-26-53
All frames and annotations from 20 datasets have been read by 2019-01-25 23:26:58.287309
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 23:27:04.698955!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0317 - mean_absolute_error: 0.1148
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 23:27:24.838512!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.0287 - mean_absolute_error: 0.1120
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 23:27:41.096783!
Epoch 1/1
654/654 [==============================] - 13s 20ms/step - loss: 0.0562 - mean_absolute_error: 0.1518
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 23:27:59.319158!
Epoch 1/1
502/502 [==============================] - 10s 21ms/step - loss: 0.0327 - mean_absolute_error: 0.1324
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 23:28:17.424579!
Epoch 1/1
772/772 [==============================] - 16s 20ms/step - loss: 0.0673 - mean_absolute_error: 0.1857
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 23:28:38.465648!
Epoch 1/1
569/569 [==============================] - 11s 20ms/step - loss: 0.0716 - mean_absolute_error: 0.1945
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 23:28:56.086104!
Epoch 1/1
634/634 [==============================] - 13s 20ms/step - loss: 0.0695 - mean_absolute_error: 0.1862
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 23:29:17.906106!
Epoch 1/1
914/914 [==============================] - 18s 20ms/step - loss: 0.0494 - mean_absolute_error: 0.1494
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 23:29:43.894356!
Epoch 1/1
745/745 [==============================] - 15s 21ms/step - loss: 0.0827 - mean_absolute_error: 0.2127
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 23:30:06.685890!
Epoch 1/1
732/732 [==============================] - 15s 20ms/step - loss: 0.0495 - mean_absolute_error: 0.1657
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 23:30:28.890471!
Epoch 1/1
726/726 [==============================] - 15s 20ms/step - loss: 0.0597 - mean_absolute_error: 0.1712
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 23:30:48.931493!
Epoch 1/1
498/498 [==============================] - 10s 20ms/step - loss: 0.0899 - mean_absolute_error: 0.2170
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 23:31:05.156695!
Epoch 1/1
614/614 [==============================] - 13s 20ms/step - loss: 0.0648 - mean_absolute_error: 0.1906
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 23:31:22.881400!
Epoch 1/1
511/511 [==============================] - 10s 20ms/step - loss: 0.0834 - mean_absolute_error: 0.2166
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 23:31:40.776040!
Epoch 1/1
744/744 [==============================] - 15s 20ms/step - loss: 0.0813 - mean_absolute_error: 0.2117
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 23:32:01.398077!
Epoch 1/1
556/556 [==============================] - 11s 21ms/step - loss: 0.0420 - mean_absolute_error: 0.1393
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 23:32:16.730034!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0250 - mean_absolute_error: 0.1037
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 23:32:30.118816!
Epoch 1/1
542/542 [==============================] - 11s 20ms/step - loss: 0.0726 - mean_absolute_error: 0.1820
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 23:32:45.971645!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0407 - mean_absolute_error: 0.1458
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 23:33:01.749333!
Epoch 1/1
572/572 [==============================] - 12s 21ms/step - loss: 0.0447 - mean_absolute_error: 0.1389
Epoch 1 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-25_23-26-53
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 23:33:15.725960
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 16.64 Degree
        The absolute mean error on Yaw angle estimation: 32.31 Degree
        The absolute mean error on Roll angle estimation: 10.70 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 21.65 Degree
        The absolute mean error on Yaw angle estimation: 29.76 Degree
        The absolute mean error on Roll angle estimation: 5.60 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.25 Degree
        The absolute mean error on Yaw angle estimation: 26.27 Degree
        The absolute mean error on Roll angle estimation: 9.45 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 23.25 Degree
        The absolute mean error on Yaw angle estimation: 34.71 Degree
        The absolute mean error on Roll angle estimation: 14.17 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.95 Degree
        The absolute mean error on Yaw angle estimations: 30.76 Degree
        The absolute mean error on Roll angle estimations: 9.98 Degree
Exp2019-01-25_23-26-53_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-25_23-26-53
All frames and annotations from 20 datasets have been read by 2019-01-25 23:34:25.937931
1. set (Dataset 6) being trained for epoch 1 by 2019-01-25 23:34:31.128874!
Epoch 1/1
542/542 [==============================] - 11s 20ms/step - loss: 0.0735 - mean_absolute_error: 0.1812
2. set (Dataset 11) being trained for epoch 1 by 2019-01-25 23:34:48.020397!
Epoch 1/1
572/572 [==============================] - 12s 21ms/step - loss: 0.0433 - mean_absolute_error: 0.1339
3. set (Dataset 10) being trained for epoch 1 by 2019-01-25 23:35:07.063689!
Epoch 1/1
726/726 [==============================] - 15s 21ms/step - loss: 0.0608 - mean_absolute_error: 0.1744
4. set (Dataset 4) being trained for epoch 1 by 2019-01-25 23:35:29.498193!
Epoch 1/1
744/744 [==============================] - 15s 20ms/step - loss: 0.0783 - mean_absolute_error: 0.2084
5. set (Dataset 23) being trained for epoch 1 by 2019-01-25 23:35:50.268723!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.0710 - mean_absolute_error: 0.1916
6. set (Dataset 13) being trained for epoch 1 by 2019-01-25 23:36:06.874644!
Epoch 1/1
485/485 [==============================] - 10s 20ms/step - loss: 0.0385 - mean_absolute_error: 0.1398
7. set (Dataset 17) being trained for epoch 1 by 2019-01-25 23:36:20.559054!
Epoch 1/1
395/395 [==============================] - 8s 20ms/step - loss: 0.0288 - mean_absolute_error: 0.1132
8. set (Dataset 1) being trained for epoch 1 by 2019-01-25 23:36:33.691170!
Epoch 1/1
498/498 [==============================] - 10s 21ms/step - loss: 0.0952 - mean_absolute_error: 0.2196
9. set (Dataset 8) being trained for epoch 1 by 2019-01-25 23:36:51.821482!
Epoch 1/1
772/772 [==============================] - 16s 21ms/step - loss: 0.0659 - mean_absolute_error: 0.1820
10. set (Dataset 7) being trained for epoch 1 by 2019-01-25 23:37:15.286184!
Epoch 1/1
745/745 [==============================] - 15s 21ms/step - loss: 0.0805 - mean_absolute_error: 0.2113
11. set (Dataset 21) being trained for epoch 1 by 2019-01-25 23:37:36.700779!
Epoch 1/1
634/634 [==============================] - 13s 20ms/step - loss: 0.0735 - mean_absolute_error: 0.1942
12. set (Dataset 22) being trained for epoch 1 by 2019-01-25 23:37:55.902630!
Epoch 1/1
665/665 [==============================] - 14s 20ms/step - loss: 0.0294 - mean_absolute_error: 0.1088
13. set (Dataset 2) being trained for epoch 1 by 2019-01-25 23:38:14.529027!
Epoch 1/1
511/511 [==============================] - 10s 20ms/step - loss: 0.0752 - mean_absolute_error: 0.2036
14. set (Dataset 24) being trained for epoch 1 by 2019-01-25 23:38:29.534510!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.0462 - mean_absolute_error: 0.1460
15. set (Dataset 15) being trained for epoch 1 by 2019-01-25 23:38:45.876440!
Epoch 1/1
654/654 [==============================] - 13s 20ms/step - loss: 0.0559 - mean_absolute_error: 0.1518
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 23:39:04.482983!
Epoch 1/1
556/556 [==============================] - 11s 20ms/step - loss: 0.0395 - mean_absolute_error: 0.1294
17. set (Dataset 18) being trained for epoch 1 by 2019-01-25 23:39:21.751709!
Epoch 1/1
614/614 [==============================] - 13s 20ms/step - loss: 0.0578 - mean_absolute_error: 0.1765
18. set (Dataset 19) being trained for epoch 1 by 2019-01-25 23:39:39.296934!
Epoch 1/1
502/502 [==============================] - 10s 20ms/step - loss: 0.0350 - mean_absolute_error: 0.1368
19. set (Dataset 12) being trained for epoch 1 by 2019-01-25 23:39:56.817267!
Epoch 1/1
732/732 [==============================] - 15s 20ms/step - loss: 0.0525 - mean_absolute_error: 0.1688
20. set (Dataset 16) being trained for epoch 1 by 2019-01-25 23:40:20.599993!
Epoch 1/1
914/914 [==============================] - 19s 20ms/step - loss: 0.0540 - mean_absolute_error: 0.1578
Epoch 1 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (23, 'M13'), (13, 'M07'), (17,
'M10'), (1, 'F01'), (8, 'M02'), (7, 'M01'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'),
(20, 'M12'), (18, 'F05'), (19, 'M11'), (12, 'M06'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-25_23-26-53
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 23:40:41.373262
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 19.66 Degree
        The absolute mean error on Yaw angle estimation: 32.57 Degree
        The absolute mean error on Roll angle estimation: 7.82 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 30.65 Degree
        The absolute mean error on Yaw angle estimation: 29.77 Degree
        The absolute mean error on Roll angle estimation: 3.69 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.64 Degree
        The absolute mean error on Yaw angle estimation: 26.40 Degree
        The absolute mean error on Roll angle estimation: 7.91 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 21.48 Degree
        The absolute mean error on Yaw angle estimation: 34.80 Degree
        The absolute mean error on Roll angle estimation: 14.20 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 23.11 Degree
        The absolute mean error on Yaw angle estimations: 30.89 Degree
        The absolute mean error on Roll angle estimations: 8.40 Degree
Exp2019-01-25_23-26-53_part2 completed!
subject3_Exp2019-01-25_23-26-53_part2.png has been saved by 2019-01-25 23:41:47.432863.
subject5_Exp2019-01-25_23-26-53_part2.png has been saved by 2019-01-25 23:41:47.622836.
subject9_Exp2019-01-25_23-26-53_part2.png has been saved by 2019-01-25 23:41:47.812736.
subject14_Exp2019-01-25_23-26-53_part2.png has been saved by 2019-01-25 23:41:48.021442.
Model Exp2019-01-25_23-26-53 has been evaluated successfully.
Model Exp2019-01-25_23-26-53 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:51:55.846169: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:51:55.942397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:51:55.942700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:51:55.942715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:51:56.098169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:51:56.098196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:51:56.098201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:51:56.098387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-51-56 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   17408
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    195
=================================================================
Total params: 134,290,438
Trainable params: 29,894
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-25_23-51-56
All frames and annotations from 20 datasets have been read by 2019-01-25 23:52:01.163672
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 23:52:07.567505!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.0284 - mean_absolute_error: 0.1103
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 23:52:26.035945!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0256 - mean_absolute_error: 0.1092
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 23:52:41.257450!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0517 - mean_absolute_error: 0.1528
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 23:52:57.946292!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0336 - mean_absolute_error: 0.1341
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 23:53:14.725493!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0670 - mean_absolute_error: 0.1859
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 23:53:33.854577!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0581 - mean_absolute_error: 0.1812
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 23:53:50.002752!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0618 - mean_absolute_error: 0.1828
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 23:54:10.427192!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0384 - mean_absolute_error: 0.1441
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 23:54:34.469446!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0805 - mean_absolute_error: 0.2126
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 23:54:55.400152!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0499 - mean_absolute_error: 0.1657
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 23:55:15.823954!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0597 - mean_absolute_error: 0.1710
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 23:55:33.785566!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0900 - mean_absolute_error: 0.2169
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 23:55:48.664525!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0493 - mean_absolute_error: 0.1713
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 23:56:04.716952!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0626 - mean_absolute_error: 0.1856
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 23:56:21.456084!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0569 - mean_absolute_error: 0.1728
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 23:56:40.338628!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0442 - mean_absolute_error: 0.1523
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 23:56:54.275994!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0238 - mean_absolute_error: 0.1115
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 23:57:06.709594!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0627 - mean_absolute_error: 0.1732
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 23:57:21.261694!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0305 - mean_absolute_error: 0.1219
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 23:57:35.894987!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0392 - mean_absolute_error: 0.1316
Epoch 1 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-25_23-51-56
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 23:57:48.100614
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.56 Degree
        The absolute mean error on Yaw angle estimation: 31.71 Degree
        The absolute mean error on Roll angle estimation: 9.63 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.73 Degree
        The absolute mean error on Yaw angle estimation: 30.31 Degree
        The absolute mean error on Roll angle estimation: 5.46 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.55 Degree
        The absolute mean error on Yaw angle estimation: 33.82 Degree
        The absolute mean error on Roll angle estimation: 8.28 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 29.63 Degree
        The absolute mean error on Yaw angle estimation: 34.54 Degree
        The absolute mean error on Roll angle estimation: 14.15 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 21.12 Degree
        The absolute mean error on Yaw angle estimations: 32.60 Degree
        The absolute mean error on Roll angle estimations: 9.38 Degree
Exp2019-01-25_23-51-56_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-25_23-51-56
All frames and annotations from 20 datasets have been read by 2019-01-25 23:58:57.637212
1. set (Dataset 6) being trained for epoch 1 by 2019-01-25 23:59:02.824821!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0573 - mean_absolute_error: 0.1679
2. set (Dataset 11) being trained for epoch 1 by 2019-01-25 23:59:18.230182!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0326 - mean_absolute_error: 0.1248
3. set (Dataset 10) being trained for epoch 1 by 2019-01-25 23:59:35.946602!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0596 - mean_absolute_error: 0.1714
4. set (Dataset 4) being trained for epoch 1 by 2019-01-25 23:59:56.431037!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0429 - mean_absolute_error: 0.1518
5. set (Dataset 23) being trained for epoch 1 by 2019-01-26 00:00:15.467587!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0611 - mean_absolute_error: 0.1873
6. set (Dataset 13) being trained for epoch 1 by 2019-01-26 00:00:30.678576!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0279 - mean_absolute_error: 0.1214
7. set (Dataset 17) being trained for epoch 1 by 2019-01-26 00:00:43.155736!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0253 - mean_absolute_error: 0.1174
8. set (Dataset 1) being trained for epoch 1 by 2019-01-26 00:00:55.210216!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0881 - mean_absolute_error: 0.2113
9. set (Dataset 8) being trained for epoch 1 by 2019-01-26 00:01:11.839785!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0485 - mean_absolute_error: 0.1553
10. set (Dataset 7) being trained for epoch 1 by 2019-01-26 00:01:33.587933!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0526 - mean_absolute_error: 0.1677
11. set (Dataset 21) being trained for epoch 1 by 2019-01-26 00:01:52.876115!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0551 - mean_absolute_error: 0.1807
12. set (Dataset 22) being trained for epoch 1 by 2019-01-26 00:02:10.754297!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0214 - mean_absolute_error: 0.1009
13. set (Dataset 2) being trained for epoch 1 by 2019-01-26 00:02:27.903116!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0540 - mean_absolute_error: 0.1757
14. set (Dataset 24) being trained for epoch 1 by 2019-01-26 00:02:41.749679!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0225 - mean_absolute_error: 0.1089
15. set (Dataset 15) being trained for epoch 1 by 2019-01-26 00:02:56.762922!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0415 - mean_absolute_error: 0.1426
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 00:03:14.027861!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0366 - mean_absolute_error: 0.1306
17. set (Dataset 18) being trained for epoch 1 by 2019-01-26 00:03:30.160569!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0406 - mean_absolute_error: 0.1589
18. set (Dataset 19) being trained for epoch 1 by 2019-01-26 00:03:46.043747!
Epoch 1/1
502/502 [==============================] - 9s 19ms/step - loss: 0.0246 - mean_absolute_error: 0.1153
19. set (Dataset 12) being trained for epoch 1 by 2019-01-26 00:04:02.666426!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0426 - mean_absolute_error: 0.1500
20. set (Dataset 16) being trained for epoch 1 by 2019-01-26 00:04:24.565911!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0341 - mean_absolute_error: 0.1388
Epoch 1 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (23, 'M13'), (13, 'M07'), (17,
'M10'), (1, 'F01'), (8, 'M02'), (7, 'M01'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'),
(20, 'M12'), (18, 'F05'), (19, 'M11'), (12, 'M06'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-25_23-51-56
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 00:04:43.110208
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 16.97 Degree
        The absolute mean error on Yaw angle estimation: 31.64 Degree
        The absolute mean error on Roll angle estimation: 9.60 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.44 Degree
        The absolute mean error on Yaw angle estimation: 28.44 Degree
        The absolute mean error on Roll angle estimation: 5.36 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 19.78 Degree
        The absolute mean error on Yaw angle estimation: 25.99 Degree
        The absolute mean error on Roll angle estimation: 9.10 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 27.69 Degree
        The absolute mean error on Yaw angle estimation: 33.56 Degree
        The absolute mean error on Roll angle estimation: 14.23 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 22.22 Degree
        The absolute mean error on Yaw angle estimations: 29.91 Degree
        The absolute mean error on Roll angle estimations: 9.57 Degree
Exp2019-01-25_23-51-56_part2 completed!
subject3_Exp2019-01-25_23-51-56_part2.png has been saved by 2019-01-26 00:05:48.797204.
subject5_Exp2019-01-25_23-51-56_part2.png has been saved by 2019-01-26 00:05:48.996840.
subject9_Exp2019-01-25_23-51-56_part2.png has been saved by 2019-01-26 00:05:49.189573.
subject14_Exp2019-01-25_23-51-56_part2.png has been saved by 2019-01-26 00:05:49.402624.
Model Exp2019-01-25_23-51-56 has been evaluated successfully.
Model Exp2019-01-25_23-51-56 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit -m "don't use fc
1024"
[master 87e1e1b] don't use fc1024
 28 files changed, 1256 insertions(+), 2419 deletions(-)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-20-22/output_Exp2019-01-25_23-20-22.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-20-22/subject1_Exp2019-01-25_23-20-22
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/output_Exp2019-01-25_23-26-53.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject14_Exp2019-01-25_23-26-5
3_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject3_Exp2019-01-25_23-26-53
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject5_Exp2019-01-25_23-26-53
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject9_Exp2019-01-25_23-26-53
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-51-56/output_Exp2019-01-25_23-51-56.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-51-56/subject14_Exp2019-01-25_23-51-5
6_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-51-56/subject3_Exp2019-01-25_23-51-56
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-51-56/subject5_Exp2019-01-25_23-51-56
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-51-56/subject9_Exp2019-01-25_23-51-56
_part2.png
 rewrite DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_/output_Last_Model.txt (100%)
 rewrite DeepRL_For_HPE/LSTM_VGG16/results/Last_Model____/output_Last_Model.txt (98%)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_____/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model______/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_______/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model________/output_Last_Model.txt
 copy DeepRL_For_HPE/LSTM_VGG16/results/{Last_Model_ => Last_Model_________}/output_Last_Model.txt (100%)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__________/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model___________/output_Last_Model.txt
 copy DeepRL_For_HPE/LSTM_VGG16/results/{Last_Model____ => Last_Model____________}/output_Last_Model.txt (10
0%)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 40, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (32/32), done.
Writing objects: 100% (40/40), 1.09 MiB | 816.00 KiB/s, done.
Total 40 (delta 18), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   dc6659e..87e1e1b  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
Already up-to-date.
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 00:12:41.927566: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 00:12:42.024262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 00:12:42.024563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 00:12:42.024576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 00:12:42.179922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 00:12:42.179948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 00:12:42.179952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 00:12:42.180131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_00-12-42 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-26_00-12-42
All frames and annotations from 20 datasets have been read by 2019-01-26 00:12:47.248707
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 00:12:53.677350!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.1278
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 00:13:12.371593!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.1189
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 00:13:27.688705!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.1498
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 00:13:44.492984!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.1282
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 00:14:01.341359!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.1931
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 00:14:20.931527!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1783
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 00:14:37.216535!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.1835
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 00:14:57.528343!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.1339
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 00:15:21.678287!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.1813
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 00:15:42.379065!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.1422
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 00:16:02.988077!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.1465
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 00:16:21.231374!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.1714
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 00:16:36.366602!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1798
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 00:16:52.773770!
Epoch 1/1
511/511 [==============================] - 10s 19ms/step - loss: 0.1880
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 00:17:09.738784!
Epoch 1/1
744/744 [==============================] - 14s 19ms/step - loss: 0.1499
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 00:17:29.049705!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.1323
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 00:17:42.834604!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.1156
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 00:17:55.400458!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1845
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 00:18:10.234248!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.1023
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 00:18:24.577182!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.1148
Epoch 1 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-26_00-12-42
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 00:18:36.843726
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 7.74 Degree
        The absolute mean error on Yaw angle estimation: 52.97 Degree
        The absolute mean error on Roll angle estimation: 13.95 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.23 Degree
        The absolute mean error on Yaw angle estimation: 27.69 Degree
        The absolute mean error on Roll angle estimation: 10.22 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 31.61 Degree
        The absolute mean error on Yaw angle estimation: 53.48 Degree
        The absolute mean error on Roll angle estimation: 15.95 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 17.83 Degree
        The absolute mean error on Yaw angle estimation: 26.55 Degree
        The absolute mean error on Roll angle estimation: 18.35 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.35 Degree
        The absolute mean error on Yaw angle estimations: 40.17 Degree
        The absolute mean error on Roll angle estimations: 14.62 Degree
Exp2019-01-26_00-12-42_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-26_00-12-42
All frames and annotations from 20 datasets have been read by 2019-01-26 00:19:46.752737
1. set (Dataset 6) being trained for epoch 1 by 2019-01-26 00:19:51.932797!
Epoch 1/1
542/542 [==============================] - 10s 19ms/step - loss: 0.1566
2. set (Dataset 11) being trained for epoch 1 by 2019-01-26 00:20:08.029365!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0954
3. set (Dataset 10) being trained for epoch 1 by 2019-01-26 00:20:25.706267!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.1265
4. set (Dataset 4) being trained for epoch 1 by 2019-01-26 00:20:46.000190!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.1311
5. set (Dataset 23) being trained for epoch 1 by 2019-01-26 00:21:04.791036!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1574
6. set (Dataset 13) being trained for epoch 1 by 2019-01-26 00:21:20.059721!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0925
7. set (Dataset 17) being trained for epoch 1 by 2019-01-26 00:21:32.605965!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.1113
8. set (Dataset 1) being trained for epoch 1 by 2019-01-26 00:21:44.967098!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.1491
9. set (Dataset 8) being trained for epoch 1 by 2019-01-26 00:22:01.463451!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.1194
10. set (Dataset 7) being trained for epoch 1 by 2019-01-26 00:22:23.102969!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.1101
11. set (Dataset 21) being trained for epoch 1 by 2019-01-26 00:22:42.784328!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1594
12. set (Dataset 22) being trained for epoch 1 by 2019-01-26 00:23:00.718334!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.1040
13. set (Dataset 2) being trained for epoch 1 by 2019-01-26 00:23:18.061942!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.1505
14. set (Dataset 24) being trained for epoch 1 by 2019-01-26 00:23:32.049163!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0963
15. set (Dataset 15) being trained for epoch 1 by 2019-01-26 00:23:47.429706!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.1198
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 00:24:04.665255!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.1137
17. set (Dataset 18) being trained for epoch 1 by 2019-01-26 00:24:21.052891!
Epoch 1/1
614/614 [==============================] - 11s 19ms/step - loss: 0.1340
18. set (Dataset 19) being trained for epoch 1 by 2019-01-26 00:24:37.367086!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.1061
19. set (Dataset 12) being trained for epoch 1 by 2019-01-26 00:24:53.797321!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.1087
20. set (Dataset 16) being trained for epoch 1 by 2019-01-26 00:25:15.892507!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.1001
Epoch 1 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (23, 'M13'), (13, 'M07'), (17,
'M10'), (1, 'F01'), (8, 'M02'), (7, 'M01'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'),
(20, 'M12'), (18, 'F05'), (19, 'M11'), (12, 'M06'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-26_00-12-42
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 00:25:34.534668
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.25 Degree
        The absolute mean error on Yaw angle estimation: 37.73 Degree
        The absolute mean error on Roll angle estimation: 16.26 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.21 Degree
        The absolute mean error on Yaw angle estimation: 25.89 Degree
        The absolute mean error on Roll angle estimation: 4.29 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.08 Degree
        The absolute mean error on Yaw angle estimation: 40.89 Degree
        The absolute mean error on Roll angle estimation: 16.39 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 15.42 Degree
        The absolute mean error on Yaw angle estimation: 28.46 Degree
        The absolute mean error on Roll angle estimation: 16.12 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 12.74 Degree
        The absolute mean error on Yaw angle estimations: 33.25 Degree
        The absolute mean error on Roll angle estimations: 13.27 Degree
Exp2019-01-26_00-12-42_part2 completed!
subject3_Exp2019-01-26_00-12-42_part2.png has been saved by 2019-01-26 00:26:40.313523.
subject5_Exp2019-01-26_00-12-42_part2.png has been saved by 2019-01-26 00:26:40.512352.
subject9_Exp2019-01-26_00-12-42_part2.png has been saved by 2019-01-26 00:26:40.711535.
subject14_Exp2019-01-26_00-12-42_part2.png has been saved by 2019-01-26 00:26:40.929039.
Model Exp2019-01-26_00-12-42 has been evaluated successfully.
Model Exp2019-01-26_00-12-42 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
remote: Counting objects: 5, done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 5 (delta 4), reused 0 (delta 0)
Unpacking objects: 100% (5/5), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   87e1e1b..256039d  master     -> origin/master
Updating 87e1e1b..256039d
Fast-forward
 .../output_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.txt          |   464 -
 .../subject14_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png       |   Bin 228286 -> 0 bytes
 .../subject3_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png        |   Bin 184926 -> 0 bytes
 .../subject5_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png        |   Bin 196018 -> 0 bytes
 .../subject9_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png        |   Bin 197645 -> 0 bytes
 .../output_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.txt          |    44 -
 .../subject14_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png       |   Bin 203701 -> 0 bytes
 .../subject3_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png        |   Bin 174982 -> 0 bytes
 .../subject5_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png        |   Bin 175522 -> 0 bytes
 .../subject9_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png        |   Bin 163637 -> 0 bytes
 .../results/Exp2019-01-24_23-58-26/output_Exp2019-01-24_23-58-26.txt   |   144 -
 .../Exp2019-01-24_23-58-26/subject14_Exp2019-01-24_23-58-26.png        |   Bin 202556 -> 0 bytes
 .../results/Exp2019-01-24_23-58-26/subject3_Exp2019-01-24_23-58-26.png |   Bin 152763 -> 0 bytes
 .../results/Exp2019-01-24_23-58-26/subject5_Exp2019-01-24_23-58-26.png |   Bin 184182 -> 0 bytes
 .../results/Exp2019-01-24_23-58-26/subject9_Exp2019-01-24_23-58-26.png |   Bin 178958 -> 0 bytes
 .../output_Exp2019-01-24_23-58-26_and_2019-01-25_01-00-50.txt          |   197 -
 .../output_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.txt          |    71 -
 .../subject14_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png       |   Bin 199709 -> 0 bytes
 .../subject3_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png        |   Bin 178917 -> 0 bytes
 .../subject5_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png        |   Bin 179552 -> 0 bytes
 .../subject9_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png        |   Bin 184311 -> 0 bytes
 .../results/Exp2019-01-25_02-12-13/output_Exp2019-01-25_02-12-13.txt   |   105 -
 .../results/Exp2019-01-25_02-12-13/subject9_Exp2019-01-25_02-12-13.png |   Bin 86502 -> 0 bytes
 .../results/Exp2019-01-25_02-13-30/output_Exp2019-01-25_02-13-30.txt   |   107 -
 .../results/Exp2019-01-25_02-13-30/subject1_Exp2019-01-25_02-13-30.png |   Bin 79968 -> 0 bytes
 .../results/Exp2019-01-25_02-17-13/output_Exp2019-01-25_02-17-13.txt   |   113 -
 .../results/Exp2019-01-25_02-17-13/subject1_Exp2019-01-25_02-17-13.png |   Bin 74133 -> 0 bytes
 .../results/Exp2019-01-25_02-20-23/output_Exp2019-01-25_02-20-23.txt   |   134 -
 .../Exp2019-01-25_02-20-23/subject14_Exp2019-01-25_02-20-23.png        |   Bin 88887 -> 0 bytes
 .../results/Exp2019-01-25_02-20-23/subject3_Exp2019-01-25_02-20-23.png |   Bin 72555 -> 0 bytes
 .../results/Exp2019-01-25_02-20-23/subject5_Exp2019-01-25_02-20-23.png |   Bin 75021 -> 0 bytes
 .../results/Exp2019-01-25_02-20-23/subject9_Exp2019-01-25_02-20-23.png |   Bin 61798 -> 0 bytes
 .../results/Exp2019-01-25_02-37-37/output_Exp2019-01-25_02-37-37.txt   |   176 -
 .../Exp2019-01-25_02-37-37/subject14_Exp2019-01-25_02-37-37.png        |   Bin 99104 -> 0 bytes
 .../results/Exp2019-01-25_02-37-37/subject3_Exp2019-01-25_02-37-37.png |   Bin 91601 -> 0 bytes
 .../results/Exp2019-01-25_02-37-37/subject5_Exp2019-01-25_02-37-37.png |   Bin 97782 -> 0 bytes
 .../results/Exp2019-01-25_02-37-37/subject9_Exp2019-01-25_02-37-37.png |   Bin 92552 -> 0 bytes
 .../results/Exp2019-01-25_03-00-06/output_Exp2019-01-25_03-00-06.txt   |   134 -
 .../Exp2019-01-25_03-00-06/subject14_Exp2019-01-25_03-00-06.png        |   Bin 96855 -> 0 bytes
 .../results/Exp2019-01-25_03-00-06/subject3_Exp2019-01-25_03-00-06.png |   Bin 57903 -> 0 bytes
 .../results/Exp2019-01-25_03-00-06/subject5_Exp2019-01-25_03-00-06.png |   Bin 90819 -> 0 bytes
 .../results/Exp2019-01-25_03-00-06/subject9_Exp2019-01-25_03-00-06.png |   Bin 92457 -> 0 bytes
 .../output_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.txt          |   851 --
 .../subject14_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png       |   Bin 202153 -> 0 bytes
 .../subject3_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png        |   Bin 173973 -> 0 bytes
 .../subject5_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png        |   Bin 174975 -> 0 bytes
 .../subject9_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png        |   Bin 192854 -> 0 bytes
 .../output_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt          |   120 -
 .../scrollback_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt      | 23553 ----------------------------
--
 .../subject14_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png       |   Bin 202136 -> 0 bytes
 .../subject3_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png        |   Bin 173956 -> 0 bytes
 .../subject5_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png        |   Bin 174962 -> 0 bytes
 .../subject9_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png        |   Bin 192849 -> 0 bytes
 .../results/Exp2019-01-25_19-22-56/output_Exp2019-01-25_19-22-56.txt   |   144 -
 .../Exp2019-01-25_19-22-56/subject14_Exp2019-01-25_19-22-56.png        |   Bin 212119 -> 0 bytes
 .../results/Exp2019-01-25_19-22-56/subject3_Exp2019-01-25_19-22-56.png |   Bin 159378 -> 0 bytes
 .../results/Exp2019-01-25_19-22-56/subject5_Exp2019-01-25_19-22-56.png |   Bin 191241 -> 0 bytes
 .../results/Exp2019-01-25_19-22-56/subject9_Exp2019-01-25_19-22-56.png |   Bin 189925 -> 0 bytes
 .../results/Exp2019-01-25_19-31-28/output_Exp2019-01-25_19-31-28.txt   |   144 -
 .../Exp2019-01-25_19-31-28/subject14_Exp2019-01-25_19-31-28.png        |   Bin 202897 -> 0 bytes
 .../results/Exp2019-01-25_19-31-28/subject3_Exp2019-01-25_19-31-28.png |   Bin 156950 -> 0 bytes
 .../results/Exp2019-01-25_19-31-28/subject5_Exp2019-01-25_19-31-28.png |   Bin 181616 -> 0 bytes
 .../results/Exp2019-01-25_19-31-28/subject9_Exp2019-01-25_19-31-28.png |   Bin 177434 -> 0 bytes
 .../output_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.txt          |    92 -
 .../subject14_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png       |   Bin 199480 -> 0 bytes
 .../subject3_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png        |   Bin 162234 -> 0 bytes
 .../subject5_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png        |   Bin 171704 -> 0 bytes
 .../subject9_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png        |   Bin 178818 -> 0 bytes
 .../results/Exp2019-01-25_19-51-44/output_Exp2019-01-25_19-51-44.txt   |   144 -
 .../Exp2019-01-25_19-51-44/subject14_Exp2019-01-25_19-51-44.png        |   Bin 214307 -> 0 bytes
 .../results/Exp2019-01-25_19-51-44/subject3_Exp2019-01-25_19-51-44.png |   Bin 164378 -> 0 bytes
 .../results/Exp2019-01-25_19-51-44/subject5_Exp2019-01-25_19-51-44.png |   Bin 178148 -> 0 bytes
 .../results/Exp2019-01-25_19-51-44/subject9_Exp2019-01-25_19-51-44.png |   Bin 177293 -> 0 bytes
 .../results/Exp2019-01-25_20-09-28/output_Exp2019-01-25_20-09-28.txt   |   148 -
 .../Exp2019-01-25_20-09-28/subject14_Exp2019-01-25_20-09-28.png        |   Bin 221003 -> 0 bytes
 .../results/Exp2019-01-25_20-09-28/subject3_Exp2019-01-25_20-09-28.png |   Bin 171524 -> 0 bytes
 .../results/Exp2019-01-25_20-09-28/subject5_Exp2019-01-25_20-09-28.png |   Bin 174791 -> 0 bytes
 .../results/Exp2019-01-25_20-09-28/subject9_Exp2019-01-25_20-09-28.png |   Bin 167111 -> 0 bytes
 .../results/Exp2019-01-25_20-27-45/output_Exp2019-01-25_20-27-45.txt   |   152 -
 .../Exp2019-01-25_20-27-45/subject14_Exp2019-01-25_20-27-45.png        |   Bin 195529 -> 0 bytes
 .../results/Exp2019-01-25_20-27-45/subject3_Exp2019-01-25_20-27-45.png |   Bin 154760 -> 0 bytes
 .../results/Exp2019-01-25_20-27-45/subject5_Exp2019-01-25_20-27-45.png |   Bin 167809 -> 0 bytes
 .../results/Exp2019-01-25_20-27-45/subject9_Exp2019-01-25_20-27-45.png |   Bin 158979 -> 0 bytes
 .../results/Exp2019-01-25_20-37-42/output_Exp2019-01-25_20-37-42.txt   |   152 -
 .../Exp2019-01-25_20-37-42/subject14_Exp2019-01-25_20-37-42.png        |   Bin 182078 -> 0 bytes
 .../results/Exp2019-01-25_20-37-42/subject3_Exp2019-01-25_20-37-42.png |   Bin 141195 -> 0 bytes
 .../results/Exp2019-01-25_20-37-42/subject5_Exp2019-01-25_20-37-42.png |   Bin 130378 -> 0 bytes
 .../results/Exp2019-01-25_20-37-42/subject9_Exp2019-01-25_20-37-42.png |   Bin 136091 -> 0 bytes
 .../results/Exp2019-01-25_20-48-22/output_Exp2019-01-25_20-48-22.txt   |   152 -
 .../Exp2019-01-25_20-48-22/subject14_Exp2019-01-25_20-48-22.png        |   Bin 135837 -> 0 bytes
 .../results/Exp2019-01-25_20-48-22/subject3_Exp2019-01-25_20-48-22.png |   Bin 118094 -> 0 bytes
 .../results/Exp2019-01-25_20-48-22/subject5_Exp2019-01-25_20-48-22.png |   Bin 117076 -> 0 bytes
 .../results/Exp2019-01-25_20-48-22/subject9_Exp2019-01-25_20-48-22.png |   Bin 117670 -> 0 bytes
 .../results/Exp2019-01-25_21-04-25/output_Exp2019-01-25_21-04-25.txt   |   152 -
 .../Exp2019-01-25_21-04-25/subject14_Exp2019-01-25_21-04-25.png        |   Bin 136648 -> 0 bytes
 .../results/Exp2019-01-25_21-04-25/subject3_Exp2019-01-25_21-04-25.png |   Bin 118774 -> 0 bytes
 .../results/Exp2019-01-25_21-04-25/subject5_Exp2019-01-25_21-04-25.png |   Bin 117789 -> 0 bytes
 .../results/Exp2019-01-25_21-04-25/subject9_Exp2019-01-25_21-04-25.png |   Bin 118207 -> 0 bytes
 .../results/Exp2019-01-25_21-15-24/output_Exp2019-01-25_21-15-24.txt   |   106 -
 .../results/Exp2019-01-25_21-16-36/output_Exp2019-01-25_21-16-36.txt   |   194 -
 .../Exp2019-01-25_21-16-36/subject14_Exp2019-01-25_21-16-36.png        |   Bin 135992 -> 0 bytes
 .../results/Exp2019-01-25_21-16-36/subject3_Exp2019-01-25_21-16-36.png |   Bin 117254 -> 0 bytes
 .../results/Exp2019-01-25_21-16-36/subject5_Exp2019-01-25_21-16-36.png |   Bin 116207 -> 0 bytes
 .../results/Exp2019-01-25_21-16-36/subject9_Exp2019-01-25_21-16-36.png |   Bin 116941 -> 0 bytes
 .../results/Exp2019-01-25_21-47-31/output_Exp2019-01-25_21-47-31.txt   |   194 -
 .../Exp2019-01-25_21-47-31/subject14_Exp2019-01-25_21-47-31.png        |   Bin 136087 -> 0 bytes
 .../results/Exp2019-01-25_21-47-31/subject3_Exp2019-01-25_21-47-31.png |   Bin 117531 -> 0 bytes
 .../results/Exp2019-01-25_21-47-31/subject5_Exp2019-01-25_21-47-31.png |   Bin 116515 -> 0 bytes
 .../results/Exp2019-01-25_21-47-31/subject9_Exp2019-01-25_21-47-31.png |   Bin 117675 -> 0 bytes
 .../results/Exp2019-01-25_23-20-22/output_Exp2019-01-25_23-20-22.txt   |   104 -
 .../Exp2019-01-25_23-20-22/subject1_Exp2019-01-25_23-20-22_part2.png   |   Bin 107516 -> 0 bytes
 .../results/Exp2019-01-25_23-26-53/output_Exp2019-01-25_23-26-53.txt   |   106 -
 .../Exp2019-01-25_23-26-53/subject14_Exp2019-01-25_23-26-53_part2.png  |   Bin 137215 -> 0 bytes
 .../Exp2019-01-25_23-26-53/subject3_Exp2019-01-25_23-26-53_part2.png   |   Bin 118900 -> 0 bytes
 .../Exp2019-01-25_23-26-53/subject5_Exp2019-01-25_23-26-53_part2.png   |   Bin 117827 -> 0 bytes
 .../Exp2019-01-25_23-26-53/subject9_Exp2019-01-25_23-26-53_part2.png   |   Bin 118233 -> 0 bytes
 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_/output_Last_Model.txt    |    99 -
 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__/output_Last_Model.txt   |   106 -
 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model___/output_Last_Model.txt  |   106 -
 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model____/output_Last_Model.txt |   105 -
 .../LSTM_VGG16/results/Last_Model_____/output_Last_Model.txt           |   103 -
 .../LSTM_VGG16/results/Last_Model______/output_Last_Model.txt          |   103 -
 .../LSTM_VGG16/results/Last_Model_______/output_Last_Model.txt         |   103 -
 .../LSTM_VGG16/results/Last_Model________/output_Last_Model.txt        |   103 -
 .../LSTM_VGG16/results/Last_Model_________/output_Last_Model.txt       |     1 -
 .../LSTM_VGG16/results/Last_Model__________/output_Last_Model.txt      |   102 -
 .../LSTM_VGG16/results/Last_Model___________/output_Last_Model.txt     |   102 -
 .../LSTM_VGG16/results/Last_Model____________/output_Last_Model.txt    |  2309 ---
 128 files changed, 31539 deletions(-)
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45/output_
Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45/subject
14_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45/subject
3_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45/subject
5_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45/subject
9_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09/output_
Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09/subject
14_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09/subject
3_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09/subject
5_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09/subject
9_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26/output_Exp2019-01-24_23-58-26.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26/subject14_Exp2019-01-24_23-58-2
6.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26/subject3_Exp2019-01-24_23-58-26
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26/subject5_Exp2019-01-24_23-58-26
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26/subject9_Exp2019-01-24_23-58-26
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26_and_2019-01-25_01-00-50/output_
Exp2019-01-24_23-58-26_and_2019-01-25_01-00-50.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/output_
Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
14_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
3_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
5_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
9_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-12-13/output_Exp2019-01-25_02-12-13.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-12-13/subject9_Exp2019-01-25_02-12-13
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-13-30/output_Exp2019-01-25_02-13-30.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-13-30/subject1_Exp2019-01-25_02-13-30
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-17-13/output_Exp2019-01-25_02-17-13.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-17-13/subject1_Exp2019-01-25_02-17-13
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/output_Exp2019-01-25_02-20-23.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject14_Exp2019-01-25_02-20-2
3.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject3_Exp2019-01-25_02-20-23
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject5_Exp2019-01-25_02-20-23
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject9_Exp2019-01-25_02-20-23
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/output_Exp2019-01-25_02-37-37.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject14_Exp2019-01-25_02-37-3
7.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject3_Exp2019-01-25_02-37-37
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject5_Exp2019-01-25_02-37-37
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject9_Exp2019-01-25_02-37-37
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/output_Exp2019-01-25_03-00-06.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject14_Exp2019-01-25_03-00-0
6.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject3_Exp2019-01-25_03-00-06
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject5_Exp2019-01-25_03-00-06
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject9_Exp2019-01-25_03-00-06
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/output_
Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
14_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
3_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
5_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
9_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/output_
Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/scrollb
ack_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
14_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
3_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
5_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
9_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/output_Exp2019-01-25_19-22-56.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject14_Exp2019-01-25_19-22-5
6.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject3_Exp2019-01-25_19-22-56
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject5_Exp2019-01-25_19-22-56
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject9_Exp2019-01-25_19-22-56
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/output_Exp2019-01-25_19-31-28.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject14_Exp2019-01-25_19-31-2
8.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject3_Exp2019-01-25_19-31-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject5_Exp2019-01-25_19-31-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject9_Exp2019-01-25_19-31-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/output_
Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
14_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
3_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
5_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
9_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/output_Exp2019-01-25_19-51-44.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject14_Exp2019-01-25_19-51-4
4.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject3_Exp2019-01-25_19-51-44
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject5_Exp2019-01-25_19-51-44
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject9_Exp2019-01-25_19-51-44
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/output_Exp2019-01-25_20-09-28.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject14_Exp2019-01-25_20-09-2
8.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject3_Exp2019-01-25_20-09-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject5_Exp2019-01-25_20-09-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject9_Exp2019-01-25_20-09-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/output_Exp2019-01-25_20-27-45.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject14_Exp2019-01-25_20-27-4
5.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject3_Exp2019-01-25_20-27-45
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject5_Exp2019-01-25_20-27-45
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject9_Exp2019-01-25_20-27-45
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/output_Exp2019-01-25_20-37-42.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject14_Exp2019-01-25_20-37-4
2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject3_Exp2019-01-25_20-37-42
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject5_Exp2019-01-25_20-37-42
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject9_Exp2019-01-25_20-37-42
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/output_Exp2019-01-25_20-48-22.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject14_Exp2019-01-25_20-48-2
2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject3_Exp2019-01-25_20-48-22
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject5_Exp2019-01-25_20-48-22
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject9_Exp2019-01-25_20-48-22
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/output_Exp2019-01-25_21-04-25.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject14_Exp2019-01-25_21-04-2
5.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject3_Exp2019-01-25_21-04-25
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject5_Exp2019-01-25_21-04-25
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject9_Exp2019-01-25_21-04-25
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-15-24/output_Exp2019-01-25_21-15-24.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/output_Exp2019-01-25_21-16-36.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject14_Exp2019-01-25_21-16-3
6.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject3_Exp2019-01-25_21-16-36
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject5_Exp2019-01-25_21-16-36
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject9_Exp2019-01-25_21-16-36
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/output_Exp2019-01-25_21-47-31.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject14_Exp2019-01-25_21-47-3
1.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject3_Exp2019-01-25_21-47-31
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject5_Exp2019-01-25_21-47-31
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject9_Exp2019-01-25_21-47-31
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-20-22/output_Exp2019-01-25_23-20-22.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-20-22/subject1_Exp2019-01-25_23-20-22
_part2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/output_Exp2019-01-25_23-26-53.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject14_Exp2019-01-25_23-26-5
3_part2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject3_Exp2019-01-25_23-26-53
_part2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject5_Exp2019-01-25_23-26-53
_part2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject9_Exp2019-01-25_23-26-53
_part2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model___/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model____/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_____/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model______/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_______/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model___________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model____________/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 00:51:05.057561: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 00:51:05.155880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 00:51:05.156146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 00:51:05.156160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 00:51:05.312125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 00:51:05.312148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 00:51:05.312155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 00:51:05.312296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 51, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/Stateful_CNN_LSTM_Configur
ation.py", line 83, in getFinalModel
    rnn.compile(optimizer=adam, loss='mean_absolute_error', loss_weights=[1.0,2.0,1.0]) #'mean_squared_error
', metrics=['mae']
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 863, in compile
    **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 677, in compile
    str(loss_weights))
ValueError: When passing a list as loss_weights, it should have one entry per model output. The model has 1
outputs, but you passed loss_weights=[1.0, 2.0, 1.0]
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 00:52:49.489444: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 00:52:49.587172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 00:52:49.587427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 00:52:49.587438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 00:52:49.743970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 00:52:49.743996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 00:52:49.744001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 00:52:49.744140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_00-52-50 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 1)                 4097
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   480
_________________________________________________________________
dense_2 (Dense)              (1, 1)                    11
=================================================================
Total params: 134,265,132
Trainable params: 4,588
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 4
num_outputs = 1

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output1_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_00-52-50
All frames and annotations from 20 datasets have been read by 2019-01-26 00:52:54.890715
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 00:53:01.289970!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.1430
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 00:53:19.734353!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.1171
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 00:53:35.066903!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.2065
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 00:53:51.770206!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.1807
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 00:54:08.631278!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.3079
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 00:54:27.857494!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.2141
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 00:54:43.904878!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1510
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 00:55:04.074129!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.1651
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 00:55:28.392161!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.2856
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 00:55:49.238753!
Epoch 1/1
732/732 [==============================] - 14s 19ms/step - loss: 0.2016
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 00:56:10.224210!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.2505
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 00:56:28.315862!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.2830
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 00:56:43.480760!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.2074
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 00:56:59.471396!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.2409
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 00:57:16.095988!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.2538
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 00:57:34.945441!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.2093
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 00:57:48.819318!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.1153
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 00:58:01.237185!
Epoch 1/1
542/542 [==============================] - 10s 19ms/step - loss: 0.1979
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 00:58:16.172935!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.1415
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 00:58:30.753657!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.2215
Epoch 1 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output1_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_00-52-50
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 00:58:43.195650
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Yaw angle estimation: 32.51 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Yaw angle estimation: 27.77 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Yaw angle estimation: 27.31 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Yaw angle estimation: 37.02 Degree
On average in 4 test subjects:
        The absolute mean error on Yaw angle estimations: 31.15 Degree
Exp2019-01-26_00-52-50_part1 completed!
subject3_Exp2019-01-26_00-52-50_part1.png has been saved by 2019-01-26 00:59:49.134975.
subject5_Exp2019-01-26_00-52-50_part1.png has been saved by 2019-01-26 00:59:49.216325.
subject9_Exp2019-01-26_00-52-50_part1.png has been saved by 2019-01-26 00:59:49.296179.
subject14_Exp2019-01-26_00-52-50_part1.png has been saved by 2019-01-26 00:59:49.387635.
Model Exp2019-01-26_00-52-50 has been evaluated successfully.
Model Exp2019-01-26_00-52-50 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 01:05:23.692171: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 01:05:23.788700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 01:05:23.789008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 01:05:23.789023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 01:05:23.946497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 01:05:23.946521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 01:05:23.946526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 01:05:23.946705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_01-05-24 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (10, 1, 4096)             134260544
_________________________________________________________________
fc3 (TimeDistributed)        (10, 1, 1)                4097
_________________________________________________________________
lstm_1 (LSTM)                (10, 10)                  480
_________________________________________________________________
dense_2 (Dense)              (10, 1)                   11
=================================================================
Total params: 134,265,132
Trainable params: 4,588
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 4
num_outputs = 1

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 10
test_batch_size = 10

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output1_BatchSize10_inEpochs1_outEpochs1_AdamOpt_lr-0.0
00100_2019-01-26_01-05-24
All frames and annotations from 20 datasets have been read by 2019-01-26 01:05:29.126806
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 01:05:35.537744!
Epoch 1/1
66/66 [==============================] - 5s 80ms/step - loss: 0.1479
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 01:05:45.981153!
Epoch 1/1
49/49 [==============================] - 3s 59ms/step - loss: 0.1227
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 01:05:55.235414!
Epoch 1/1
65/65 [==============================] - 4s 60ms/step - loss: 0.2288
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 01:06:03.985440!
Epoch 1/1
50/50 [==============================] - 3s 60ms/step - loss: 0.2557
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 01:06:14.809869!
Epoch 1/1
77/77 [==============================] - 5s 60ms/step - loss: 0.3590
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 01:06:24.855336!
Epoch 1/1
56/56 [==============================] - 3s 60ms/step - loss: 0.2382
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 01:06:34.201865!
Epoch 1/1
63/63 [==============================] - 4s 60ms/step - loss: 0.1661
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 01:06:46.677050!
Epoch 1/1
91/91 [==============================] - 5s 60ms/step - loss: 0.1875
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 01:06:59.690941!
Epoch 1/1
74/74 [==============================] - 4s 60ms/step - loss: 0.3776
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 01:07:11.486219!
Epoch 1/1
73/73 [==============================] - 4s 60ms/step - loss: 0.2518
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 01:07:23.137220!
Epoch 1/1
72/72 [==============================] - 4s 60ms/step - loss: 0.2975
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 01:07:32.529139!
Epoch 1/1
49/49 [==============================] - 3s 60ms/step - loss: 0.3581
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 01:07:41.403071!
Epoch 1/1
61/61 [==============================] - 4s 60ms/step - loss: 0.2099
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 01:07:50.142890!
Epoch 1/1
51/51 [==============================] - 3s 60ms/step - loss: 0.2932
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 01:08:00.659596!
Epoch 1/1
74/74 [==============================] - 4s 60ms/step - loss: 0.4256
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 01:08:10.547297!
Epoch 1/1
55/55 [==============================] - 3s 60ms/step - loss: 0.1525
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 01:08:17.666403!
Epoch 1/1
39/39 [==============================] - 2s 60ms/step - loss: 0.0883
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 01:08:25.257406!
Epoch 1/1
54/54 [==============================] - 3s 60ms/step - loss: 0.1841
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 01:08:33.382369!
Epoch 1/1
48/48 [==============================] - 3s 60ms/step - loss: 0.2108
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 01:08:41.924109!
Epoch 1/1
57/57 [==============================] - 3s 60ms/step - loss: 0.2851
Epoch 1 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output1_BatchSize10_inEpochs1_outEpochs1_AdamOpt_lr-0
.000100_2019-01-26_01-05-24
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 01:08:47.500035
For the Subject 3 (F03):
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 62, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runCNN_LSTM.py", line 45, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record =
record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 140, in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs
, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 110, in evaluateSubject
    matrix = numpy.concatenate((test_labels[start_index:, i:i+1], predictions[:, i:i+1]), axis=1)
TypeError: list indices must be integers or slices, not tuple
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
remote: Counting objects: 17, done.
remote: Compressing objects: 100% (17/17), done.
remote: Total 17 (delta 8), reused 0 (delta 0)
Unpacking objects: 100% (17/17), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   256039d..8330c29  master     -> origin/master
Updating 256039d..8330c29
error: Your local changes to the following files would be overwritten by merge:
        DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py
        DeepRL_For_HPE/LSTM_VGG16/Stateful_CNN_LSTM_Configuration.py
Please, commit your changes or stash them before you can merge.
Aborting
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git reset --hard HEAD
HEAD is now at 256039d Emptying results folder
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
Updating 256039d..8330c29
Fast-forward
 DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py                         |  51 +++++++++++----------
 DeepRL_For_HPE/DeepRL_For_HPE.sln                                    |   2 +-
 DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/EstimationPlotter.py |   6 +--
 .../{LSTM_VGG16 => FC_RNN_Evaluater}/EvaluationRecorder.py           |  74 ++++++++++++++++---------------
 .../LSTM_VGG16Helper.py => FC_RNN_Evaluater/FC_RNN_Evaluater.py}     |  51 +++++----------------
 .../LSTM_VGG16.pyproj => FC_RNN_Evaluater/FC_RNN_Evaluater.pyproj}   |  17 +++----
 .../{LSTM_VGG16 => FC_RNN_Evaluater}/NeighborFolderimporter.py       |   0
 .../Quick_Scripts/LSTM_VGG16Helper.py                                |   0
 .../Quick_Scripts/tf_trainLSTM_VGG16.py                              |   0
 .../Quick_Scripts/trainLSTM_VGG16.py                                 |   0
 .../Stateful_FC_RNN_Configuration.py}                                |  53 +++++++++++++++-------
 .../Stateless_FC_RNN_Configuration.py}                               |  14 +++---
 DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/VGG_model.txt        |   0
 DeepRL_For_HPE/FC_RNN_Evaluater/__init__.py                          |   5 +++
 .../continueFC_RNN_Experiment.py}                                    |  18 ++++----
 .../results/Exp2019-01-25_20-19-34/output_Exp2019-01-25_20-19-34.txt |   0
 .../Exp2019-01-25_20-19-34/subject14_Exp2019-01-25_20-19-34.png      | Bin
 .../Exp2019-01-25_20-19-34/subject3_Exp2019-01-25_20-19-34.png       | Bin
 .../Exp2019-01-25_20-19-34/subject5_Exp2019-01-25_20-19-34.png       | Bin
 .../Exp2019-01-25_20-19-34/subject9_Exp2019-01-25_20-19-34.png       | Bin
 .../results/Exp2019-01-25_23-51-56/output_Exp2019-01-25_23-51-56.txt |   0
 .../subject14_Exp2019-01-25_23-51-56_part2.png                       | Bin
 .../Exp2019-01-25_23-51-56/subject3_Exp2019-01-25_23-51-56_part2.png | Bin
 .../Exp2019-01-25_23-51-56/subject5_Exp2019-01-25_23-51-56_part2.png | Bin
 .../Exp2019-01-25_23-51-56/subject9_Exp2019-01-25_23-51-56_part2.png | Bin
 .../runCNN_LSTM.py => FC_RNN_Evaluater/runFC_RNN_Experiment.py}      |  38 +++++-----------
 DeepRL_For_HPE/Note_Files/commands.txt                               |   4 +-
 27 files changed, 158 insertions(+), 175 deletions(-)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/EstimationPlotter.py (92%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/EvaluationRecorder.py (89%)
 rename DeepRL_For_HPE/{LSTM_VGG16/LSTM_VGG16Helper.py => FC_RNN_Evaluater/FC_RNN_Evaluater.py} (78%)
 rename DeepRL_For_HPE/{LSTM_VGG16/LSTM_VGG16.pyproj => FC_RNN_Evaluater/FC_RNN_Evaluater.pyproj} (82%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/NeighborFolderimporter.py (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/Quick_Scripts/LSTM_VGG16Helper.py (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/Quick_Scripts/tf_trainLSTM_VGG16.py (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/Quick_Scripts/trainLSTM_VGG16.py (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16/Stateful_CNN_LSTM_Configuration.py => FC_RNN_Evaluater/Stateful_FC_RNN_Co
nfiguration.py} (61%)
 rename DeepRL_For_HPE/{LSTM_VGG16/CNN_LSTM_Configuration.py => FC_RNN_Evaluater/Stateless_FC_RNN_Configurat
ion.py} (91%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/VGG_model.txt (100%)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/__init__.py
 rename DeepRL_For_HPE/{LSTM_VGG16/continueTrainigCNN_LSTM.py => FC_RNN_Evaluater/continueFC_RNN_Experiment.
py} (83%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_20-19-34/output_Exp2019-01-25_
20-19-34.txt (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_20-19-34/subject14_Exp2019-01-
25_20-19-34.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_20-19-34/subject3_Exp2019-01-2
5_20-19-34.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_20-19-34/subject5_Exp2019-01-2
5_20-19-34.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_20-19-34/subject9_Exp2019-01-2
5_20-19-34.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_23-51-56/output_Exp2019-01-25_
23-51-56.txt (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_23-51-56/subject14_Exp2019-01-
25_23-51-56_part2.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_23-51-56/subject3_Exp2019-01-2
5_23-51-56_part2.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_23-51-56/subject5_Exp2019-01-2
5_23-51-56_part2.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_23-51-56/subject9_Exp2019-01-2
5_23-51-56_part2.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16/runCNN_LSTM.py => FC_RNN_Evaluater/runFC_RNN_Experiment.py} (74%)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ cd ..
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE$ cd FC_RNN_Evaluater/
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:33:43.019413: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:33:43.117506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:33:43.117764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:33:43.117775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:33:43.272818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:33:43.272844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:33:43.272849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:33:43.272988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 37, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateful_FC_RNN_Conf
iguration.py", line 103, in getFinalModel
    modelID = modelID + '_%s' % now()[:-7].replace(' ', '_').replace(':', '-')
NameError: name 'now' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:41:46.350962: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:41:46.448568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:41:46.448825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:41:46.448837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:41:46.603651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:41:46.603677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:41:46.603682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:41:46.603820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 37, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateful_FC_RNN_Conf
iguration.py", line 104, in getFinalModel
    modelID = modelID + '_%s' % now()[:-7].replace(' ', '_').replace(':', '-')
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateful_FC_RNN_Conf
iguration.py", line 8, in now
    def now(): return str(datetime.datetime.now())
NameError: name 'datetime' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:42:26.881057: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:42:26.977631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:42:26.977896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:42:26.977909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:42:27.133122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:42:27.133149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:42:27.133154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:42:27.133295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 37, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
ValueError: not enough values to unpack (expected 4, got 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:46:00.779877: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:46:00.877528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:46:00.877793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:46:00.877805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:46:01.032793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:46:01.032818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:46:01.032823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:46:01.032959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-46-01 has been started to be evaluated.
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 41, in runCNN_LSTM
    printLog(get_model_summary(vgg_model), record = record)
  File "runFC_RNN_Experiment.py", line 13, in get_model_summary
    stream = io.StringIO()
NameError: name 'io' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:47:06.392190: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:47:06.488823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:47:06.489082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:47:06.489095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:47:06.644446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:47:06.644472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:47:06.644477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:47:06.644614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-47-07 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 43, in runCNN_LSTM
    saveConfiguration(confFile = confFile, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.p
y", line 22, in saveConfiguration
    with open(confFile, 'r') as conf:
FileNotFoundError: [Errno 2] No such file or directory: 'Stateful_CNN_LSTM_Configuration.py'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:48:06.521179: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:48:06.617631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:48:06.617892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:48:06.617905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:48:06.773032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:48:06.773059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:48:06.773065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:48:06.773204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-48-07 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-48-07
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_i
nput = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 47, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 36, in trainImageModelForEpochs
    random.Random(4).shuffle(trainingSubjects)
NameError: name 'random' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:49:20.514872: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:49:20.611778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:49:20.612038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:49:20.612052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:49:20.766916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:49:20.766942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:49:20.766951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:49:20.767096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-49-21 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-49-21
All frames and annotations from 1 datasets have been read by 2019-01-26 03:49:22.299620
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_i
nput = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 48, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 39, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 22, in trainImageModelOnSets
    printLog('%d. set (Dataset %d) being trained for epoch %d by %s!' % (c+1, trainingSubjects[c], epoch+1,
now()), record = record)
NameError: name 'now' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:51:18.718144: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:51:18.816303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:51:18.816561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:51:18.816574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:51:18.973477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:51:18.973503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:51:18.973508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:51:18.973646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-51-19 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-51-19
All frames and annotations from 1 datasets have been read by 2019-01-26 03:51:20.506803
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 03:51:29.416103!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2063
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_03-51-19
The subjects will be tested: [(9, 'M03')]
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 30, in runCNN_LSTM_ExperimentWithModel
    full_model, means, results = evaluateCNN_LSTM(full_model, label_rescaling_factor = label_rescaling_facto
r,
NameError: name 'label_rescaling_factor' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:54:08.987256: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:54:09.084941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:54:09.085198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:54:09.085210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:54:09.240557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:54:09.240582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:54:09.240587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:54:09.240722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-54-09 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-54-09
All frames and annotations from 1 datasets have been read by 2019-01-26 03:54:10.801423
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 03:54:19.723145!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2129
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_03-54-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 03:54:38.189740
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 32, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record =
record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 114, in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs
, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 90, in evaluateSubject
    absolute_mean_error = np.abs(differences).mean()
NameError: name 'np' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:55:53.070851: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:55:53.168750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:55:53.169009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:55:53.169021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:55:53.324774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:55:53.324801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:55:53.324806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:55:53.324946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-55-54 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-55-54
All frames and annotations from 1 datasets have been read by 2019-01-26 03:55:54.883566
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 03:56:03.790556!
Epoch 1/1
882/882 [==============================] - 17s 20ms/step - loss: 0.2365
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_03-55-54
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 03:56:22.552461
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 14.25 Degree
        The absolute mean error on Yaw angle estimation: 17.34 Degree
        The absolute mean error on Roll angle estimation: 18.53 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 14.25 Degree
        The absolute mean error on Yaw angle estimations: 17.34 Degree
        The absolute mean error on Roll angle estimations: 18.53 Degree
Exp2019-01-26_03-55-54_part1 completed!
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EstimationPlotter.py
", line 37, in drawResults
    f = drawPlotsForSubj(outputs, subject, BIWI_Subject_IDs[subject], modelStr, num_outputs, angles = angles
)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EstimationPlotter.py
", line 29, in drawPlotsForSubj
    cell.set_ylim([-label_rescaling_factor, label_rescaling_factor])
NameError: name 'label_rescaling_factor' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:57:40.680883: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:57:40.778094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:57:40.778395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:57:40.778409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:57:40.934473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:57:40.934498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:57:40.934503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:57:40.934681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-57-41 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-57-41
All frames and annotations from 1 datasets have been read by 2019-01-26 03:57:42.576001
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 03:57:51.470764!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2710
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_03-57-41
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 03:58:16.281421
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 15.81 Degree
        The absolute mean error on Yaw angle estimation: 20.35 Degree
        The absolute mean error on Roll angle estimation: 8.50 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.81 Degree
        The absolute mean error on Yaw angle estimations: 20.35 Degree
        The absolute mean error on Roll angle estimations: 8.50 Degree
Exp2019-01-26_03-57-41_part1 completed!
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EstimationPlotter.py
", line 42, in drawResults
    f.savefig(addModelFolder(CURRENT_MODEL, fileName), bbox_inches='tight')
NameError: name 'addModelFolder' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:01:56.497215: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:01:56.594076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:01:56.594381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:01:56.594396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:01:56.749528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:01:56.749556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:01:56.749561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:01:56.749745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-01-57 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_04-01-57
All frames and annotations from 1 datasets have been read by 2019-01-26 04:01:58.299366
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:02:07.199021!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1852
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_04-01-57
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:02:25.869609
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 14.82 Degree
        The absolute mean error on Yaw angle estimation: 18.24 Degree
        The absolute mean error on Roll angle estimation: 4.23 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 14.82 Degree
        The absolute mean error on Yaw angle estimations: 18.24 Degree
        The absolute mean error on Roll angle estimations: 4.23 Degree
Exp2019-01-26_04-01-57_part1 completed!
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EstimationPlotter.py
", line 43, in drawResults
    printLog(fileName, 'has been saved by %s.' % now(), record = save)
NameError: name 'printLog' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 8, in <module>
    from EvaluationRecorder import *
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.p
y", line 9, in <module>
    import shutil, os, numpy as np, datatime
ModuleNotFoundError: No module named 'datatime'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:05:13.204314: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:05:13.300833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:05:13.301139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:05:13.301154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:05:13.456636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:05:13.456660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:05:13.456665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:05:13.456845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-05-14 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_04-05-14
All frames and annotations from 1 datasets have been read by 2019-01-26 04:05:14.973562
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:05:23.943874!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2299
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_04-05-14
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:05:42.509464
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 20.74 Degree
        The absolute mean error on Yaw angle estimation: 20.95 Degree
        The absolute mean error on Roll angle estimation: 8.36 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.74 Degree
        The absolute mean error on Yaw angle estimations: 20.95 Degree
        The absolute mean error on Roll angle estimations: 8.36 Degree
Exp2019-01-26_04-05-14_part1 completed!
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EstimationPlotter.py
", line 43, in drawResults
    printLog(fileName, 'has been saved by %s.' % now(), record = save)
NameError: name 'printLog' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:06:28.629338: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:06:28.709179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:06:28.709476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:06:28.709490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:06:28.864187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:06:28.864214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:06:28.864218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:06:28.864405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-06-29 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_04-06-29
All frames and annotations from 1 datasets have been read by 2019-01-26 04:06:30.443397
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:06:39.332947!
Epoch 1/1
882/882 [==============================] - 17s 20ms/step - loss: 0.2223
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_04-06-29
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:06:58.244987
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.46 Degree
        The absolute mean error on Yaw angle estimation: 25.36 Degree
        The absolute mean error on Roll angle estimation: 7.35 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.46 Degree
        The absolute mean error on Yaw angle estimations: 25.36 Degree
        The absolute mean error on Roll angle estimations: 7.35 Degree
Exp2019-01-26_04-06-29_part1 completed!
subject9_Exp2019-01-26_04-06-29_part1.png has been saved by 2019-01-26 04:07:15.780947.
Model Exp2019-01-26_04-06-29 has been evaluated successfully.
Model Exp2019-01-26_04-06-29 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:10:15.867587: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:10:15.964455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:10:15.964711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:10:15.964722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:10:16.120441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:10:16.120468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:10:16.120473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:10:16.120615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-10-16 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-26_04-10-16
All frames and annotations from 1 datasets have been read by 2019-01-26 04:10:17.800543
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:10:26.686713!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2587
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-26_04-10-16
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:10:51.618520
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 19.45 Degree
        The absolute mean error on Yaw angle estimation: 22.98 Degree
        The absolute mean error on Roll angle estimation: 9.85 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 19.45 Degree
        The absolute mean error on Yaw angle estimations: 22.98 Degree
        The absolute mean error on Roll angle estimations: 9.85 Degree
Exp2019-01-26_04-10-16_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-26_04-10-16
All frames and annotations from 1 datasets have been read by 2019-01-26 04:11:14.690885
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:11:23.566395!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2025
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-26_04-10-16
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:11:47.332976
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.94 Degree
        The absolute mean error on Yaw angle estimation: 15.61 Degree
        The absolute mean error on Roll angle estimation: 16.81 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.94 Degree
        The absolute mean error on Yaw angle estimations: 15.61 Degree
        The absolute mean error on Roll angle estimations: 16.81 Degree
Exp2019-01-26_04-10-16_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-26_04-10-16
All frames and annotations from 1 datasets have been read by 2019-01-26 04:12:10.285470
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:12:19.151822!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1813
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-26_04-10-16
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:12:42.406155
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 15.44 Degree
        The absolute mean error on Yaw angle estimation: 16.99 Degree
        The absolute mean error on Roll angle estimation: 20.66 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.44 Degree
        The absolute mean error on Yaw angle estimations: 16.99 Degree
        The absolute mean error on Roll angle estimations: 20.66 Degree
Exp2019-01-26_04-10-16_part3 completed!
subject9_Exp2019-01-26_04-10-16.png has been saved by 2019-01-26 04:13:04.775866.
Model Exp2019-01-26_04-10-16 has been evaluated successfully.
Model Exp2019-01-26_04-10-16 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "fixing
 import errors"
[master ed724e7] fixing import errors
 18 files changed, 434 insertions(+), 17 deletions(-)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-10-16/output_Exp2019-01-26_04-1
0-16.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-10-16/subject9_Exp2019-01-26_04
-10-16.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-12-42/output_Exp2019-01-26_00-12-42.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-12-42/subject14_Exp2019-01-26_00-12-4
2_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-12-42/subject3_Exp2019-01-26_00-12-42
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-12-42/subject5_Exp2019-01-26_00-12-42
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-12-42/subject9_Exp2019-01-26_00-12-42
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-52-50/output_Exp2019-01-26_00-52-50.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-52-50/subject14_Exp2019-01-26_00-52-5
0_part1.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-52-50/subject3_Exp2019-01-26_00-52-50
_part1.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-52-50/subject5_Exp2019-01-26_00-52-50
_part1.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-52-50/subject9_Exp2019-01-26_00-52-50
_part1.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 29, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (27/27), done.
Writing objects: 100% (29/29), 1.12 MiB | 1.02 MiB/s, done.
Total 29 (delta 11), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   8330c29..ed724e7  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Everything up-to-date
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:18:08.957832: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:18:09.055721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:18:09.055984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:18:09.055999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:18:09.214162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:18:09.214190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:18:09.214198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:18:09.214341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_w
eights_tf_dim_ordering_tf_kernels.h5
96116736/96112376 [==============================] - 3s 0us/step
Model Exp2019-01-26_04-18-29 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 2048)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              2098176
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_l
r-0.000100_2019-01-26_04-18-29
All frames and annotations from 1 datasets have been read by 2019-01-26 04:18:30.404352
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:18:39.282865!
Epoch 1/1
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_i
nput = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 48, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 39, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 29, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1877, in train_on
_batch
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1476, in _standar
dize_user_data
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 123, in _standard
ize_input_data
    str(data_shape))
ValueError: Error when checking input: expected tdCNN_input to have shape (1, 299, 299, 3) but got array wit
h shape (1, 224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:23:24.718995: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:23:24.815953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:23:24.816211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:23:24.816230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:23:24.973638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:23:24.973665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:23:24.973669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:23:24.973816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-23-41 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 2048)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              2098176
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_l
r-0.000100_2019-01-26_04-23-41
All frames and annotations from 1 datasets have been read by 2019-01-26 04:23:42.383138
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:23:51.273039!
Epoch 1/1
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_i
nput = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 48, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 39, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 29, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1877, in train_on
_batch
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1476, in _standar
dize_user_data
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 123, in _standard
ize_input_data
    str(data_shape))
ValueError: Error when checking input: expected tdCNN_input to have shape (1, 299, 299, 3) but got array wit
h shape (1, 224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:25:17.160113: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:25:17.258330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:25:17.258632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:25:17.258646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:25:17.417035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:25:17.417061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:25:17.417066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:25:17.417245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-25-33 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 2048)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              2098176
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_l
r-0.000100_2019-01-26_04-25-33
All frames and annotations from 1 datasets have been read by 2019-01-26 04:25:34.774293
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:25:43.666742!
Epoch 1/1
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_i
nput = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 48, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 39, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 29, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1877, in train_on
_batch
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1476, in _standar
dize_user_data
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 123, in _standard
ize_input_data
    str(data_shape))
ValueError: Error when checking input: expected tdCNN_input to have shape (1, 299, 299, 3) but got array wit
h shape (1, 224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:28:48.573417: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:28:48.670272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:28:48.670536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:28:48.670551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:28:48.826268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:28:48.826296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:28:48.826305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:28:48.826451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-28-49 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 04:28:54.047867
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 04:29:00.442153!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.2228
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 04:29:24.008025!
Epoch 1/1
492/492 [==============================] - 13s 25ms/step - loss: 0.1842
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 04:29:42.960016!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.2238
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 04:30:04.142948!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1564
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 04:30:24.700792!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.2204
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 04:30:50.176104!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.2073
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 04:31:10.019732!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.2010
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 04:31:35.182952!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1556
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 04:32:05.959504!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.2257
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 04:32:32.248763!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.1805
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 04:32:58.596110!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1888
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 04:33:21.691104!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.2175
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 04:33:40.065292!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.2058
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 04:34:00.696278!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.2369
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 04:34:21.055728!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.2100
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 04:34:45.127164!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.1549
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 04:35:02.980311!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.1113
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 04:35:17.873040!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1806
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 04:35:35.946084!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.1475
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 04:35:53.898483!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.1368
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:36:12.687372
1. set (Dataset 6) being trained for epoch 2 by 2019-01-26 04:36:17.863106!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1746
2. set (Dataset 11) being trained for epoch 2 by 2019-01-26 04:36:36.954885!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.1286
3. set (Dataset 10) being trained for epoch 2 by 2019-01-26 04:36:58.816780!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1656
4. set (Dataset 4) being trained for epoch 2 by 2019-01-26 04:37:24.215415!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.1856
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 04:37:48.097318!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1965
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 04:38:07.103853!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.1225
7. set (Dataset 17) being trained for epoch 2 by 2019-01-26 04:38:23.311534!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.1133
8. set (Dataset 1) being trained for epoch 2 by 2019-01-26 04:38:38.016834!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1979
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 04:38:58.596935!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1561
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 04:39:25.509463!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.1581
11. set (Dataset 21) being trained for epoch 2 by 2019-01-26 04:39:50.068361!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1873
12. set (Dataset 22) being trained for epoch 2 by 2019-01-26 04:40:12.266179!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.1034
13. set (Dataset 2) being trained for epoch 2 by 2019-01-26 04:40:34.357408!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.2106
14. set (Dataset 24) being trained for epoch 2 by 2019-01-26 04:40:51.781517!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.1173
15. set (Dataset 15) being trained for epoch 2 by 2019-01-26 04:41:10.889728!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.1358
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 04:41:33.479932!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.1172
17. set (Dataset 18) being trained for epoch 2 by 2019-01-26 04:41:53.051489!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1529
18. set (Dataset 19) being trained for epoch 2 by 2019-01-26 04:42:13.228258!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1125
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 04:42:33.319423!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.1461
20. set (Dataset 16) being trained for epoch 2 by 2019-01-26 04:43:00.063280!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1159
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:43:27.577297
1. set (Dataset 19) being trained for epoch 3 by 2019-01-26 04:43:32.450320!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.1012
2. set (Dataset 16) being trained for epoch 3 by 2019-01-26 04:43:53.600948!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1014
3. set (Dataset 21) being trained for epoch 3 by 2019-01-26 04:44:22.243162!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1541
4. set (Dataset 15) being trained for epoch 3 by 2019-01-26 04:44:44.480331!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.1154
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 04:45:05.510978!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.1072
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 04:45:25.607692!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.1200
7. set (Dataset 18) being trained for epoch 3 by 2019-01-26 04:45:49.765595!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1377
8. set (Dataset 22) being trained for epoch 3 by 2019-01-26 04:46:11.389184!
Epoch 1/1
665/665 [==============================] - 18s 26ms/step - loss: 0.0811
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 04:46:34.424081!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1434
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 04:46:56.362502!
Epoch 1/1
772/772 [==============================] - 21s 27ms/step - loss: 0.1282
11. set (Dataset 17) being trained for epoch 3 by 2019-01-26 04:47:20.757338!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0972
12. set (Dataset 6) being trained for epoch 3 by 2019-01-26 04:47:35.794624!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1520
13. set (Dataset 24) being trained for epoch 3 by 2019-01-26 04:47:53.916679!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0802
14. set (Dataset 11) being trained for epoch 3 by 2019-01-26 04:48:11.541992!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.1024
15. set (Dataset 10) being trained for epoch 3 by 2019-01-26 04:48:33.061723!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1253
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 04:48:56.679181!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0939
17. set (Dataset 2) being trained for epoch 3 by 2019-01-26 04:49:15.698703!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1578
18. set (Dataset 4) being trained for epoch 3 by 2019-01-26 04:49:35.819931!
Epoch 1/1
744/744 [==============================] - 20s 26ms/step - loss: 0.1303
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 04:50:03.140728!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.1213
20. set (Dataset 1) being trained for epoch 3 by 2019-01-26 04:50:26.528219!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1554
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:50:43.835969
1. set (Dataset 4) being trained for epoch 4 by 2019-01-26 04:50:51.196768!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.1126
2. set (Dataset 1) being trained for epoch 4 by 2019-01-26 04:51:15.310824!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1445
3. set (Dataset 17) being trained for epoch 4 by 2019-01-26 04:51:31.610824!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0977
4. set (Dataset 10) being trained for epoch 4 by 2019-01-26 04:51:49.216550!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1057
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 04:52:14.767318!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0974
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 04:52:40.808372!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1095
7. set (Dataset 2) being trained for epoch 4 by 2019-01-26 04:53:04.804114!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1155
8. set (Dataset 6) being trained for epoch 4 by 2019-01-26 04:53:22.774811!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1431
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 04:53:41.136011!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0822
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 04:53:59.151675!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1345
11. set (Dataset 18) being trained for epoch 4 by 2019-01-26 04:54:20.049230!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1298
12. set (Dataset 19) being trained for epoch 4 by 2019-01-26 04:54:40.200105!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1119
13. set (Dataset 11) being trained for epoch 4 by 2019-01-26 04:54:58.739402!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0816
14. set (Dataset 16) being trained for epoch 4 by 2019-01-26 04:55:21.805977!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0948
15. set (Dataset 21) being trained for epoch 4 by 2019-01-26 04:55:50.532712!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1401
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 04:56:11.941354!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0887
17. set (Dataset 24) being trained for epoch 4 by 2019-01-26 04:56:30.814452!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0753
18. set (Dataset 15) being trained for epoch 4 by 2019-01-26 04:56:49.660685!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0995
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 04:57:14.014141!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1109
20. set (Dataset 22) being trained for epoch 4 by 2019-01-26 04:57:39.561226!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0713
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:58:01.056796
1. set (Dataset 15) being trained for epoch 5 by 2019-01-26 04:58:07.388715!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0923
2. set (Dataset 22) being trained for epoch 5 by 2019-01-26 04:58:30.759055!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0691
3. set (Dataset 18) being trained for epoch 5 by 2019-01-26 04:58:53.033295!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.1152
4. set (Dataset 21) being trained for epoch 5 by 2019-01-26 04:59:14.942068!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1315
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 04:59:38.376003!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1039
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 05:00:04.655816!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1010
7. set (Dataset 24) being trained for epoch 5 by 2019-01-26 05:00:28.700652!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0723
8. set (Dataset 19) being trained for epoch 5 by 2019-01-26 05:00:46.114431!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0957
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 05:01:05.923313!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0936
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 05:01:29.344750!
Epoch 1/1
485/485 [==============================] - 13s 27ms/step - loss: 0.0817
11. set (Dataset 2) being trained for epoch 5 by 2019-01-26 05:01:47.372845!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.1111
12. set (Dataset 4) being trained for epoch 5 by 2019-01-26 05:02:08.040595!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.1129
13. set (Dataset 16) being trained for epoch 5 by 2019-01-26 05:02:36.247872!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0953
14. set (Dataset 1) being trained for epoch 5 by 2019-01-26 05:03:04.829364!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1278
15. set (Dataset 17) being trained for epoch 5 by 2019-01-26 05:03:20.865410!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0883
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 05:03:36.409832!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0873
17. set (Dataset 11) being trained for epoch 5 by 2019-01-26 05:03:55.920227!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0749
18. set (Dataset 10) being trained for epoch 5 by 2019-01-26 05:04:17.962527!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.1015
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 05:04:42.096181!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1320
20. set (Dataset 6) being trained for epoch 5 by 2019-01-26 05:05:01.590447!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1228
Epoch 5 completed!
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24,
'M14'), (19, 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'),
 (20, 'M12'), (11, 'M05'), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 05:05:16.893796
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 36.25 Degree
        The absolute mean error on Yaw angle estimation: 38.61 Degree
        The absolute mean error on Roll angle estimation: 9.42 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.63 Degree
        The absolute mean error on Yaw angle estimation: 30.99 Degree
        The absolute mean error on Roll angle estimation: 3.44 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 19.30 Degree
        The absolute mean error on Yaw angle estimation: 29.05 Degree
        The absolute mean error on Roll angle estimation: 7.98 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 12.10 Degree
        The absolute mean error on Yaw angle estimation: 33.15 Degree
        The absolute mean error on Roll angle estimation: 13.42 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.82 Degree
        The absolute mean error on Yaw angle estimations: 32.95 Degree
        The absolute mean error on Roll angle estimations: 8.56 Degree
Exp2019-01-26_04-28-49_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 05:06:46.281809
1. set (Dataset 10) being trained for epoch 1 by 2019-01-26 05:06:53.487837!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0907
2. set (Dataset 6) being trained for epoch 1 by 2019-01-26 05:07:16.836326!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1160
3. set (Dataset 2) being trained for epoch 1 by 2019-01-26 05:07:35.576914!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1048
4. set (Dataset 17) being trained for epoch 1 by 2019-01-26 05:07:52.301772!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0842
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 05:08:09.869102!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0984
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 05:08:34.489780!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1229
7. set (Dataset 11) being trained for epoch 1 by 2019-01-26 05:08:54.552133!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0702
8. set (Dataset 4) being trained for epoch 1 by 2019-01-26 05:09:16.792885!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1058
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 05:09:43.094568!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0948
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 05:10:09.666108!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0862
11. set (Dataset 24) being trained for epoch 1 by 2019-01-26 05:10:33.400735!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0744
12. set (Dataset 15) being trained for epoch 1 by 2019-01-26 05:10:51.983250!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0928
13. set (Dataset 1) being trained for epoch 1 by 2019-01-26 05:11:13.412373!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1173
14. set (Dataset 22) being trained for epoch 1 by 2019-01-26 05:11:32.154269!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0708
15. set (Dataset 18) being trained for epoch 1 by 2019-01-26 05:11:54.848362!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1170
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 05:12:15.664876!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0814
17. set (Dataset 16) being trained for epoch 1 by 2019-01-26 05:12:38.861152!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0840
18. set (Dataset 21) being trained for epoch 1 by 2019-01-26 05:13:07.396020!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1312
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 05:13:28.481224!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0840
20. set (Dataset 19) being trained for epoch 1 by 2019-01-26 05:13:45.686781!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0903
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:14:02.883146
1. set (Dataset 21) being trained for epoch 2 by 2019-01-26 05:14:08.885982!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1232
2. set (Dataset 19) being trained for epoch 2 by 2019-01-26 05:14:29.884001!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0862
3. set (Dataset 24) being trained for epoch 2 by 2019-01-26 05:14:46.810803!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0670
4. set (Dataset 18) being trained for epoch 2 by 2019-01-26 05:15:05.137386!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.1030
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 05:15:26.632837!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1153
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 05:15:46.201917!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0798
7. set (Dataset 16) being trained for epoch 2 by 2019-01-26 05:16:07.618261!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0812
8. set (Dataset 15) being trained for epoch 2 by 2019-01-26 05:16:36.542357!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0878
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 05:17:00.648825!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0965
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 05:17:28.041790!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0963
11. set (Dataset 11) being trained for epoch 2 by 2019-01-26 05:17:52.853509!
Epoch 1/1
572/572 [==============================] - 15s 27ms/step - loss: 0.0673
12. set (Dataset 10) being trained for epoch 2 by 2019-01-26 05:18:15.448685!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0876
13. set (Dataset 22) being trained for epoch 2 by 2019-01-26 05:18:40.700612!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0681
14. set (Dataset 6) being trained for epoch 2 by 2019-01-26 05:19:03.085189!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1190
15. set (Dataset 2) being trained for epoch 2 by 2019-01-26 05:19:22.274887!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1067
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 05:19:40.273199!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0803
17. set (Dataset 1) being trained for epoch 2 by 2019-01-26 05:19:59.083827!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1123
18. set (Dataset 17) being trained for epoch 2 by 2019-01-26 05:20:15.080463!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0803
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 05:20:32.538847!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0813
20. set (Dataset 4) being trained for epoch 2 by 2019-01-26 05:20:58.154980!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1083
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:21:21.422255
1. set (Dataset 17) being trained for epoch 3 by 2019-01-26 05:21:25.174028!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0803
2. set (Dataset 4) being trained for epoch 3 by 2019-01-26 05:21:42.522719!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0993
3. set (Dataset 11) being trained for epoch 3 by 2019-01-26 05:22:07.585812!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0697
4. set (Dataset 2) being trained for epoch 3 by 2019-01-26 05:22:27.138369!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1028
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 05:22:45.045150!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0759
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 05:23:04.475726!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0787
7. set (Dataset 1) being trained for epoch 3 by 2019-01-26 05:23:28.167714!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1104
8. set (Dataset 10) being trained for epoch 3 by 2019-01-26 05:23:48.192718!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0830
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 05:24:12.006273!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1211
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 05:24:33.827329!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0915
11. set (Dataset 16) being trained for epoch 3 by 2019-01-26 05:25:01.870736!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0853
12. set (Dataset 21) being trained for epoch 3 by 2019-01-26 05:25:31.176226!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1280
13. set (Dataset 6) being trained for epoch 3 by 2019-01-26 05:25:52.561853!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1125
14. set (Dataset 19) being trained for epoch 3 by 2019-01-26 05:26:10.902529!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0869
15. set (Dataset 24) being trained for epoch 3 by 2019-01-26 05:26:28.624758!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0641
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 05:26:46.896239!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0737
17. set (Dataset 22) being trained for epoch 3 by 2019-01-26 05:27:07.205811!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0646
18. set (Dataset 18) being trained for epoch 3 by 2019-01-26 05:27:29.612889!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.1046
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 05:27:52.836162!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0943
20. set (Dataset 15) being trained for epoch 3 by 2019-01-26 05:28:18.574130!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0844
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:28:39.201414
1. set (Dataset 18) being trained for epoch 4 by 2019-01-26 05:28:45.075009!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1008
2. set (Dataset 15) being trained for epoch 4 by 2019-01-26 05:29:06.895639!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0819
3. set (Dataset 16) being trained for epoch 4 by 2019-01-26 05:29:31.959484!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0797
4. set (Dataset 24) being trained for epoch 4 by 2019-01-26 05:29:59.834317!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0624
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 05:30:19.358131!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0809
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 05:30:45.142863!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0931
7. set (Dataset 22) being trained for epoch 4 by 2019-01-26 05:31:10.120943!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0610
8. set (Dataset 21) being trained for epoch 4 by 2019-01-26 05:31:33.125414!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1230
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 05:31:53.959870!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0791
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 05:32:11.372467!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1143
11. set (Dataset 1) being trained for epoch 4 by 2019-01-26 05:32:30.954705!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1090
12. set (Dataset 17) being trained for epoch 4 by 2019-01-26 05:32:47.624325!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0776
13. set (Dataset 19) being trained for epoch 4 by 2019-01-26 05:33:02.574953!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0810
14. set (Dataset 4) being trained for epoch 4 by 2019-01-26 05:33:23.114068!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1004
15. set (Dataset 11) being trained for epoch 4 by 2019-01-26 05:33:47.776908!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0676
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 05:34:07.405923!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0769
17. set (Dataset 6) being trained for epoch 4 by 2019-01-26 05:34:26.611031!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1106
18. set (Dataset 2) being trained for epoch 4 by 2019-01-26 05:34:45.181404!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1011
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 05:35:05.598144!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0899
20. set (Dataset 10) being trained for epoch 4 by 2019-01-26 05:35:32.158380!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0848
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:35:55.067866
1. set (Dataset 2) being trained for epoch 5 by 2019-01-26 05:36:00.146684!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0933
2. set (Dataset 10) being trained for epoch 5 by 2019-01-26 05:36:20.274787!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0827
3. set (Dataset 1) being trained for epoch 5 by 2019-01-26 05:36:44.417882!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0984
4. set (Dataset 11) being trained for epoch 5 by 2019-01-26 05:37:02.549156!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0696
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 05:37:24.203492!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0904
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 05:37:50.393732!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0840
7. set (Dataset 6) being trained for epoch 5 by 2019-01-26 05:38:15.485552!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1144
8. set (Dataset 17) being trained for epoch 5 by 2019-01-26 05:38:33.326186!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0759
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 05:38:50.940982!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0752
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 05:39:14.606836!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0728
11. set (Dataset 22) being trained for epoch 5 by 2019-01-26 05:39:33.270108!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0625
12. set (Dataset 18) being trained for epoch 5 by 2019-01-26 05:39:56.128320!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1029
13. set (Dataset 4) being trained for epoch 5 by 2019-01-26 05:40:18.692437!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0972
14. set (Dataset 15) being trained for epoch 5 by 2019-01-26 05:40:43.591286!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0859
15. set (Dataset 16) being trained for epoch 5 by 2019-01-26 05:41:08.869091!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0798
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 05:41:36.923325!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0750
17. set (Dataset 19) being trained for epoch 5 by 2019-01-26 05:41:55.575194!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0813
18. set (Dataset 24) being trained for epoch 5 by 2019-01-26 05:42:12.656292!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0597
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 05:42:30.447787!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1114
20. set (Dataset 21) being trained for epoch 5 by 2019-01-26 05:42:51.455067!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1217
Epoch 5 completed!
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F0
6'), (17, 'M10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'),
(20, 'M12'), (19, 'M11'), (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 05:43:09.288916
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.89 Degree
        The absolute mean error on Yaw angle estimation: 27.86 Degree
        The absolute mean error on Roll angle estimation: 24.26 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.54 Degree
        The absolute mean error on Yaw angle estimation: 28.96 Degree
        The absolute mean error on Roll angle estimation: 5.38 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 25.41 Degree
        The absolute mean error on Yaw angle estimation: 24.68 Degree
        The absolute mean error on Roll angle estimation: 10.63 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.51 Degree
        The absolute mean error on Yaw angle estimation: 28.15 Degree
        The absolute mean error on Roll angle estimation: 14.35 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.09 Degree
        The absolute mean error on Yaw angle estimations: 27.41 Degree
        The absolute mean error on Roll angle estimations: 13.65 Degree
Exp2019-01-26_04-28-49_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 05:44:38.355710
1. set (Dataset 24) being trained for epoch 1 by 2019-01-26 05:44:43.027356!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0613
2. set (Dataset 21) being trained for epoch 1 by 2019-01-26 05:45:01.290534!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1124
3. set (Dataset 22) being trained for epoch 1 by 2019-01-26 05:45:23.491057!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0607
4. set (Dataset 16) being trained for epoch 1 by 2019-01-26 05:45:49.049931!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0774
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 05:46:20.013710!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0886
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 05:46:44.627027!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1069
7. set (Dataset 19) being trained for epoch 1 by 2019-01-26 05:47:03.763761!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0823
8. set (Dataset 18) being trained for epoch 1 by 2019-01-26 05:47:22.257770!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0986
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 05:47:45.462492!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0871
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 05:48:11.846161!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0799
11. set (Dataset 6) being trained for epoch 1 by 2019-01-26 05:48:35.172784!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1044
12. set (Dataset 2) being trained for epoch 1 by 2019-01-26 05:48:53.755356!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0928
13. set (Dataset 15) being trained for epoch 1 by 2019-01-26 05:49:13.371753!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0816
14. set (Dataset 10) being trained for epoch 1 by 2019-01-26 05:49:37.606429!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0832
15. set (Dataset 1) being trained for epoch 1 by 2019-01-26 05:50:01.667421!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0957
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 05:50:19.472535!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0730
17. set (Dataset 4) being trained for epoch 1 by 2019-01-26 05:50:40.668341!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0950
18. set (Dataset 11) being trained for epoch 1 by 2019-01-26 05:51:05.356600!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0679
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 05:51:24.961216!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0706
20. set (Dataset 17) being trained for epoch 1 by 2019-01-26 05:51:40.950399!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0731
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:51:55.270329
1. set (Dataset 11) being trained for epoch 2 by 2019-01-26 05:52:00.964363!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0679
2. set (Dataset 17) being trained for epoch 2 by 2019-01-26 05:52:19.577005!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0738
3. set (Dataset 6) being trained for epoch 2 by 2019-01-26 05:52:34.612518!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1066
4. set (Dataset 1) being trained for epoch 2 by 2019-01-26 05:52:53.501477!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0987
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 05:53:11.624896!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1073
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 05:53:31.478954!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0677
7. set (Dataset 4) being trained for epoch 2 by 2019-01-26 05:53:51.126394!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0920
8. set (Dataset 2) being trained for epoch 2 by 2019-01-26 05:54:14.841847!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0914
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 05:54:35.500334!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0862
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 05:55:02.991929!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0808
11. set (Dataset 19) being trained for epoch 2 by 2019-01-26 05:55:26.978931!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0841
12. set (Dataset 24) being trained for epoch 2 by 2019-01-26 05:55:44.149627!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0633
13. set (Dataset 10) being trained for epoch 2 by 2019-01-26 05:56:03.493398!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0799
14. set (Dataset 21) being trained for epoch 2 by 2019-01-26 05:56:27.968519!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1205
15. set (Dataset 22) being trained for epoch 2 by 2019-01-26 05:56:51.068281!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0598
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 05:57:12.749650!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0727
17. set (Dataset 15) being trained for epoch 2 by 2019-01-26 05:57:33.092315!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0796
18. set (Dataset 16) being trained for epoch 2 by 2019-01-26 05:57:58.513862!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0790
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 05:58:28.795697!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0750
20. set (Dataset 18) being trained for epoch 2 by 2019-01-26 05:58:53.436519!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0988
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:59:13.268392
1. set (Dataset 16) being trained for epoch 3 by 2019-01-26 05:59:21.989897!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0765
2. set (Dataset 18) being trained for epoch 3 by 2019-01-26 05:59:51.115318!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0931
3. set (Dataset 19) being trained for epoch 3 by 2019-01-26 06:00:11.396071!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0789
4. set (Dataset 22) being trained for epoch 3 by 2019-01-26 06:00:30.249512!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0594
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 06:00:52.433249!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0705
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 06:01:12.092859!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0724
7. set (Dataset 15) being trained for epoch 3 by 2019-01-26 06:01:37.516640!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0788
8. set (Dataset 24) being trained for epoch 3 by 2019-01-26 06:01:59.446643!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0591
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 06:02:17.336046!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1073
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 06:02:39.534748!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0852
11. set (Dataset 4) being trained for epoch 3 by 2019-01-26 06:03:06.604413!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.0903
12. set (Dataset 11) being trained for epoch 3 by 2019-01-26 06:03:30.568696!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0637
13. set (Dataset 21) being trained for epoch 3 by 2019-01-26 06:03:51.391728!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1173
14. set (Dataset 17) being trained for epoch 3 by 2019-01-26 06:04:11.049850!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0746
15. set (Dataset 6) being trained for epoch 3 by 2019-01-26 06:04:26.327237!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1021
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 06:04:45.320249!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0706
17. set (Dataset 10) being trained for epoch 3 by 2019-01-26 06:05:06.418178!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0793
18. set (Dataset 1) being trained for epoch 3 by 2019-01-26 06:05:30.634262!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0940
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 06:05:50.727014!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0824
20. set (Dataset 2) being trained for epoch 3 by 2019-01-26 06:06:14.500544!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0957
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:06:31.283130
1. set (Dataset 1) being trained for epoch 4 by 2019-01-26 06:06:36.314993!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0910
2. set (Dataset 2) being trained for epoch 4 by 2019-01-26 06:06:54.098015!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0827
3. set (Dataset 4) being trained for epoch 4 by 2019-01-26 06:07:14.794473!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0930
4. set (Dataset 6) being trained for epoch 4 by 2019-01-26 06:07:39.018507!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1079
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 06:07:59.878546!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0704
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 06:08:25.962126!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0815
7. set (Dataset 10) being trained for epoch 4 by 2019-01-26 06:08:52.029688!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0760
8. set (Dataset 11) being trained for epoch 4 by 2019-01-26 06:09:15.735446!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0651
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 06:09:35.034384!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0710
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 06:09:52.735269!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1054
11. set (Dataset 15) being trained for epoch 4 by 2019-01-26 06:10:13.221679!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0811
12. set (Dataset 16) being trained for epoch 4 by 2019-01-26 06:10:38.337325!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0746
13. set (Dataset 17) being trained for epoch 4 by 2019-01-26 06:11:05.635006!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0725
14. set (Dataset 18) being trained for epoch 4 by 2019-01-26 06:11:21.894900!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0971
15. set (Dataset 19) being trained for epoch 4 by 2019-01-26 06:11:42.608912!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0777
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 06:11:59.194360!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0650
17. set (Dataset 21) being trained for epoch 4 by 2019-01-26 06:12:19.059406!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1165
18. set (Dataset 22) being trained for epoch 4 by 2019-01-26 06:12:41.797711!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0589
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 06:13:05.914645!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0843
20. set (Dataset 24) being trained for epoch 4 by 2019-01-26 06:13:30.460105!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0582
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:13:47.177261
1. set (Dataset 22) being trained for epoch 5 by 2019-01-26 06:13:53.572974!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0550
2. set (Dataset 24) being trained for epoch 5 by 2019-01-26 06:14:15.402954!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0565
3. set (Dataset 15) being trained for epoch 5 by 2019-01-26 06:14:34.558485!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0759
4. set (Dataset 19) being trained for epoch 5 by 2019-01-26 06:14:55.689539!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0734
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 06:15:15.817698!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0830
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 06:15:42.047288!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0817
7. set (Dataset 21) being trained for epoch 5 by 2019-01-26 06:16:07.299976!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1131
8. set (Dataset 16) being trained for epoch 5 by 2019-01-26 06:16:32.376623!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0726
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 06:17:02.938477!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0716
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 06:17:26.482968!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0684
11. set (Dataset 10) being trained for epoch 5 by 2019-01-26 06:17:46.133796!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0748
12. set (Dataset 1) being trained for epoch 5 by 2019-01-26 06:18:09.704676!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0884
13. set (Dataset 18) being trained for epoch 5 by 2019-01-26 06:18:28.488470!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.1006
14. set (Dataset 2) being trained for epoch 5 by 2019-01-26 06:18:49.848012!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0859
15. set (Dataset 4) being trained for epoch 5 by 2019-01-26 06:19:09.930339!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0874
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 06:19:34.222813!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0729
17. set (Dataset 17) being trained for epoch 5 by 2019-01-26 06:19:51.313291!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0715
18. set (Dataset 6) being trained for epoch 5 by 2019-01-26 06:20:06.918240!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1027
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 06:20:26.629349!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1072
20. set (Dataset 11) being trained for epoch 5 by 2019-01-26 06:20:46.774843!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0617
Epoch 5 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21,
'F02'), (16, 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 06:21:03.480772
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.73 Degree
        The absolute mean error on Yaw angle estimation: 28.78 Degree
        The absolute mean error on Roll angle estimation: 15.75 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.48 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 4.81 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 27.43 Degree
        The absolute mean error on Yaw angle estimation: 21.95 Degree
        The absolute mean error on Roll angle estimation: 8.51 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.05 Degree
        The absolute mean error on Yaw angle estimation: 28.14 Degree
        The absolute mean error on Roll angle estimation: 13.34 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.67 Degree
        The absolute mean error on Yaw angle estimations: 26.39 Degree
        The absolute mean error on Roll angle estimations: 10.60 Degree
Exp2019-01-26_04-28-49_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 06:22:32.365899
1. set (Dataset 6) being trained for epoch 1 by 2019-01-26 06:22:37.549368!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1010
2. set (Dataset 11) being trained for epoch 1 by 2019-01-26 06:22:56.963269!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0593
3. set (Dataset 10) being trained for epoch 1 by 2019-01-26 06:23:18.452463!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0750
4. set (Dataset 4) being trained for epoch 1 by 2019-01-26 06:23:44.316888!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0889
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 06:24:10.461272!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0798
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 06:24:36.360582!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1076
7. set (Dataset 17) being trained for epoch 1 by 2019-01-26 06:24:54.416247!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0735
8. set (Dataset 1) being trained for epoch 1 by 2019-01-26 06:25:09.296393!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0914
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 06:25:29.823345!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0793
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 06:25:55.999272!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0657
11. set (Dataset 21) being trained for epoch 1 by 2019-01-26 06:26:20.101883!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1142
12. set (Dataset 22) being trained for epoch 1 by 2019-01-26 06:26:42.279079!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0572
13. set (Dataset 2) being trained for epoch 1 by 2019-01-26 06:27:04.056751!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0861
14. set (Dataset 24) being trained for epoch 1 by 2019-01-26 06:27:21.750660!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0624
15. set (Dataset 15) being trained for epoch 1 by 2019-01-26 06:27:40.477855!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0793
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 06:28:02.135544!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0658
17. set (Dataset 18) being trained for epoch 1 by 2019-01-26 06:28:22.189104!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0928
18. set (Dataset 19) being trained for epoch 1 by 2019-01-26 06:28:42.701004!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0768
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 06:29:00.490016!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0665
20. set (Dataset 16) being trained for epoch 1 by 2019-01-26 06:29:21.899223!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0724
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:29:48.744448
1. set (Dataset 19) being trained for epoch 2 by 2019-01-26 06:29:53.654326!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0749
2. set (Dataset 16) being trained for epoch 2 by 2019-01-26 06:30:14.667899!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0684
3. set (Dataset 21) being trained for epoch 2 by 2019-01-26 06:30:43.448925!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1090
4. set (Dataset 15) being trained for epoch 2 by 2019-01-26 06:31:06.258474!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0747
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 06:31:28.983845!
Epoch 1/1
569/569 [==============================] - 15s 25ms/step - loss: 0.0996
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 06:31:48.399290!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0662
7. set (Dataset 18) being trained for epoch 2 by 2019-01-26 06:32:06.403444!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0904
8. set (Dataset 22) being trained for epoch 2 by 2019-01-26 06:32:28.469473!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0566
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 06:32:52.688130!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0808
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 06:33:19.922978!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0769
11. set (Dataset 17) being trained for epoch 2 by 2019-01-26 06:33:42.335483!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0722
12. set (Dataset 6) being trained for epoch 2 by 2019-01-26 06:33:57.233743!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0988
13. set (Dataset 24) being trained for epoch 2 by 2019-01-26 06:34:15.173460!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0584
14. set (Dataset 11) being trained for epoch 2 by 2019-01-26 06:34:33.131784!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0605
15. set (Dataset 10) being trained for epoch 2 by 2019-01-26 06:34:54.902859!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0728
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 06:35:18.908211!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0699
17. set (Dataset 2) being trained for epoch 2 by 2019-01-26 06:35:38.348906!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0899
18. set (Dataset 4) being trained for epoch 2 by 2019-01-26 06:35:58.265034!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0887
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 06:36:24.177876!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0680
20. set (Dataset 1) being trained for epoch 2 by 2019-01-26 06:36:47.589158!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0869
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:37:04.294037
1. set (Dataset 4) being trained for epoch 3 by 2019-01-26 06:37:11.657852!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0834
2. set (Dataset 1) being trained for epoch 3 by 2019-01-26 06:37:35.699017!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0836
3. set (Dataset 17) being trained for epoch 3 by 2019-01-26 06:37:51.458126!
Epoch 1/1
395/395 [==============================] - 8s 20ms/step - loss: 0.0738
4. set (Dataset 10) being trained for epoch 3 by 2019-01-26 06:38:06.526138!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0714
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 06:38:29.685901!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0657
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 06:38:48.919626!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0645
7. set (Dataset 2) being trained for epoch 3 by 2019-01-26 06:39:12.318343!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0800
8. set (Dataset 6) being trained for epoch 3 by 2019-01-26 06:39:30.598890!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1073
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 06:39:49.771989!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1025
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 06:40:11.655101!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0776
11. set (Dataset 18) being trained for epoch 3 by 2019-01-26 06:40:36.950867!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0948
12. set (Dataset 19) being trained for epoch 3 by 2019-01-26 06:40:57.642910!
Epoch 1/1
502/502 [==============================] - 13s 27ms/step - loss: 0.0761
13. set (Dataset 11) being trained for epoch 3 by 2019-01-26 06:41:16.899161!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0585
14. set (Dataset 16) being trained for epoch 3 by 2019-01-26 06:41:40.376685!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0711
15. set (Dataset 21) being trained for epoch 3 by 2019-01-26 06:42:09.952505!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1091
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 06:42:31.562126!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0641
17. set (Dataset 24) being trained for epoch 3 by 2019-01-26 06:42:50.197140!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0564
18. set (Dataset 15) being trained for epoch 3 by 2019-01-26 06:43:09.116688!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0729
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 06:43:33.512179!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0797
20. set (Dataset 22) being trained for epoch 3 by 2019-01-26 06:43:59.101112!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0531
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:44:20.072421
1. set (Dataset 15) being trained for epoch 4 by 2019-01-26 06:44:26.430470!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0717
2. set (Dataset 22) being trained for epoch 4 by 2019-01-26 06:44:48.896138!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0540
3. set (Dataset 18) being trained for epoch 4 by 2019-01-26 06:45:11.719960!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0918
4. set (Dataset 21) being trained for epoch 4 by 2019-01-26 06:45:33.763074!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1085
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 06:45:56.786181!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0693
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 06:46:22.638520!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0779
7. set (Dataset 24) being trained for epoch 4 by 2019-01-26 06:46:46.033958!
Epoch 1/1
492/492 [==============================] - 13s 25ms/step - loss: 0.0610
8. set (Dataset 19) being trained for epoch 4 by 2019-01-26 06:47:03.546703!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0732
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 06:47:20.951917!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0656
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 06:47:39.038326!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0999
11. set (Dataset 2) being trained for epoch 4 by 2019-01-26 06:47:58.236340!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0817
12. set (Dataset 4) being trained for epoch 4 by 2019-01-26 06:48:18.467803!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0868
13. set (Dataset 16) being trained for epoch 4 by 2019-01-26 06:48:45.533716!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0732
14. set (Dataset 1) being trained for epoch 4 by 2019-01-26 06:49:13.197691!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0894
15. set (Dataset 17) being trained for epoch 4 by 2019-01-26 06:49:29.304670!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0698
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 06:49:44.041615!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0636
17. set (Dataset 11) being trained for epoch 4 by 2019-01-26 06:50:03.619269!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0585
18. set (Dataset 10) being trained for epoch 4 by 2019-01-26 06:50:25.343654!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0718
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 06:50:51.643237!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0776
20. set (Dataset 6) being trained for epoch 4 by 2019-01-26 06:51:16.848257!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1035
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:51:35.340047
1. set (Dataset 10) being trained for epoch 5 by 2019-01-26 06:51:42.554755!
Epoch 1/1
726/726 [==============================] - 19s 27ms/step - loss: 0.0709
2. set (Dataset 6) being trained for epoch 5 by 2019-01-26 06:52:07.067375!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0963
3. set (Dataset 2) being trained for epoch 5 by 2019-01-26 06:52:25.742531!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0825
4. set (Dataset 17) being trained for epoch 5 by 2019-01-26 06:52:42.232237!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0715
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 06:52:59.709024!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0732
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 06:53:25.998008!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0748
7. set (Dataset 11) being trained for epoch 5 by 2019-01-26 06:53:51.306157!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0563
8. set (Dataset 4) being trained for epoch 5 by 2019-01-26 06:54:13.107043!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0865
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 06:54:39.092066!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0657
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 06:55:02.043762!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0634
11. set (Dataset 24) being trained for epoch 5 by 2019-01-26 06:55:19.296313!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0609
12. set (Dataset 15) being trained for epoch 5 by 2019-01-26 06:55:38.686235!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0736
13. set (Dataset 1) being trained for epoch 5 by 2019-01-26 06:56:00.062741!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0869
14. set (Dataset 22) being trained for epoch 5 by 2019-01-26 06:56:19.089110!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0553
15. set (Dataset 18) being trained for epoch 5 by 2019-01-26 06:56:41.661206!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0942
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 06:57:02.345884!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0635
17. set (Dataset 16) being trained for epoch 5 by 2019-01-26 06:57:25.126876!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0706
18. set (Dataset 21) being trained for epoch 5 by 2019-01-26 06:57:54.141694!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1065
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 06:58:15.501673!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1000
20. set (Dataset 19) being trained for epoch 5 by 2019-01-26 06:58:34.975049!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0745
Epoch 5 completed!
The subjects are trained: [(10, 'M04'), (6, 'F06'), (2, 'F02'), (17, 'M10'), (7, 'M01'), (8, 'M02'), (11, 'M
05'), (4, 'F04'), (12, 'M06'), (13, 'M07'), (24, 'M14'), (15, 'F03'), (1, 'F01'), (22, 'M01'), (18, 'F05'),
(20, 'M12'), (16, 'M09'), (21, 'F02'), (23, 'M13'), (19, 'M11')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 06:58:49.971824
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.08 Degree
        The absolute mean error on Yaw angle estimation: 24.35 Degree
        The absolute mean error on Roll angle estimation: 15.10 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.11 Degree
        The absolute mean error on Yaw angle estimation: 28.24 Degree
        The absolute mean error on Roll angle estimation: 3.55 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.88 Degree
        The absolute mean error on Yaw angle estimation: 23.90 Degree
        The absolute mean error on Roll angle estimation: 7.48 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.77 Degree
        The absolute mean error on Yaw angle estimation: 29.53 Degree
        The absolute mean error on Roll angle estimation: 13.02 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.21 Degree
        The absolute mean error on Yaw angle estimations: 26.51 Degree
        The absolute mean error on Roll angle estimations: 9.79 Degree
Exp2019-01-26_04-28-49_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 07:00:18.968931
1. set (Dataset 21) being trained for epoch 1 by 2019-01-26 07:00:24.974574!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1082
2. set (Dataset 19) being trained for epoch 1 by 2019-01-26 07:00:46.229432!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0719
3. set (Dataset 24) being trained for epoch 1 by 2019-01-26 07:01:03.896012!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0556
4. set (Dataset 18) being trained for epoch 1 by 2019-01-26 07:01:22.617814!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0873
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 07:01:46.156159!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0835
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 07:02:11.205423!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0978
7. set (Dataset 16) being trained for epoch 1 by 2019-01-26 07:02:34.900086!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0700
8. set (Dataset 15) being trained for epoch 1 by 2019-01-26 07:03:03.902567!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0756
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 07:03:27.507377!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0789
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 07:03:53.028437!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.0649
11. set (Dataset 11) being trained for epoch 1 by 2019-01-26 07:04:16.677250!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0591
12. set (Dataset 10) being trained for epoch 1 by 2019-01-26 07:04:38.516222!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0718
13. set (Dataset 22) being trained for epoch 1 by 2019-01-26 07:05:02.752774!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0547
14. set (Dataset 6) being trained for epoch 1 by 2019-01-26 07:05:24.574888!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0999
15. set (Dataset 2) being trained for epoch 1 by 2019-01-26 07:05:43.835108!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0779
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 07:06:01.936891!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0661
17. set (Dataset 1) being trained for epoch 1 by 2019-01-26 07:06:21.110182!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0919
18. set (Dataset 17) being trained for epoch 1 by 2019-01-26 07:06:37.964055!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0714
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 07:06:53.286172!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0644
20. set (Dataset 4) being trained for epoch 1 by 2019-01-26 07:07:12.998053!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0859
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:07:36.282711
1. set (Dataset 17) being trained for epoch 2 by 2019-01-26 07:07:40.027415!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0696
2. set (Dataset 4) being trained for epoch 2 by 2019-01-26 07:07:57.717044!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0831
3. set (Dataset 11) being trained for epoch 2 by 2019-01-26 07:08:22.671893!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0591
4. set (Dataset 2) being trained for epoch 2 by 2019-01-26 07:08:42.743856!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0796
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 07:09:01.413696!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1011
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 07:09:20.549134!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0618
7. set (Dataset 1) being trained for epoch 2 by 2019-01-26 07:09:37.575631!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0829
8. set (Dataset 10) being trained for epoch 2 by 2019-01-26 07:09:57.288650!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0698
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 07:10:23.674479!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0749
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 07:10:50.289780!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0746
11. set (Dataset 16) being trained for epoch 2 by 2019-01-26 07:11:18.395470!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0721
12. set (Dataset 21) being trained for epoch 2 by 2019-01-26 07:11:47.806766!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1106
13. set (Dataset 6) being trained for epoch 2 by 2019-01-26 07:12:09.461733!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0963
14. set (Dataset 19) being trained for epoch 2 by 2019-01-26 07:12:27.809420!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0712
15. set (Dataset 24) being trained for epoch 2 by 2019-01-26 07:12:45.280018!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0567
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 07:13:02.983279!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0623
17. set (Dataset 22) being trained for epoch 2 by 2019-01-26 07:13:23.600502!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0539
18. set (Dataset 18) being trained for epoch 2 by 2019-01-26 07:13:46.626894!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0864
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 07:14:09.685006!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0715
20. set (Dataset 15) being trained for epoch 2 by 2019-01-26 07:14:34.227531!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0773
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:14:55.476377
1. set (Dataset 18) being trained for epoch 3 by 2019-01-26 07:15:01.371624!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0886
2. set (Dataset 15) being trained for epoch 3 by 2019-01-26 07:15:23.230052!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0706
3. set (Dataset 16) being trained for epoch 3 by 2019-01-26 07:15:48.551778!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0669
4. set (Dataset 24) being trained for epoch 3 by 2019-01-26 07:16:16.025434!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0543
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 07:16:33.498287!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0648
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 07:16:53.601546!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0663
7. set (Dataset 22) being trained for epoch 3 by 2019-01-26 07:17:18.920329!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0533
8. set (Dataset 21) being trained for epoch 3 by 2019-01-26 07:17:41.665276!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1080
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 07:18:03.540026!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0990
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 07:18:25.941365!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0785
11. set (Dataset 1) being trained for epoch 3 by 2019-01-26 07:18:50.556563!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0874
12. set (Dataset 17) being trained for epoch 3 by 2019-01-26 07:19:06.922002!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0679
13. set (Dataset 19) being trained for epoch 3 by 2019-01-26 07:19:22.126006!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0723
14. set (Dataset 4) being trained for epoch 3 by 2019-01-26 07:19:41.758889!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0858
15. set (Dataset 11) being trained for epoch 3 by 2019-01-26 07:20:05.989063!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0570
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 07:20:25.846769!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0663
17. set (Dataset 6) being trained for epoch 3 by 2019-01-26 07:20:44.899385!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0956
18. set (Dataset 2) being trained for epoch 3 by 2019-01-26 07:21:03.355762!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0780
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 07:21:23.836741!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0733
20. set (Dataset 10) being trained for epoch 3 by 2019-01-26 07:21:49.945170!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0710
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:22:12.219164
1. set (Dataset 2) being trained for epoch 4 by 2019-01-26 07:22:17.293133!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0762
2. set (Dataset 10) being trained for epoch 4 by 2019-01-26 07:22:37.266400!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0673
3. set (Dataset 1) being trained for epoch 4 by 2019-01-26 07:23:00.319661!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0842
4. set (Dataset 11) being trained for epoch 4 by 2019-01-26 07:23:18.680572!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0564
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 07:23:40.720390!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0616
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 07:24:06.668910!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0736
7. set (Dataset 6) being trained for epoch 4 by 2019-01-26 07:24:30.513360!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0989
8. set (Dataset 17) being trained for epoch 4 by 2019-01-26 07:24:47.466546!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0713
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 07:25:02.287289!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0603
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 07:25:19.716311!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1027
11. set (Dataset 22) being trained for epoch 4 by 2019-01-26 07:25:40.561218!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0522
12. set (Dataset 18) being trained for epoch 4 by 2019-01-26 07:26:02.998499!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0895
13. set (Dataset 4) being trained for epoch 4 by 2019-01-26 07:26:26.148324!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0899
14. set (Dataset 15) being trained for epoch 4 by 2019-01-26 07:26:51.458968!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0734
15. set (Dataset 16) being trained for epoch 4 by 2019-01-26 07:27:16.972203!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0682
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 07:27:45.717291!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0615
17. set (Dataset 19) being trained for epoch 4 by 2019-01-26 07:28:04.728431!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0704
18. set (Dataset 24) being trained for epoch 4 by 2019-01-26 07:28:22.194574!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0551
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 07:28:42.069694!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0771
20. set (Dataset 21) being trained for epoch 4 by 2019-01-26 07:29:07.733035!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1047
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:29:28.251958
1. set (Dataset 24) being trained for epoch 5 by 2019-01-26 07:29:32.928084!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0554
2. set (Dataset 21) being trained for epoch 5 by 2019-01-26 07:29:51.614716!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1062
3. set (Dataset 22) being trained for epoch 5 by 2019-01-26 07:30:14.432678!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0513
4. set (Dataset 16) being trained for epoch 5 by 2019-01-26 07:30:40.616721!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0674
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 07:31:11.588170!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0747
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 07:31:37.580869!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0733
7. set (Dataset 19) being trained for epoch 5 by 2019-01-26 07:32:02.306500!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0728
8. set (Dataset 18) being trained for epoch 5 by 2019-01-26 07:32:21.431948!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0893
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 07:32:44.180337!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0655
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 07:33:07.663397!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0593
11. set (Dataset 6) being trained for epoch 5 by 2019-01-26 07:33:25.060136!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0975
12. set (Dataset 2) being trained for epoch 5 by 2019-01-26 07:33:43.535147!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0791
13. set (Dataset 15) being trained for epoch 5 by 2019-01-26 07:34:02.995378!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0715
14. set (Dataset 10) being trained for epoch 5 by 2019-01-26 07:34:26.540607!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0724
15. set (Dataset 1) being trained for epoch 5 by 2019-01-26 07:34:50.182225!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0843
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 07:35:08.172488!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0666
17. set (Dataset 4) being trained for epoch 5 by 2019-01-26 07:35:29.077850!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0831
18. set (Dataset 11) being trained for epoch 5 by 2019-01-26 07:35:53.621620!
Epoch 1/1
572/572 [==============================] - 15s 27ms/step - loss: 0.0553
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 07:36:14.354175!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0998
20. set (Dataset 17) being trained for epoch 5 by 2019-01-26 07:36:32.529632!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0692
Epoch 5 completed!
The subjects are trained: [(24, 'M14'), (21, 'F02'), (22, 'M01'), (16, 'M09'), (7, 'M01'), (8, 'M02'), (19,
'M11'), (18, 'F05'), (12, 'M06'), (13, 'M07'), (6, 'F06'), (2, 'F02'), (15, 'F03'), (10, 'M04'), (1, 'F01'),
 (20, 'M12'), (4, 'F04'), (11, 'M05'), (23, 'M13'), (17, 'M10')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 07:36:44.945946
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.64 Degree
        The absolute mean error on Yaw angle estimation: 29.55 Degree
        The absolute mean error on Roll angle estimation: 13.68 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.39 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 3.36 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 28.40 Degree
        The absolute mean error on Yaw angle estimation: 21.91 Degree
        The absolute mean error on Roll angle estimation: 7.64 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 17.03 Degree
        The absolute mean error on Yaw angle estimation: 28.84 Degree
        The absolute mean error on Roll angle estimation: 13.32 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.62 Degree
        The absolute mean error on Yaw angle estimations: 26.75 Degree
        The absolute mean error on Roll angle estimations: 9.50 Degree
Exp2019-01-26_04-28-49_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 07:38:13.760168
1. set (Dataset 11) being trained for epoch 1 by 2019-01-26 07:38:19.476468!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0561
2. set (Dataset 17) being trained for epoch 1 by 2019-01-26 07:38:37.523606!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0695
3. set (Dataset 6) being trained for epoch 1 by 2019-01-26 07:38:52.835590!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0959
4. set (Dataset 1) being trained for epoch 1 by 2019-01-26 07:39:11.158400!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0903
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 07:39:31.974097!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0744
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 07:39:56.390712!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0982
7. set (Dataset 4) being trained for epoch 1 by 2019-01-26 07:40:18.217087!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0811
8. set (Dataset 2) being trained for epoch 1 by 2019-01-26 07:40:42.113672!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0773
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 07:41:02.341650!
Epoch 1/1
745/745 [==============================] - 20s 26ms/step - loss: 0.0762
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 07:41:29.173613!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0589
11. set (Dataset 19) being trained for epoch 1 by 2019-01-26 07:41:53.285005!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0746
12. set (Dataset 24) being trained for epoch 1 by 2019-01-26 07:42:11.113035!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0577
13. set (Dataset 10) being trained for epoch 1 by 2019-01-26 07:42:30.466367!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0719
14. set (Dataset 21) being trained for epoch 1 by 2019-01-26 07:42:55.009466!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1089
15. set (Dataset 22) being trained for epoch 1 by 2019-01-26 07:43:17.347919!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0533
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 07:43:39.598660!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0624
17. set (Dataset 15) being trained for epoch 1 by 2019-01-26 07:43:59.641668!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0724
18. set (Dataset 16) being trained for epoch 1 by 2019-01-26 07:44:25.196358!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0674
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 07:44:52.843080!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0631
20. set (Dataset 18) being trained for epoch 1 by 2019-01-26 07:45:10.985443!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0898
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:45:30.832964
1. set (Dataset 16) being trained for epoch 2 by 2019-01-26 07:45:39.560001!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0646
2. set (Dataset 18) being trained for epoch 2 by 2019-01-26 07:46:08.604152!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0807
3. set (Dataset 19) being trained for epoch 2 by 2019-01-26 07:46:29.011900!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0727
4. set (Dataset 22) being trained for epoch 2 by 2019-01-26 07:46:47.904968!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0527
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 07:47:09.606779!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0971
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 07:47:29.341320!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0629
7. set (Dataset 15) being trained for epoch 2 by 2019-01-26 07:47:48.141713!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0702
8. set (Dataset 24) being trained for epoch 2 by 2019-01-26 07:48:09.096391!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0521
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 07:48:29.059040!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0758
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 07:48:56.520902!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0764
11. set (Dataset 4) being trained for epoch 2 by 2019-01-26 07:49:22.946224!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0811
12. set (Dataset 11) being trained for epoch 2 by 2019-01-26 07:49:47.460491!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0570
13. set (Dataset 21) being trained for epoch 2 by 2019-01-26 07:50:08.449487!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1066
14. set (Dataset 17) being trained for epoch 2 by 2019-01-26 07:50:28.385538!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0733
15. set (Dataset 6) being trained for epoch 2 by 2019-01-26 07:50:43.172187!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0943
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 07:51:02.167556!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0633
17. set (Dataset 10) being trained for epoch 2 by 2019-01-26 07:51:23.498855!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0732
18. set (Dataset 1) being trained for epoch 2 by 2019-01-26 07:51:47.190611!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0886
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 07:52:07.465784!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0598
20. set (Dataset 2) being trained for epoch 2 by 2019-01-26 07:52:30.802860!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0787
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:52:48.021722
1. set (Dataset 1) being trained for epoch 3 by 2019-01-26 07:52:53.059088!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0825
2. set (Dataset 2) being trained for epoch 3 by 2019-01-26 07:53:10.749161!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0724
3. set (Dataset 4) being trained for epoch 3 by 2019-01-26 07:53:30.833814!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0806
4. set (Dataset 6) being trained for epoch 3 by 2019-01-26 07:53:54.925029!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0998
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 07:54:13.271986!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0606
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 07:54:32.533976!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0608
7. set (Dataset 10) being trained for epoch 3 by 2019-01-26 07:54:57.974745!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0662
8. set (Dataset 11) being trained for epoch 3 by 2019-01-26 07:55:22.385488!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0553
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 07:55:42.096824!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0993
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 07:56:04.567371!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0742
11. set (Dataset 15) being trained for epoch 3 by 2019-01-26 07:56:30.913814!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0771
12. set (Dataset 16) being trained for epoch 3 by 2019-01-26 07:56:56.403145!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0664
13. set (Dataset 17) being trained for epoch 3 by 2019-01-26 07:57:23.254364!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0673
14. set (Dataset 18) being trained for epoch 3 by 2019-01-26 07:57:39.040620!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0873
15. set (Dataset 19) being trained for epoch 3 by 2019-01-26 07:57:59.178552!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0703
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 07:58:17.738052!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0592
17. set (Dataset 21) being trained for epoch 3 by 2019-01-26 07:58:37.579680!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1058
18. set (Dataset 22) being trained for epoch 3 by 2019-01-26 07:59:00.104717!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0504
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 07:59:24.393397!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0780
20. set (Dataset 24) being trained for epoch 3 by 2019-01-26 07:59:47.713737!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0556
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 08:00:04.571187
1. set (Dataset 22) being trained for epoch 4 by 2019-01-26 08:00:10.960619!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0514
2. set (Dataset 24) being trained for epoch 4 by 2019-01-26 08:00:32.805237!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0534
3. set (Dataset 15) being trained for epoch 4 by 2019-01-26 08:00:51.459594!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0702
4. set (Dataset 19) being trained for epoch 4 by 2019-01-26 08:01:12.680806!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0655
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 08:01:32.813718!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0674
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 08:01:58.771406!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0740
7. set (Dataset 21) being trained for epoch 4 by 2019-01-26 08:02:24.272413!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1082
8. set (Dataset 16) being trained for epoch 4 by 2019-01-26 08:02:48.851880!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0675
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 08:03:16.820251!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0616
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 08:03:34.864238!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0964
11. set (Dataset 10) being trained for epoch 4 by 2019-01-26 08:03:56.708783!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0724
12. set (Dataset 1) being trained for epoch 4 by 2019-01-26 08:04:20.506022!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0853
13. set (Dataset 18) being trained for epoch 4 by 2019-01-26 08:04:39.530026!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0905
14. set (Dataset 2) being trained for epoch 4 by 2019-01-26 08:05:00.047339!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0752
15. set (Dataset 4) being trained for epoch 4 by 2019-01-26 08:05:20.299933!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0832
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 08:05:44.483862!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0653
17. set (Dataset 17) being trained for epoch 4 by 2019-01-26 08:06:02.229564!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0684
18. set (Dataset 6) being trained for epoch 4 by 2019-01-26 08:06:17.654992!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0950
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 08:06:39.338423!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0739
20. set (Dataset 11) being trained for epoch 4 by 2019-01-26 08:07:04.665108!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0528
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 08:07:24.032018
1. set (Dataset 6) being trained for epoch 5 by 2019-01-26 08:07:29.216261!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0919
2. set (Dataset 11) being trained for epoch 5 by 2019-01-26 08:07:48.782211!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0525
3. set (Dataset 10) being trained for epoch 5 by 2019-01-26 08:08:10.759908!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0692
4. set (Dataset 4) being trained for epoch 5 by 2019-01-26 08:08:36.859104!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0805
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 08:09:03.540836!
Epoch 1/1
745/745 [==============================] - 20s 26ms/step - loss: 0.0748
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 08:09:30.970720!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0723
7. set (Dataset 17) being trained for epoch 5 by 2019-01-26 08:09:54.585840!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0682
8. set (Dataset 1) being trained for epoch 5 by 2019-01-26 08:10:09.464996!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0863
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 08:10:29.073630!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0595
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 08:10:52.447912!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0590
11. set (Dataset 21) being trained for epoch 5 by 2019-01-26 08:11:10.613418!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1097
12. set (Dataset 22) being trained for epoch 5 by 2019-01-26 08:11:32.742420!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0532
13. set (Dataset 2) being trained for epoch 5 by 2019-01-26 08:11:55.222057!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0768
14. set (Dataset 24) being trained for epoch 5 by 2019-01-26 08:12:12.532791!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0564
15. set (Dataset 15) being trained for epoch 5 by 2019-01-26 08:12:31.395378!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0740
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 08:12:52.955901!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0624
17. set (Dataset 18) being trained for epoch 5 by 2019-01-26 08:13:13.236528!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0840
18. set (Dataset 19) being trained for epoch 5 by 2019-01-26 08:13:33.948401!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0686
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 08:13:51.863731!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0950
20. set (Dataset 16) being trained for epoch 5 by 2019-01-26 08:14:15.452437!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0640
Epoch 5 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (7, 'M01'), (8, 'M02'), (17, 'M
10'), (1, 'F01'), (12, 'M06'), (13, 'M07'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'),
(20, 'M12'), (18, 'F05'), (19, 'M11'), (23, 'M13'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 08:14:40.592843
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.24 Degree
        The absolute mean error on Yaw angle estimation: 23.13 Degree
        The absolute mean error on Roll angle estimation: 15.32 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 14.24 Degree
        The absolute mean error on Yaw angle estimation: 27.71 Degree
        The absolute mean error on Roll angle estimation: 3.81 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 26.10 Degree
        The absolute mean error on Yaw angle estimation: 26.98 Degree
        The absolute mean error on Roll angle estimation: 7.45 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.52 Degree
        The absolute mean error on Yaw angle estimation: 30.62 Degree
        The absolute mean error on Roll angle estimation: 13.39 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.53 Degree
        The absolute mean error on Yaw angle estimations: 27.11 Degree
        The absolute mean error on Roll angle estimations: 9.99 Degree
Exp2019-01-26_04-28-49_part6 completed!
subject3_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.343922.
subject5_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.543420.
subject9_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.741643.
subject14_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.957915.
Model Exp2019-01-26_04-28-49 has been evaluated successfully.
Model Exp2019-01-26_04-28-49 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-26_04-28-49 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.p
y", line 52, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 260, in load_model
    f = h5py.File(filepath, mode='r')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = '/home/mcicek/Datasets/Keras_Models/Exp2019-01-26_
04-28-49.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)