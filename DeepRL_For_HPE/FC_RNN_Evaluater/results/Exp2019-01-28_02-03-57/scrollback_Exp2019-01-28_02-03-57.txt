__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 2048)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              2098176
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2
019-01-26_04-25-33
All frames and annotations from 1 datasets have been read by 2019-01-26 04:25:34.774293
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:25:43.666742!
Epoch 1/1
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, record = Fals
e)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_input = prepr
ocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 48, i
n trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 39, i
n trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_outputs, batch_
size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 29, i
n trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_generator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1877, in train_on_batch
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1476, in _standardize_user_da
ta
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 123, in _standardize_input_da
ta
    str(data_shape))
ValueError: Error when checking input: expected tdCNN_input to have shape (1, 299, 299, 3) but got array with shape (1,
224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:28:48.573417: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:28:48.670272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-26 04:28:48.670536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:28:48.670551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-26 04:28:48.826268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-26 04:28:48.826296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:28:48.826305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:28:48.826451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-28-49 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 04:28:54.047867
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 04:29:00.442153!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.2228
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 04:29:24.008025!
Epoch 1/1
492/492 [==============================] - 13s 25ms/step - loss: 0.1842
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 04:29:42.960016!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.2238
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 04:30:04.142948!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1564
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 04:30:24.700792!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.2204
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 04:30:50.176104!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.2073
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 04:31:10.019732!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.2010
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 04:31:35.182952!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1556
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 04:32:05.959504!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.2257
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 04:32:32.248763!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.1805
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 04:32:58.596110!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1888
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 04:33:21.691104!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.2175
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 04:33:40.065292!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.2058
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 04:34:00.696278!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.2369
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 04:34:21.055728!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.2100
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 04:34:45.127164!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.1549
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 04:35:02.980311!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.1113
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 04:35:17.873040!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1806
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 04:35:35.946084!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.1475
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 04:35:53.898483!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.1368
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:36:12.687372
1. set (Dataset 6) being trained for epoch 2 by 2019-01-26 04:36:17.863106!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1746
2. set (Dataset 11) being trained for epoch 2 by 2019-01-26 04:36:36.954885!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.1286
3. set (Dataset 10) being trained for epoch 2 by 2019-01-26 04:36:58.816780!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1656
4. set (Dataset 4) being trained for epoch 2 by 2019-01-26 04:37:24.215415!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.1856
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 04:37:48.097318!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1965
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 04:38:07.103853!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.1225
7. set (Dataset 17) being trained for epoch 2 by 2019-01-26 04:38:23.311534!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.1133
8. set (Dataset 1) being trained for epoch 2 by 2019-01-26 04:38:38.016834!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1979
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 04:38:58.596935!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1561
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 04:39:25.509463!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.1581
11. set (Dataset 21) being trained for epoch 2 by 2019-01-26 04:39:50.068361!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1873
12. set (Dataset 22) being trained for epoch 2 by 2019-01-26 04:40:12.266179!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.1034
13. set (Dataset 2) being trained for epoch 2 by 2019-01-26 04:40:34.357408!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.2106
14. set (Dataset 24) being trained for epoch 2 by 2019-01-26 04:40:51.781517!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.1173
15. set (Dataset 15) being trained for epoch 2 by 2019-01-26 04:41:10.889728!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.1358
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 04:41:33.479932!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.1172
17. set (Dataset 18) being trained for epoch 2 by 2019-01-26 04:41:53.051489!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1529
18. set (Dataset 19) being trained for epoch 2 by 2019-01-26 04:42:13.228258!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1125
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 04:42:33.319423!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.1461
20. set (Dataset 16) being trained for epoch 2 by 2019-01-26 04:43:00.063280!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1159
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:43:27.577297
1. set (Dataset 19) being trained for epoch 3 by 2019-01-26 04:43:32.450320!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.1012
2. set (Dataset 16) being trained for epoch 3 by 2019-01-26 04:43:53.600948!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1014
3. set (Dataset 21) being trained for epoch 3 by 2019-01-26 04:44:22.243162!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1541
4. set (Dataset 15) being trained for epoch 3 by 2019-01-26 04:44:44.480331!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.1154
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 04:45:05.510978!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.1072
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 04:45:25.607692!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.1200
7. set (Dataset 18) being trained for epoch 3 by 2019-01-26 04:45:49.765595!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1377
8. set (Dataset 22) being trained for epoch 3 by 2019-01-26 04:46:11.389184!
Epoch 1/1
665/665 [==============================] - 18s 26ms/step - loss: 0.0811
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 04:46:34.424081!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1434
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 04:46:56.362502!
Epoch 1/1
772/772 [==============================] - 21s 27ms/step - loss: 0.1282
11. set (Dataset 17) being trained for epoch 3 by 2019-01-26 04:47:20.757338!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0972
12. set (Dataset 6) being trained for epoch 3 by 2019-01-26 04:47:35.794624!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1520
13. set (Dataset 24) being trained for epoch 3 by 2019-01-26 04:47:53.916679!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0802
14. set (Dataset 11) being trained for epoch 3 by 2019-01-26 04:48:11.541992!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.1024
15. set (Dataset 10) being trained for epoch 3 by 2019-01-26 04:48:33.061723!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1253
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 04:48:56.679181!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0939
17. set (Dataset 2) being trained for epoch 3 by 2019-01-26 04:49:15.698703!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1578
18. set (Dataset 4) being trained for epoch 3 by 2019-01-26 04:49:35.819931!
Epoch 1/1
744/744 [==============================] - 20s 26ms/step - loss: 0.1303
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 04:50:03.140728!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.1213
20. set (Dataset 1) being trained for epoch 3 by 2019-01-26 04:50:26.528219!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1554
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:50:43.835969
1. set (Dataset 4) being trained for epoch 4 by 2019-01-26 04:50:51.196768!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.1126
2. set (Dataset 1) being trained for epoch 4 by 2019-01-26 04:51:15.310824!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1445
3. set (Dataset 17) being trained for epoch 4 by 2019-01-26 04:51:31.610824!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0977
4. set (Dataset 10) being trained for epoch 4 by 2019-01-26 04:51:49.216550!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1057
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 04:52:14.767318!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0974
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 04:52:40.808372!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1095
7. set (Dataset 2) being trained for epoch 4 by 2019-01-26 04:53:04.804114!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1155
8. set (Dataset 6) being trained for epoch 4 by 2019-01-26 04:53:22.774811!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1431
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 04:53:41.136011!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0822
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 04:53:59.151675!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1345
11. set (Dataset 18) being trained for epoch 4 by 2019-01-26 04:54:20.049230!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1298
12. set (Dataset 19) being trained for epoch 4 by 2019-01-26 04:54:40.200105!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1119
13. set (Dataset 11) being trained for epoch 4 by 2019-01-26 04:54:58.739402!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0816
14. set (Dataset 16) being trained for epoch 4 by 2019-01-26 04:55:21.805977!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0948
15. set (Dataset 21) being trained for epoch 4 by 2019-01-26 04:55:50.532712!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1401
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 04:56:11.941354!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0887
17. set (Dataset 24) being trained for epoch 4 by 2019-01-26 04:56:30.814452!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0753
18. set (Dataset 15) being trained for epoch 4 by 2019-01-26 04:56:49.660685!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0995
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 04:57:14.014141!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1109
20. set (Dataset 22) being trained for epoch 4 by 2019-01-26 04:57:39.561226!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0713
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:58:01.056796
1. set (Dataset 15) being trained for epoch 5 by 2019-01-26 04:58:07.388715!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0923
2. set (Dataset 22) being trained for epoch 5 by 2019-01-26 04:58:30.759055!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0691
3. set (Dataset 18) being trained for epoch 5 by 2019-01-26 04:58:53.033295!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.1152
4. set (Dataset 21) being trained for epoch 5 by 2019-01-26 04:59:14.942068!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1315
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 04:59:38.376003!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1039
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 05:00:04.655816!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1010
7. set (Dataset 24) being trained for epoch 5 by 2019-01-26 05:00:28.700652!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0723
8. set (Dataset 19) being trained for epoch 5 by 2019-01-26 05:00:46.114431!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0957
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 05:01:05.923313!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0936
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 05:01:29.344750!
Epoch 1/1
485/485 [==============================] - 13s 27ms/step - loss: 0.0817
11. set (Dataset 2) being trained for epoch 5 by 2019-01-26 05:01:47.372845!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.1111
12. set (Dataset 4) being trained for epoch 5 by 2019-01-26 05:02:08.040595!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.1129
13. set (Dataset 16) being trained for epoch 5 by 2019-01-26 05:02:36.247872!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0953
14. set (Dataset 1) being trained for epoch 5 by 2019-01-26 05:03:04.829364!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1278
15. set (Dataset 17) being trained for epoch 5 by 2019-01-26 05:03:20.865410!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0883
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 05:03:36.409832!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0873
17. set (Dataset 11) being trained for epoch 5 by 2019-01-26 05:03:55.920227!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0749
18. set (Dataset 10) being trained for epoch 5 by 2019-01-26 05:04:17.962527!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.1015
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 05:04:42.096181!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1320
20. set (Dataset 6) being trained for epoch 5 by 2019-01-26 05:05:01.590447!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1228
Epoch 5 completed!
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24, 'M14'), (19,
 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'), (20, 'M12'), (11, 'M05'
), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 05:05:16.893796
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 36.25 Degree
        The absolute mean error on Yaw angle estimation: 38.61 Degree
        The absolute mean error on Roll angle estimation: 9.42 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.63 Degree
        The absolute mean error on Yaw angle estimation: 30.99 Degree
        The absolute mean error on Roll angle estimation: 3.44 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 19.30 Degree
        The absolute mean error on Yaw angle estimation: 29.05 Degree
        The absolute mean error on Roll angle estimation: 7.98 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 12.10 Degree
        The absolute mean error on Yaw angle estimation: 33.15 Degree
        The absolute mean error on Roll angle estimation: 13.42 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.82 Degree
        The absolute mean error on Yaw angle estimations: 32.95 Degree
        The absolute mean error on Roll angle estimations: 8.56 Degree
Exp2019-01-26_04-28-49_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 05:06:46.281809
1. set (Dataset 10) being trained for epoch 1 by 2019-01-26 05:06:53.487837!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0907
2. set (Dataset 6) being trained for epoch 1 by 2019-01-26 05:07:16.836326!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1160
3. set (Dataset 2) being trained for epoch 1 by 2019-01-26 05:07:35.576914!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1048
4. set (Dataset 17) being trained for epoch 1 by 2019-01-26 05:07:52.301772!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0842
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 05:08:09.869102!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0984
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 05:08:34.489780!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1229
7. set (Dataset 11) being trained for epoch 1 by 2019-01-26 05:08:54.552133!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0702
8. set (Dataset 4) being trained for epoch 1 by 2019-01-26 05:09:16.792885!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1058
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 05:09:43.094568!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0948
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 05:10:09.666108!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0862
11. set (Dataset 24) being trained for epoch 1 by 2019-01-26 05:10:33.400735!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0744
12. set (Dataset 15) being trained for epoch 1 by 2019-01-26 05:10:51.983250!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0928
13. set (Dataset 1) being trained for epoch 1 by 2019-01-26 05:11:13.412373!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1173
14. set (Dataset 22) being trained for epoch 1 by 2019-01-26 05:11:32.154269!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0708
15. set (Dataset 18) being trained for epoch 1 by 2019-01-26 05:11:54.848362!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1170
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 05:12:15.664876!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0814
17. set (Dataset 16) being trained for epoch 1 by 2019-01-26 05:12:38.861152!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0840
18. set (Dataset 21) being trained for epoch 1 by 2019-01-26 05:13:07.396020!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1312
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 05:13:28.481224!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0840
20. set (Dataset 19) being trained for epoch 1 by 2019-01-26 05:13:45.686781!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0903
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:14:02.883146
1. set (Dataset 21) being trained for epoch 2 by 2019-01-26 05:14:08.885982!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1232
2. set (Dataset 19) being trained for epoch 2 by 2019-01-26 05:14:29.884001!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0862
3. set (Dataset 24) being trained for epoch 2 by 2019-01-26 05:14:46.810803!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0670
4. set (Dataset 18) being trained for epoch 2 by 2019-01-26 05:15:05.137386!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.1030
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 05:15:26.632837!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1153
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 05:15:46.201917!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0798
7. set (Dataset 16) being trained for epoch 2 by 2019-01-26 05:16:07.618261!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0812
8. set (Dataset 15) being trained for epoch 2 by 2019-01-26 05:16:36.542357!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0878
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 05:17:00.648825!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0965
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 05:17:28.041790!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0963
11. set (Dataset 11) being trained for epoch 2 by 2019-01-26 05:17:52.853509!
Epoch 1/1
572/572 [==============================] - 15s 27ms/step - loss: 0.0673
12. set (Dataset 10) being trained for epoch 2 by 2019-01-26 05:18:15.448685!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0876
13. set (Dataset 22) being trained for epoch 2 by 2019-01-26 05:18:40.700612!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0681
14. set (Dataset 6) being trained for epoch 2 by 2019-01-26 05:19:03.085189!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1190
15. set (Dataset 2) being trained for epoch 2 by 2019-01-26 05:19:22.274887!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1067
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 05:19:40.273199!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0803
17. set (Dataset 1) being trained for epoch 2 by 2019-01-26 05:19:59.083827!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1123
18. set (Dataset 17) being trained for epoch 2 by 2019-01-26 05:20:15.080463!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0803
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 05:20:32.538847!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0813
20. set (Dataset 4) being trained for epoch 2 by 2019-01-26 05:20:58.154980!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1083
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:21:21.422255
1. set (Dataset 17) being trained for epoch 3 by 2019-01-26 05:21:25.174028!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0803
2. set (Dataset 4) being trained for epoch 3 by 2019-01-26 05:21:42.522719!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0993
3. set (Dataset 11) being trained for epoch 3 by 2019-01-26 05:22:07.585812!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0697
4. set (Dataset 2) being trained for epoch 3 by 2019-01-26 05:22:27.138369!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1028
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 05:22:45.045150!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0759
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 05:23:04.475726!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0787
7. set (Dataset 1) being trained for epoch 3 by 2019-01-26 05:23:28.167714!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1104
8. set (Dataset 10) being trained for epoch 3 by 2019-01-26 05:23:48.192718!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0830
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 05:24:12.006273!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1211
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 05:24:33.827329!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0915
11. set (Dataset 16) being trained for epoch 3 by 2019-01-26 05:25:01.870736!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0853
12. set (Dataset 21) being trained for epoch 3 by 2019-01-26 05:25:31.176226!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1280
13. set (Dataset 6) being trained for epoch 3 by 2019-01-26 05:25:52.561853!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1125
14. set (Dataset 19) being trained for epoch 3 by 2019-01-26 05:26:10.902529!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0869
15. set (Dataset 24) being trained for epoch 3 by 2019-01-26 05:26:28.624758!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0641
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 05:26:46.896239!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0737
17. set (Dataset 22) being trained for epoch 3 by 2019-01-26 05:27:07.205811!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0646
18. set (Dataset 18) being trained for epoch 3 by 2019-01-26 05:27:29.612889!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.1046
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 05:27:52.836162!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0943
20. set (Dataset 15) being trained for epoch 3 by 2019-01-26 05:28:18.574130!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0844
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:28:39.201414
1. set (Dataset 18) being trained for epoch 4 by 2019-01-26 05:28:45.075009!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1008
2. set (Dataset 15) being trained for epoch 4 by 2019-01-26 05:29:06.895639!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0819
3. set (Dataset 16) being trained for epoch 4 by 2019-01-26 05:29:31.959484!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0797
4. set (Dataset 24) being trained for epoch 4 by 2019-01-26 05:29:59.834317!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0624
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 05:30:19.358131!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0809
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 05:30:45.142863!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0931
7. set (Dataset 22) being trained for epoch 4 by 2019-01-26 05:31:10.120943!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0610
8. set (Dataset 21) being trained for epoch 4 by 2019-01-26 05:31:33.125414!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1230
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 05:31:53.959870!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0791
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 05:32:11.372467!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1143
11. set (Dataset 1) being trained for epoch 4 by 2019-01-26 05:32:30.954705!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1090
12. set (Dataset 17) being trained for epoch 4 by 2019-01-26 05:32:47.624325!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0776
13. set (Dataset 19) being trained for epoch 4 by 2019-01-26 05:33:02.574953!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0810
14. set (Dataset 4) being trained for epoch 4 by 2019-01-26 05:33:23.114068!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1004
15. set (Dataset 11) being trained for epoch 4 by 2019-01-26 05:33:47.776908!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0676
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 05:34:07.405923!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0769
17. set (Dataset 6) being trained for epoch 4 by 2019-01-26 05:34:26.611031!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1106
18. set (Dataset 2) being trained for epoch 4 by 2019-01-26 05:34:45.181404!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1011
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 05:35:05.598144!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0899
20. set (Dataset 10) being trained for epoch 4 by 2019-01-26 05:35:32.158380!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0848
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:35:55.067866
1. set (Dataset 2) being trained for epoch 5 by 2019-01-26 05:36:00.146684!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0933
2. set (Dataset 10) being trained for epoch 5 by 2019-01-26 05:36:20.274787!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0827
3. set (Dataset 1) being trained for epoch 5 by 2019-01-26 05:36:44.417882!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0984
4. set (Dataset 11) being trained for epoch 5 by 2019-01-26 05:37:02.549156!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0696
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 05:37:24.203492!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0904
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 05:37:50.393732!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0840
7. set (Dataset 6) being trained for epoch 5 by 2019-01-26 05:38:15.485552!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1144
8. set (Dataset 17) being trained for epoch 5 by 2019-01-26 05:38:33.326186!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0759
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 05:38:50.940982!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0752
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 05:39:14.606836!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0728
11. set (Dataset 22) being trained for epoch 5 by 2019-01-26 05:39:33.270108!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0625
12. set (Dataset 18) being trained for epoch 5 by 2019-01-26 05:39:56.128320!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1029
13. set (Dataset 4) being trained for epoch 5 by 2019-01-26 05:40:18.692437!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0972
14. set (Dataset 15) being trained for epoch 5 by 2019-01-26 05:40:43.591286!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0859
15. set (Dataset 16) being trained for epoch 5 by 2019-01-26 05:41:08.869091!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0798
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 05:41:36.923325!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0750
17. set (Dataset 19) being trained for epoch 5 by 2019-01-26 05:41:55.575194!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0813
18. set (Dataset 24) being trained for epoch 5 by 2019-01-26 05:42:12.656292!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0597
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 05:42:30.447787!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1114
20. set (Dataset 21) being trained for epoch 5 by 2019-01-26 05:42:51.455067!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1217
Epoch 5 completed!
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F06'), (17, 'M
10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'), (20, 'M12'), (19, 'M11')
, (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 05:43:09.288916
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.89 Degree
        The absolute mean error on Yaw angle estimation: 27.86 Degree
        The absolute mean error on Roll angle estimation: 24.26 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.54 Degree
        The absolute mean error on Yaw angle estimation: 28.96 Degree
        The absolute mean error on Roll angle estimation: 5.38 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 25.41 Degree
        The absolute mean error on Yaw angle estimation: 24.68 Degree
        The absolute mean error on Roll angle estimation: 10.63 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.51 Degree
        The absolute mean error on Yaw angle estimation: 28.15 Degree
        The absolute mean error on Roll angle estimation: 14.35 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.09 Degree
        The absolute mean error on Yaw angle estimations: 27.41 Degree
        The absolute mean error on Roll angle estimations: 13.65 Degree
Exp2019-01-26_04-28-49_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 05:44:38.355710
1. set (Dataset 24) being trained for epoch 1 by 2019-01-26 05:44:43.027356!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0613
2. set (Dataset 21) being trained for epoch 1 by 2019-01-26 05:45:01.290534!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1124
3. set (Dataset 22) being trained for epoch 1 by 2019-01-26 05:45:23.491057!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0607
4. set (Dataset 16) being trained for epoch 1 by 2019-01-26 05:45:49.049931!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0774
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 05:46:20.013710!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0886
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 05:46:44.627027!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1069
7. set (Dataset 19) being trained for epoch 1 by 2019-01-26 05:47:03.763761!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0823
8. set (Dataset 18) being trained for epoch 1 by 2019-01-26 05:47:22.257770!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0986
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 05:47:45.462492!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0871
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 05:48:11.846161!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0799
11. set (Dataset 6) being trained for epoch 1 by 2019-01-26 05:48:35.172784!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1044
12. set (Dataset 2) being trained for epoch 1 by 2019-01-26 05:48:53.755356!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0928
13. set (Dataset 15) being trained for epoch 1 by 2019-01-26 05:49:13.371753!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0816
14. set (Dataset 10) being trained for epoch 1 by 2019-01-26 05:49:37.606429!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0832
15. set (Dataset 1) being trained for epoch 1 by 2019-01-26 05:50:01.667421!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0957
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 05:50:19.472535!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0730
17. set (Dataset 4) being trained for epoch 1 by 2019-01-26 05:50:40.668341!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0950
18. set (Dataset 11) being trained for epoch 1 by 2019-01-26 05:51:05.356600!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0679
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 05:51:24.961216!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0706
20. set (Dataset 17) being trained for epoch 1 by 2019-01-26 05:51:40.950399!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0731
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:51:55.270329
1. set (Dataset 11) being trained for epoch 2 by 2019-01-26 05:52:00.964363!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0679
2. set (Dataset 17) being trained for epoch 2 by 2019-01-26 05:52:19.577005!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0738
3. set (Dataset 6) being trained for epoch 2 by 2019-01-26 05:52:34.612518!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1066
4. set (Dataset 1) being trained for epoch 2 by 2019-01-26 05:52:53.501477!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0987
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 05:53:11.624896!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1073
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 05:53:31.478954!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0677
7. set (Dataset 4) being trained for epoch 2 by 2019-01-26 05:53:51.126394!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0920
8. set (Dataset 2) being trained for epoch 2 by 2019-01-26 05:54:14.841847!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0914
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 05:54:35.500334!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0862
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 05:55:02.991929!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0808
11. set (Dataset 19) being trained for epoch 2 by 2019-01-26 05:55:26.978931!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0841
12. set (Dataset 24) being trained for epoch 2 by 2019-01-26 05:55:44.149627!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0633
13. set (Dataset 10) being trained for epoch 2 by 2019-01-26 05:56:03.493398!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0799
14. set (Dataset 21) being trained for epoch 2 by 2019-01-26 05:56:27.968519!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1205
15. set (Dataset 22) being trained for epoch 2 by 2019-01-26 05:56:51.068281!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0598
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 05:57:12.749650!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0727
17. set (Dataset 15) being trained for epoch 2 by 2019-01-26 05:57:33.092315!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0796
18. set (Dataset 16) being trained for epoch 2 by 2019-01-26 05:57:58.513862!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0790
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 05:58:28.795697!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0750
20. set (Dataset 18) being trained for epoch 2 by 2019-01-26 05:58:53.436519!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0988
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:59:13.268392
1. set (Dataset 16) being trained for epoch 3 by 2019-01-26 05:59:21.989897!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0765
2. set (Dataset 18) being trained for epoch 3 by 2019-01-26 05:59:51.115318!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0931
3. set (Dataset 19) being trained for epoch 3 by 2019-01-26 06:00:11.396071!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0789
4. set (Dataset 22) being trained for epoch 3 by 2019-01-26 06:00:30.249512!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0594
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 06:00:52.433249!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0705
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 06:01:12.092859!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0724
7. set (Dataset 15) being trained for epoch 3 by 2019-01-26 06:01:37.516640!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0788
8. set (Dataset 24) being trained for epoch 3 by 2019-01-26 06:01:59.446643!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0591
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 06:02:17.336046!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1073
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 06:02:39.534748!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0852
11. set (Dataset 4) being trained for epoch 3 by 2019-01-26 06:03:06.604413!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.0903
12. set (Dataset 11) being trained for epoch 3 by 2019-01-26 06:03:30.568696!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0637
13. set (Dataset 21) being trained for epoch 3 by 2019-01-26 06:03:51.391728!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1173
14. set (Dataset 17) being trained for epoch 3 by 2019-01-26 06:04:11.049850!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0746
15. set (Dataset 6) being trained for epoch 3 by 2019-01-26 06:04:26.327237!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1021
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 06:04:45.320249!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0706
17. set (Dataset 10) being trained for epoch 3 by 2019-01-26 06:05:06.418178!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0793
18. set (Dataset 1) being trained for epoch 3 by 2019-01-26 06:05:30.634262!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0940
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 06:05:50.727014!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0824
20. set (Dataset 2) being trained for epoch 3 by 2019-01-26 06:06:14.500544!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0957
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:06:31.283130
1. set (Dataset 1) being trained for epoch 4 by 2019-01-26 06:06:36.314993!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0910
2. set (Dataset 2) being trained for epoch 4 by 2019-01-26 06:06:54.098015!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0827
3. set (Dataset 4) being trained for epoch 4 by 2019-01-26 06:07:14.794473!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0930
4. set (Dataset 6) being trained for epoch 4 by 2019-01-26 06:07:39.018507!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1079
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 06:07:59.878546!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0704
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 06:08:25.962126!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0815
7. set (Dataset 10) being trained for epoch 4 by 2019-01-26 06:08:52.029688!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0760
8. set (Dataset 11) being trained for epoch 4 by 2019-01-26 06:09:15.735446!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0651
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 06:09:35.034384!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0710
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 06:09:52.735269!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1054
11. set (Dataset 15) being trained for epoch 4 by 2019-01-26 06:10:13.221679!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0811
12. set (Dataset 16) being trained for epoch 4 by 2019-01-26 06:10:38.337325!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0746
13. set (Dataset 17) being trained for epoch 4 by 2019-01-26 06:11:05.635006!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0725
14. set (Dataset 18) being trained for epoch 4 by 2019-01-26 06:11:21.894900!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0971
15. set (Dataset 19) being trained for epoch 4 by 2019-01-26 06:11:42.608912!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0777
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 06:11:59.194360!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0650
17. set (Dataset 21) being trained for epoch 4 by 2019-01-26 06:12:19.059406!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1165
18. set (Dataset 22) being trained for epoch 4 by 2019-01-26 06:12:41.797711!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0589
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 06:13:05.914645!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0843
20. set (Dataset 24) being trained for epoch 4 by 2019-01-26 06:13:30.460105!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0582
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:13:47.177261
1. set (Dataset 22) being trained for epoch 5 by 2019-01-26 06:13:53.572974!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0550
2. set (Dataset 24) being trained for epoch 5 by 2019-01-26 06:14:15.402954!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0565
3. set (Dataset 15) being trained for epoch 5 by 2019-01-26 06:14:34.558485!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0759
4. set (Dataset 19) being trained for epoch 5 by 2019-01-26 06:14:55.689539!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0734
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 06:15:15.817698!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0830
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 06:15:42.047288!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0817
7. set (Dataset 21) being trained for epoch 5 by 2019-01-26 06:16:07.299976!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1131
8. set (Dataset 16) being trained for epoch 5 by 2019-01-26 06:16:32.376623!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0726
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 06:17:02.938477!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0716
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 06:17:26.482968!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0684
11. set (Dataset 10) being trained for epoch 5 by 2019-01-26 06:17:46.133796!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0748
12. set (Dataset 1) being trained for epoch 5 by 2019-01-26 06:18:09.704676!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0884
13. set (Dataset 18) being trained for epoch 5 by 2019-01-26 06:18:28.488470!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.1006
14. set (Dataset 2) being trained for epoch 5 by 2019-01-26 06:18:49.848012!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0859
15. set (Dataset 4) being trained for epoch 5 by 2019-01-26 06:19:09.930339!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0874
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 06:19:34.222813!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0729
17. set (Dataset 17) being trained for epoch 5 by 2019-01-26 06:19:51.313291!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0715
18. set (Dataset 6) being trained for epoch 5 by 2019-01-26 06:20:06.918240!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1027
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 06:20:26.629349!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1072
20. set (Dataset 11) being trained for epoch 5 by 2019-01-26 06:20:46.774843!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0617
Epoch 5 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21, 'F02'), (16,
 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 06:21:03.480772
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.73 Degree
        The absolute mean error on Yaw angle estimation: 28.78 Degree
        The absolute mean error on Roll angle estimation: 15.75 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.48 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 4.81 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 27.43 Degree
        The absolute mean error on Yaw angle estimation: 21.95 Degree
        The absolute mean error on Roll angle estimation: 8.51 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.05 Degree
        The absolute mean error on Yaw angle estimation: 28.14 Degree
        The absolute mean error on Roll angle estimation: 13.34 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.67 Degree
        The absolute mean error on Yaw angle estimations: 26.39 Degree
        The absolute mean error on Roll angle estimations: 10.60 Degree
Exp2019-01-26_04-28-49_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 06:22:32.365899
1. set (Dataset 6) being trained for epoch 1 by 2019-01-26 06:22:37.549368!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1010
2. set (Dataset 11) being trained for epoch 1 by 2019-01-26 06:22:56.963269!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0593
3. set (Dataset 10) being trained for epoch 1 by 2019-01-26 06:23:18.452463!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0750
4. set (Dataset 4) being trained for epoch 1 by 2019-01-26 06:23:44.316888!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0889
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 06:24:10.461272!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0798
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 06:24:36.360582!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1076
7. set (Dataset 17) being trained for epoch 1 by 2019-01-26 06:24:54.416247!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0735
8. set (Dataset 1) being trained for epoch 1 by 2019-01-26 06:25:09.296393!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0914
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 06:25:29.823345!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0793
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 06:25:55.999272!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0657
11. set (Dataset 21) being trained for epoch 1 by 2019-01-26 06:26:20.101883!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1142
12. set (Dataset 22) being trained for epoch 1 by 2019-01-26 06:26:42.279079!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0572
13. set (Dataset 2) being trained for epoch 1 by 2019-01-26 06:27:04.056751!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0861
14. set (Dataset 24) being trained for epoch 1 by 2019-01-26 06:27:21.750660!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0624
15. set (Dataset 15) being trained for epoch 1 by 2019-01-26 06:27:40.477855!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0793
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 06:28:02.135544!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0658
17. set (Dataset 18) being trained for epoch 1 by 2019-01-26 06:28:22.189104!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0928
18. set (Dataset 19) being trained for epoch 1 by 2019-01-26 06:28:42.701004!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0768
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 06:29:00.490016!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0665
20. set (Dataset 16) being trained for epoch 1 by 2019-01-26 06:29:21.899223!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0724
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:29:48.744448
1. set (Dataset 19) being trained for epoch 2 by 2019-01-26 06:29:53.654326!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0749
2. set (Dataset 16) being trained for epoch 2 by 2019-01-26 06:30:14.667899!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0684
3. set (Dataset 21) being trained for epoch 2 by 2019-01-26 06:30:43.448925!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1090
4. set (Dataset 15) being trained for epoch 2 by 2019-01-26 06:31:06.258474!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0747
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 06:31:28.983845!
Epoch 1/1
569/569 [==============================] - 15s 25ms/step - loss: 0.0996
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 06:31:48.399290!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0662
7. set (Dataset 18) being trained for epoch 2 by 2019-01-26 06:32:06.403444!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0904
8. set (Dataset 22) being trained for epoch 2 by 2019-01-26 06:32:28.469473!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0566
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 06:32:52.688130!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0808
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 06:33:19.922978!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0769
11. set (Dataset 17) being trained for epoch 2 by 2019-01-26 06:33:42.335483!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0722
12. set (Dataset 6) being trained for epoch 2 by 2019-01-26 06:33:57.233743!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0988
13. set (Dataset 24) being trained for epoch 2 by 2019-01-26 06:34:15.173460!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0584
14. set (Dataset 11) being trained for epoch 2 by 2019-01-26 06:34:33.131784!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0605
15. set (Dataset 10) being trained for epoch 2 by 2019-01-26 06:34:54.902859!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0728
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 06:35:18.908211!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0699
17. set (Dataset 2) being trained for epoch 2 by 2019-01-26 06:35:38.348906!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0899
18. set (Dataset 4) being trained for epoch 2 by 2019-01-26 06:35:58.265034!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0887
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 06:36:24.177876!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0680
20. set (Dataset 1) being trained for epoch 2 by 2019-01-26 06:36:47.589158!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0869
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:37:04.294037
1. set (Dataset 4) being trained for epoch 3 by 2019-01-26 06:37:11.657852!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0834
2. set (Dataset 1) being trained for epoch 3 by 2019-01-26 06:37:35.699017!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0836
3. set (Dataset 17) being trained for epoch 3 by 2019-01-26 06:37:51.458126!
Epoch 1/1
395/395 [==============================] - 8s 20ms/step - loss: 0.0738
4. set (Dataset 10) being trained for epoch 3 by 2019-01-26 06:38:06.526138!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0714
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 06:38:29.685901!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0657
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 06:38:48.919626!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0645
7. set (Dataset 2) being trained for epoch 3 by 2019-01-26 06:39:12.318343!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0800
8. set (Dataset 6) being trained for epoch 3 by 2019-01-26 06:39:30.598890!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1073
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 06:39:49.771989!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1025
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 06:40:11.655101!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0776
11. set (Dataset 18) being trained for epoch 3 by 2019-01-26 06:40:36.950867!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0948
12. set (Dataset 19) being trained for epoch 3 by 2019-01-26 06:40:57.642910!
Epoch 1/1
502/502 [==============================] - 13s 27ms/step - loss: 0.0761
13. set (Dataset 11) being trained for epoch 3 by 2019-01-26 06:41:16.899161!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0585
14. set (Dataset 16) being trained for epoch 3 by 2019-01-26 06:41:40.376685!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0711
15. set (Dataset 21) being trained for epoch 3 by 2019-01-26 06:42:09.952505!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1091
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 06:42:31.562126!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0641
17. set (Dataset 24) being trained for epoch 3 by 2019-01-26 06:42:50.197140!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0564
18. set (Dataset 15) being trained for epoch 3 by 2019-01-26 06:43:09.116688!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0729
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 06:43:33.512179!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0797
20. set (Dataset 22) being trained for epoch 3 by 2019-01-26 06:43:59.101112!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0531
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:44:20.072421
1. set (Dataset 15) being trained for epoch 4 by 2019-01-26 06:44:26.430470!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0717
2. set (Dataset 22) being trained for epoch 4 by 2019-01-26 06:44:48.896138!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0540
3. set (Dataset 18) being trained for epoch 4 by 2019-01-26 06:45:11.719960!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0918
4. set (Dataset 21) being trained for epoch 4 by 2019-01-26 06:45:33.763074!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1085
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 06:45:56.786181!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0693
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 06:46:22.638520!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0779
7. set (Dataset 24) being trained for epoch 4 by 2019-01-26 06:46:46.033958!
Epoch 1/1
492/492 [==============================] - 13s 25ms/step - loss: 0.0610
8. set (Dataset 19) being trained for epoch 4 by 2019-01-26 06:47:03.546703!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0732
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 06:47:20.951917!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0656
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 06:47:39.038326!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0999
11. set (Dataset 2) being trained for epoch 4 by 2019-01-26 06:47:58.236340!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0817
12. set (Dataset 4) being trained for epoch 4 by 2019-01-26 06:48:18.467803!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0868
13. set (Dataset 16) being trained for epoch 4 by 2019-01-26 06:48:45.533716!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0732
14. set (Dataset 1) being trained for epoch 4 by 2019-01-26 06:49:13.197691!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0894
15. set (Dataset 17) being trained for epoch 4 by 2019-01-26 06:49:29.304670!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0698
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 06:49:44.041615!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0636
17. set (Dataset 11) being trained for epoch 4 by 2019-01-26 06:50:03.619269!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0585
18. set (Dataset 10) being trained for epoch 4 by 2019-01-26 06:50:25.343654!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0718
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 06:50:51.643237!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0776
20. set (Dataset 6) being trained for epoch 4 by 2019-01-26 06:51:16.848257!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1035
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:51:35.340047
1. set (Dataset 10) being trained for epoch 5 by 2019-01-26 06:51:42.554755!
Epoch 1/1
726/726 [==============================] - 19s 27ms/step - loss: 0.0709
2. set (Dataset 6) being trained for epoch 5 by 2019-01-26 06:52:07.067375!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0963
3. set (Dataset 2) being trained for epoch 5 by 2019-01-26 06:52:25.742531!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0825
4. set (Dataset 17) being trained for epoch 5 by 2019-01-26 06:52:42.232237!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0715
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 06:52:59.709024!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0732
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 06:53:25.998008!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0748
7. set (Dataset 11) being trained for epoch 5 by 2019-01-26 06:53:51.306157!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0563
8. set (Dataset 4) being trained for epoch 5 by 2019-01-26 06:54:13.107043!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0865
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 06:54:39.092066!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0657
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 06:55:02.043762!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0634
11. set (Dataset 24) being trained for epoch 5 by 2019-01-26 06:55:19.296313!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0609
12. set (Dataset 15) being trained for epoch 5 by 2019-01-26 06:55:38.686235!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0736
13. set (Dataset 1) being trained for epoch 5 by 2019-01-26 06:56:00.062741!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0869
14. set (Dataset 22) being trained for epoch 5 by 2019-01-26 06:56:19.089110!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0553
15. set (Dataset 18) being trained for epoch 5 by 2019-01-26 06:56:41.661206!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0942
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 06:57:02.345884!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0635
17. set (Dataset 16) being trained for epoch 5 by 2019-01-26 06:57:25.126876!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0706
18. set (Dataset 21) being trained for epoch 5 by 2019-01-26 06:57:54.141694!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1065
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 06:58:15.501673!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1000
20. set (Dataset 19) being trained for epoch 5 by 2019-01-26 06:58:34.975049!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0745
Epoch 5 completed!
The subjects are trained: [(10, 'M04'), (6, 'F06'), (2, 'F02'), (17, 'M10'), (7, 'M01'), (8, 'M02'), (11, 'M05'), (4, 'F
04'), (12, 'M06'), (13, 'M07'), (24, 'M14'), (15, 'F03'), (1, 'F01'), (22, 'M01'), (18, 'F05'), (20, 'M12'), (16, 'M09')
, (21, 'F02'), (23, 'M13'), (19, 'M11')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 06:58:49.971824
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.08 Degree
        The absolute mean error on Yaw angle estimation: 24.35 Degree
        The absolute mean error on Roll angle estimation: 15.10 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.11 Degree
        The absolute mean error on Yaw angle estimation: 28.24 Degree
        The absolute mean error on Roll angle estimation: 3.55 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.88 Degree
        The absolute mean error on Yaw angle estimation: 23.90 Degree
        The absolute mean error on Roll angle estimation: 7.48 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.77 Degree
        The absolute mean error on Yaw angle estimation: 29.53 Degree
        The absolute mean error on Roll angle estimation: 13.02 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.21 Degree
        The absolute mean error on Yaw angle estimations: 26.51 Degree
        The absolute mean error on Roll angle estimations: 9.79 Degree
Exp2019-01-26_04-28-49_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 07:00:18.968931
1. set (Dataset 21) being trained for epoch 1 by 2019-01-26 07:00:24.974574!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1082
2. set (Dataset 19) being trained for epoch 1 by 2019-01-26 07:00:46.229432!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0719
3. set (Dataset 24) being trained for epoch 1 by 2019-01-26 07:01:03.896012!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0556
4. set (Dataset 18) being trained for epoch 1 by 2019-01-26 07:01:22.617814!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0873
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 07:01:46.156159!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0835
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 07:02:11.205423!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0978
7. set (Dataset 16) being trained for epoch 1 by 2019-01-26 07:02:34.900086!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0700
8. set (Dataset 15) being trained for epoch 1 by 2019-01-26 07:03:03.902567!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0756
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 07:03:27.507377!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0789
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 07:03:53.028437!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.0649
11. set (Dataset 11) being trained for epoch 1 by 2019-01-26 07:04:16.677250!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0591
12. set (Dataset 10) being trained for epoch 1 by 2019-01-26 07:04:38.516222!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0718
13. set (Dataset 22) being trained for epoch 1 by 2019-01-26 07:05:02.752774!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0547
14. set (Dataset 6) being trained for epoch 1 by 2019-01-26 07:05:24.574888!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0999
15. set (Dataset 2) being trained for epoch 1 by 2019-01-26 07:05:43.835108!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0779
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 07:06:01.936891!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0661
17. set (Dataset 1) being trained for epoch 1 by 2019-01-26 07:06:21.110182!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0919
18. set (Dataset 17) being trained for epoch 1 by 2019-01-26 07:06:37.964055!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0714
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 07:06:53.286172!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0644
20. set (Dataset 4) being trained for epoch 1 by 2019-01-26 07:07:12.998053!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0859
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:07:36.282711
1. set (Dataset 17) being trained for epoch 2 by 2019-01-26 07:07:40.027415!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0696
2. set (Dataset 4) being trained for epoch 2 by 2019-01-26 07:07:57.717044!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0831
3. set (Dataset 11) being trained for epoch 2 by 2019-01-26 07:08:22.671893!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0591
4. set (Dataset 2) being trained for epoch 2 by 2019-01-26 07:08:42.743856!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0796
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 07:09:01.413696!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1011
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 07:09:20.549134!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0618
7. set (Dataset 1) being trained for epoch 2 by 2019-01-26 07:09:37.575631!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0829
8. set (Dataset 10) being trained for epoch 2 by 2019-01-26 07:09:57.288650!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0698
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 07:10:23.674479!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0749
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 07:10:50.289780!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0746
11. set (Dataset 16) being trained for epoch 2 by 2019-01-26 07:11:18.395470!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0721
12. set (Dataset 21) being trained for epoch 2 by 2019-01-26 07:11:47.806766!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1106
13. set (Dataset 6) being trained for epoch 2 by 2019-01-26 07:12:09.461733!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0963
14. set (Dataset 19) being trained for epoch 2 by 2019-01-26 07:12:27.809420!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0712
15. set (Dataset 24) being trained for epoch 2 by 2019-01-26 07:12:45.280018!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0567
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 07:13:02.983279!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0623
17. set (Dataset 22) being trained for epoch 2 by 2019-01-26 07:13:23.600502!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0539
18. set (Dataset 18) being trained for epoch 2 by 2019-01-26 07:13:46.626894!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0864
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 07:14:09.685006!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0715
20. set (Dataset 15) being trained for epoch 2 by 2019-01-26 07:14:34.227531!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0773
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:14:55.476377
1. set (Dataset 18) being trained for epoch 3 by 2019-01-26 07:15:01.371624!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0886
2. set (Dataset 15) being trained for epoch 3 by 2019-01-26 07:15:23.230052!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0706
3. set (Dataset 16) being trained for epoch 3 by 2019-01-26 07:15:48.551778!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0669
4. set (Dataset 24) being trained for epoch 3 by 2019-01-26 07:16:16.025434!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0543
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 07:16:33.498287!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0648
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 07:16:53.601546!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0663
7. set (Dataset 22) being trained for epoch 3 by 2019-01-26 07:17:18.920329!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0533
8. set (Dataset 21) being trained for epoch 3 by 2019-01-26 07:17:41.665276!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1080
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 07:18:03.540026!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0990
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 07:18:25.941365!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0785
11. set (Dataset 1) being trained for epoch 3 by 2019-01-26 07:18:50.556563!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0874
12. set (Dataset 17) being trained for epoch 3 by 2019-01-26 07:19:06.922002!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0679
13. set (Dataset 19) being trained for epoch 3 by 2019-01-26 07:19:22.126006!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0723
14. set (Dataset 4) being trained for epoch 3 by 2019-01-26 07:19:41.758889!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0858
15. set (Dataset 11) being trained for epoch 3 by 2019-01-26 07:20:05.989063!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0570
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 07:20:25.846769!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0663
17. set (Dataset 6) being trained for epoch 3 by 2019-01-26 07:20:44.899385!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0956
18. set (Dataset 2) being trained for epoch 3 by 2019-01-26 07:21:03.355762!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0780
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 07:21:23.836741!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0733
20. set (Dataset 10) being trained for epoch 3 by 2019-01-26 07:21:49.945170!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0710
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:22:12.219164
1. set (Dataset 2) being trained for epoch 4 by 2019-01-26 07:22:17.293133!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0762
2. set (Dataset 10) being trained for epoch 4 by 2019-01-26 07:22:37.266400!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0673
3. set (Dataset 1) being trained for epoch 4 by 2019-01-26 07:23:00.319661!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0842
4. set (Dataset 11) being trained for epoch 4 by 2019-01-26 07:23:18.680572!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0564
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 07:23:40.720390!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0616
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 07:24:06.668910!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0736
7. set (Dataset 6) being trained for epoch 4 by 2019-01-26 07:24:30.513360!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0989
8. set (Dataset 17) being trained for epoch 4 by 2019-01-26 07:24:47.466546!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0713
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 07:25:02.287289!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0603
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 07:25:19.716311!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1027
11. set (Dataset 22) being trained for epoch 4 by 2019-01-26 07:25:40.561218!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0522
12. set (Dataset 18) being trained for epoch 4 by 2019-01-26 07:26:02.998499!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0895
13. set (Dataset 4) being trained for epoch 4 by 2019-01-26 07:26:26.148324!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0899
14. set (Dataset 15) being trained for epoch 4 by 2019-01-26 07:26:51.458968!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0734
15. set (Dataset 16) being trained for epoch 4 by 2019-01-26 07:27:16.972203!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0682
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 07:27:45.717291!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0615
17. set (Dataset 19) being trained for epoch 4 by 2019-01-26 07:28:04.728431!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0704
18. set (Dataset 24) being trained for epoch 4 by 2019-01-26 07:28:22.194574!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0551
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 07:28:42.069694!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0771
20. set (Dataset 21) being trained for epoch 4 by 2019-01-26 07:29:07.733035!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1047
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:29:28.251958
1. set (Dataset 24) being trained for epoch 5 by 2019-01-26 07:29:32.928084!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0554
2. set (Dataset 21) being trained for epoch 5 by 2019-01-26 07:29:51.614716!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1062
3. set (Dataset 22) being trained for epoch 5 by 2019-01-26 07:30:14.432678!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0513
4. set (Dataset 16) being trained for epoch 5 by 2019-01-26 07:30:40.616721!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0674
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 07:31:11.588170!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0747
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 07:31:37.580869!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0733
7. set (Dataset 19) being trained for epoch 5 by 2019-01-26 07:32:02.306500!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0728
8. set (Dataset 18) being trained for epoch 5 by 2019-01-26 07:32:21.431948!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0893
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 07:32:44.180337!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0655
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 07:33:07.663397!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0593
11. set (Dataset 6) being trained for epoch 5 by 2019-01-26 07:33:25.060136!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0975
12. set (Dataset 2) being trained for epoch 5 by 2019-01-26 07:33:43.535147!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0791
13. set (Dataset 15) being trained for epoch 5 by 2019-01-26 07:34:02.995378!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0715
14. set (Dataset 10) being trained for epoch 5 by 2019-01-26 07:34:26.540607!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0724
15. set (Dataset 1) being trained for epoch 5 by 2019-01-26 07:34:50.182225!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0843
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 07:35:08.172488!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0666
17. set (Dataset 4) being trained for epoch 5 by 2019-01-26 07:35:29.077850!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0831
18. set (Dataset 11) being trained for epoch 5 by 2019-01-26 07:35:53.621620!
Epoch 1/1
572/572 [==============================] - 15s 27ms/step - loss: 0.0553
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 07:36:14.354175!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0998
20. set (Dataset 17) being trained for epoch 5 by 2019-01-26 07:36:32.529632!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0692
Epoch 5 completed!
The subjects are trained: [(24, 'M14'), (21, 'F02'), (22, 'M01'), (16, 'M09'), (7, 'M01'), (8, 'M02'), (19, 'M11'), (18,
 'F05'), (12, 'M06'), (13, 'M07'), (6, 'F06'), (2, 'F02'), (15, 'F03'), (10, 'M04'), (1, 'F01'), (20, 'M12'), (4, 'F04')
, (11, 'M05'), (23, 'M13'), (17, 'M10')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 07:36:44.945946
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.64 Degree
        The absolute mean error on Yaw angle estimation: 29.55 Degree
        The absolute mean error on Roll angle estimation: 13.68 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.39 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 3.36 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 28.40 Degree
        The absolute mean error on Yaw angle estimation: 21.91 Degree
        The absolute mean error on Roll angle estimation: 7.64 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 17.03 Degree
        The absolute mean error on Yaw angle estimation: 28.84 Degree
        The absolute mean error on Roll angle estimation: 13.32 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.62 Degree
        The absolute mean error on Yaw angle estimations: 26.75 Degree
        The absolute mean error on Roll angle estimations: 9.50 Degree
Exp2019-01-26_04-28-49_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 07:38:13.760168
1. set (Dataset 11) being trained for epoch 1 by 2019-01-26 07:38:19.476468!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0561
2. set (Dataset 17) being trained for epoch 1 by 2019-01-26 07:38:37.523606!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0695
3. set (Dataset 6) being trained for epoch 1 by 2019-01-26 07:38:52.835590!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0959
4. set (Dataset 1) being trained for epoch 1 by 2019-01-26 07:39:11.158400!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0903
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 07:39:31.974097!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0744
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 07:39:56.390712!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0982
7. set (Dataset 4) being trained for epoch 1 by 2019-01-26 07:40:18.217087!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0811
8. set (Dataset 2) being trained for epoch 1 by 2019-01-26 07:40:42.113672!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0773
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 07:41:02.341650!
Epoch 1/1
745/745 [==============================] - 20s 26ms/step - loss: 0.0762
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 07:41:29.173613!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0589
11. set (Dataset 19) being trained for epoch 1 by 2019-01-26 07:41:53.285005!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0746
12. set (Dataset 24) being trained for epoch 1 by 2019-01-26 07:42:11.113035!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0577
13. set (Dataset 10) being trained for epoch 1 by 2019-01-26 07:42:30.466367!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0719
14. set (Dataset 21) being trained for epoch 1 by 2019-01-26 07:42:55.009466!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1089
15. set (Dataset 22) being trained for epoch 1 by 2019-01-26 07:43:17.347919!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0533
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 07:43:39.598660!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0624
17. set (Dataset 15) being trained for epoch 1 by 2019-01-26 07:43:59.641668!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0724
18. set (Dataset 16) being trained for epoch 1 by 2019-01-26 07:44:25.196358!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0674
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 07:44:52.843080!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0631
20. set (Dataset 18) being trained for epoch 1 by 2019-01-26 07:45:10.985443!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0898
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:45:30.832964
1. set (Dataset 16) being trained for epoch 2 by 2019-01-26 07:45:39.560001!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0646
2. set (Dataset 18) being trained for epoch 2 by 2019-01-26 07:46:08.604152!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0807
3. set (Dataset 19) being trained for epoch 2 by 2019-01-26 07:46:29.011900!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0727
4. set (Dataset 22) being trained for epoch 2 by 2019-01-26 07:46:47.904968!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0527
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 07:47:09.606779!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0971
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 07:47:29.341320!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0629
7. set (Dataset 15) being trained for epoch 2 by 2019-01-26 07:47:48.141713!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0702
8. set (Dataset 24) being trained for epoch 2 by 2019-01-26 07:48:09.096391!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0521
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 07:48:29.059040!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0758
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 07:48:56.520902!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0764
11. set (Dataset 4) being trained for epoch 2 by 2019-01-26 07:49:22.946224!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0811
12. set (Dataset 11) being trained for epoch 2 by 2019-01-26 07:49:47.460491!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0570
13. set (Dataset 21) being trained for epoch 2 by 2019-01-26 07:50:08.449487!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1066
14. set (Dataset 17) being trained for epoch 2 by 2019-01-26 07:50:28.385538!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0733
15. set (Dataset 6) being trained for epoch 2 by 2019-01-26 07:50:43.172187!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0943
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 07:51:02.167556!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0633
17. set (Dataset 10) being trained for epoch 2 by 2019-01-26 07:51:23.498855!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0732
18. set (Dataset 1) being trained for epoch 2 by 2019-01-26 07:51:47.190611!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0886
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 07:52:07.465784!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0598
20. set (Dataset 2) being trained for epoch 2 by 2019-01-26 07:52:30.802860!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0787
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:52:48.021722
1. set (Dataset 1) being trained for epoch 3 by 2019-01-26 07:52:53.059088!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0825
2. set (Dataset 2) being trained for epoch 3 by 2019-01-26 07:53:10.749161!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0724
3. set (Dataset 4) being trained for epoch 3 by 2019-01-26 07:53:30.833814!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0806
4. set (Dataset 6) being trained for epoch 3 by 2019-01-26 07:53:54.925029!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0998
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 07:54:13.271986!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0606
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 07:54:32.533976!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0608
7. set (Dataset 10) being trained for epoch 3 by 2019-01-26 07:54:57.974745!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0662
8. set (Dataset 11) being trained for epoch 3 by 2019-01-26 07:55:22.385488!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0553
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 07:55:42.096824!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0993
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 07:56:04.567371!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0742
11. set (Dataset 15) being trained for epoch 3 by 2019-01-26 07:56:30.913814!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0771
12. set (Dataset 16) being trained for epoch 3 by 2019-01-26 07:56:56.403145!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0664
13. set (Dataset 17) being trained for epoch 3 by 2019-01-26 07:57:23.254364!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0673
14. set (Dataset 18) being trained for epoch 3 by 2019-01-26 07:57:39.040620!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0873
15. set (Dataset 19) being trained for epoch 3 by 2019-01-26 07:57:59.178552!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0703
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 07:58:17.738052!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0592
17. set (Dataset 21) being trained for epoch 3 by 2019-01-26 07:58:37.579680!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1058
18. set (Dataset 22) being trained for epoch 3 by 2019-01-26 07:59:00.104717!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0504
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 07:59:24.393397!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0780
20. set (Dataset 24) being trained for epoch 3 by 2019-01-26 07:59:47.713737!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0556
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 08:00:04.571187
1. set (Dataset 22) being trained for epoch 4 by 2019-01-26 08:00:10.960619!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0514
2. set (Dataset 24) being trained for epoch 4 by 2019-01-26 08:00:32.805237!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0534
3. set (Dataset 15) being trained for epoch 4 by 2019-01-26 08:00:51.459594!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0702
4. set (Dataset 19) being trained for epoch 4 by 2019-01-26 08:01:12.680806!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0655
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 08:01:32.813718!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0674
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 08:01:58.771406!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0740
7. set (Dataset 21) being trained for epoch 4 by 2019-01-26 08:02:24.272413!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1082
8. set (Dataset 16) being trained for epoch 4 by 2019-01-26 08:02:48.851880!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0675
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 08:03:16.820251!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0616
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 08:03:34.864238!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0964
11. set (Dataset 10) being trained for epoch 4 by 2019-01-26 08:03:56.708783!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0724
12. set (Dataset 1) being trained for epoch 4 by 2019-01-26 08:04:20.506022!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0853
13. set (Dataset 18) being trained for epoch 4 by 2019-01-26 08:04:39.530026!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0905
14. set (Dataset 2) being trained for epoch 4 by 2019-01-26 08:05:00.047339!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0752
15. set (Dataset 4) being trained for epoch 4 by 2019-01-26 08:05:20.299933!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0832
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 08:05:44.483862!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0653
17. set (Dataset 17) being trained for epoch 4 by 2019-01-26 08:06:02.229564!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0684
18. set (Dataset 6) being trained for epoch 4 by 2019-01-26 08:06:17.654992!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0950
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 08:06:39.338423!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0739
20. set (Dataset 11) being trained for epoch 4 by 2019-01-26 08:07:04.665108!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0528
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 08:07:24.032018
1. set (Dataset 6) being trained for epoch 5 by 2019-01-26 08:07:29.216261!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0919
2. set (Dataset 11) being trained for epoch 5 by 2019-01-26 08:07:48.782211!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0525
3. set (Dataset 10) being trained for epoch 5 by 2019-01-26 08:08:10.759908!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0692
4. set (Dataset 4) being trained for epoch 5 by 2019-01-26 08:08:36.859104!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0805
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 08:09:03.540836!
Epoch 1/1
745/745 [==============================] - 20s 26ms/step - loss: 0.0748
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 08:09:30.970720!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0723
7. set (Dataset 17) being trained for epoch 5 by 2019-01-26 08:09:54.585840!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0682
8. set (Dataset 1) being trained for epoch 5 by 2019-01-26 08:10:09.464996!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0863
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 08:10:29.073630!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0595
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 08:10:52.447912!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0590
11. set (Dataset 21) being trained for epoch 5 by 2019-01-26 08:11:10.613418!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1097
12. set (Dataset 22) being trained for epoch 5 by 2019-01-26 08:11:32.742420!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0532
13. set (Dataset 2) being trained for epoch 5 by 2019-01-26 08:11:55.222057!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0768
14. set (Dataset 24) being trained for epoch 5 by 2019-01-26 08:12:12.532791!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0564
15. set (Dataset 15) being trained for epoch 5 by 2019-01-26 08:12:31.395378!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0740
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 08:12:52.955901!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0624
17. set (Dataset 18) being trained for epoch 5 by 2019-01-26 08:13:13.236528!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0840
18. set (Dataset 19) being trained for epoch 5 by 2019-01-26 08:13:33.948401!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0686
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 08:13:51.863731!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0950
20. set (Dataset 16) being trained for epoch 5 by 2019-01-26 08:14:15.452437!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0640
Epoch 5 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (7, 'M01'), (8, 'M02'), (17, 'M10'), (1, 'F
01'), (12, 'M06'), (13, 'M07'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'), (20, 'M12'), (18, 'F05')
, (19, 'M11'), (23, 'M13'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 08:14:40.592843
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.24 Degree
        The absolute mean error on Yaw angle estimation: 23.13 Degree
        The absolute mean error on Roll angle estimation: 15.32 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 14.24 Degree
        The absolute mean error on Yaw angle estimation: 27.71 Degree
        The absolute mean error on Roll angle estimation: 3.81 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 26.10 Degree
        The absolute mean error on Yaw angle estimation: 26.98 Degree
        The absolute mean error on Roll angle estimation: 7.45 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.52 Degree
        The absolute mean error on Yaw angle estimation: 30.62 Degree
        The absolute mean error on Roll angle estimation: 13.39 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.53 Degree
        The absolute mean error on Yaw angle estimations: 27.11 Degree
        The absolute mean error on Roll angle estimations: 9.99 Degree
Exp2019-01-26_04-28-49_part6 completed!
subject3_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.343922.
subject5_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.543420.
subject9_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.741643.
subject14_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.957915.
Model Exp2019-01-26_04-28-49 has been evaluated successfully.
Model Exp2019-01-26_04-28-49 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-26_04-28-49 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.py", line 52,
 in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 260, in load_model
    f = h5py.File(filepath, mode='r')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = '/home/mcicek/Datasets/Keras_Models/Exp2019-01-26_04-28-49.h5'
, errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 13:37:42.044613: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 13:37:42.141623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-26 13:37:42.141885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 13:37:42.141897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-26 13:37:42.297498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-26 13:37:42.297525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 13:37:42.297529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 13:37:42.297666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_13-37-43 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 13:37:47.496164
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 13:37:53.897956!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.2134
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 13:38:17.428122!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.1537
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 13:38:35.826012!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.1627
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 13:38:57.157694!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1557
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 13:39:17.582164!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.1922
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 13:39:41.965695!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1938
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 13:40:02.478005!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1931
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 13:40:27.363862!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1485
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 13:40:58.079955!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.1751
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 13:41:24.479678!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.1414
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 13:41:50.880939!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.1474
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 13:42:13.757991!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1588
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 13:42:31.935777!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.1739
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 13:42:52.737112!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1646
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 13:43:12.947118!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1508
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 13:43:37.021597!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.1290
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 13:43:54.862511!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.1054
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 13:44:09.850323!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1716
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 13:44:28.180793!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.1019
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 13:44:45.863761!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.1089
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 13:45:04.521469
1. set (Dataset 6) being trained for epoch 2 by 2019-01-26 13:45:09.706344!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1545
2. set (Dataset 11) being trained for epoch 2 by 2019-01-26 13:45:28.570247!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0893
3. set (Dataset 10) being trained for epoch 2 by 2019-01-26 13:45:50.489836!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1188
4. set (Dataset 4) being trained for epoch 2 by 2019-01-26 13:46:16.252836!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.1267
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 13:46:40.262662!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1590
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 13:46:59.524336!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0916
7. set (Dataset 17) being trained for epoch 2 by 2019-01-26 13:47:15.329745!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.1010
8. set (Dataset 1) being trained for epoch 2 by 2019-01-26 13:47:30.105209!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1335
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 13:47:50.052174!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.1162
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 13:48:17.559026!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1081
11. set (Dataset 21) being trained for epoch 2 by 2019-01-26 13:48:42.280547!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1569
12. set (Dataset 22) being trained for epoch 2 by 2019-01-26 13:49:05.128362!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0871
13. set (Dataset 2) being trained for epoch 2 by 2019-01-26 13:49:27.366885!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1146
14. set (Dataset 24) being trained for epoch 2 by 2019-01-26 13:49:45.027558!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0905
15. set (Dataset 15) being trained for epoch 2 by 2019-01-26 13:50:03.433363!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.1117
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 13:50:25.737346!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0934
17. set (Dataset 18) being trained for epoch 2 by 2019-01-26 13:50:45.510993!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1411
18. set (Dataset 19) being trained for epoch 2 by 2019-01-26 13:51:05.647126!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1030
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 13:51:25.720885!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.1043
20. set (Dataset 16) being trained for epoch 2 by 2019-01-26 13:51:53.325940!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0994
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 13:52:20.929231
1. set (Dataset 19) being trained for epoch 3 by 2019-01-26 13:52:25.806612!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0918
2. set (Dataset 16) being trained for epoch 3 by 2019-01-26 13:52:47.585047!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0907
3. set (Dataset 21) being trained for epoch 3 by 2019-01-26 13:53:16.953899!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1418
4. set (Dataset 15) being trained for epoch 3 by 2019-01-26 13:53:39.021422!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.1024
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 13:54:01.136997!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0883
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 13:54:20.773588!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0873
7. set (Dataset 18) being trained for epoch 3 by 2019-01-26 13:54:44.983117!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1255
8. set (Dataset 22) being trained for epoch 3 by 2019-01-26 13:55:06.658491!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0741
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 13:55:29.571770!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1308
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 13:55:51.945928!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1072
11. set (Dataset 17) being trained for epoch 3 by 2019-01-26 13:56:14.805050!
Epoch 1/1
395/395 [==============================] - 11s 27ms/step - loss: 0.0876
12. set (Dataset 6) being trained for epoch 3 by 2019-01-26 13:56:30.578744!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1422
13. set (Dataset 24) being trained for epoch 3 by 2019-01-26 13:56:48.763587!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0733
14. set (Dataset 11) being trained for epoch 3 by 2019-01-26 13:57:06.880216!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0777
15. set (Dataset 10) being trained for epoch 3 by 2019-01-26 13:57:28.530031!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0960
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 13:57:51.998912!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0825
17. set (Dataset 2) being trained for epoch 3 by 2019-01-26 13:58:10.875157!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1068
18. set (Dataset 4) being trained for epoch 3 by 2019-01-26 13:58:30.760695!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.1154
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 13:58:57.795830!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0960
20. set (Dataset 1) being trained for epoch 3 by 2019-01-26 13:59:22.097381!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1149
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 13:59:38.836575
1. set (Dataset 4) being trained for epoch 4 by 2019-01-26 13:59:46.216868!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1029
2. set (Dataset 1) being trained for epoch 4 by 2019-01-26 14:00:10.228407!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1006
3. set (Dataset 17) being trained for epoch 4 by 2019-01-26 14:00:26.117745!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0861
4. set (Dataset 10) being trained for epoch 4 by 2019-01-26 14:00:43.553951!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0870
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 14:01:08.957133!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0800
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 14:01:35.209997!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0923
7. set (Dataset 2) being trained for epoch 4 by 2019-01-26 14:01:58.979624!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0928
8. set (Dataset 6) being trained for epoch 4 by 2019-01-26 14:02:17.108006!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1296
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 14:02:36.248809!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0765
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 14:02:53.633478!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1278
11. set (Dataset 18) being trained for epoch 4 by 2019-01-26 14:03:13.834329!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1228
12. set (Dataset 19) being trained for epoch 4 by 2019-01-26 14:03:34.280314!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0917
13. set (Dataset 11) being trained for epoch 4 by 2019-01-26 14:03:52.973101!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0704
14. set (Dataset 16) being trained for epoch 4 by 2019-01-26 14:04:16.136605!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0918
15. set (Dataset 21) being trained for epoch 4 by 2019-01-26 14:04:45.354851!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1347
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 14:05:06.858769!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0763
17. set (Dataset 24) being trained for epoch 4 by 2019-01-26 14:05:25.821704!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0739
18. set (Dataset 15) being trained for epoch 4 by 2019-01-26 14:05:44.538353!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0889
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 14:06:08.338253!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0963
20. set (Dataset 22) being trained for epoch 4 by 2019-01-26 14:06:34.493181!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0659
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:06:56.001676
1. set (Dataset 15) being trained for epoch 5 by 2019-01-26 14:07:02.350262!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0907
2. set (Dataset 22) being trained for epoch 5 by 2019-01-26 14:07:24.954878!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0629
3. set (Dataset 18) being trained for epoch 5 by 2019-01-26 14:07:47.222554!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1152
4. set (Dataset 21) being trained for epoch 5 by 2019-01-26 14:08:08.506640!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1261
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 14:08:32.683875!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0944
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 14:08:59.336003!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0897
7. set (Dataset 24) being trained for epoch 5 by 2019-01-26 14:09:23.357499!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0710
8. set (Dataset 19) being trained for epoch 5 by 2019-01-26 14:09:40.108793!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0900
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 14:09:59.885154!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0820
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 14:10:22.924358!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0757
11. set (Dataset 2) being trained for epoch 5 by 2019-01-26 14:10:39.682459!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0959
12. set (Dataset 4) being trained for epoch 5 by 2019-01-26 14:10:59.791402!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0988
13. set (Dataset 16) being trained for epoch 5 by 2019-01-26 14:11:27.180528!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0876
14. set (Dataset 1) being trained for epoch 5 by 2019-01-26 14:11:55.313939!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1049
15. set (Dataset 17) being trained for epoch 5 by 2019-01-26 14:12:11.944553!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0873
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 14:12:27.214537!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0772
17. set (Dataset 11) being trained for epoch 5 by 2019-01-26 14:12:46.572954!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0681
18. set (Dataset 10) being trained for epoch 5 by 2019-01-26 14:13:08.055256!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0824
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 14:13:32.359488!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1233
20. set (Dataset 6) being trained for epoch 5 by 2019-01-26 14:13:52.056260!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1204
Epoch 5 completed!
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24, 'M14'), (19,
 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'), (20, 'M12'), (11, 'M05'
), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 14:14:07.707520
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.43 Degree
        The absolute mean error on Yaw angle estimation: 32.58 Degree
        The absolute mean error on Roll angle estimation: 14.18 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.77 Degree
        The absolute mean error on Yaw angle estimation: 28.26 Degree
        The absolute mean error on Roll angle estimation: 3.76 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 25.82 Degree
        The absolute mean error on Yaw angle estimation: 27.21 Degree
        The absolute mean error on Roll angle estimation: 8.53 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 15.19 Degree
        The absolute mean error on Yaw angle estimation: 26.62 Degree
        The absolute mean error on Roll angle estimation: 13.37 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.80 Degree
        The absolute mean error on Yaw angle estimations: 28.67 Degree
        The absolute mean error on Roll angle estimations: 9.96 Degree
Exp2019-01-26_13-37-43_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 14:15:36.631298
1. set (Dataset 10) being trained for epoch 1 by 2019-01-26 14:15:43.894822!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0806
2. set (Dataset 6) being trained for epoch 1 by 2019-01-26 14:16:07.691099!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1075
3. set (Dataset 2) being trained for epoch 1 by 2019-01-26 14:16:26.981420!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0848
4. set (Dataset 17) being trained for epoch 1 by 2019-01-26 14:16:43.496499!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0839
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 14:17:01.099752!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0848
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 14:17:25.378604!
Epoch 1/1
569/569 [==============================] - 15s 25ms/step - loss: 0.1156
7. set (Dataset 11) being trained for epoch 1 by 2019-01-26 14:17:45.669515!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0668
8. set (Dataset 4) being trained for epoch 1 by 2019-01-26 14:18:07.459276!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0945
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 14:18:33.578293!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0850
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 14:18:59.921008!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0698
11. set (Dataset 24) being trained for epoch 1 by 2019-01-26 14:19:22.859512!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0709
12. set (Dataset 15) being trained for epoch 1 by 2019-01-26 14:19:41.322784!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0905
13. set (Dataset 1) being trained for epoch 1 by 2019-01-26 14:20:02.815468!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0977
14. set (Dataset 22) being trained for epoch 1 by 2019-01-26 14:20:21.977093!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0649
15. set (Dataset 18) being trained for epoch 1 by 2019-01-26 14:20:44.434148!
Epoch 1/1
614/614 [==============================] - 16s 27ms/step - loss: 0.1079
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 14:21:06.282758!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0746
17. set (Dataset 16) being trained for epoch 1 by 2019-01-26 14:21:29.064175!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0826
18. set (Dataset 21) being trained for epoch 1 by 2019-01-26 14:21:57.827123!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1227
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 14:22:18.219033!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0772
20. set (Dataset 19) being trained for epoch 1 by 2019-01-26 14:22:35.056914!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0848
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:22:52.561555
1. set (Dataset 21) being trained for epoch 2 by 2019-01-26 14:22:58.592694!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1177
2. set (Dataset 19) being trained for epoch 2 by 2019-01-26 14:23:19.214919!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0805
3. set (Dataset 24) being trained for epoch 2 by 2019-01-26 14:23:36.919947!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0658
4. set (Dataset 18) being trained for epoch 2 by 2019-01-26 14:23:55.483760!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0984
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 14:24:16.490951!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1094
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 14:24:35.525900!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0719
7. set (Dataset 16) being trained for epoch 2 by 2019-01-26 14:24:56.465532!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0784
8. set (Dataset 15) being trained for epoch 2 by 2019-01-26 14:25:26.515800!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0824
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 14:25:50.671417!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0854
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 14:26:17.385570!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0831
11. set (Dataset 11) being trained for epoch 2 by 2019-01-26 14:26:41.670344!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0637
12. set (Dataset 10) being trained for epoch 2 by 2019-01-26 14:27:03.043151!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0772
13. set (Dataset 22) being trained for epoch 2 by 2019-01-26 14:27:28.548327!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0610
14. set (Dataset 6) being trained for epoch 2 by 2019-01-26 14:27:50.083163!
Epoch 1/1
542/542 [==============================] - 19s 34ms/step - loss: 0.1109
15. set (Dataset 2) being trained for epoch 2 by 2019-01-26 14:28:13.914915!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0843
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 14:28:32.537184!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0704
17. set (Dataset 1) being trained for epoch 2 by 2019-01-26 14:28:51.658246!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0978
18. set (Dataset 17) being trained for epoch 2 by 2019-01-26 14:29:07.850546!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0800
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 14:29:25.241590!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0711
20. set (Dataset 4) being trained for epoch 2 by 2019-01-26 14:29:51.462921!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0922
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:30:14.554037
1. set (Dataset 17) being trained for epoch 3 by 2019-01-26 14:30:18.297763!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0801
2. set (Dataset 4) being trained for epoch 3 by 2019-01-26 14:30:35.806538!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0854
3. set (Dataset 11) being trained for epoch 3 by 2019-01-26 14:31:00.196995!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0640
4. set (Dataset 2) being trained for epoch 3 by 2019-01-26 14:31:19.830724!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0831
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 14:31:37.756635!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0701
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 14:31:57.522353!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0692
7. set (Dataset 1) being trained for epoch 3 by 2019-01-26 14:32:20.584402!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0901
8. set (Dataset 10) being trained for epoch 3 by 2019-01-26 14:32:40.230857!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0744
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 14:33:04.376025!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1157
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 14:33:27.193604!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0784
11. set (Dataset 16) being trained for epoch 3 by 2019-01-26 14:33:54.900027!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0800
12. set (Dataset 21) being trained for epoch 3 by 2019-01-26 14:34:23.670919!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1205
13. set (Dataset 6) being trained for epoch 3 by 2019-01-26 14:34:44.401241!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1039
14. set (Dataset 19) being trained for epoch 3 by 2019-01-26 14:35:02.652340!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0833
15. set (Dataset 24) being trained for epoch 3 by 2019-01-26 14:35:20.053081!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0613
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 14:35:37.213640!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0671
17. set (Dataset 22) being trained for epoch 3 by 2019-01-26 14:35:57.395653!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0584
18. set (Dataset 18) being trained for epoch 3 by 2019-01-26 14:36:19.582540!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0981
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 14:36:43.209261!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0826
20. set (Dataset 15) being trained for epoch 3 by 2019-01-26 14:37:08.046709!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0827
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:37:28.835226
1. set (Dataset 18) being trained for epoch 4 by 2019-01-26 14:37:34.741326!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0962
2. set (Dataset 15) being trained for epoch 4 by 2019-01-26 14:37:56.411012!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0786
3. set (Dataset 16) being trained for epoch 4 by 2019-01-26 14:38:21.895220!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0727
4. set (Dataset 24) being trained for epoch 4 by 2019-01-26 14:38:50.204041!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0595
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 14:39:10.261066!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0717
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 14:39:36.658326!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0778
7. set (Dataset 22) being trained for epoch 4 by 2019-01-26 14:40:01.592545!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0590
8. set (Dataset 21) being trained for epoch 4 by 2019-01-26 14:40:24.038604!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1149
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 14:40:44.774689!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0690
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 14:41:02.443332!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1090
11. set (Dataset 1) being trained for epoch 4 by 2019-01-26 14:41:22.117326!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0941
12. set (Dataset 17) being trained for epoch 4 by 2019-01-26 14:41:38.320863!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0777
13. set (Dataset 19) being trained for epoch 4 by 2019-01-26 14:41:53.014440!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0800
14. set (Dataset 4) being trained for epoch 4 by 2019-01-26 14:42:13.100160!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0921
15. set (Dataset 11) being trained for epoch 4 by 2019-01-26 14:42:37.908668!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0609
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 14:42:57.846931!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0687
17. set (Dataset 6) being trained for epoch 4 by 2019-01-26 14:43:17.306900!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0986
18. set (Dataset 2) being trained for epoch 4 by 2019-01-26 14:43:36.298587!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0835
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 14:43:56.903694!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0804
20. set (Dataset 10) being trained for epoch 4 by 2019-01-26 14:44:23.278208!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0745
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:44:45.259911
1. set (Dataset 2) being trained for epoch 5 by 2019-01-26 14:44:50.346808!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0803
2. set (Dataset 10) being trained for epoch 5 by 2019-01-26 14:45:10.257256!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0710
3. set (Dataset 1) being trained for epoch 5 by 2019-01-26 14:45:33.129980!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0863
4. set (Dataset 11) being trained for epoch 5 by 2019-01-26 14:45:51.535776!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0590
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 14:46:13.206475!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0731
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 14:46:39.971290!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0760
7. set (Dataset 6) being trained for epoch 5 by 2019-01-26 14:47:05.145133!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1072
8. set (Dataset 17) being trained for epoch 5 by 2019-01-26 14:47:22.073022!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0794
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 14:47:39.193066!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0648
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 14:48:02.522140!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0632
11. set (Dataset 22) being trained for epoch 5 by 2019-01-26 14:48:21.298966!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0586
12. set (Dataset 18) being trained for epoch 5 by 2019-01-26 14:48:44.434982!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0992
13. set (Dataset 4) being trained for epoch 5 by 2019-01-26 14:49:07.565375!
Epoch 1/1
744/744 [==============================] - 20s 27ms/step - loss: 0.0859
14. set (Dataset 15) being trained for epoch 5 by 2019-01-26 14:49:33.957366!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0804
15. set (Dataset 16) being trained for epoch 5 by 2019-01-26 14:49:59.240280!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0741
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 14:50:27.181489!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0629
17. set (Dataset 19) being trained for epoch 5 by 2019-01-26 14:50:45.838729!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0785
18. set (Dataset 24) being trained for epoch 5 by 2019-01-26 14:51:03.296781!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0595
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 14:51:20.446707!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1070
20. set (Dataset 21) being trained for epoch 5 by 2019-01-26 14:51:40.713904!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1156
Epoch 5 completed!
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F06'), (17, 'M
10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'), (20, 'M12'), (19, 'M11')
, (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 14:51:59.193524
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.48 Degree
        The absolute mean error on Yaw angle estimation: 27.58 Degree
        The absolute mean error on Roll angle estimation: 21.84 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.26 Degree
        The absolute mean error on Yaw angle estimation: 28.83 Degree
        The absolute mean error on Roll angle estimation: 4.95 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.45 Degree
        The absolute mean error on Yaw angle estimation: 26.92 Degree
        The absolute mean error on Roll angle estimation: 9.55 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.09 Degree
        The absolute mean error on Yaw angle estimation: 30.08 Degree
        The absolute mean error on Roll angle estimation: 14.75 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.57 Degree
        The absolute mean error on Yaw angle estimations: 28.35 Degree
        The absolute mean error on Roll angle estimations: 12.77 Degree
Exp2019-01-26_13-37-43_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 14:53:28.013376
1. set (Dataset 24) being trained for epoch 1 by 2019-01-26 14:53:32.683865!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0556
2. set (Dataset 21) being trained for epoch 1 by 2019-01-26 14:53:51.583583!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1053
3. set (Dataset 22) being trained for epoch 1 by 2019-01-26 14:54:13.858749!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0570
4. set (Dataset 16) being trained for epoch 1 by 2019-01-26 14:54:40.095091!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0724
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 14:55:10.621983!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0798
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 14:55:35.651794!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1042
7. set (Dataset 19) being trained for epoch 1 by 2019-01-26 14:55:54.796342!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0755
8. set (Dataset 18) being trained for epoch 1 by 2019-01-26 14:56:12.892481!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0896
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 14:56:35.666188!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0765
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 14:57:01.360934!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0680
11. set (Dataset 6) being trained for epoch 1 by 2019-01-26 14:57:25.408468!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0960
12. set (Dataset 2) being trained for epoch 1 by 2019-01-26 14:57:43.921788!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0760
13. set (Dataset 15) being trained for epoch 1 by 2019-01-26 14:58:03.410819!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0798
14. set (Dataset 10) being trained for epoch 1 by 2019-01-26 14:58:26.882251!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0747
15. set (Dataset 1) being trained for epoch 1 by 2019-01-26 14:58:50.511601!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0844
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 14:59:08.683661!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0666
17. set (Dataset 4) being trained for epoch 1 by 2019-01-26 14:59:30.163833!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.0846
18. set (Dataset 11) being trained for epoch 1 by 2019-01-26 14:59:54.010793!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0596
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 15:00:13.509759!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0662
20. set (Dataset 17) being trained for epoch 1 by 2019-01-26 15:00:29.177627!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0744
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:00:44.021509
1. set (Dataset 11) being trained for epoch 2 by 2019-01-26 15:00:49.721220!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0565
2. set (Dataset 17) being trained for epoch 2 by 2019-01-26 15:01:08.042173!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0719
3. set (Dataset 6) being trained for epoch 2 by 2019-01-26 15:01:23.260939!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0997
4. set (Dataset 1) being trained for epoch 2 by 2019-01-26 15:01:41.969715!
Epoch 1/1
498/498 [==============================] - 13s 27ms/step - loss: 0.0848
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 15:02:00.744307!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1043
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 15:02:19.546123!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0668
7. set (Dataset 4) being trained for epoch 2 by 2019-01-26 15:02:39.382976!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0821
8. set (Dataset 2) being trained for epoch 2 by 2019-01-26 15:03:03.346989!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0785
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 15:03:24.184414!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0758
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 15:03:51.442214!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0734
11. set (Dataset 19) being trained for epoch 2 by 2019-01-26 15:04:15.111560!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0796
12. set (Dataset 24) being trained for epoch 2 by 2019-01-26 15:04:32.531618!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0584
13. set (Dataset 10) being trained for epoch 2 by 2019-01-26 15:04:50.832725!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0698
14. set (Dataset 21) being trained for epoch 2 by 2019-01-26 15:05:15.200154!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1133
15. set (Dataset 22) being trained for epoch 2 by 2019-01-26 15:05:37.696727!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0552
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 15:05:59.711142!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0644
17. set (Dataset 15) being trained for epoch 2 by 2019-01-26 15:06:19.705067!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0777
18. set (Dataset 16) being trained for epoch 2 by 2019-01-26 15:06:45.316879!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0719
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 15:07:15.426576!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0651
20. set (Dataset 18) being trained for epoch 2 by 2019-01-26 15:07:39.970224!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0942
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:07:59.804347
1. set (Dataset 16) being trained for epoch 3 by 2019-01-26 15:08:08.531468!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0687
2. set (Dataset 18) being trained for epoch 3 by 2019-01-26 15:08:37.692824!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0860
3. set (Dataset 19) being trained for epoch 3 by 2019-01-26 15:08:58.000695!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0728
4. set (Dataset 22) being trained for epoch 3 by 2019-01-26 15:09:16.887071!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0549
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 15:09:38.574291!
Epoch 1/1
485/485 [==============================] - 13s 27ms/step - loss: 0.0667
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 15:09:58.896934!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0617
7. set (Dataset 15) being trained for epoch 3 by 2019-01-26 15:10:24.259197!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0755
8. set (Dataset 24) being trained for epoch 3 by 2019-01-26 15:10:45.263651!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0557
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 15:11:03.498922!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1008
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 15:11:25.928887!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0757
11. set (Dataset 4) being trained for epoch 3 by 2019-01-26 15:11:53.526348!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0847
12. set (Dataset 11) being trained for epoch 3 by 2019-01-26 15:12:17.991874!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0577
13. set (Dataset 21) being trained for epoch 3 by 2019-01-26 15:12:38.579035!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1129
14. set (Dataset 17) being trained for epoch 3 by 2019-01-26 15:12:58.912190!
Epoch 1/1
395/395 [==============================] - 11s 27ms/step - loss: 0.0767
15. set (Dataset 6) being trained for epoch 3 by 2019-01-26 15:13:14.906001!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0906
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 15:13:33.759581!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0600
17. set (Dataset 10) being trained for epoch 3 by 2019-01-26 15:13:55.440825!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0734
18. set (Dataset 1) being trained for epoch 3 by 2019-01-26 15:14:18.694931!
Epoch 1/1
498/498 [==============================] - 13s 27ms/step - loss: 0.0831
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 15:14:39.758589!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0715
20. set (Dataset 2) being trained for epoch 3 by 2019-01-26 15:15:03.620327!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0723
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:15:21.093228
1. set (Dataset 1) being trained for epoch 4 by 2019-01-26 15:15:26.141843!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0784
2. set (Dataset 2) being trained for epoch 4 by 2019-01-26 15:15:44.034335!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0656
3. set (Dataset 4) being trained for epoch 4 by 2019-01-26 15:16:04.373438!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0778
4. set (Dataset 6) being trained for epoch 4 by 2019-01-26 15:16:28.173172!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0969
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 15:16:49.626348!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0581
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 15:17:15.916878!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0699
7. set (Dataset 10) being trained for epoch 4 by 2019-01-26 15:17:42.215000!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0656
8. set (Dataset 11) being trained for epoch 4 by 2019-01-26 15:18:06.302530!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0526
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 15:18:25.986049!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0638
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 15:18:43.921722!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1043
11. set (Dataset 15) being trained for epoch 4 by 2019-01-26 15:19:04.582571!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0772
12. set (Dataset 16) being trained for epoch 4 by 2019-01-26 15:19:29.985996!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0682
13. set (Dataset 17) being trained for epoch 4 by 2019-01-26 15:19:57.252664!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0717
14. set (Dataset 18) being trained for epoch 4 by 2019-01-26 15:20:13.196367!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0890
15. set (Dataset 19) being trained for epoch 4 by 2019-01-26 15:20:33.763898!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0732
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 15:20:52.082695!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0595
17. set (Dataset 21) being trained for epoch 4 by 2019-01-26 15:21:11.724748!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1048
18. set (Dataset 22) being trained for epoch 4 by 2019-01-26 15:21:33.904824!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0535
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 15:21:58.527580!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0730
20. set (Dataset 24) being trained for epoch 4 by 2019-01-26 15:22:22.924578!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0574
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:22:39.761675
1. set (Dataset 22) being trained for epoch 5 by 2019-01-26 15:22:46.171505!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0514
2. set (Dataset 24) being trained for epoch 5 by 2019-01-26 15:23:08.056579!
Epoch 1/1
492/492 [==============================] - 13s 25ms/step - loss: 0.0513
3. set (Dataset 15) being trained for epoch 5 by 2019-01-26 15:23:26.987199!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0744
4. set (Dataset 19) being trained for epoch 5 by 2019-01-26 15:23:48.803197!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0723
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 15:24:09.478706!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0744
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 15:24:36.570734!
Epoch 1/1
772/772 [==============================] - 21s 27ms/step - loss: 0.0702
7. set (Dataset 21) being trained for epoch 5 by 2019-01-26 15:25:03.568212!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1103
8. set (Dataset 16) being trained for epoch 5 by 2019-01-26 15:25:27.941645!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0702
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 15:25:58.472955!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0618
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 15:26:21.856302!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0623
11. set (Dataset 10) being trained for epoch 5 by 2019-01-26 15:26:41.507061!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0709
12. set (Dataset 1) being trained for epoch 5 by 2019-01-26 15:27:04.858152!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0811
13. set (Dataset 18) being trained for epoch 5 by 2019-01-26 15:27:22.955725!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0924
14. set (Dataset 2) being trained for epoch 5 by 2019-01-26 15:27:43.988864!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0718
15. set (Dataset 4) being trained for epoch 5 by 2019-01-26 15:28:04.251771!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0815
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 15:28:28.784851!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0659
17. set (Dataset 17) being trained for epoch 5 by 2019-01-26 15:28:46.651534!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0720
18. set (Dataset 6) being trained for epoch 5 by 2019-01-26 15:29:01.677170!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0953
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 15:29:21.050426!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0983
20. set (Dataset 11) being trained for epoch 5 by 2019-01-26 15:29:41.296736!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0551
Epoch 5 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21, 'F02'), (16,
 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 15:29:57.340457
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.03 Degree
        The absolute mean error on Yaw angle estimation: 32.77 Degree
        The absolute mean error on Roll angle estimation: 14.60 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.85 Degree
        The absolute mean error on Yaw angle estimation: 25.93 Degree
        The absolute mean error on Roll angle estimation: 4.87 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 26.39 Degree
        The absolute mean error on Yaw angle estimation: 26.14 Degree
        The absolute mean error on Roll angle estimation: 9.19 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.17 Degree
        The absolute mean error on Yaw angle estimation: 30.83 Degree
        The absolute mean error on Roll angle estimation: 14.46 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.86 Degree
        The absolute mean error on Yaw angle estimations: 28.91 Degree
        The absolute mean error on Roll angle estimations: 10.78 Degree
Exp2019-01-26_13-37-43_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 15:31:26.022829
1. set (Dataset 6) being trained for epoch 1 by 2019-01-26 15:31:31.209621!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0885
2. set (Dataset 11) being trained for epoch 1 by 2019-01-26 15:31:50.559580!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0526
3. set (Dataset 10) being trained for epoch 1 by 2019-01-26 15:32:12.645515!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0641
4. set (Dataset 4) being trained for epoch 1 by 2019-01-26 15:32:39.257999!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0763
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 15:33:06.133257!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0729
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 15:33:31.019260!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1046
7. set (Dataset 17) being trained for epoch 1 by 2019-01-26 15:33:48.722368!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0718
8. set (Dataset 1) being trained for epoch 1 by 2019-01-26 15:34:04.095278!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0844
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 15:34:24.430393!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0700
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 15:34:50.428321!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0580
11. set (Dataset 21) being trained for epoch 1 by 2019-01-26 15:35:15.230250!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1072
12. set (Dataset 22) being trained for epoch 1 by 2019-01-26 15:35:37.557405!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0546
13. set (Dataset 2) being trained for epoch 1 by 2019-01-26 15:35:59.544554!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0717
14. set (Dataset 24) being trained for epoch 1 by 2019-01-26 15:36:16.864275!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0606
15. set (Dataset 15) being trained for epoch 1 by 2019-01-26 15:36:35.410444!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0729
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 15:36:56.920170!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0595
17. set (Dataset 18) being trained for epoch 1 by 2019-01-26 15:37:16.517392!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0869
18. set (Dataset 19) being trained for epoch 1 by 2019-01-26 15:37:37.477243!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0717
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 15:37:55.651253!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0637
20. set (Dataset 16) being trained for epoch 1 by 2019-01-26 15:38:16.772632!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0693
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:38:44.121402
1. set (Dataset 19) being trained for epoch 2 by 2019-01-26 15:38:49.028269!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0678
2. set (Dataset 16) being trained for epoch 2 by 2019-01-26 15:39:10.359793!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0648
3. set (Dataset 21) being trained for epoch 2 by 2019-01-26 15:39:39.705659!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1059
4. set (Dataset 15) being trained for epoch 2 by 2019-01-26 15:40:02.349958!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0736
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 15:40:24.647596!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0958
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 15:40:44.093892!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0611
7. set (Dataset 18) being trained for epoch 2 by 2019-01-26 15:41:02.327292!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0835
8. set (Dataset 22) being trained for epoch 2 by 2019-01-26 15:41:24.502344!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0504
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 15:41:49.415675!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0751
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 15:42:16.261823!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0693
11. set (Dataset 17) being trained for epoch 2 by 2019-01-26 15:42:39.304281!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0724
12. set (Dataset 6) being trained for epoch 2 by 2019-01-26 15:42:54.182426!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0920
13. set (Dataset 24) being trained for epoch 2 by 2019-01-26 15:43:12.227640!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0529
14. set (Dataset 11) being trained for epoch 2 by 2019-01-26 15:43:29.707274!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0538
15. set (Dataset 10) being trained for epoch 2 by 2019-01-26 15:43:51.142722!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0646
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 15:44:15.210325!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0629
17. set (Dataset 2) being trained for epoch 2 by 2019-01-26 15:44:34.544753!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0742
18. set (Dataset 4) being trained for epoch 2 by 2019-01-26 15:44:54.992131!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0771
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 15:45:21.132024!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0604
20. set (Dataset 1) being trained for epoch 2 by 2019-01-26 15:45:44.421336!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0774
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:46:01.458284
1. set (Dataset 4) being trained for epoch 3 by 2019-01-26 15:46:08.847242!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0745
2. set (Dataset 1) being trained for epoch 3 by 2019-01-26 15:46:32.261423!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0753
3. set (Dataset 17) being trained for epoch 3 by 2019-01-26 15:46:48.748779!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0736
4. set (Dataset 10) being trained for epoch 3 by 2019-01-26 15:47:05.944224!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0633
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 15:47:28.861424!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0586
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 15:47:48.331539!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0575
7. set (Dataset 2) being trained for epoch 3 by 2019-01-26 15:48:12.147193!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0680
8. set (Dataset 6) being trained for epoch 3 by 2019-01-26 15:48:30.303864!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0920
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 15:48:49.181829!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0941
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 15:49:11.303334!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0702
11. set (Dataset 18) being trained for epoch 3 by 2019-01-26 15:49:36.432195!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0902
12. set (Dataset 19) being trained for epoch 3 by 2019-01-26 15:49:56.472461!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0699
13. set (Dataset 11) being trained for epoch 3 by 2019-01-26 15:50:14.624550!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0541
14. set (Dataset 16) being trained for epoch 3 by 2019-01-26 15:50:37.927450!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0644
15. set (Dataset 21) being trained for epoch 3 by 2019-01-26 15:51:07.133659!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1037
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 15:51:28.673899!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0594
17. set (Dataset 24) being trained for epoch 3 by 2019-01-26 15:51:47.409754!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0519
18. set (Dataset 15) being trained for epoch 3 by 2019-01-26 15:52:06.064945!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0699
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 15:52:29.918049!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0697
20. set (Dataset 22) being trained for epoch 3 by 2019-01-26 15:52:55.414448!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0506
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:53:16.990405
1. set (Dataset 15) being trained for epoch 4 by 2019-01-26 15:53:23.379986!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0697
2. set (Dataset 22) being trained for epoch 4 by 2019-01-26 15:53:46.599274!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0499
3. set (Dataset 18) being trained for epoch 4 by 2019-01-26 15:54:09.676822!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0848
4. set (Dataset 21) being trained for epoch 4 by 2019-01-26 15:54:31.845267!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1044
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 15:54:55.046066!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0597
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 15:55:21.534973!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0685
7. set (Dataset 24) being trained for epoch 4 by 2019-01-26 15:55:45.040976!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0544
8. set (Dataset 19) being trained for epoch 4 by 2019-01-26 15:56:02.348301!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0647
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 15:56:19.567199!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0607
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 15:56:36.983751!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0968
11. set (Dataset 2) being trained for epoch 4 by 2019-01-26 15:56:56.414260!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0721
12. set (Dataset 4) being trained for epoch 4 by 2019-01-26 15:57:16.764899!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0779
13. set (Dataset 16) being trained for epoch 4 by 2019-01-26 15:57:43.989061!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0670
14. set (Dataset 1) being trained for epoch 4 by 2019-01-26 15:58:11.981977!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0806
15. set (Dataset 17) being trained for epoch 4 by 2019-01-26 15:58:28.465115!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0678
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 15:58:44.332699!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0611
17. set (Dataset 11) being trained for epoch 4 by 2019-01-26 15:59:03.861195!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0522
18. set (Dataset 10) being trained for epoch 4 by 2019-01-26 15:59:25.516166!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0632
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 15:59:51.240423!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0710
20. set (Dataset 6) being trained for epoch 4 by 2019-01-26 16:00:15.725953!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0948
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:00:34.098364
1. set (Dataset 10) being trained for epoch 5 by 2019-01-26 16:00:41.317723!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0646
2. set (Dataset 6) being trained for epoch 5 by 2019-01-26 16:01:04.767443!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0869
3. set (Dataset 2) being trained for epoch 5 by 2019-01-26 16:01:23.669140!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0714
4. set (Dataset 17) being trained for epoch 5 by 2019-01-26 16:01:40.262674!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0696
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 16:01:57.587131!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0649
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 16:02:24.143803!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0657
7. set (Dataset 11) being trained for epoch 5 by 2019-01-26 16:02:49.187910!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0514
8. set (Dataset 4) being trained for epoch 5 by 2019-01-26 16:03:11.379922!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0731
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 16:03:37.521355!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0591
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 16:04:01.281888!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0598
11. set (Dataset 24) being trained for epoch 5 by 2019-01-26 16:04:18.189454!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0578
12. set (Dataset 15) being trained for epoch 5 by 2019-01-26 16:04:36.755933!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0700
13. set (Dataset 1) being trained for epoch 5 by 2019-01-26 16:04:58.299822!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0809
14. set (Dataset 22) being trained for epoch 5 by 2019-01-26 16:05:17.054063!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0514
15. set (Dataset 18) being trained for epoch 5 by 2019-01-26 16:05:39.426668!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0835
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 16:06:00.170539!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0591
17. set (Dataset 16) being trained for epoch 5 by 2019-01-26 16:06:22.762796!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0643
18. set (Dataset 21) being trained for epoch 5 by 2019-01-26 16:06:51.575732!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1013
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 16:07:12.770027!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0944
20. set (Dataset 19) being trained for epoch 5 by 2019-01-26 16:07:32.081873!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0670
Epoch 5 completed!
The subjects are trained: [(10, 'M04'), (6, 'F06'), (2, 'F02'), (17, 'M10'), (7, 'M01'), (8, 'M02'), (11, 'M05'), (4, 'F
04'), (12, 'M06'), (13, 'M07'), (24, 'M14'), (15, 'F03'), (1, 'F01'), (22, 'M01'), (18, 'F05'), (20, 'M12'), (16, 'M09')
, (21, 'F02'), (23, 'M13'), (19, 'M11')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 16:07:46.712039
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 7.65 Degree
        The absolute mean error on Yaw angle estimation: 27.07 Degree
        The absolute mean error on Roll angle estimation: 15.22 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.97 Degree
        The absolute mean error on Yaw angle estimation: 26.83 Degree
        The absolute mean error on Roll angle estimation: 3.93 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 25.70 Degree
        The absolute mean error on Yaw angle estimation: 25.65 Degree
        The absolute mean error on Roll angle estimation: 6.89 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 16.03 Degree
        The absolute mean error on Yaw angle estimation: 31.47 Degree
        The absolute mean error on Roll angle estimation: 13.76 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.84 Degree
        The absolute mean error on Yaw angle estimations: 27.76 Degree
        The absolute mean error on Roll angle estimations: 9.95 Degree
Exp2019-01-26_13-37-43_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 16:09:15.448359
1. set (Dataset 21) being trained for epoch 1 by 2019-01-26 16:09:21.455918!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0995
2. set (Dataset 19) being trained for epoch 1 by 2019-01-26 16:09:42.330645!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0658
3. set (Dataset 24) being trained for epoch 1 by 2019-01-26 16:10:00.033674!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0510
4. set (Dataset 18) being trained for epoch 1 by 2019-01-26 16:10:19.012394!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0801
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 16:10:41.986488!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0744
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 16:11:06.864552!
Epoch 1/1
569/569 [==============================] - 15s 25ms/step - loss: 0.0925
7. set (Dataset 16) being trained for epoch 1 by 2019-01-26 16:11:30.336567!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0626
8. set (Dataset 15) being trained for epoch 1 by 2019-01-26 16:11:59.535800!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0704
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 16:12:23.652063!
Epoch 1/1
745/745 [==============================] - 20s 26ms/step - loss: 0.0663
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 16:12:50.691746!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0575
11. set (Dataset 11) being trained for epoch 1 by 2019-01-26 16:13:15.058794!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0509
12. set (Dataset 10) being trained for epoch 1 by 2019-01-26 16:13:36.809569!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0613
13. set (Dataset 22) being trained for epoch 1 by 2019-01-26 16:14:01.538245!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0516
14. set (Dataset 6) being trained for epoch 1 by 2019-01-26 16:14:23.051677!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0948
15. set (Dataset 2) being trained for epoch 1 by 2019-01-26 16:14:41.637042!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0706
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 16:14:59.534141!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0601
17. set (Dataset 1) being trained for epoch 1 by 2019-01-26 16:15:18.377651!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0809
18. set (Dataset 17) being trained for epoch 1 by 2019-01-26 16:15:35.102194!
Epoch 1/1
395/395 [==============================] - 10s 27ms/step - loss: 0.0687
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 16:15:50.440488!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0600
20. set (Dataset 4) being trained for epoch 1 by 2019-01-26 16:16:09.880021!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0722
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:16:33.356972
1. set (Dataset 17) being trained for epoch 2 by 2019-01-26 16:16:37.102283!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0669
2. set (Dataset 4) being trained for epoch 2 by 2019-01-26 16:16:54.547558!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0693
3. set (Dataset 11) being trained for epoch 2 by 2019-01-26 16:17:18.965668!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0507
4. set (Dataset 2) being trained for epoch 2 by 2019-01-26 16:17:38.515934!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0666
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 16:17:56.689288!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0959
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 16:18:15.945438!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0573
7. set (Dataset 1) being trained for epoch 2 by 2019-01-26 16:18:33.083658!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0760
8. set (Dataset 10) being trained for epoch 2 by 2019-01-26 16:18:52.833091!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0633
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 16:19:19.455341!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0676
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 16:19:45.833543!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0651
11. set (Dataset 16) being trained for epoch 2 by 2019-01-26 16:20:13.369937!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0680
12. set (Dataset 21) being trained for epoch 2 by 2019-01-26 16:20:42.714073!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1023
13. set (Dataset 6) being trained for epoch 2 by 2019-01-26 16:21:04.639960!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0882
14. set (Dataset 19) being trained for epoch 2 by 2019-01-26 16:21:23.361145!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0640
15. set (Dataset 24) being trained for epoch 2 by 2019-01-26 16:21:40.827094!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0530
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 16:21:58.654456!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0543
17. set (Dataset 22) being trained for epoch 2 by 2019-01-26 16:22:18.416295!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0489
18. set (Dataset 18) being trained for epoch 2 by 2019-01-26 16:22:41.282349!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0825
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 16:23:03.894807!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0596
20. set (Dataset 15) being trained for epoch 2 by 2019-01-26 16:23:28.388406!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0676
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:23:49.040657
1. set (Dataset 18) being trained for epoch 3 by 2019-01-26 16:23:55.054799!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0805
2. set (Dataset 15) being trained for epoch 3 by 2019-01-26 16:24:16.651404!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0651
3. set (Dataset 16) being trained for epoch 3 by 2019-01-26 16:24:41.811652!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0629
4. set (Dataset 24) being trained for epoch 3 by 2019-01-26 16:25:09.454454!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0530
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 16:25:26.721698!
Epoch 1/1
485/485 [==============================] - 11s 24ms/step - loss: 0.0607
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 16:25:45.517937!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0551
7. set (Dataset 22) being trained for epoch 3 by 2019-01-26 16:26:10.322323!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0468
8. set (Dataset 21) being trained for epoch 3 by 2019-01-26 16:26:33.596709!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1036
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 16:26:54.912398!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0915
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 16:27:17.293376!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0705
11. set (Dataset 1) being trained for epoch 3 by 2019-01-26 16:27:42.286234!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0784
12. set (Dataset 17) being trained for epoch 3 by 2019-01-26 16:27:58.547606!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0679
13. set (Dataset 19) being trained for epoch 3 by 2019-01-26 16:28:13.210919!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0660
14. set (Dataset 4) being trained for epoch 3 by 2019-01-26 16:28:33.076904!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0726
15. set (Dataset 11) being trained for epoch 3 by 2019-01-26 16:28:57.614080!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0535
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 16:29:17.249998!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0623
17. set (Dataset 6) being trained for epoch 3 by 2019-01-26 16:29:36.449165!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0855
18. set (Dataset 2) being trained for epoch 3 by 2019-01-26 16:29:55.119490!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0668
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 16:30:15.373297!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0669
20. set (Dataset 10) being trained for epoch 3 by 2019-01-26 16:30:41.602066!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0631
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:31:04.298281
1. set (Dataset 2) being trained for epoch 4 by 2019-01-26 16:31:09.387196!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0695
2. set (Dataset 10) being trained for epoch 4 by 2019-01-26 16:31:29.525710!
Epoch 1/1
726/726 [==============================] - 19s 27ms/step - loss: 0.0609
3. set (Dataset 1) being trained for epoch 4 by 2019-01-26 16:31:53.873614!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0734
4. set (Dataset 11) being trained for epoch 4 by 2019-01-26 16:32:11.992026!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0506
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 16:32:33.884836!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0562
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 16:33:00.418875!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0647
7. set (Dataset 6) being trained for epoch 4 by 2019-01-26 16:33:24.563486!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0933
8. set (Dataset 17) being trained for epoch 4 by 2019-01-26 16:33:42.317598!
Epoch 1/1
395/395 [==============================] - 11s 27ms/step - loss: 0.0680
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 16:33:57.675023!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0580
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 16:34:15.327010!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0933
11. set (Dataset 22) being trained for epoch 4 by 2019-01-26 16:34:36.357546!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0486
12. set (Dataset 18) being trained for epoch 4 by 2019-01-26 16:34:59.111737!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0784
13. set (Dataset 4) being trained for epoch 4 by 2019-01-26 16:35:22.287548!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0767
14. set (Dataset 15) being trained for epoch 4 by 2019-01-26 16:35:47.059930!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0685
15. set (Dataset 16) being trained for epoch 4 by 2019-01-26 16:36:11.705119!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0630
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 16:36:40.170278!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0545
17. set (Dataset 19) being trained for epoch 4 by 2019-01-26 16:36:59.155749!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0642
18. set (Dataset 24) being trained for epoch 4 by 2019-01-26 16:37:17.040741!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0513
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 16:37:37.244257!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0702
20. set (Dataset 21) being trained for epoch 4 by 2019-01-26 16:38:02.889262!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1000
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:38:23.728616
1. set (Dataset 24) being trained for epoch 5 by 2019-01-26 16:38:28.404836!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0504
2. set (Dataset 21) being trained for epoch 5 by 2019-01-26 16:38:47.204659!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.0993
3. set (Dataset 22) being trained for epoch 5 by 2019-01-26 16:39:09.963603!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0481
4. set (Dataset 16) being trained for epoch 5 by 2019-01-26 16:39:35.776611!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0608
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 16:40:07.038945!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0644
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 16:40:33.245417!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0684
7. set (Dataset 19) being trained for epoch 5 by 2019-01-26 16:40:57.753317!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0652
8. set (Dataset 18) being trained for epoch 5 by 2019-01-26 16:41:16.413715!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0832
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 16:41:38.847877!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0565
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 16:42:02.026829!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0573
11. set (Dataset 6) being trained for epoch 5 by 2019-01-26 16:42:19.738542!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0863
12. set (Dataset 2) being trained for epoch 5 by 2019-01-26 16:42:38.074033!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0659
13. set (Dataset 15) being trained for epoch 5 by 2019-01-26 16:42:57.453970!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0700
14. set (Dataset 10) being trained for epoch 5 by 2019-01-26 16:43:21.859920!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0617
15. set (Dataset 1) being trained for epoch 5 by 2019-01-26 16:43:45.313149!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0741
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 16:44:03.566733!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0595
17. set (Dataset 4) being trained for epoch 5 by 2019-01-26 16:44:25.092482!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0699
18. set (Dataset 11) being trained for epoch 5 by 2019-01-26 16:44:49.256455!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0489
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 16:45:09.311314!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0905
20. set (Dataset 17) being trained for epoch 5 by 2019-01-26 16:45:27.482368!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0677
Epoch 5 completed!
The subjects are trained: [(24, 'M14'), (21, 'F02'), (22, 'M01'), (16, 'M09'), (7, 'M01'), (8, 'M02'), (19, 'M11'), (18,
 'F05'), (12, 'M06'), (13, 'M07'), (6, 'F06'), (2, 'F02'), (15, 'F03'), (10, 'M04'), (1, 'F01'), (20, 'M12'), (4, 'F04')
, (11, 'M05'), (23, 'M13'), (17, 'M10')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 16:45:39.737107
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 7.24 Degree
        The absolute mean error on Yaw angle estimation: 30.34 Degree
        The absolute mean error on Roll angle estimation: 12.49 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 7.41 Degree
        The absolute mean error on Yaw angle estimation: 25.61 Degree
        The absolute mean error on Roll angle estimation: 4.46 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 31.00 Degree
        The absolute mean error on Yaw angle estimation: 25.10 Degree
        The absolute mean error on Roll angle estimation: 5.99 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.66 Degree
        The absolute mean error on Yaw angle estimation: 29.90 Degree
        The absolute mean error on Roll angle estimation: 14.03 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.83 Degree
        The absolute mean error on Yaw angle estimations: 27.74 Degree
        The absolute mean error on Roll angle estimations: 9.24 Degree
Exp2019-01-26_13-37-43_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 16:47:08.552155
1. set (Dataset 11) being trained for epoch 1 by 2019-01-26 16:47:14.254657!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0502
2. set (Dataset 17) being trained for epoch 1 by 2019-01-26 16:47:31.832847!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0667
3. set (Dataset 6) being trained for epoch 1 by 2019-01-26 16:47:47.276031!
Epoch 1/1
542/542 [==============================] - 15s 27ms/step - loss: 0.0834
4. set (Dataset 1) being trained for epoch 1 by 2019-01-26 16:48:06.961337!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0765
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 16:48:27.476170!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0676
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 16:48:52.670483!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0925
7. set (Dataset 4) being trained for epoch 1 by 2019-01-26 16:49:14.450330!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0708
8. set (Dataset 2) being trained for epoch 1 by 2019-01-26 16:49:38.499024!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0620
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 16:49:59.066549!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0622
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 16:50:24.841588!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0530
11. set (Dataset 19) being trained for epoch 1 by 2019-01-26 16:50:48.635618!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0640
12. set (Dataset 24) being trained for epoch 1 by 2019-01-26 16:51:06.196973!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0513
13. set (Dataset 10) being trained for epoch 1 by 2019-01-26 16:51:26.567616!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0610
14. set (Dataset 21) being trained for epoch 1 by 2019-01-26 16:51:51.187248!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0976
15. set (Dataset 22) being trained for epoch 1 by 2019-01-26 16:52:13.382875!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0476
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 16:52:35.807660!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0576
17. set (Dataset 15) being trained for epoch 1 by 2019-01-26 16:52:56.353703!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0651
18. set (Dataset 16) being trained for epoch 1 by 2019-01-26 16:53:21.266610!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0595
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 16:53:49.635463!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0556
20. set (Dataset 18) being trained for epoch 1 by 2019-01-26 16:54:07.900380!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0792
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:54:27.717076
1. set (Dataset 16) being trained for epoch 2 by 2019-01-26 16:54:36.466012!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0602
2. set (Dataset 18) being trained for epoch 2 by 2019-01-26 16:55:05.585875!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0758
3. set (Dataset 19) being trained for epoch 2 by 2019-01-26 16:55:26.459891!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0646
4. set (Dataset 22) being trained for epoch 2 by 2019-01-26 16:55:45.617379!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0484
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 16:56:07.786053!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0891
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 16:56:26.803923!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0571
7. set (Dataset 15) being trained for epoch 2 by 2019-01-26 16:56:45.429181!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0666
8. set (Dataset 24) being trained for epoch 2 by 2019-01-26 16:57:06.433972!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0505
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 16:57:26.981123!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0671
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 16:57:54.708229!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0654
11. set (Dataset 4) being trained for epoch 2 by 2019-01-26 16:58:20.979987!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0698
12. set (Dataset 11) being trained for epoch 2 by 2019-01-26 16:58:45.827603!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0495
13. set (Dataset 21) being trained for epoch 2 by 2019-01-26 16:59:06.041720!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1027
14. set (Dataset 17) being trained for epoch 2 by 2019-01-26 16:59:25.807133!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0691
15. set (Dataset 6) being trained for epoch 2 by 2019-01-26 16:59:40.558558!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0864
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 16:59:59.471893!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0555
17. set (Dataset 10) being trained for epoch 2 by 2019-01-26 17:00:21.174942!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0642
18. set (Dataset 1) being trained for epoch 2 by 2019-01-26 17:00:44.374979!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0748
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 17:01:04.393295!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0525
20. set (Dataset 2) being trained for epoch 2 by 2019-01-26 17:01:28.438906!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0662
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 17:01:45.893065
1. set (Dataset 1) being trained for epoch 3 by 2019-01-26 17:01:50.934464!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0790
2. set (Dataset 2) being trained for epoch 3 by 2019-01-26 17:02:08.432710!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0616
3. set (Dataset 4) being trained for epoch 3 by 2019-01-26 17:02:28.937703!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0708
4. set (Dataset 6) being trained for epoch 3 by 2019-01-26 17:02:53.247164!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0857
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 17:03:11.613937!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0556
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 17:03:30.970740!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0508
7. set (Dataset 10) being trained for epoch 3 by 2019-01-26 17:03:57.123069!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0589
8. set (Dataset 11) being trained for epoch 3 by 2019-01-26 17:04:20.910822!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0454
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 17:04:40.792617!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0939
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 17:05:03.286982!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0665
11. set (Dataset 15) being trained for epoch 3 by 2019-01-26 17:05:28.773460!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0680
12. set (Dataset 16) being trained for epoch 3 by 2019-01-26 17:05:54.056530!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0593
13. set (Dataset 17) being trained for epoch 3 by 2019-01-26 17:06:20.886557!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0661
14. set (Dataset 18) being trained for epoch 3 by 2019-01-26 17:06:37.021396!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.0780
15. set (Dataset 19) being trained for epoch 3 by 2019-01-26 17:06:56.984316!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0607
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 17:07:15.626451!
Epoch 1/1
556/556 [==============================] - 15s 26ms/step - loss: 0.0531
17. set (Dataset 21) being trained for epoch 3 by 2019-01-26 17:07:36.308932!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0962
18. set (Dataset 22) being trained for epoch 3 by 2019-01-26 17:07:58.608925!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0471
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 17:08:22.820943!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0622
20. set (Dataset 24) being trained for epoch 3 by 2019-01-26 17:08:46.566974!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0531
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 17:09:03.625996
1. set (Dataset 22) being trained for epoch 4 by 2019-01-26 17:09:10.012108!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0468
2. set (Dataset 24) being trained for epoch 4 by 2019-01-26 17:09:31.345373!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0477
3. set (Dataset 15) being trained for epoch 4 by 2019-01-26 17:09:50.125437!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0636
4. set (Dataset 19) being trained for epoch 4 by 2019-01-26 17:10:11.264555!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0626
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 17:10:31.695056!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0550
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 17:10:57.889170!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0639
7. set (Dataset 21) being trained for epoch 4 by 2019-01-26 17:11:23.283327!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1014
8. set (Dataset 16) being trained for epoch 4 by 2019-01-26 17:11:47.768877!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0635
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 17:12:15.977478!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0556
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 17:12:34.164758!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0883
11. set (Dataset 10) being trained for epoch 4 by 2019-01-26 17:12:55.913165!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0617
12. set (Dataset 1) being trained for epoch 4 by 2019-01-26 17:13:19.661703!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0721
13. set (Dataset 18) being trained for epoch 4 by 2019-01-26 17:13:38.715243!
Epoch 1/1
614/614 [==============================] - 17s 28ms/step - loss: 0.0804
14. set (Dataset 2) being trained for epoch 4 by 2019-01-26 17:14:00.879291!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0667
15. set (Dataset 4) being trained for epoch 4 by 2019-01-26 17:14:21.437279!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0708
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 17:14:45.611616!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0558
17. set (Dataset 17) being trained for epoch 4 by 2019-01-26 17:15:03.497338!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0642
18. set (Dataset 6) being trained for epoch 4 by 2019-01-26 17:15:18.858449!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0851
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 17:15:40.787378!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0696
20. set (Dataset 11) being trained for epoch 4 by 2019-01-26 17:16:06.303843!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0479
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 17:16:25.259243
1. set (Dataset 6) being trained for epoch 5 by 2019-01-26 17:16:30.453304!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0802
2. set (Dataset 11) being trained for epoch 5 by 2019-01-26 17:16:49.830887!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0479
3. set (Dataset 10) being trained for epoch 5 by 2019-01-26 17:17:11.553813!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0579
4. set (Dataset 4) being trained for epoch 5 by 2019-01-26 17:17:37.315399!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0682
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 17:18:03.395014!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0616
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 17:18:30.121674!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0645
7. set (Dataset 17) being trained for epoch 5 by 2019-01-26 17:18:53.627234!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0692
8. set (Dataset 1) being trained for epoch 5 by 2019-01-26 17:19:08.876277!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0729
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 17:19:28.352156!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0513
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 17:19:51.285722!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0519
11. set (Dataset 21) being trained for epoch 5 by 2019-01-26 17:20:10.199372!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.0980
12. set (Dataset 22) being trained for epoch 5 by 2019-01-26 17:20:33.342570!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0472
13. set (Dataset 2) being trained for epoch 5 by 2019-01-26 17:20:55.709579!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0634
14. set (Dataset 24) being trained for epoch 5 by 2019-01-26 17:21:13.082804!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0522
15. set (Dataset 15) being trained for epoch 5 by 2019-01-26 17:21:31.715084!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0655
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 17:21:53.275933!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0549
17. set (Dataset 18) being trained for epoch 5 by 2019-01-26 17:22:13.521691!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0795
18. set (Dataset 19) being trained for epoch 5 by 2019-01-26 17:22:34.102147!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0599
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 17:22:51.896655!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0876
20. set (Dataset 16) being trained for epoch 5 by 2019-01-26 17:23:15.005906!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0607
Epoch 5 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (7, 'M01'), (8, 'M02'), (17, 'M10'), (1, 'F
01'), (12, 'M06'), (13, 'M07'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'), (20, 'M12'), (18, 'F05')
, (19, 'M11'), (23, 'M13'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 17:23:40.588003
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.11 Degree
        The absolute mean error on Yaw angle estimation: 28.69 Degree
        The absolute mean error on Roll angle estimation: 16.96 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.79 Degree
        The absolute mean error on Yaw angle estimation: 25.86 Degree
        The absolute mean error on Roll angle estimation: 3.97 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 27.42 Degree
        The absolute mean error on Yaw angle estimation: 26.50 Degree
        The absolute mean error on Roll angle estimation: 6.38 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.56 Degree
        The absolute mean error on Yaw angle estimation: 31.54 Degree
        The absolute mean error on Roll angle estimation: 13.50 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.97 Degree
        The absolute mean error on Yaw angle estimations: 28.15 Degree
        The absolute mean error on Roll angle estimations: 10.20 Degree
Exp2019-01-26_13-37-43_part6 completed!
Exp2019-01-26_13-37-43.h5 has been saved.
subject3_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:05.490411.
subject5_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:05.689277.
subject9_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:05.885095.
subject14_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:06.100548.
Model Exp2019-01-26_13-37-43 has been evaluated successfully.
Model Exp2019-01-26_13-37-43 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-26_13-37-43 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 11:51:35.266816: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 11:51:35.364688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 11:51:35.364997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 11:51:35.365012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 11:51:35.520660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 11:51:35.520686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 11:51:35.520691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 11:51:35.520869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-26_13-37-43.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model Exp2019-01-26_13-37-43_and_2019-01-27_11-51-36
All frames and annotations from 20 datasets have been read by 2019-01-27 11:51:41.303554
1. set (Dataset 22) being trained for epoch 1 by 2019-01-27 11:51:47.702222!
Epoch 1/1
2019-01-27 11:51:48.937122: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_array_ops.cc:121
 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does not exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_tf_
sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in __e
xit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Ten
sorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3 = Ten
sorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorArrayReadV3/Enter"], sour
ce="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"](training/Adam/gradients/dropout025
/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, training/Adam/gradients/dropout025/while/TensorAr
rayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^training/Adam/gradients/Sub_2/_619)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 48, i
n trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 39, i
n trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_outputs, batch_
size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 29, i
n trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_generator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482, in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Ten
sorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3 = Ten
sorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorArrayReadV3/Enter"], sour
ce="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"](training/Adam/gradients/dropout025
/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, training/Adam/gradients/dropout025/while/TensorAr
rayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^training/Adam/gradients/Sub_2/_619)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3', define
d at:
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.py", line 52,
 in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_train_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519, in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 488, in gradie
nts
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 625, in _Gradi
entsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 379, in _Maybe
Compile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 625, in <lambd
a>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line 104, in _Te
nsorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line 849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line 241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 6221, in te
nsor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in
_apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, in create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.py", line 52,
 in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserialize
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deserialize_keras
_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771, in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 3202, in whi
le_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2940, in Bui
ldLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line 2877, in _Bu
ildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754, in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line 861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does
 not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3 = Ten
sorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorArrayReadV3/Enter"], sour
ce="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"](training/Adam/gradients/dropout025
/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, training/Adam/gradients/dropout025/while/TensorAr
rayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^training/Adam/gradients/Sub_2/_619)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 11:56:53.344926: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 11:56:53.442347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 11:56:53.442603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 11:56:53.442616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 11:56:53.597546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 11:56:53.597572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 11:56:53.597577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 11:56:53.597719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_11-56-54 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-27_11-56-54
All frames and annotations from 1 datasets have been read by 2019-01-27 11:56:55.212496
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 11:57:04.107940!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 561.1528 - mean_absolute_error: 18.3496
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 11:57:29.215899
1. set (Dataset 9) being trained for epoch 2 by 2019-01-27 11:57:38.100679!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 552.5623 - mean_absolute_error: 18.1862
Epoch 2 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 11:58:01.075357
1. set (Dataset 9) being trained for epoch 3 by 2019-01-27 11:58:09.943610!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 546.9199 - mean_absolute_error: 18.0749
Epoch 3 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 11:58:33.073716
1. set (Dataset 9) being trained for epoch 4 by 2019-01-27 11:58:41.959666!
Epoch 1/1
303/882 [=========>....................] - ETA: 14s - loss: 534.0697 - mean_absolute_error: 17.7895^C
Model Exp2019-01-27_11-56-54_part1 has been interrupted.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 11:59:17.787248: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 11:59:17.885464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 11:59:17.885726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 11:59:17.885738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 11:59:18.041228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 11:59:18.041254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 11:59:18.041259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 11:59:18.041394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_11-59-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-27_11-59-18
All frames and annotations from 1 datasets have been read by 2019-01-27 11:59:19.688533
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 11:59:28.548652!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 561.1989 - mean_absolute_error: 18.3361
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-27_11-59-18
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 11:59:53.249342
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 1848.72 Degree
        The absolute mean error on Yaw angle estimation: 2441.66 Degree
        The absolute mean error on Roll angle estimation: 637.17 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 1848.72 Degree
        The absolute mean error on Yaw angle estimations: 2441.66 Degree
        The absolute mean error on Roll angle estimations: 637.17 Degree
Exp2019-01-27_11-59-18_part1 completed!
Exp2019-01-27_11-59-18.h5 has been saved.
subject9_Exp2019-01-27_11-59-18.png has been saved by 2019-01-27 12:00:15.977069.
Model Exp2019-01-27_11-59-18 has been evaluated successfully.
Model Exp2019-01-27_11-59-18 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:14:24.593442: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:14:24.689153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:14:24.689455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:14:24.689470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:14:24.844834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:14:24.844858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:14:24.844863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:14:24.845042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-14-25 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-27_13-14-25
All frames and annotations from 1 datasets have been read by 2019-01-27 13:14:26.511408
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:14:35.402054!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 571.6917 - mean_absolute_error: 18.6066
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-27_13-14-25
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:15:01.310412
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 20.74 Degree
        The absolute mean error on Yaw angle estimation: 27.23 Degree
        The absolute mean error on Roll angle estimation: 7.47 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.74 Degree
        The absolute mean error on Yaw angle estimations: 27.23 Degree
        The absolute mean error on Roll angle estimations: 7.47 Degree
Exp2019-01-27_13-14-25_part1 completed!
Exp2019-01-27_13-14-25.h5 has been saved.
subject9_Exp2019-01-27_13-14-25.png has been saved by 2019-01-27 13:15:24.025377.
Model Exp2019-01-27_13-14-25 has been evaluated successfully.
Model Exp2019-01-27_13-14-25 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:25:49.065244: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:25:49.161134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:25:49.161393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:25:49.161412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:25:49.317764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:25:49.317791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:25:49.317796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:25:49.317934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-25-50 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-27_13-25-50
All frames and annotations from 1 datasets have been read by 2019-01-27 13:25:50.872564
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:25:59.772653!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 568.6984 - mean_absolute_error: 18.4823
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-27_13-25-50
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:26:18.258004
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 20.06 Degree
        The absolute mean error on Yaw angle estimation: 27.29 Degree
        The absolute mean error on Roll angle estimation: 7.17 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.06 Degree
        The absolute mean error on Yaw angle estimations: 27.29 Degree
        The absolute mean error on Roll angle estimations: 7.17 Degree
Exp2019-01-27_13-25-50_part1 completed!
Exp2019-01-27_13-25-50.h5 has been saved.
subject9_Exp2019-01-27_13-25-50.png has been saved by 2019-01-27 13:26:35.981633.
Model Exp2019-01-27_13-25-50 has been evaluated successfully.
Model Exp2019-01-27_13-25-50 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:35:43.280212: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:35:43.378279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:35:43.378539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:35:43.378554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:35:43.533794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:35:43.533820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:35:43.533828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:35:43.533967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-35-44 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.001000_2019-01
-27_13-35-44
All frames and annotations from 1 datasets have been read by 2019-01-27 13:35:45.097587
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:35:53.993757!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2081
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.001000_2019-
01-27_13-35-44
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:36:12.285508
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.20 Degree
        The absolute mean error on Yaw angle estimation: 34.32 Degree
        The absolute mean error on Roll angle estimation: 10.73 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.20 Degree
        The absolute mean error on Yaw angle estimations: 34.32 Degree
        The absolute mean error on Roll angle estimations: 10.73 Degree
Exp2019-01-27_13-35-44_part1 completed!
Exp2019-01-27_13-35-44.h5 has been saved.
subject9_Exp2019-01-27_13-35-44.png has been saved by 2019-01-27 13:36:30.003511.
Model Exp2019-01-27_13-35-44 has been evaluated successfully.
Model Exp2019-01-27_13-35-44 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:36:51.400187: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:36:51.480776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:36:51.481041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:36:51.481056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:36:51.636542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:36:51.636570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:36:51.636579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:36:51.636720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-36-52 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.01
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.010000_2019-01
-27_13-36-52
All frames and annotations from 1 datasets have been read by 2019-01-27 13:36:53.171766
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:37:02.060342!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2109
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.010000_2019-
01-27_13-36-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:37:20.526908
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.27 Degree
        The absolute mean error on Yaw angle estimation: 26.28 Degree
        The absolute mean error on Roll angle estimation: 7.14 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 18.27 Degree
        The absolute mean error on Yaw angle estimations: 26.28 Degree
        The absolute mean error on Roll angle estimations: 7.14 Degree
Exp2019-01-27_13-36-52_part1 completed!
Exp2019-01-27_13-36-52.h5 has been saved.
subject9_Exp2019-01-27_13-36-52.png has been saved by 2019-01-27 13:37:38.307345.
Model Exp2019-01-27_13-36-52 has been evaluated successfully.
Model Exp2019-01-27_13-36-52 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:39:31.969676: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:39:32.067036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:39:32.067313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:39:32.067325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:39:32.222315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:39:32.222340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:39:32.222345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:39:32.222488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-39-33 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-27_13-39-33
All frames and annotations from 1 datasets have been read by 2019-01-27 13:39:33.866209
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:39:42.750884!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.3898
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-27_13-39-33
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:40:07.461262
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.67 Degree
        The absolute mean error on Yaw angle estimation: 33.01 Degree
        The absolute mean error on Roll angle estimation: 11.71 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.67 Degree
        The absolute mean error on Yaw angle estimations: 33.01 Degree
        The absolute mean error on Roll angle estimations: 11.71 Degree
Exp2019-01-27_13-39-33_part1 completed!
Exp2019-01-27_13-39-33.h5 has been saved.
subject9_Exp2019-01-27_13-39-33.png has been saved by 2019-01-27 13:40:30.273942.
Model Exp2019-01-27_13-39-33 has been evaluated successfully.
Model Exp2019-01-27_13-39-33 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:41:29.649761: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:41:29.748526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:41:29.748787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:41:29.748804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:41:29.904282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:41:29.904306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:41:29.904311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:41:29.904456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-41-30 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000010_2019-01
-27_13-41-30
All frames and annotations from 1 datasets have been read by 2019-01-27 13:41:31.534768
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:41:40.407342!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2185
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000010_2019-
01-27_13-41-30
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:42:05.188256
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.48 Degree
        The absolute mean error on Yaw angle estimation: 19.37 Degree
        The absolute mean error on Roll angle estimation: 11.52 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.48 Degree
        The absolute mean error on Yaw angle estimations: 19.37 Degree
        The absolute mean error on Roll angle estimations: 11.52 Degree
Exp2019-01-27_13-41-30_part1 completed!
Exp2019-01-27_13-41-30.h5 has been saved.
subject9_Exp2019-01-27_13-41-30.png has been saved by 2019-01-27 13:42:27.969351.
Model Exp2019-01-27_13-41-30 has been evaluated successfully.
Model Exp2019-01-27_13-41-30 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:43:19.044773: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:43:19.143056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:43:19.143316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:43:19.143334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:43:19.298473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:43:19.298499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:43:19.298504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:43:19.298643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-43-20 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.000001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000001_2019-01
-27_13-43-20
All frames and annotations from 1 datasets have been read by 2019-01-27 13:43:20.995841
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:43:29.888634!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.3066
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000001_2019-
01-27_13-43-20
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:43:55.435424
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.65 Degree
        The absolute mean error on Yaw angle estimation: 58.86 Degree
        The absolute mean error on Roll angle estimation: 13.83 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 16.65 Degree
        The absolute mean error on Yaw angle estimations: 58.86 Degree
        The absolute mean error on Roll angle estimations: 13.83 Degree
Exp2019-01-27_13-43-20_part1 completed!
Exp2019-01-27_13-43-20.h5 has been saved.
subject9_Exp2019-01-27_13-43-20.png has been saved by 2019-01-27 13:44:18.221699.
Model Exp2019-01-27_13-43-20 has been evaluated successfully.
Model Exp2019-01-27_13-43-20 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:44:41.602901: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:44:41.682597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:44:41.682852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:44:41.682865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:44:41.837607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:44:41.837631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:44:41.837636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:44:41.837777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-44-42 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-27_13-44-42
All frames and annotations from 1 datasets have been read by 2019-01-27 13:44:43.446538
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:44:52.329254!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2836
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-27_13-44-42
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:45:16.772193
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 13.25 Degree
        The absolute mean error on Yaw angle estimation: 22.60 Degree
        The absolute mean error on Roll angle estimation: 8.22 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 13.25 Degree
        The absolute mean error on Yaw angle estimations: 22.60 Degree
        The absolute mean error on Roll angle estimations: 8.22 Degree
Exp2019-01-27_13-44-42_part1 completed!
Exp2019-01-27_13-44-42.h5 has been saved.
subject9_Exp2019-01-27_13-44-42.png has been saved by 2019-01-27 13:45:39.565177.
Model Exp2019-01-27_13-44-42 has been evaluated successfully.
Model Exp2019-01-27_13-44-42 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:46:18.055774: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:46:18.153497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:46:18.153762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:46:18.153775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:46:18.309810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:46:18.309837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:46:18.309842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:46:18.309980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-46-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-27_13-46-18
All frames and annotations from 1 datasets have been read by 2019-01-27 13:46:19.828002
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:46:28.727202!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1927
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-27_13-46-18
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:46:47.092500
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.64 Degree
        The absolute mean error on Yaw angle estimation: 19.06 Degree
        The absolute mean error on Roll angle estimation: 10.65 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.64 Degree
        The absolute mean error on Yaw angle estimations: 19.06 Degree
        The absolute mean error on Roll angle estimations: 10.65 Degree
Exp2019-01-27_13-46-18_part1 completed!
Exp2019-01-27_13-46-18.h5 has been saved.
subject9_Exp2019-01-27_13-46-18.png has been saved by 2019-01-27 13:47:04.915175.
Model Exp2019-01-27_13-46-18 has been evaluated successfully.
Model Exp2019-01-27_13-46-18 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:54:21.987780: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:54:22.084073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:54:22.084330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:54:22.084342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:54:22.239653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:54:22.239680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:54:22.239685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:54:22.239831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-54-23 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-01
-27_13-54-23
All frames and annotations from 1 datasets have been read by 2019-01-27 13:54:23.874366
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:54:32.746147!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 0.2940
Epoch 1 completed!
Exp2019-01-27_13-54-23_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000100_2019-
01-27_13-54-23
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:54:58.244899
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 32.75 Degree
        The absolute mean error on Yaw angle estimation: 47.04 Degree
        The absolute mean error on Roll angle estimation: 7.60 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 32.75 Degree
        The absolute mean error on Yaw angle estimations: 47.04 Degree
        The absolute mean error on Roll angle estimations: 7.60 Degree
Exp2019-01-27_13-54-23_part1 completed!
Exp2019-01-27_13-54-23.h5 has been saved.
subject9_Exp2019-01-27_13-54-23.png has been saved by 2019-01-27 13:55:20.896750.
Model Exp2019-01-27_13-54-23 has been evaluated successfully.
Model Exp2019-01-27_13-54-23 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:55:58.877868: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:55:58.975696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:55:58.975951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:55:58.975964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:55:59.131706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:55:59.131732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:55:59.131737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:55:59.131875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-55-59 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000010_2019-01
-27_13-55-59
All frames and annotations from 1 datasets have been read by 2019-01-27 13:56:00.769742
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:56:09.636566!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2676
Epoch 1 completed!
Exp2019-01-27_13-55-59_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.000010_2019-
01-27_13-55-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:56:35.010867
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 32.57 Degree
        The absolute mean error on Yaw angle estimation: 22.06 Degree
        The absolute mean error on Roll angle estimation: 20.35 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 32.57 Degree
        The absolute mean error on Yaw angle estimations: 22.06 Degree
        The absolute mean error on Roll angle estimations: 20.35 Degree
Exp2019-01-27_13-55-59_part1 completed!
Exp2019-01-27_13-55-59.h5 has been saved.
subject9_Exp2019-01-27_13-55-59.png has been saved by 2019-01-27 13:56:57.626130.
Model Exp2019-01-27_13-55-59 has been evaluated successfully.
Model Exp2019-01-27_13-55-59 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:57:55.885697: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:57:55.983644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 13:57:55.983905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:57:55.983917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 13:57:56.138800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 13:57:56.138826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:57:56.138831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:57:56.138966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-57-56 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 10
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019-0
1-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 13:57:57.775817
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:58:06.669573!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2257
Epoch 1 completed!
Exp2019-01-27_13-57-56_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019
-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:58:32.017535
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.54 Degree
        The absolute mean error on Yaw angle estimation: 23.62 Degree
        The absolute mean error on Roll angle estimation: 14.57 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 12.54 Degree
        The absolute mean error on Yaw angle estimations: 23.62 Degree
        The absolute mean error on Roll angle estimations: 14.57 Degree
Exp2019-01-27_13-57-56_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019-0
1-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 13:58:54.976084
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:59:03.779226!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.2100
Epoch 1 completed!
Exp2019-01-27_13-57-56_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019
-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:59:26.953275
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.54 Degree
        The absolute mean error on Yaw angle estimation: 21.65 Degree
        The absolute mean error on Roll angle estimation: 15.12 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.54 Degree
        The absolute mean error on Yaw angle estimations: 21.65 Degree
        The absolute mean error on Roll angle estimations: 15.12 Degree
Exp2019-01-27_13-57-56_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019-0
1-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 13:59:49.888616
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:59:58.688990!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.2033
Epoch 1 completed!
Exp2019-01-27_13-57-56_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019
-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:00:22.263114
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.26 Degree
        The absolute mean error on Yaw angle estimation: 20.37 Degree
        The absolute mean error on Roll angle estimation: 13.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.26 Degree
        The absolute mean error on Yaw angle estimations: 20.37 Degree
        The absolute mean error on Roll angle estimations: 13.37 Degree
Exp2019-01-27_13-57-56_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019-0
1-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:00:45.154755
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:00:54.002816!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1953
Epoch 1 completed!
Exp2019-01-27_13-57-56_part4.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019
-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:01:16.972010
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.83 Degree
        The absolute mean error on Yaw angle estimation: 19.05 Degree
        The absolute mean error on Roll angle estimation: 9.71 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.83 Degree
        The absolute mean error on Yaw angle estimations: 19.05 Degree
        The absolute mean error on Roll angle estimations: 9.71 Degree
Exp2019-01-27_13-57-56_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019-0
1-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:01:39.820111
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:01:48.623665!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1938
Epoch 1 completed!
Exp2019-01-27_13-57-56_part5.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019
-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:02:11.496574
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.23 Degree
        The absolute mean error on Yaw angle estimation: 20.05 Degree
        The absolute mean error on Roll angle estimation: 7.67 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 12.23 Degree
        The absolute mean error on Yaw angle estimations: 20.05 Degree
        The absolute mean error on Roll angle estimations: 7.67 Degree
Exp2019-01-27_13-57-56_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019-0
1-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:02:34.365482
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:02:43.210671!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1874
Epoch 1 completed!
Exp2019-01-27_13-57-56_part6.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019
-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:03:06.544020
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.95 Degree
        The absolute mean error on Yaw angle estimation: 17.27 Degree
        The absolute mean error on Roll angle estimation: 7.98 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.95 Degree
        The absolute mean error on Yaw angle estimations: 17.27 Degree
        The absolute mean error on Roll angle estimations: 7.98 Degree
Exp2019-01-27_13-57-56_part6 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019-0
1-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:03:29.498885
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:03:38.322744!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1814
Epoch 1 completed!
Exp2019-01-27_13-57-56_part7.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019
-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:04:01.518257
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.15 Degree
        The absolute mean error on Yaw angle estimation: 17.84 Degree
        The absolute mean error on Roll angle estimation: 14.80 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 12.15 Degree
        The absolute mean error on Yaw angle estimations: 17.84 Degree
        The absolute mean error on Roll angle estimations: 14.80 Degree
Exp2019-01-27_13-57-56_part7 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019-0
1-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:04:24.432853
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:04:33.233569!
Epoch 1/1
882/882 [==============================] - 21s 24ms/step - loss: 0.1780
Epoch 1 completed!
Exp2019-01-27_13-57-56_part8.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019
-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:04:55.710285
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.23 Degree
        The absolute mean error on Yaw angle estimation: 17.05 Degree
        The absolute mean error on Roll angle estimation: 5.60 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.23 Degree
        The absolute mean error on Yaw angle estimations: 17.05 Degree
        The absolute mean error on Roll angle estimations: 5.60 Degree
Exp2019-01-27_13-57-56_part8 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019-0
1-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:05:18.610510
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:05:27.420508!
Epoch 1/1
882/882 [==============================] - 21s 24ms/step - loss: 0.1765
Epoch 1 completed!
Exp2019-01-27_13-57-56_part9.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019
-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:05:50.080239
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.06 Degree
        The absolute mean error on Yaw angle estimation: 16.22 Degree
        The absolute mean error on Roll angle estimation: 10.89 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.06 Degree
        The absolute mean error on Yaw angle estimations: 16.22 Degree
        The absolute mean error on Roll angle estimations: 10.89 Degree
Exp2019-01-27_13-57-56_part9 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019-0
1-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:06:12.968372
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:06:21.765093!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1763
Epoch 1 completed!
Exp2019-01-27_13-57-56_part10.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000010_2019
-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:06:44.596901
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.00 Degree
        The absolute mean error on Yaw angle estimation: 15.07 Degree
        The absolute mean error on Roll angle estimation: 8.79 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.00 Degree
        The absolute mean error on Yaw angle estimations: 15.07 Degree
        The absolute mean error on Roll angle estimations: 8.79 Degree
Exp2019-01-27_13-57-56_part10 completed!
Exp2019-01-27_13-57-56.h5 has been saved.
subject9_Exp2019-01-27_13-57-56.png has been saved by 2019-01-27 14:07:07.085305.
Model Exp2019-01-27_13-57-56 has been evaluated successfully.
Model Exp2019-01-27_13-57-56 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:12:08.064165: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:12:08.161692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 14:12:08.161949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:12:08.161962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 14:12:08.317579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 14:12:08.317603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:12:08.317607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:12:08.317750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-12-09 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 10
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:12:09.868808
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:12:18.758786!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1528
Epoch 1 completed!
Exp2019-01-27_14-12-09_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:12:37.055194
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.56 Degree
        The absolute mean error on Yaw angle estimation: 11.10 Degree
        The absolute mean error on Roll angle estimation: 3.78 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.56 Degree
        The absolute mean error on Yaw angle estimations: 11.10 Degree
        The absolute mean error on Roll angle estimations: 3.78 Degree
Exp2019-01-27_14-12-09_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:12:55.111877
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:13:03.981355!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0974
Epoch 1 completed!
Exp2019-01-27_14-12-09_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:13:20.712950
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.91 Degree
        The absolute mean error on Yaw angle estimation: 29.45 Degree
        The absolute mean error on Roll angle estimation: 4.58 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.91 Degree
        The absolute mean error on Yaw angle estimations: 29.45 Degree
        The absolute mean error on Roll angle estimations: 4.58 Degree
Exp2019-01-27_14-12-09_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:13:38.719507
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:13:47.612435!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0982
Epoch 1 completed!
Exp2019-01-27_14-12-09_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:14:04.328626
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.08 Degree
        The absolute mean error on Yaw angle estimation: 6.44 Degree
        The absolute mean error on Roll angle estimation: 3.11 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 4.08 Degree
        The absolute mean error on Yaw angle estimations: 6.44 Degree
        The absolute mean error on Roll angle estimations: 3.11 Degree
Exp2019-01-27_14-12-09_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:14:22.341498
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:14:31.227592!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 0.0778
Epoch 1 completed!
Exp2019-01-27_14-12-09_part4.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:14:47.746107
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.09 Degree
        The absolute mean error on Yaw angle estimation: 4.83 Degree
        The absolute mean error on Roll angle estimation: 2.56 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.09 Degree
        The absolute mean error on Yaw angle estimations: 4.83 Degree
        The absolute mean error on Roll angle estimations: 2.56 Degree
Exp2019-01-27_14-12-09_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:15:05.793326
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:15:14.667683!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 0.0697
Epoch 1 completed!
Exp2019-01-27_14-12-09_part5.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:15:31.070599
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.07 Degree
        The absolute mean error on Yaw angle estimation: 4.55 Degree
        The absolute mean error on Roll angle estimation: 2.26 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 4.07 Degree
        The absolute mean error on Yaw angle estimations: 4.55 Degree
        The absolute mean error on Roll angle estimations: 2.26 Degree
Exp2019-01-27_14-12-09_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:15:49.091542
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:15:57.967979!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0669
Epoch 1 completed!
Exp2019-01-27_14-12-09_part6.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:16:14.901958
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.64 Degree
        The absolute mean error on Yaw angle estimation: 4.86 Degree
        The absolute mean error on Roll angle estimation: 1.93 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.64 Degree
        The absolute mean error on Yaw angle estimations: 4.86 Degree
        The absolute mean error on Roll angle estimations: 1.93 Degree
Exp2019-01-27_14-12-09_part6 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:16:32.937169
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:16:41.816754!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0619
Epoch 1 completed!
Exp2019-01-27_14-12-09_part7.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:16:58.759968
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.21 Degree
        The absolute mean error on Yaw angle estimation: 6.93 Degree
        The absolute mean error on Roll angle estimation: 2.26 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.21 Degree
        The absolute mean error on Yaw angle estimations: 6.93 Degree
        The absolute mean error on Roll angle estimations: 2.26 Degree
Exp2019-01-27_14-12-09_part7 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:17:16.815489
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:17:25.690155!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0575
Epoch 1 completed!
Exp2019-01-27_14-12-09_part8.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:17:42.661223
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.90 Degree
        The absolute mean error on Yaw angle estimation: 3.49 Degree
        The absolute mean error on Roll angle estimation: 2.17 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.90 Degree
        The absolute mean error on Yaw angle estimations: 3.49 Degree
        The absolute mean error on Roll angle estimations: 2.17 Degree
Exp2019-01-27_14-12-09_part8 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:18:00.695985
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:18:09.561088!
Epoch 1/1
882/882 [==============================] - 15s 18ms/step - loss: 0.0564
Epoch 1 completed!
Exp2019-01-27_14-12-09_part9.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:18:26.137622
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.13 Degree
        The absolute mean error on Yaw angle estimation: 3.93 Degree
        The absolute mean error on Roll angle estimation: 2.71 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.13 Degree
        The absolute mean error on Yaw angle estimations: 3.93 Degree
        The absolute mean error on Roll angle estimations: 2.71 Degree
Exp2019-01-27_14-12-09_part9 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:18:44.216116
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:18:53.085128!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0537
Epoch 1 completed!
Exp2019-01-27_14-12-09_part10.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:19:10.244000
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.78 Degree
        The absolute mean error on Yaw angle estimation: 6.33 Degree
        The absolute mean error on Roll angle estimation: 1.94 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.78 Degree
        The absolute mean error on Yaw angle estimations: 6.33 Degree
        The absolute mean error on Roll angle estimations: 1.94 Degree
Exp2019-01-27_14-12-09_part10 completed!
Exp2019-01-27_14-12-09.h5 has been saved.
subject9_Exp2019-01-27_14-12-09.png has been saved by 2019-01-27 14:19:27.955675.
Model Exp2019-01-27_14-12-09 has been evaluated successfully.
Model Exp2019-01-27_14-12-09 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:22:50.513479: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:22:50.650116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 14:22:50.650383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:22:50.650398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 14:22:50.805215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 14:22:50.805239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:22:50.805244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:22:50.805379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-22-51 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 10
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:22:52.336614
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:23:01.233406!
Epoch 1/1
882/882 [==============================] - 17s 20ms/step - loss: 0.1814
Epoch 1 completed!
Exp2019-01-27_14-22-51_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:23:20.524612
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.54 Degree
        The absolute mean error on Yaw angle estimation: 24.20 Degree
        The absolute mean error on Roll angle estimation: 7.94 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.54 Degree
        The absolute mean error on Yaw angle estimations: 24.20 Degree
        The absolute mean error on Roll angle estimations: 7.94 Degree
Exp2019-01-27_14-22-51_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:23:38.644955
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:23:47.535765!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1530
Epoch 1 completed!
Exp2019-01-27_14-22-51_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:24:04.341496
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.36 Degree
        The absolute mean error on Yaw angle estimation: 11.74 Degree
        The absolute mean error on Roll angle estimation: 8.61 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.36 Degree
        The absolute mean error on Yaw angle estimations: 11.74 Degree
        The absolute mean error on Roll angle estimations: 8.61 Degree
Exp2019-01-27_14-22-51_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:24:22.336096
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:24:31.224476!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1379
Epoch 1 completed!
Exp2019-01-27_14-22-51_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:24:48.042813
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.13 Degree
        The absolute mean error on Yaw angle estimation: 14.27 Degree
        The absolute mean error on Roll angle estimation: 5.87 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.13 Degree
        The absolute mean error on Yaw angle estimations: 14.27 Degree
        The absolute mean error on Roll angle estimations: 5.87 Degree
Exp2019-01-27_14-22-51_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:25:06.099666
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:25:14.967599!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1240
Epoch 1 completed!
Exp2019-01-27_14-22-51_part4.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:25:31.871593
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.11 Degree
        The absolute mean error on Yaw angle estimation: 14.73 Degree
        The absolute mean error on Roll angle estimation: 5.53 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.11 Degree
        The absolute mean error on Yaw angle estimations: 14.73 Degree
        The absolute mean error on Roll angle estimations: 5.53 Degree
Exp2019-01-27_14-22-51_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:25:49.948082
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:25:58.877512!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1127
Epoch 1 completed!
Exp2019-01-27_14-22-51_part5.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:26:15.707754
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 13.96 Degree
        The absolute mean error on Yaw angle estimation: 13.96 Degree
        The absolute mean error on Roll angle estimation: 9.96 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 13.96 Degree
        The absolute mean error on Yaw angle estimations: 13.96 Degree
        The absolute mean error on Roll angle estimations: 9.96 Degree
Exp2019-01-27_14-22-51_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:26:33.713023
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:26:42.584351!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1134
Epoch 1 completed!
Exp2019-01-27_14-22-51_part6.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:26:59.591930
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.31 Degree
        The absolute mean error on Yaw angle estimation: 13.77 Degree
        The absolute mean error on Roll angle estimation: 5.05 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 8.31 Degree
        The absolute mean error on Yaw angle estimations: 13.77 Degree
        The absolute mean error on Roll angle estimations: 5.05 Degree
Exp2019-01-27_14-22-51_part6 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:27:17.610320
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:27:26.491399!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1022
Epoch 1 completed!
Exp2019-01-27_14-22-51_part7.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:27:43.732378
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.73 Degree
        The absolute mean error on Yaw angle estimation: 12.42 Degree
        The absolute mean error on Roll angle estimation: 4.29 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.73 Degree
        The absolute mean error on Yaw angle estimations: 12.42 Degree
        The absolute mean error on Roll angle estimations: 4.29 Degree
Exp2019-01-27_14-22-51_part7 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:28:01.799529
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:28:10.669395!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1000
Epoch 1 completed!
Exp2019-01-27_14-22-51_part8.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:28:28.011063
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 11.97 Degree
        The absolute mean error on Yaw angle estimation: 10.27 Degree
        The absolute mean error on Roll angle estimation: 6.08 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.97 Degree
        The absolute mean error on Yaw angle estimations: 10.27 Degree
        The absolute mean error on Roll angle estimations: 6.08 Degree
Exp2019-01-27_14-22-51_part8 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:28:46.030915
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:28:54.916290!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0954
Epoch 1 completed!
Exp2019-01-27_14-22-51_part9.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:29:12.107028
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.79 Degree
        The absolute mean error on Yaw angle estimation: 12.49 Degree
        The absolute mean error on Roll angle estimation: 5.46 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.79 Degree
        The absolute mean error on Yaw angle estimations: 12.49 Degree
        The absolute mean error on Roll angle estimations: 5.46 Degree
Exp2019-01-27_14-22-51_part9 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019-0
1-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:29:30.425215
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:29:39.305690!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0940
Epoch 1 completed!
Exp2019-01-27_14-22-51_part10.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.000100_2019
-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:29:56.096427
^[[B
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 11.99 Degree
        The absolute mean error on Yaw angle estimation: 11.38 Degree
        The absolute mean error on Roll angle estimation: 6.25 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.99 Degree
        The absolute mean error on Yaw angle estimations: 11.38 Degree
        The absolute mean error on Roll angle estimations: 6.25 Degree
Exp2019-01-27_14-22-51_part10 completed!
Exp2019-01-27_14-22-51.h5 has been saved.
subject9_Exp2019-01-27_14-22-51.png has been saved by 2019-01-27 14:30:13.810628.
Model Exp2019-01-27_14-22-51 has been evaluated successfully.
Model Exp2019-01-27_14-22-51 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:36:59.292172: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:36:59.389454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 14:36:59.389717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:36:59.389730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 14:36:59.545699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 14:36:59.545725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:36:59.545730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:36:59.545869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-37-00 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_14-37-00
All frames and annotations from 1 datasets have been read by 2019-01-27 14:37:01.097217
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:37:10.135526!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1785
Epoch 1 completed!
Exp2019-01-27_14-37-00_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_14-37-00
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:37:28.701735
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 13.42 Degree
        The absolute mean error on Yaw angle estimation: 25.46 Degree
        The absolute mean error on Roll angle estimation: 6.02 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 13.42 Degree
        The absolute mean error on Yaw angle estimations: 25.46 Degree
        The absolute mean error on Roll angle estimations: 6.02 Degree
Exp2019-01-27_14-37-00_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_14-37-00
All frames and annotations from 1 datasets have been read by 2019-01-27 14:37:46.723326
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:37:55.599698!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1463
Epoch 1 completed!
Exp2019-01-27_14-37-00_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_14-37-00
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:38:12.734572
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.80 Degree
        The absolute mean error on Yaw angle estimation: 20.47 Degree
        The absolute mean error on Roll angle estimation: 4.20 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 8.80 Degree
        The absolute mean error on Yaw angle estimations: 20.47 Degree
        The absolute mean error on Roll angle estimations: 4.20 Degree
Exp2019-01-27_14-37-00_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_14-37-00
All frames and annotations from 1 datasets have been read by 2019-01-27 14:38:30.715357
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:38:39.589163!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1358
Epoch 1 completed!
Exp2019-01-27_14-37-00_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_14-37-00
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:38:56.811656
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 9.34 Degree
        The absolute mean error on Yaw angle estimation: 15.15 Degree
        The absolute mean error on Roll angle estimation: 3.78 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 9.34 Degree
        The absolute mean error on Yaw angle estimations: 15.15 Degree
        The absolute mean error on Roll angle estimations: 3.78 Degree
Exp2019-01-27_14-37-00_part3 completed!
Exp2019-01-27_14-37-00.h5 has been saved.
subject9_Exp2019-01-27_14-37-00.png has been saved by 2019-01-27 14:39:14.497177.
Model Exp2019-01-27_14-37-00 has been evaluated successfully.
Model Exp2019-01-27_14-37-00 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:44:54.499002: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:44:54.595898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 14:44:54.596158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:44:54.596174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 14:44:54.750996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 14:44:54.751022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:44:54.751027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:44:54.751169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-44-55 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_14-44-55
All frames and annotations from 1 datasets have been read by 2019-01-27 14:44:56.326835
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:45:05.231495!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 18.0210
Epoch 1 completed!
Exp2019-01-27_14-44-55_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_14-44-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:45:23.834406
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.95 Degree
        The absolute mean error on Yaw angle estimation: 27.62 Degree
        The absolute mean error on Roll angle estimation: 6.91 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.95 Degree
        The absolute mean error on Yaw angle estimations: 27.62 Degree
        The absolute mean error on Roll angle estimations: 6.91 Degree
Exp2019-01-27_14-44-55_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_14-44-55
All frames and annotations from 1 datasets have been read by 2019-01-27 14:45:41.911863
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:45:50.815900!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 15.1474
Epoch 1 completed!
Exp2019-01-27_14-44-55_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_14-44-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:46:07.944194
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.33 Degree
        The absolute mean error on Yaw angle estimation: 51.37 Degree
        The absolute mean error on Roll angle estimation: 11.31 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.33 Degree
        The absolute mean error on Yaw angle estimations: 51.37 Degree
        The absolute mean error on Roll angle estimations: 11.31 Degree
Exp2019-01-27_14-44-55_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_14-44-55
All frames and annotations from 1 datasets have been read by 2019-01-27 14:46:25.939242
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:46:34.826481!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 14.7312
Epoch 1 completed!
Exp2019-01-27_14-44-55_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_14-44-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:46:51.759112
7For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.61 Degree
        The absolute mean error on Yaw angle estimation: 24.38 Degree
        The absolute mean error on Roll angle estimation: 9.90 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.61 Degree
        The absolute mean error on Yaw angle estimations: 24.38 Degree
        The absolute mean error on Roll angle estimations: 9.90 Degree
Exp2019-01-27_14-44-55_part3 completed!
Exp2019-01-27_14-44-55.h5 has been saved.
subject9_Exp2019-01-27_14-44-55.png has been saved by 2019-01-27 14:47:09.458016.
Model Exp2019-01-27_14-44-55 has been evaluated successfully.
Model Exp2019-01-27_14-44-55 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:06:26.706710: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:06:26.804736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:06:26.805046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:06:26.805060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:06:26.960746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:06:26.960773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:06:26.960778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:06:26.960958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-06-27 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-06-27
All frames and annotations from 1 datasets have been read by 2019-01-27 17:06:28.499394
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:06:37.386609!
Epoch 1/1
882/882 [==============================] - 16s 19ms/step - loss: 0.2803
Epoch 1 completed!
Exp2019-01-27_17-06-27_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-06-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:06:55.392619
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.37 Degree
        The absolute mean error on Yaw angle estimation: 27.08 Degree
        The absolute mean error on Roll angle estimation: 7.21 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 18.37 Degree
        The absolute mean error on Yaw angle estimations: 27.08 Degree
        The absolute mean error on Roll angle estimations: 7.21 Degree
Exp2019-01-27_17-06-27_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-06-27
All frames and annotations from 1 datasets have been read by 2019-01-27 17:07:13.549917
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:07:22.460788!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1686
Epoch 1 completed!
Exp2019-01-27_17-06-27_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-06-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:07:39.313559
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.90 Degree
        The absolute mean error on Yaw angle estimation: 26.35 Degree
        The absolute mean error on Roll angle estimation: 6.72 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.90 Degree
        The absolute mean error on Yaw angle estimations: 26.35 Degree
        The absolute mean error on Roll angle estimations: 6.72 Degree
Exp2019-01-27_17-06-27_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-06-27
All frames and annotations from 1 datasets have been read by 2019-01-27 17:07:57.303622
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:08:06.208615!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 0.3985
Epoch 1 completed!
Exp2019-01-27_17-06-27_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-06-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:08:22.727079
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 96.42 Degree
        The absolute mean error on Yaw angle estimation: 24.90 Degree
        The absolute mean error on Roll angle estimation: 20.49 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 96.42 Degree
        The absolute mean error on Yaw angle estimations: 24.90 Degree
        The absolute mean error on Roll angle estimations: 20.49 Degree
Exp2019-01-27_17-06-27_part3 completed!
Exp2019-01-27_17-06-27.h5 has been saved.
subject9_Exp2019-01-27_17-06-27.png has been saved by 2019-01-27 17:08:40.336369.
Model Exp2019-01-27_17-06-27 has been evaluated successfully.
Model Exp2019-01-27_17-06-27 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:11:07.981136: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:11:08.079072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:11:08.079329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:11:08.079342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:11:08.234938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:11:08.234964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:11:08.234972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:11:08.235112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-11-08 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-11-08
All frames and annotations from 1 datasets have been read by 2019-01-27 17:11:09.772831
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:11:18.668204!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 423.8770
Epoch 1 completed!
Exp2019-01-27_17-11-08_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-11-08
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:11:37.070157
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 82.24 Degree
        The absolute mean error on Yaw angle estimation: 161.83 Degree
        The absolute mean error on Roll angle estimation: 43.80 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 82.24 Degree
        The absolute mean error on Yaw angle estimations: 161.83 Degree
        The absolute mean error on Roll angle estimations: 43.80 Degree
Exp2019-01-27_17-11-08_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-11-08
All frames and annotations from 1 datasets have been read by 2019-01-27 17:11:55.115036
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:12:04.009826!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 268.4798
Epoch 1 completed!
Exp2019-01-27_17-11-08_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-11-08
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:12:20.789575
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5466.06 Degree
        The absolute mean error on Yaw angle estimation: 2227.08 Degree
        The absolute mean error on Roll angle estimation: 8169.04 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5466.06 Degree
        The absolute mean error on Yaw angle estimations: 2227.08 Degree
        The absolute mean error on Roll angle estimations: 8169.04 Degree
Exp2019-01-27_17-11-08_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-11-08
All frames and annotations from 1 datasets have been read by 2019-01-27 17:12:38.850034
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:12:47.740026!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 182.1086
Epoch 1 completed!
Exp2019-01-27_17-11-08_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-11-08
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:13:04.144654
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 21.67 Degree
        The absolute mean error on Yaw angle estimation: 53.25 Degree
        The absolute mean error on Roll angle estimation: 13.67 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 21.67 Degree
        The absolute mean error on Yaw angle estimations: 53.25 Degree
        The absolute mean error on Roll angle estimations: 13.67 Degree
Exp2019-01-27_17-11-08_part3 completed!
Exp2019-01-27_17-11-08.h5 has been saved.
subject9_Exp2019-01-27_17-11-08.png has been saved by 2019-01-27 17:13:21.784073.
Model Exp2019-01-27_17-11-08 has been evaluated successfully.
Model Exp2019-01-27_17-11-08 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:13:58.513362: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:13:58.596645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:13:58.596958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:13:58.596973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:13:58.752296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:13:58.752321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:13:58.752326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:13:58.752507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-13-59 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-13-59
All frames and annotations from 1 datasets have been read by 2019-01-27 17:14:00.270949
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:14:09.169790!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 5111.3121 - mean_absolute_error: 58.9947
Epoch 1 completed!
Exp2019-01-27_17-13-59_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-13-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:14:27.554966
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 258.08 Degree
        The absolute mean error on Yaw angle estimation: 154.25 Degree
        The absolute mean error on Roll angle estimation: 200.68 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 258.08 Degree
        The absolute mean error on Yaw angle estimations: 154.25 Degree
        The absolute mean error on Roll angle estimations: 200.68 Degree
Exp2019-01-27_17-13-59_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-13-59
All frames and annotations from 1 datasets have been read by 2019-01-27 17:14:45.661435
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:14:54.559045!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 3502.2638 - mean_absolute_error: 45.6296
Epoch 1 completed!
Exp2019-01-27_17-13-59_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-13-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:15:11.027305
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 222.44 Degree
        The absolute mean error on Yaw angle estimation: 96.18 Degree
        The absolute mean error on Roll angle estimation: 161.77 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 222.44 Degree
        The absolute mean error on Yaw angle estimations: 96.18 Degree
        The absolute mean error on Roll angle estimations: 161.77 Degree
Exp2019-01-27_17-13-59_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-13-59
All frames and annotations from 1 datasets have been read by 2019-01-27 17:15:29.051575
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:15:37.925067!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 2396.5377 - mean_absolute_error: 37.6626
Epoch 1 completed!
Exp2019-01-27_17-13-59_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-13-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:15:54.623638
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 127.72 Degree
        The absolute mean error on Yaw angle estimation: 36.24 Degree
        The absolute mean error on Roll angle estimation: 46.21 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 127.72 Degree
        The absolute mean error on Yaw angle estimations: 36.24 Degree
        The absolute mean error on Roll angle estimations: 46.21 Degree
Exp2019-01-27_17-13-59_part3 completed!
Exp2019-01-27_17-13-59.h5 has been saved.
subject9_Exp2019-01-27_17-13-59.png has been saved by 2019-01-27 17:16:12.281103.
Model Exp2019-01-27_17-13-59 has been evaluated successfully.
Model Exp2019-01-27_17-13-59 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:37:51.423768: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:37:51.519611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:37:51.519866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:37:51.519880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:37:51.675047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:37:51.675074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:37:51.675082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:37:51.675220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-37-52 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-37-52
All frames and annotations from 1 datasets have been read by 2019-01-27 17:37:53.223646
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:38:02.111424!
Epoch 1/1
882/882 [==============================] - 16s 19ms/step - loss: 20.5013
Epoch 1 completed!
Exp2019-01-27_17-37-52_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-37-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:38:20.198796
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 20.21 Degree
        The absolute mean error on Yaw angle estimation: 22.74 Degree
        The absolute mean error on Roll angle estimation: 39.90 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.21 Degree
        The absolute mean error on Yaw angle estimations: 22.74 Degree
        The absolute mean error on Roll angle estimations: 39.90 Degree
Exp2019-01-27_17-37-52_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-37-52
All frames and annotations from 1 datasets have been read by 2019-01-27 17:38:38.216487
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:38:47.094084!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 15.5593
Epoch 1 completed!
Exp2019-01-27_17-37-52_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-37-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:39:03.953601
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: nan Degree
        The absolute mean error on Yaw angle estimation: nan Degree
        The absolute mean error on Roll angle estimation: nan Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: nan Degree
        The absolute mean error on Yaw angle estimations: nan Degree
        The absolute mean error on Roll angle estimations: nan Degree
Exp2019-01-27_17-37-52_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_17-37-52
All frames and annotations from 1 datasets have been read by 2019-01-27 17:39:21.904712
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:39:30.782926!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 14.6536
Epoch 1 completed!
Exp2019-01-27_17-37-52_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_17-37-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:39:47.288458
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 53.75 Degree
        The absolute mean error on Yaw angle estimation: 53.80 Degree
        The absolute mean error on Roll angle estimation: 45.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 53.75 Degree
        The absolute mean error on Yaw angle estimations: 53.80 Degree
        The absolute mean error on Roll angle estimations: 45.37 Degree
Exp2019-01-27_17-37-52_part3 completed!
Exp2019-01-27_17-37-52.h5 has been saved.
subject9_Exp2019-01-27_17-37-52.png has been saved by 2019-01-27 17:40:04.934461.
Model Exp2019-01-27_17-37-52 has been evaluated successfully.
Model Exp2019-01-27_17-37-52 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:41:44.141756: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:41:44.239114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:41:44.239370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:41:44.239382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:41:44.394416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:41:44.394442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:41:44.394446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:41:44.394584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-41-45 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01
-27_17-41-45
All frames and annotations from 1 datasets have been read by 2019-01-27 17:41:45.911711
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:41:54.805160!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 17.7891
Epoch 1 completed!
Exp2019-01-27_17-41-45_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-
01-27_17-41-45
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:42:13.349871
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 19.15 Degree
        The absolute mean error on Yaw angle estimation: 26.85 Degree
        The absolute mean error on Roll angle estimation: 6.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 19.15 Degree
        The absolute mean error on Yaw angle estimations: 26.85 Degree
        The absolute mean error on Roll angle estimations: 6.37 Degree
Exp2019-01-27_17-41-45_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01
-27_17-41-45
All frames and annotations from 1 datasets have been read by 2019-01-27 17:42:31.474049
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:42:40.382404!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 17.3497
Epoch 1 completed!
Exp2019-01-27_17-41-45_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-
01-27_17-41-45
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:42:57.337922
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.55 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 6.13 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 18.55 Degree
        The absolute mean error on Yaw angle estimations: 26.69 Degree
        The absolute mean error on Roll angle estimations: 6.13 Degree
Exp2019-01-27_17-41-45_part2 completed!
Exp2019-01-27_17-41-45.h5 has been saved.
subject9_Exp2019-01-27_17-41-45.png has been saved by 2019-01-27 17:43:14.978960.
Model Exp2019-01-27_17-41-45 has been evaluated successfully.
Model Exp2019-01-27_17-41-45 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:44:39.001451: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:44:39.099178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:44:39.099434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:44:39.099447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:44:39.254759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:44:39.254784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:44:39.254789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:44:39.254931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-44-39 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01
-27_17-44-39
All frames and annotations from 1 datasets have been read by 2019-01-27 17:44:40.787081
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:44:49.683324!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1456
Epoch 1 completed!
Exp2019-01-27_17-44-39_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-
01-27_17-44-39
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:45:07.929362
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 0.06 Degree
        The absolute mean error on Yaw angle estimation: 0.07 Degree
        The absolute mean error on Roll angle estimation: 0.04 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 0.06 Degree
        The absolute mean error on Yaw angle estimations: 0.07 Degree
        The absolute mean error on Roll angle estimations: 0.04 Degree
Exp2019-01-27_17-44-39_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01
-27_17-44-39
All frames and annotations from 1 datasets have been read by 2019-01-27 17:45:26.023434
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:45:34.894620!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0834
Epoch 1 completed!
Exp2019-01-27_17-44-39_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-
01-27_17-44-39
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:45:51.901139
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 0.04 Degree
        The absolute mean error on Yaw angle estimation: 0.07 Degree
        The absolute mean error on Roll angle estimation: 0.03 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 0.04 Degree
        The absolute mean error on Yaw angle estimations: 0.07 Degree
        The absolute mean error on Roll angle estimations: 0.03 Degree
Exp2019-01-27_17-44-39_part2 completed!
Exp2019-01-27_17-44-39.h5 has been saved.
subject9_Exp2019-01-27_17-44-39.png has been saved by 2019-01-27 17:46:09.511789.
Model Exp2019-01-27_17-44-39 has been evaluated successfully.
Model Exp2019-01-27_17-44-39 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:47:11.540781: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:47:11.640112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:47:11.640371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:47:11.640383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:47:11.796872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:47:11.796897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:47:11.796901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:47:11.797041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-47-12 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01
-27_17-47-12
All frames and annotations from 1 datasets have been read by 2019-01-27 17:47:13.339605
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:47:22.252820!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1451
Epoch 1 completed!
Exp2019-01-27_17-47-12_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-
01-27_17-47-12
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:47:40.685115
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.96 Degree
        The absolute mean error on Yaw angle estimation: 16.55 Degree
        The absolute mean error on Roll angle estimation: 7.15 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.96 Degree
        The absolute mean error on Yaw angle estimations: 16.55 Degree
        The absolute mean error on Roll angle estimations: 7.15 Degree
Exp2019-01-27_17-47-12_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-01
-27_17-47-12
All frames and annotations from 1 datasets have been read by 2019-01-27 17:47:58.757568
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:48:07.651269!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0906
Epoch 1 completed!
Exp2019-01-27_17-47-12_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.000100_2019-
01-27_17-47-12
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:48:24.654124
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.32 Degree
        The absolute mean error on Yaw angle estimation: 5.24 Degree
        The absolute mean error on Roll angle estimation: 4.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.32 Degree
        The absolute mean error on Yaw angle estimations: 5.24 Degree
        The absolute mean error on Roll angle estimations: 4.30 Degree
Exp2019-01-27_17-47-12_part2 completed!
Exp2019-01-27_17-47-12.h5 has been saved.
subject9_Exp2019-01-27_17-47-12.png has been saved by 2019-01-27 17:48:42.299472.
Model Exp2019-01-27_17-47-12 has been evaluated successfully.
Model Exp2019-01-27_17-47-12 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-27_17-47-12
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:56:06.162525: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:56:06.259846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:56:06.260107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:56:06.260119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:56:06.425913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:56:06.425939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:56:06.425943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:56:06.426081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-56-07
The subjects will be tested: [(9, 'M03')]
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 64, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, record = record)
TypeError: evaluateCNN_LSTM() missing 1 required positional argument: 'angles'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-27_17-47-12
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:57:18.819874: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:57:18.917262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:57:18.917521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:57:18.917535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:57:19.072422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:57:19.072448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:57:19.072453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:57:19.072589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-57-19
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:57:20.878985
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 5.32 Degree
        The absolute mean error on Yaw angle estimation: 5.24 Degree
        The absolute mean error on Roll angle estimation: 4.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.32 Degree
        The absolute mean error on Yaw angle estimations: 5.24 Degree
        The absolute mean error on Roll angle estimations: 4.30 Degree
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 64, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record = record)
ValueError: too many values to unpack (expected 2)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-27_17-47-12
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:58:50.386018: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:58:50.482385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:58:50.482642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:58:50.482654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:58:50.637249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:58:50.637272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:58:50.637277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:58:50.637417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:58:52.478750
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 5.32 Degree
        The absolute mean error on Yaw angle estimation: 5.24 Degree
        The absolute mean error on Roll angle estimation: 4.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.32 Degree
        The absolute mean error on Yaw angle estimations: 5.24 Degree
        The absolute mean error on Roll angle estimations: 4.30 Degree
subject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51.png has been saved by 2019-01-27 17:59:10.617293.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51 has been evaluated successfully.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-27_17-47-12 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:59:25.695772: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:59:25.776512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 17:59:25.776819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:59:25.776833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 17:59:25.931452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 17:59:25.931479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:59:25.931484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:59:25.931665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26
All frames and annotations from 1 datasets have been read by 2019-01-27 17:59:27.765131
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:59:36.674876!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.0744
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 17:59:54.241822
1. set (Dataset 9) being trained for epoch 2 by 2019-01-27 18:00:03.102253!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0643
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 18:00:20.122391
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.99 Degree
        The absolute mean error on Yaw angle estimation: 5.12 Degree
        The absolute mean error on Roll angle estimation: 2.68 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.99 Degree
        The absolute mean error on Yaw angle estimations: 5.12 Degree
        The absolute mean error on Roll angle estimations: 2.68 Degree
subject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26.png has been saved by 2019-01-27 18:00:37.610534.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26 has been evaluated successfully.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "keep tanh"
[master 68e0104] keep tanh
 70 files changed, 3673 insertions(+), 2172 deletions(-)
 rename DeepRL_For_HPE/FC_RNN_Evaluater/results/{Last_Model/output_Last_Model.txt => Exp2019-01-26_04-28-49/output_Exp20
19-01-26_04-28-49.txt} (89%)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject14_Exp2019-01-26_04-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject3_Exp2019-01-26_04-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject5_Exp2019-01-26_04-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject9_Exp2019-01-26_04-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/output_Exp2019-01-26_13-37-43.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject14_Exp2019-01-26_13-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject3_Exp2019-01-26_13-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject5_Exp2019-01-26_13-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject9_Exp2019-01-26_13-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_11-59-18/output_Exp2019-01-27_11-59-18.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_11-59-18/subject9_Exp2019-01-27_11-59-18.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-14-25/output_Exp2019-01-27_13-14-25.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-14-25/subject9_Exp2019-01-27_13-14-25.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-25-50/output_Exp2019-01-27_13-25-50.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-25-50/subject9_Exp2019-01-27_13-25-50.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-35-44/output_Exp2019-01-27_13-35-44.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-35-44/subject9_Exp2019-01-27_13-35-44.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-36-52/output_Exp2019-01-27_13-36-52.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-36-52/subject9_Exp2019-01-27_13-36-52.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-39-33/output_Exp2019-01-27_13-39-33.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-39-33/subject9_Exp2019-01-27_13-39-33.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-41-30/output_Exp2019-01-27_13-41-30.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-41-30/subject9_Exp2019-01-27_13-41-30.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-43-20/output_Exp2019-01-27_13-43-20.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-43-20/subject9_Exp2019-01-27_13-43-20.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-44-42/output_Exp2019-01-27_13-44-42.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-44-42/subject9_Exp2019-01-27_13-44-42.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-46-18/output_Exp2019-01-27_13-46-18.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-46-18/subject9_Exp2019-01-27_13-46-18.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-54-23/output_Exp2019-01-27_13-54-23.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-54-23/subject9_Exp2019-01-27_13-54-23.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-55-59/output_Exp2019-01-27_13-55-59.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-55-59/subject9_Exp2019-01-27_13-55-59.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-57-56/output_Exp2019-01-27_13-57-56.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-57-56/subject9_Exp2019-01-27_13-57-56.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-12-09/output_Exp2019-01-27_14-12-09.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-12-09/subject9_Exp2019-01-27_14-12-09.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-22-51/output_Exp2019-01-27_14-22-51.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-22-51/subject9_Exp2019-01-27_14-22-51.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-37-00/output_Exp2019-01-27_14-37-00.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-37-00/subject9_Exp2019-01-27_14-37-00.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-44-55/output_Exp2019-01-27_14-44-55.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-44-55/subject9_Exp2019-01-27_14-44-55.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-06-27/output_Exp2019-01-27_17-06-27.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-06-27/subject9_Exp2019-01-27_17-06-27.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-11-08/output_Exp2019-01-27_17-11-08.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-11-08/subject9_Exp2019-01-27_17-11-08.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-13-59/output_Exp2019-01-27_17-13-59.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-13-59/subject9_Exp2019-01-27_17-13-59.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-37-52/output_Exp2019-01-27_17-37-52.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-37-52/subject9_Exp2019-01-27_17-37-52.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-41-45/output_Exp2019-01-27_17-41-45.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-41-45/subject9_Exp2019-01-27_17-41-45.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-44-39/output_Exp2019-01-27_17-44-39.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-44-39/subject9_Exp2019-01-27_17-44-39.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12/output_Exp2019-01-27_17-47-12.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12/subject9_Exp2019-01-27_17-47-12.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51/output_Exp201
9-01-27_17-47-12_and_2019-01-27_17-58-51.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51/subject9_Exp2
019-01-27_17-47-12_and_2019-01-27_17-58-51.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26/output_Exp201
9-01-27_17-47-12_and_2019-01-27_17-59-26.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26/subject9_Exp2
019-01-27_17-47-12_and_2019-01-27_17-59-26.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model__/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model___/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 101, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (101/101), done.
Writing objects: 100% (101/101), 4.99 MiB | 697.00 KiB/s, done.
Total 101 (delta 34), reused 0 (delta 0)
remote: Resolving deltas: 100% (34/34), completed with 8 local objects.
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   fea1f6f..68e0104  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:15:52.548327: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:15:52.646140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 18:15:52.646395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:15:52.646407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 18:15:52.801675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 18:15:52.801708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:15:52.801713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:15:52.801850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-15-53 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-27_18-15-53
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, record = reco
rd)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, preprocess_in
put = preprocess_input)
TypeError: trainCNN_LSTM() got an unexpected keyword argument 'exp'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:18:36.279976: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:18:36.377512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 18:18:36.377780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:18:36.377792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 18:18:36.533234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 18:18:36.533260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:18:36.533265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:18:36.533404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-18-37 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-27_18-18-37
All frames and annotations from 24 datasets have been read by 2019-01-27 18:18:43.082578
1. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:18:47.765704!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.1347
^C
Model Exp2019-01-27_18-18-37_part1 has been interrupted.
Exp2019-01-27_18-18-37_part1.h5 has been saved.
Model Exp2019-01-27_18-18-37_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "adding experiment
count to output"
[master 7b03ced] adding experiment count to output
 6 files changed, 214 insertions(+), 16 deletions(-)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_18-18-37_part1/output_Exp2019-01-27_18-18-37_p
art1.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 14, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (11/11), done.
Writing objects: 100% (14/14), 2.33 KiB | 0 bytes/s, done.
Total 14 (delta 8), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   68e0104..7b03ced  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:21:07.912509: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:21:08.007845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 18:21:08.008151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:21:08.008166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 18:21:08.163946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 18:21:08.163971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:21:08.163976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:21:08.164154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-21-08 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-27_18-21-08
All frames and annotations from 20 datasets have been read by 2019-01-27 18:21:13.332003
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:21:19.740762!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.0988
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, preprocess_in
put = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 50, i
n trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, exp = exp, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 40, i
n trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_outputs, batch_
size, in_epochs, exp = exp, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 22, i
n trainImageModelOnSets
    exp = ' in Experiment %d' % (exp) if exp != -1 else ''
TypeError: %d format: a number is required, not str
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:36:49.313944: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:36:49.410235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 18:36:49.410499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:36:49.410513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 18:36:49.566088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 18:36:49.566116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:36:49.566124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:36:49.566264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-36-50 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-27_18-36-50
All frames and annotations from 20 datasets have been read by 2019-01-27 18:36:54.600296
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:37:00.996602!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.1272
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, preprocess_in
put = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 50, i
n trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, exp = exp, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 40, i
n trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_outputs, batch_
size, in_epochs, exp = exp, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 22, i
n trainImageModelOnSets
    exp = ' in Experiment %d' % (exp) if exp != -1 else ''
TypeError: %d format: a number is required, not str
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:38:17.980728: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:38:18.079080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 18:38:18.079343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:38:18.079357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 18:38:18.234477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 18:38:18.234503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:38:18.234511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:38:18.234659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-38-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-27_18-38-18
All frames and annotations from 20 datasets have been read by 2019-01-27 18:38:23.546454
^C
Model Exp2019-01-27_18-38-18_part1 has been interrupted.
Exp2019-01-27_18-38-18_part1.h5 has been saved.
Model Exp2019-01-27_18-38-18_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:39:24.754830: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:39:24.851524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 18:39:24.851826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:39:24.851841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 18:39:25.007701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 18:39:25.007727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:39:25.007732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:39:25.007914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-39-25 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-27_18-39-25
All frames and annotations from 20 datasets have been read by 2019-01-27 18:39:30.089324
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:39:36.501210!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.1046
2. set (Dataset 24) being trained for epoch 1 in Experiment  in Experiment 1 by 2019-01-27 18:39:54.679586!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0894
3. set (Dataset 15) being trained for epoch 1 in Experiment  in Experiment  in Experiment 1 by 2019-01-27 18:40:09.89274
4!
Epoch 1/1
585/654 [=========================>....] - ETA: 1s - loss: 0.1114^C
Model Exp2019-01-27_18-39-25_part1 has been interrupted.
Exp2019-01-27_18-39-25_part1.h5 has been saved.
Model Exp2019-01-27_18-39-25_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:42:46.900277: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:42:46.998566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 18:42:46.998830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:42:46.998848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 18:42:47.153779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 18:42:47.153801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:42:47.153805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:42:47.153942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-42-47 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 18:42:52.167787
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:42:58.567546!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.1332
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:43:16.446032!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.1030
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:43:31.678422!
Epoch 1/1
654/654 [==============================] - 11s 17ms/step - loss: 0.1170
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:43:47.988977!
Epoch 1/1
502/502 [==============================] - 9s 17ms/step - loss: 0.0964
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:44:09.966421!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.1351
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:44:28.904202!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1431
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:44:45.044640!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1318
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:45:05.035664!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0905
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:45:34.811942!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.1055
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:00.325613!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0923
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:20.764274!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.1623
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:41.652583!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.1402
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:56.999856!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1333
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:47:16.391078!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.1216
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:47:38.782751!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.1247
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:47:57.759704!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.1025
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:11.897512!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0824
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:24.302934!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1393
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:39.105880!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0769
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:57.259225!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0916
Epoch 1 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 18:49:11.641579
1. set (Dataset 6) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:49:16.843642!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1243
2. set (Dataset 11) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:49:32.332765!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0815
3. set (Dataset 10) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:49:49.609311!
Epoch 1/1
726/726 [==============================] - 12s 17ms/step - loss: 0.0987
4. set (Dataset 4) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:09.565960!
Epoch 1/1
744/744 [==============================] - 13s 17ms/step - loss: 0.0966
5. set (Dataset 23) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:28.081221!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1182
6. set (Dataset 13) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:43.183428!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0649
7. set (Dataset 17) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:55.520042!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0757
8. set (Dataset 1) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:51:07.798232!
Epoch 1/1
498/498 [==============================] - 9s 17ms/step - loss: 0.1162
9. set (Dataset 8) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:51:24.291008!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0975
10. set (Dataset 7) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:51:45.440368!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0630
11. set (Dataset 21) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:04.910538!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1129
12. set (Dataset 22) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:22.621362!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0575
13. set (Dataset 2) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:39.854964!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.1026
14. set (Dataset 24) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:53.871213!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0714
15. set (Dataset 15) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:08.998806!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0879
16. set (Dataset 20) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:26.052559!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0688
17. set (Dataset 18) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:42.068783!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1124
18. set (Dataset 19) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:57.886035!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0907
19. set (Dataset 12) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:54:14.117165!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0760
20. set (Dataset 16) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:54:36.113482!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0702
Epoch 2 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 18:54:57.024012
1. set (Dataset 19) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:01.904439!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0744
2. set (Dataset 16) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:19.604927!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0600
3. set (Dataset 21) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:42.182104!
Epoch 1/1
634/634 [==============================] - 11s 17ms/step - loss: 0.1023
4. set (Dataset 15) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:59.596651!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0770
5. set (Dataset 13) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:56:16.487845!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0635
6. set (Dataset 12) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:56:32.477186!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0669
7. set (Dataset 18) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:56:51.712415!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0923
8. set (Dataset 22) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:57:09.303466!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0501
9. set (Dataset 23) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:57:26.623977!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0977
10. set (Dataset 8) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:57:44.512881!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0903
11. set (Dataset 17) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:02.272823!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0711
12. set (Dataset 6) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:14.621591!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1086
13. set (Dataset 24) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:29.121195!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0588
14. set (Dataset 11) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:43.409428!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0676
15. set (Dataset 10) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:00.701733!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0780
16. set (Dataset 20) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:18.981708!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0635
17. set (Dataset 2) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:34.087527!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0843
18. set (Dataset 4) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:50.615252!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0904
19. set (Dataset 7) being trained for epoch 3 in Experiment 1 by 2019-01-27 19:00:11.650108!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0574
20. set (Dataset 1) being trained for epoch 3 in Experiment 1 by 2019-01-27 19:00:30.196131!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0908
Epoch 3 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:00:43.906526
1. set (Dataset 4) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:00:51.292632!
Epoch 1/1
744/744 [==============================] - 13s 17ms/step - loss: 0.0823
2. set (Dataset 1) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:09.220723!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0817
3. set (Dataset 17) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:21.795295!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.0696
4. set (Dataset 10) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:36.474761!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0665
5. set (Dataset 12) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:56.701255!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0626
6. set (Dataset 7) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:02:17.237521!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0487
7. set (Dataset 2) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:02:36.093606!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0725
8. set (Dataset 6) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:02:50.623866!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0963
9. set (Dataset 13) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:05.486486!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0602
10. set (Dataset 23) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:19.718768!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0938
11. set (Dataset 18) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:35.988681!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0930
12. set (Dataset 19) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:51.910868!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0711
13. set (Dataset 11) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:04:06.815106!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0594
14. set (Dataset 16) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:04:25.667899!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0643
15. set (Dataset 21) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:04:47.867466!
Epoch 1/1
634/634 [==============================] - 11s 17ms/step - loss: 0.1013
16. set (Dataset 20) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:04.400685!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0604
17. set (Dataset 24) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:19.161419!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0536
18. set (Dataset 15) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:34.469924!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0665
19. set (Dataset 8) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:53.785055!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0807
20. set (Dataset 22) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:06:13.877914!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0522
Epoch 4 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:06:30.065417
1. set (Dataset 15) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:06:36.424523!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0591
2. set (Dataset 22) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:06:54.411195!
Epoch 1/1
665/665 [==============================] - 12s 17ms/step - loss: 0.0525
3. set (Dataset 18) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:07:11.864472!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0834
4. set (Dataset 21) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:07:28.807315!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0908
5. set (Dataset 7) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:07:47.877576!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0462
6. set (Dataset 8) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:09.394655!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0667
7. set (Dataset 24) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:28.237416!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0523
8. set (Dataset 19) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:41.859859!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0664
9. set (Dataset 12) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:58.221870!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0637
10. set (Dataset 13) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:09:16.402229!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0589
11. set (Dataset 2) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:09:30.330811!
Epoch 1/1
511/511 [==============================] - 9s 17ms/step - loss: 0.0660
12. set (Dataset 4) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:09:46.495917!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0799
13. set (Dataset 16) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:08.533742!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0567
14. set (Dataset 1) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:29.877989!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0775
15. set (Dataset 17) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:42.680985!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0704
16. set (Dataset 20) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:54.988202!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0580
17. set (Dataset 11) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:11:10.829872!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0492
18. set (Dataset 10) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:11:28.531363!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0584
19. set (Dataset 23) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:11:47.079723!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0916
20. set (Dataset 6) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:12:02.366878!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0908
Epoch 5 for Experiment 1 completed!
Exp2019-01-27_18-42-47_part1.h5 has been saved.
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24, 'M14'), (19,
 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'), (20, 'M12'), (11, 'M05'
), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-27_18-42-47
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 19:12:14.444907
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.69 Degree
        The absolute mean error on Yaw angle estimation: 29.11 Degree
        The absolute mean error on Roll angle estimation: 10.60 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 16.35 Degree
        The absolute mean error on Yaw angle estimation: 28.69 Degree
        The absolute mean error on Roll angle estimation: 4.37 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.51 Degree
        The absolute mean error on Yaw angle estimation: 27.13 Degree
        The absolute mean error on Roll angle estimation: 6.82 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 14.23 Degree
        The absolute mean error on Yaw angle estimation: 26.67 Degree
        The absolute mean error on Roll angle estimation: 13.22 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.95 Degree
        The absolute mean error on Yaw angle estimations: 27.90 Degree
        The absolute mean error on Roll angle estimations: 8.75 Degree
Exp2019-01-27_18-42-47_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 19:13:45.881361
1. set (Dataset 10) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:13:53.105883!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0515
2. set (Dataset 6) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:11.232820!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0794
3. set (Dataset 2) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:26.018407!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0633
4. set (Dataset 17) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:38.917258!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0665
5. set (Dataset 8) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:53.816884!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0662
6. set (Dataset 23) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:15:13.404074!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0859
7. set (Dataset 11) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:15:29.481044!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0440
8. set (Dataset 4) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:15:46.996451!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0761
9. set (Dataset 7) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:16:08.061893!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0497
10. set (Dataset 12) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:16:28.893406!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0558
11. set (Dataset 24) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:16:47.101295!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0529
12. set (Dataset 15) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:02.157010!
Epoch 1/1
654/654 [==============================] - 11s 17ms/step - loss: 0.0630
13. set (Dataset 1) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:18.628263!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0766
14. set (Dataset 22) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:34.136817!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0493
15. set (Dataset 18) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:51.912838!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0826
16. set (Dataset 20) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:18:08.350942!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0552
17. set (Dataset 16) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:18:27.260683!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0558
18. set (Dataset 21) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:18:49.696981!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0912
19. set (Dataset 13) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:19:05.785579!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0575
20. set (Dataset 19) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:19:19.311789!
Epoch 1/1
502/502 [==============================] - 9s 17ms/step - loss: 0.0695
Epoch 1 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:19:32.436378
1. set (Dataset 21) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:19:38.461040!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0894
2. set (Dataset 19) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:19:54.845932!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0644
3. set (Dataset 24) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:08.663520!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0496
4. set (Dataset 18) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:23.375669!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0764
5. set (Dataset 23) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:39.674408!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0852
6. set (Dataset 13) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:54.754717!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0561
7. set (Dataset 16) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:21:12.347048!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0519
8. set (Dataset 15) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:21:35.495497!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0623
9. set (Dataset 8) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:21:55.127988!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0654
10. set (Dataset 7) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:22:16.433160!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0499
11. set (Dataset 11) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:22:35.514674!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0438
12. set (Dataset 10) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:22:53.199325!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0512
13. set (Dataset 22) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:12.349267!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0480
14. set (Dataset 6) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:29.528190!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0818
15. set (Dataset 2) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:44.260465!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0630
16. set (Dataset 20) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:58.968805!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0546
17. set (Dataset 1) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:24:13.936020!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0700
18. set (Dataset 17) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:24:26.725974!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0668
19. set (Dataset 12) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:24:41.178591!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0543
20. set (Dataset 4) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:25:01.567008!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0776
Epoch 2 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:25:19.202612
1. set (Dataset 17) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:25:22.957722!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0647
2. set (Dataset 4) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:25:37.684372!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0706
3. set (Dataset 11) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:25:57.095653!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0420
4. set (Dataset 2) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:26:12.445397!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0591
5. set (Dataset 13) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:26:26.658676!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0554
6. set (Dataset 12) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:26:42.694142!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0522
7. set (Dataset 1) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:01.165068!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0634
8. set (Dataset 10) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:17.541401!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0515
9. set (Dataset 23) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:36.211270!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0862
10. set (Dataset 8) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:54.364591!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0637
11. set (Dataset 16) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:28:17.221706!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0554
12. set (Dataset 21) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:28:39.842309!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0845
13. set (Dataset 6) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:28:56.693202!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0806
14. set (Dataset 19) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:11.488908!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0634
15. set (Dataset 24) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:25.224984!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0515
16. set (Dataset 20) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:39.422913!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0500
17. set (Dataset 22) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:55.895098!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0489
18. set (Dataset 18) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:30:13.784069!
Epoch 1/1
614/614 [==============================] - 11s 17ms/step - loss: 0.0733
19. set (Dataset 7) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:30:32.050765!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0507
20. set (Dataset 15) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:30:52.081075!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0611
Epoch 3 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:31:08.185533
1. set (Dataset 18) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:31:14.067362!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0689
2. set (Dataset 15) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:31:31.660767!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0579
3. set (Dataset 16) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:31:52.515029!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0528
4. set (Dataset 24) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:32:13.452763!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0482
5. set (Dataset 12) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:32:29.640838!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0500
6. set (Dataset 7) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:32:50.356766!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0447
7. set (Dataset 22) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:10.151733!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0452
8. set (Dataset 21) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:27.908699!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0838
9. set (Dataset 13) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:44.262728!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0562
10. set (Dataset 23) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:58.745057!
Epoch 1/1
569/569 [==============================] - 10s 17ms/step - loss: 0.0823
11. set (Dataset 1) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:13.668643!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0615
12. set (Dataset 17) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:26.262834!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0596
13. set (Dataset 19) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:38.325985!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0669
14. set (Dataset 4) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:54.786776!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0718
15. set (Dataset 11) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:14.019041!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0448
16. set (Dataset 20) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:29.774079!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0518
17. set (Dataset 6) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:45.025572!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0772
18. set (Dataset 2) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:59.940590!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0578
19. set (Dataset 8) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:36:16.975018!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0641
20. set (Dataset 10) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:36:37.983661!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0497
Epoch 4 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:36:55.087002
1. set (Dataset 2) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:00.175476!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0534
2. set (Dataset 10) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:16.571052!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0472
3. set (Dataset 1) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:34.387931!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0606
4. set (Dataset 11) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:49.327006!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0385
5. set (Dataset 7) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:38:07.289313!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0463
6. set (Dataset 8) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:38:28.359984!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0577
7. set (Dataset 6) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:38:47.577598!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0746
8. set (Dataset 17) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:01.233167!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0620
9. set (Dataset 12) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:15.800441!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0492
10. set (Dataset 13) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:34.032954!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0549
11. set (Dataset 22) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:49.140482!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0453
12. set (Dataset 18) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:40:06.950934!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0731
13. set (Dataset 4) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:40:25.611824!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0684
14. set (Dataset 15) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:40:45.387282!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0600
15. set (Dataset 16) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:05.911249!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0545
16. set (Dataset 20) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:27.543195!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0522
17. set (Dataset 19) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:42.451760!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0629
18. set (Dataset 24) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:56.228560!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0485
19. set (Dataset 23) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:42:10.292714!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0821
20. set (Dataset 21) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:42:26.813089!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0852
Epoch 5 for Experiment 2 completed!
Exp2019-01-27_18-42-47_part2.h5 has been saved.
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F06'), (17, 'M
10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'), (20, 'M12'), (19, 'M11')
, (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-27_18-42-47
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 19:42:40.488846
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 14.47 Degree
        The absolute mean error on Yaw angle estimation: 26.24 Degree
        The absolute mean error on Roll angle estimation: 15.04 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.47 Degree
        The absolute mean error on Yaw angle estimation: 26.70 Degree
        The absolute mean error on Roll angle estimation: 4.47 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 30.05 Degree
        The absolute mean error on Yaw angle estimation: 28.99 Degree
        The absolute mean error on Roll angle estimation: 9.91 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 13.73 Degree
        The absolute mean error on Yaw angle estimation: 29.61 Degree
        The absolute mean error on Roll angle estimation: 13.48 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.18 Degree
        The absolute mean error on Yaw angle estimations: 27.89 Degree
        The absolute mean error on Roll angle estimations: 10.72 Degree
Exp2019-01-27_18-42-47_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 19:43:50.425386
1. set (Dataset 24) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:43:55.096427!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0478
2. set (Dataset 21) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:44:10.022010!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0777
3. set (Dataset 22) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:44:27.924823!
Epoch 1/1
665/665 [==============================] - 12s 17ms/step - loss: 0.0424
4. set (Dataset 16) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:44:48.306731!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0502
5. set (Dataset 8) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:45:12.700088!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.0618
6. set (Dataset 23) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:45:31.680416!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0782
7. set (Dataset 19) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:45:46.840017!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0628
8. set (Dataset 18) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:01.666710!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0685
9. set (Dataset 7) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:20.535815!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0447
10. set (Dataset 12) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:40.802422!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0478
11. set (Dataset 6) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:59.304550!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0766
12. set (Dataset 2) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:47:14.135651!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0566
13. set (Dataset 15) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:47:29.506956!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0570
14. set (Dataset 10) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:47:48.436347!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0481
15. set (Dataset 1) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:06.394290!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0608
16. set (Dataset 20) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:21.139462!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0514
17. set (Dataset 4) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:38.545496!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0633
18. set (Dataset 11) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:57.493891!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0441
19. set (Dataset 13) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:49:12.346897!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0563
20. set (Dataset 17) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:49:25.081186!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0589
Epoch 1 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:49:36.584927
1. set (Dataset 11) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:49:42.289848!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0387
2. set (Dataset 17) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:49:56.407994!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0579
3. set (Dataset 6) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:08.529818!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0742
4. set (Dataset 1) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:23.311135!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0644
5. set (Dataset 23) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:37.607357!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0774
6. set (Dataset 13) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:52.611932!
Epoch 1/1
485/485 [==============================] - 8s 17ms/step - loss: 0.0546
7. set (Dataset 4) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:51:08.469662!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0563
8. set (Dataset 2) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:51:27.223317!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0585
9. set (Dataset 8) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:51:44.445292!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0624
10. set (Dataset 7) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:06.177440!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0458
11. set (Dataset 19) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:24.295945!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0649
12. set (Dataset 24) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:38.025858!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0520
13. set (Dataset 10) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:54.182522!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0460
14. set (Dataset 21) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:53:13.065940!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0802
15. set (Dataset 22) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:53:30.978319!
Epoch 1/1
665/665 [==============================] - 12s 17ms/step - loss: 0.0418
16. set (Dataset 20) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:53:48.129821!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0492
17. set (Dataset 15) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:54:04.374761!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0593
18. set (Dataset 16) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:54:24.798719!
Epoch 1/1
914/914 [==============================] - 16s 17ms/step - loss: 0.0506
19. set (Dataset 12) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:54:47.871470!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0518
20. set (Dataset 18) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:55:06.784013!
Epoch 1/1
614/614 [==============================] - 10s 17ms/step - loss: 0.0681
Epoch 2 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:55:21.594316
1. set (Dataset 16) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:55:30.326945!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0472
2. set (Dataset 18) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:55:52.580136!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0645
3. set (Dataset 19) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:08.651444!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0605
4. set (Dataset 22) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:24.298028!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0435
5. set (Dataset 13) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:41.321855!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0548
6. set (Dataset 12) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:57.329044!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0460
7. set (Dataset 15) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:57:17.146784!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0573
8. set (Dataset 24) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:57:33.759377!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0486
9. set (Dataset 23) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:57:48.195378!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0792
10. set (Dataset 8) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:58:06.187953!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0603
11. set (Dataset 4) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:58:27.664460!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0601
12. set (Dataset 11) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:58:46.772628!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0427
13. set (Dataset 21) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:02.915899!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0811
14. set (Dataset 17) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:17.838180!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0608
15. set (Dataset 6) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:30.103961!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0735
16. set (Dataset 20) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:45.124216!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0504
17. set (Dataset 10) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:02.223293!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0485
18. set (Dataset 1) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:20.004548!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0625
19. set (Dataset 7) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:36.371809!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0469
20. set (Dataset 2) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:54.505796!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0580
Epoch 3 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:01:08.009750
1. set (Dataset 1) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:01:13.060101!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0585
2. set (Dataset 2) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:01:27.218336!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0492
3. set (Dataset 4) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:01:43.877738!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0583
4. set (Dataset 6) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:02.395349!
Epoch 1/1
542/542 [==============================] - 9s 17ms/step - loss: 0.0707
5. set (Dataset 12) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:19.140511!
Epoch 1/1
732/732 [==============================] - 13s 17ms/step - loss: 0.0489
6. set (Dataset 7) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:39.518390!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0454
7. set (Dataset 10) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:59.870638!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0471
8. set (Dataset 11) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:03:18.449754!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0397
9. set (Dataset 13) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:03:33.736004!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0529
10. set (Dataset 23) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:03:48.067782!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0742
11. set (Dataset 15) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:04.755729!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0604
12. set (Dataset 16) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:25.225887!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0495
13. set (Dataset 17) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:45.315949!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0586
14. set (Dataset 18) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:58.186891!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0647
15. set (Dataset 19) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:05:14.124219!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0625
16. set (Dataset 20) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:05:28.723316!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0504
17. set (Dataset 21) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:05:45.174951!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0803
18. set (Dataset 22) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:06:03.049500!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0423
19. set (Dataset 8) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:06:22.553963!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0605
20. set (Dataset 24) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:06:41.050421!
Epoch 1/1
492/492 [==============================] - 9s 19ms/step - loss: 0.0502
Epoch 4 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:06:54.715849
1. set (Dataset 22) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:01.109039!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0396
2. set (Dataset 24) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:17.505229!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0461
3. set (Dataset 15) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:32.809575!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0564
4. set (Dataset 19) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:49.916013!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0617
5. set (Dataset 7) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:08:06.670525!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0447
6. set (Dataset 8) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:08:27.687181!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0591
7. set (Dataset 21) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:08:47.637308!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0774
8. set (Dataset 16) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:09:07.855799!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0489
9. set (Dataset 12) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:09:31.685479!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0487
10. set (Dataset 13) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:09:49.699038!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0522
11. set (Dataset 10) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:05.729591!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0475
12. set (Dataset 1) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:24.075266!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0629
13. set (Dataset 18) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:38.977272!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0652
14. set (Dataset 2) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:54.967810!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0581
15. set (Dataset 4) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:11.612821!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0569
16. set (Dataset 20) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:30.109813!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0537
17. set (Dataset 17) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:43.802444!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0560
18. set (Dataset 6) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:56.252467!
Epoch 1/1
542/542 [==============================] - 9s 17ms/step - loss: 0.0721
19. set (Dataset 23) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:12:11.214010!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0778
20. set (Dataset 11) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:12:27.428342!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0424
Epoch 5 for Experiment 3 completed!
Exp2019-01-27_18-42-47_part3.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21, 'F02'), (16,
 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019
-01-27_18-42-47
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 20:12:40.011331
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 12.71 Degree
        The absolute mean error on Yaw angle estimation: 39.63 Degree
        The absolute mean error on Roll angle estimation: 15.27 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.30 Degree
        The absolute mean error on Yaw angle estimation: 23.38 Degree
        The absolute mean error on Roll angle estimation: 3.83 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 29.03 Degree
        The absolute mean error on Yaw angle estimation: 24.58 Degree
        The absolute mean error on Roll angle estimation: 12.18 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.08 Degree
        The absolute mean error on Yaw angle estimation: 27.01 Degree
        The absolute mean error on Roll angle estimation: 12.92 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.53 Degree
        The absolute mean error on Yaw angle estimations: 28.65 Degree
        The absolute mean error on Roll angle estimations: 11.05 Degree
Exp2019-01-27_18-42-47_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.000100_2019-0
1-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 20:13:49.742692
1. set (Dataset 6) being trained for epoch 1 in Experiment 4 by 2019-01-27 20:13:54.931341!
Epoch 1/1
533/542 [============================>.] - ETA: 0s - loss: 0.0684^C
Model Exp2019-01-27_18-42-47_part4 has been interrupted.
Exp2019-01-27_18-42-47_part4.h5 has been saved.
Model Exp2019-01-27_18-42-47_part4 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-27_18-42-47_part4
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 20:15:02.889499: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 20:15:02.987129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 20:15:02.987433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 20:15:02.987446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 20:15:03.143127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 20:15:03.143154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 20:15:03.143158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 20:15:03.143337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_18-42-47_part4.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-27_18-42-47_part4_and_2019-01-27_20-15-04
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 20:15:06.137359
For the Subject 3 (F03):
730/730 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 14.14 Degree
        The absolute mean error on Yaw angle estimation: 42.17 Degree
        The absolute mean error on Roll angle estimation: 16.91 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.27 Degree
        The absolute mean error on Yaw angle estimation: 25.17 Degree
        The absolute mean error on Roll angle estimation: 4.36 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 27.32 Degree
        The absolute mean error on Yaw angle estimation: 25.95 Degree
        The absolute mean error on Roll angle estimation: 12.79 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 19.02 Degree
        The absolute mean error on Yaw angle estimation: 27.32 Degree
        The absolute mean error on Roll angle estimation: 12.62 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.69 Degree
        The absolute mean error on Yaw angle estimations: 30.15 Degree
        The absolute mean error on Roll angle estimations: 11.67 Degree
subject3_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.239660.
subject5_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.438394.
subject9_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.637757.
subject14_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.855475.
Model Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04 has been evaluated successfully.
Model Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN_Experiment.
py Exp2019-01-27_18-42-47_part4 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 20:16:37.328762: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 20:16:37.426327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 20:16:37.426634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 20:16:37.426648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 20:16:37.582479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 20:16:37.582506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 20:16:37.582510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 20:16:37.582691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_18-42-47_part4.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model Exp2019-01-27_18-42-47_part4_and_2019-01-27_20-16-38
All frames and annotations from 20 datasets have been read by 2019-01-27 20:16:42.978898
num_ou1. set (Dataset 22) being trained for epoch 1 by 2019-01-27 20:16:49.379813!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.0427
2. set (Dataset 24) being trained for epoch 1 by 2019-01-27 20:17:07.191014!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0462
3. set (Dataset 15) being trained for epoch 1 by 2019-01-27 20:17:22.511085!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0568
4. set (Dataset 19) being trained for epoch 1 by 2019-01-27 20:17:39.144240!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0626
5. set (Dataset 8) being trained for epoch 1 by 2019-01-27 20:17:55.810600!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.0616
6. set (Dataset 23) being trained for epoch 1 by 2019-01-27 20:18:14.734283!
Epoch 1/1
569/569 [==============================] - 465s 817ms/step - loss: 0.0763
7. set (Dataset 21) being trained for epoch 1 by 2019-01-27 20:26:05.906729!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0768
8. set (Dataset 16) being trained for epoch 1 by 2019-01-27 20:26:26.225650!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0487
9. set (Dataset 7) being trained for epoch 1 by 2019-01-27 20:26:50.103557!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0427
10. set (Dataset 12) being trained for epoch 1 by 2019-01-27 20:27:10.736443!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0467
11. set (Dataset 10) being trained for epoch 1 by 2019-01-27 20:27:31.263412!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0459
12. set (Dataset 1) being trained for epoch 1 by 2019-01-27 20:27:49.638384!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0600
13. set (Dataset 18) being trained for epoch 1 by 2019-01-27 20:28:04.398874!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0671
14. set (Dataset 2) being trained for epoch 1 by 2019-01-27 20:28:20.293867!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0528
15. set (Dataset 4) being trained for epoch 1 by 2019-01-27 20:28:37.096561!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0568
16. set (Dataset 20) being trained for epoch 1 by 2019-01-27 20:28:55.615559!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0511
17. set (Dataset 17) being trained for epoch 1 by 2019-01-27 20:29:09.244205!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0571
18. set (Dataset 6) being trained for epoch 1 by 2019-01-27 20:29:21.720068!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0714
19. set (Dataset 13) being trained for epoch 1 by 2019-01-27 20:29:36.409357!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0539
20. set (Dataset 11) being trained for epoch 1 by 2019-01-27 20:29:51.071109!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0400
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:30:05.816863
1. set (Dataset 6) being trained for epoch 2 by 2019-01-27 20:30:11.004281!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0665
2. set (Dataset 11) being trained for epoch 2 by 2019-01-27 20:30:26.719338!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0357
3. set (Dataset 10) being trained for epoch 2 by 2019-01-27 20:30:44.251493!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0467
4. set (Dataset 4) being trained for epoch 2 by 2019-01-27 20:31:04.626094!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0529
5. set (Dataset 23) being trained for epoch 2 by 2019-01-27 20:31:23.468434!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0757
6. set (Dataset 13) being trained for epoch 2 by 2019-01-27 20:31:38.407350!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0493
7. set (Dataset 17) being trained for epoch 2 by 2019-01-27 20:31:50.836893!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0591
8. set (Dataset 1) being trained for epoch 2 by 2019-01-27 20:32:02.797215!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0607
9. set (Dataset 8) being trained for epoch 2 by 2019-01-27 20:32:19.687959!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0604
10. set (Dataset 7) being trained for epoch 2 by 2019-01-27 20:32:41.043037!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0441
11. set (Dataset 21) being trained for epoch 2 by 2019-01-27 20:33:00.816332!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0732
12. set (Dataset 22) being trained for epoch 2 by 2019-01-27 20:33:18.457238!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0407
13. set (Dataset 2) being trained for epoch 2 by 2019-01-27 20:33:35.711971!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0548
14. set (Dataset 24) being trained for epoch 2 by 2019-01-27 20:33:49.460636!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0490
15. set (Dataset 15) being trained for epoch 2 by 2019-01-27 20:34:04.511515!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0574
16. set (Dataset 20) being trained for epoch 2 by 2019-01-27 20:34:21.542546!
Epoch 1/1
556/556 [==============================] - 11s 19ms/step - loss: 0.0476
17. set (Dataset 18) being trained for epoch 2 by 2019-01-27 20:34:38.026058!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0630
18. set (Dataset 19) being trained for epoch 2 by 2019-01-27 20:34:53.981046!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0641
19. set (Dataset 12) being trained for epoch 2 by 2019-01-27 20:35:10.344117!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0486
20. set (Dataset 16) being trained for epoch 2 by 2019-01-27 20:35:32.234644!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0488
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:35:53.232482
1. set (Dataset 19) being trained for epoch 3 by 2019-01-27 20:35:58.108308!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0629
2. set (Dataset 16) being trained for epoch 3 by 2019-01-27 20:36:15.953686!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0472
3. set (Dataset 21) being trained for epoch 3 by 2019-01-27 20:36:38.214522!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0774
4. set (Dataset 15) being trained for epoch 3 by 2019-01-27 20:36:55.940200!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0564
5. set (Dataset 13) being trained for epoch 3 by 2019-01-27 20:37:12.660753!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0506
6. set (Dataset 12) being trained for epoch 3 by 2019-01-27 20:37:28.931160!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0463
7. set (Dataset 18) being trained for epoch 3 by 2019-01-27 20:37:47.991680!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0617
8. set (Dataset 22) being trained for epoch 3 by 2019-01-27 20:38:05.464460!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0417
9. set (Dataset 23) being trained for epoch 3 by 2019-01-27 20:38:22.947596!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0740
10. set (Dataset 8) being trained for epoch 3 by 2019-01-27 20:38:40.918620!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0591
11. set (Dataset 17) being trained for epoch 3 by 2019-01-27 20:38:58.739418!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0560
12. set (Dataset 6) being trained for epoch 3 by 2019-01-27 20:39:11.271983!
Epoch 1/1
542/542 [==============================] - 9s 17ms/step - loss: 0.0706
13. set (Dataset 24) being trained for epoch 3 by 2019-01-27 20:39:25.383027!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0449
14. set (Dataset 11) being trained for epoch 3 by 2019-01-27 20:39:39.885464!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0408
15. set (Dataset 10) being trained for epoch 3 by 2019-01-27 20:39:57.406201!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0460
16. set (Dataset 20) being trained for epoch 3 by 2019-01-27 20:40:15.883576!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0503
17. set (Dataset 2) being trained for epoch 3 by 2019-01-27 20:40:31.029417!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0535
18. set (Dataset 4) being trained for epoch 3 by 2019-01-27 20:40:47.666244!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0553
19. set (Dataset 7) being trained for epoch 3 by 2019-01-27 20:41:08.402141!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0492
20. set (Dataset 1) being trained for epoch 3 by 2019-01-27 20:41:27.067396!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0605
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:41:40.251766
1. set (Dataset 4) being trained for epoch 4 by 2019-01-27 20:41:47.639391!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0526
2. set (Dataset 1) being trained for epoch 4 by 2019-01-27 20:42:06.113909!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0604
3. set (Dataset 17) being trained for epoch 4 by 2019-01-27 20:42:18.829758!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0546
4. set (Dataset 10) being trained for epoch 4 by 2019-01-27 20:42:33.179431!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0457
5. set (Dataset 12) being trained for epoch 4 by 2019-01-27 20:42:53.229785!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0470
6. set (Dataset 7) being trained for epoch 4 by 2019-01-27 20:43:13.994688!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0426
7. set (Dataset 2) being trained for epoch 4 by 2019-01-27 20:43:32.301068!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0531
8. set (Dataset 6) being trained for epoch 4 by 2019-01-27 20:43:46.750586!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0719
9. set (Dataset 13) being trained for epoch 4 by 2019-01-27 20:44:01.205251!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0497
10. set (Dataset 23) being trained for epoch 4 by 2019-01-27 20:44:15.290187!
Epoch 1/1
569/569 [==============================] - 10s 17ms/step - loss: 0.0714
11. set (Dataset 18) being trained for epoch 4 by 2019-01-27 20:44:31.179157!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0638
12. set (Dataset 19) being trained for epoch 4 by 2019-01-27 20:44:47.122092!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0622
13. set (Dataset 11) being trained for epoch 4 by 2019-01-27 20:45:02.095960!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0439
14. set (Dataset 16) being trained for epoch 4 by 2019-01-27 20:45:21.254981!
Epoch 1/1
914/914 [==============================] - 16s 17ms/step - loss: 0.0461
15. set (Dataset 21) being trained for epoch 4 by 2019-01-27 20:45:43.237209!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0744
16. set (Dataset 20) being trained for epoch 4 by 2019-01-27 20:46:00.128440!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0467
17. set (Dataset 24) being trained for epoch 4 by 2019-01-27 20:46:15.174648!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0463
18. set (Dataset 15) being trained for epoch 4 by 2019-01-27 20:46:30.365198!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0592
19. set (Dataset 8) being trained for epoch 4 by 2019-01-27 20:46:49.732452!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0593
20. set (Dataset 22) being trained for epoch 4 by 2019-01-27 20:47:10.169264!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0414
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:47:26.281842
1. set (Dataset 15) being trained for epoch 5 by 2019-01-27 20:47:32.616670!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0537
2. set (Dataset 22) being trained for epoch 5 by 2019-01-27 20:47:50.843992!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0402
3. set (Dataset 18) being trained for epoch 5 by 2019-01-27 20:48:08.900213!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0623
4. set (Dataset 21) being trained for epoch 5 by 2019-01-27 20:48:26.029583!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0736
5. set (Dataset 7) being trained for epoch 5 by 2019-01-27 20:48:44.850779!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0440
6. set (Dataset 8) being trained for epoch 5 by 2019-01-27 20:49:05.999875!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0571
7. set (Dataset 24) being trained for epoch 5 by 2019-01-27 20:49:24.281479!
Epoch 1/1
492/492 [==============================] - 9s 19ms/step - loss: 0.0478
8. set (Dataset 19) being trained for epoch 5 by 2019-01-27 20:49:38.436123!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0640
9. set (Dataset 12) being trained for epoch 5 by 2019-01-27 20:49:54.674435!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0477
10. set (Dataset 13) being trained for epoch 5 by 2019-01-27 20:50:12.734018!
Epoch 1/1
485/485 [==============================] - 8s 17ms/step - loss: 0.0482
11. set (Dataset 2) being trained for epoch 5 by 2019-01-27 20:50:26.313955!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0514
12. set (Dataset 4) being trained for epoch 5 by 2019-01-27 20:50:43.113362!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0560
13. set (Dataset 16) being trained for epoch 5 by 2019-01-27 20:51:05.571285!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0488
14. set (Dataset 1) being trained for epoch 5 by 2019-01-27 20:51:26.812667!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0604
15. set (Dataset 17) being trained for epoch 5 by 2019-01-27 20:51:39.623408!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0557
16. set (Dataset 20) being trained for epoch 5 by 2019-01-27 20:51:51.726683!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0466
17. set (Dataset 11) being trained for epoch 5 by 2019-01-27 20:52:07.450193!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0396
18. set (Dataset 10) being trained for epoch 5 by 2019-01-27 20:52:25.160107!
Epoch 1/1
726/726 [==============================] - 13s 19ms/step - loss: 0.0463
19. set (Dataset 23) being trained for epoch 5 by 2019-01-27 20:52:44.140889!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0721
20. set (Dataset 6) being trained for epoch 5 by 2019-01-27 20:52:59.557697!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0712
Epoch 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:53:13.730410
1. set (Dataset 10) being trained for epoch 6 by 2019-01-27 20:53:20.941388!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0448
2. set (Dataset 6) being trained for epoch 6 by 2019-01-27 20:53:39.154065!
Epoch 1/1
542/542 [==============================] - 9s 17ms/step - loss: 0.0674
3. set (Dataset 2) being trained for epoch 6 by 2019-01-27 20:53:53.695783!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0508
4. set (Dataset 17) being trained for epoch 6 by 2019-01-27 20:54:06.798744!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0553
5. set (Dataset 8) being trained for epoch 6 by 2019-01-27 20:54:21.595368!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0594
6. set (Dataset 23) being trained for epoch 6 by 2019-01-27 20:54:40.843555!
Epoch 1/1
569/569 [==============================] - 11s 19ms/step - loss: 0.0708
7. set (Dataset 11) being trained for epoch 6 by 2019-01-27 20:54:57.127957!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0383
8. set (Dataset 4) being trained for epoch 6 by 2019-01-27 20:55:14.803252!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0539
9. set (Dataset 7) being trained for epoch 6 by 2019-01-27 20:55:35.533672!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0487
10. set (Dataset 12) being trained for epoch 6 by 2019-01-27 20:55:56.122462!
Epoch 1/1
732/732 [==============================] - 14s 19ms/step - loss: 0.0464
11. set (Dataset 24) being trained for epoch 6 by 2019-01-27 20:56:14.495113!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0470
12. set (Dataset 15) being trained for epoch 6 by 2019-01-27 20:56:29.763210!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0553
13. set (Dataset 1) being trained for epoch 6 by 2019-01-27 20:56:46.751536!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0623
14. set (Dataset 22) being trained for epoch 6 by 2019-01-27 20:57:02.298452!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0405
15. set (Dataset 18) being trained for epoch 6 by 2019-01-27 20:57:19.977092!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0638
16. set (Dataset 20) being trained for epoch 6 by 2019-01-27 20:57:36.302073!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0454
17. set (Dataset 16) being trained for epoch 6 by 2019-01-27 20:57:54.989413!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0451
18. set (Dataset 21) being trained for epoch 6 by 2019-01-27 20:58:17.317649!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0765
19. set (Dataset 13) being trained for epoch 6 by 2019-01-27 20:58:33.762031!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0506
20. set (Dataset 19) being trained for epoch 6 by 2019-01-27 20:58:47.532448!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0632
Epoch 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:59:01.119648
1. set (Dataset 21) being trained for epoch 7 by 2019-01-27 20:59:07.152782!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0764
2. set (Dataset 19) being trained for epoch 7 by 2019-01-27 20:59:23.615786!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0655
3. set (Dataset 24) being trained for epoch 7 by 2019-01-27 20:59:37.474553!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0458
4. set (Dataset 18) being trained for epoch 7 by 2019-01-27 20:59:52.357822!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0602
5. set (Dataset 23) being trained for epoch 7 by 2019-01-27 21:00:09.131903!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0702
6. set (Dataset 13) being trained for epoch 7 by 2019-01-27 21:00:24.145693!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0492
7. set (Dataset 16) being trained for epoch 7 by 2019-01-27 21:00:41.765525!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0441
8. set (Dataset 15) being trained for epoch 7 by 2019-01-27 21:01:04.565467!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0557
9. set (Dataset 8) being trained for epoch 7 by 2019-01-27 21:01:24.292293!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0594
10. set (Dataset 7) being trained for epoch 7 by 2019-01-27 21:01:45.613123!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0415
11. set (Dataset 11) being trained for epoch 7 by 2019-01-27 21:02:04.681426!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0400
12. set (Dataset 10) being trained for epoch 7 by 2019-01-27 21:02:22.328591!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0453
13. set (Dataset 22) being trained for epoch 7 by 2019-01-27 21:02:41.727726!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0406
14. set (Dataset 6) being trained for epoch 7 by 2019-01-27 21:02:59.024777!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0730
15. set (Dataset 2) being trained for epoch 7 by 2019-01-27 21:03:14.044452!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0563
16. set (Dataset 20) being trained for epoch 7 by 2019-01-27 21:03:28.792972!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0458
17. set (Dataset 1) being trained for epoch 7 by 2019-01-27 21:03:44.018371!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0577
18. set (Dataset 17) being trained for epoch 7 by 2019-01-27 21:03:56.848867!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0535
19. set (Dataset 12) being trained for epoch 7 by 2019-01-27 21:04:11.171620!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0463
20. set (Dataset 4) being trained for epoch 7 by 2019-01-27 21:04:31.545405!
Epoch 1/1
744/744 [==============================] - 13s 17ms/step - loss: 0.0548
Epoch 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 21:04:48.605977
1. set (Dataset 17) being trained for epoch 8 by 2019-01-27 21:04:52.351968!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0579
2. set (Dataset 4) being trained for epoch 8 by 2019-01-27 21:05:06.722248!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0509
3. set (Dataset 11) being trained for epoch 8 by 2019-01-27 21:05:26.089432!
Epoch 1/1
572/572 [==============================] - 11s 19ms/step - loss: 0.0388
4. set (Dataset 2) being trained for epoch 8 by 2019-01-27 21:05:41.798442!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0511
5. set (Dataset 13) being trained for epoch 8 by 2019-01-27 21:05:55.851851!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0474
6. set (Dataset 12) being trained for epoch 8 by 2019-01-27 21:06:11.942714!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0453
7. set (Dataset 1) being trained for epoch 8 by 2019-01-27 21:06:30.153561!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0575
8. set (Dataset 10) being trained for epoch 8 by 2019-01-27 21:06:46.246435!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0453
9. set (Dataset 23) being trained for epoch 8 by 2019-01-27 21:07:04.571913!
Epoch 1/1
569/569 [==============================] - 11s 19ms/step - loss: 0.0695
10. set (Dataset 8) being trained for epoch 8 by 2019-01-27 21:07:22.898936!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0575
11. set (Dataset 16) being trained for epoch 8 by 2019-01-27 21:07:45.850992!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0452
12. set (Dataset 21) being trained for epoch 8 by 2019-01-27 21:08:08.301790!
Epoch 1/1
634/634 [==============================] - 11s 17ms/step - loss: 0.0720
13. set (Dataset 6) being trained for epoch 8 by 2019-01-27 21:08:24.565410!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0680
14. set (Dataset 19) being trained for epoch 8 by 2019-01-27 21:08:39.068700!
Epoch 1/1
502/502 [==============================] - 9s 17ms/step - loss: 0.0682
15. set (Dataset 24) being trained for epoch 8 by 2019-01-27 21:08:52.536034!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0473
16. set (Dataset 20) being trained for epoch 8 by 2019-01-27 21:09:07.010266!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0473
17. set (Dataset 22) being trained for epoch 8 by 2019-01-27 21:09:23.610244!
Epoch 1/1
665/665 [==============================] - 12s 19ms/step - loss: 0.0408
18. set (Dataset 18) being trained for epoch 8 by 2019-01-27 21:09:41.905047!
Epoch 1/1
614/614 [==============================] - 11s 17ms/step - loss: 0.0628
19. set (Dataset 7) being trained for epoch 8 by 2019-01-27 21:10:00.217548!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0431
20. set (Dataset 15) being trained for epoch 8 by 2019-01-27 21:10:20.290278!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0573
Epoch 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 21:10:36.659523
1. set (Dataset 18) being trained for epoch 9 by 2019-01-27 21:10:42.541369!
Epoch 1/1
 96/614 [===>..........................] - ETA: 9s - loss: 0.0667^C
Model Exp1-27_18-42-47_part4_and_2019-01-27_20-16-38 has been interrupted.
Exp1-27_18-42-47_part4_and_2019-01-27_20-16-38.h5 has been saved.
Model Exp1-27_18-42-47_part4_and_2019-01-27_20-16-38 has been recorded successfully.
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python  continueFC_RNN_Experiment
.py Exp1-27_18-42-47_part4_and_2019-01-27_20-16-38
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 21:11:30.005331: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 21:11:30.101752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 21:11:30.102007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 21:11:30.102019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 21:11:30.258017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 21:11:30.258045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 21:11:30.258049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 21:11:30.258187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Exp1-27_18-42-47_part4_and_2019-01-27_20-16-38.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04'), (11, 'M0
5'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'), (20, 'M12'), (21, 'F02')
, (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp1-27_18-42-47_part4_and_2019-01-27_20-16-38_and_2019-01-27_21-11-31
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 21:11:33.163705
For the Subject 3 (F03):
730/730 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 10.59 Degree
        The absolute mean error on Yaw angle estimation: 26.66 Degree
        The absolute mean error on Roll angle estimation: 11.30 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.82 Degree
        The absolute mean error on Yaw angle estimation: 20.28 Degree
        The absolute mean error on Roll angle estimation: 3.14 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 30.67 Degree
        The absolute mean error on Yaw angle estimation: 25.09 Degree
        The absolute mean error on Roll angle estimation: 12.66 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 18.85 Degree
        The absolute mean error on Yaw angle estimation: 27.95 Degree
        The absolute mean error on Roll angle estimation: 13.44 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.98 Degree
        The absolute mean error on Yaw angle estimations: 24.99 Degree
        The absolute mean error on Roll angle estimations: 10.14 Degree
subject3_Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31.png has been saved by 2019-01-27 21:12:39.405316.
subject5_Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31.png has been saved by 2019-01-27 21:12:39.605792.
subject9_Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31.png has been saved by 2019-01-27 21:12:39.805600.
subject14_Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31.png has been saved by 2019-01-27 21:12:40.026663.
Model Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31 has been evaluated successfully.
Model Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "don't use fc layer
s"
[master 3975539] don't use fc layers
 7 files changed, 288 insertions(+), 55 deletions(-)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp1-27_18-42-47_part4_and_2019-01-27_20-16-38/output_Exp1-2
7_18-42-47_part4_and_2019-01-27_20-16-38.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31/output_Exp201
9-01-27_20-16-38_and_2019-01-27_21-11-31.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31/subject14_Exp
2019-01-27_20-16-38_and_2019-01-27_21-11-31.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31/subject3_Exp2
019-01-27_20-16-38_and_2019-01-27_21-11-31.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31/subject5_Exp2
019-01-27_20-16-38_and_2019-01-27_21-11-31.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_20-16-38_and_2019-01-27_21-11-31/subject9_Exp2
019-01-27_20-16-38_and_2019-01-27_21-11-31.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 13, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (13/13), done.
Writing objects: 100% (13/13), 649.75 KiB | 0 bytes/s, done.
Total 13 (delta 4), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   426eef8..3975539  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git pull
remote: Counting objects: 35, done.
remote: Compressing objects: 100% (34/34), done.
remote: Total 35 (delta 19), reused 0 (delta 0)
Unpacking objects: 100% (35/35), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   3975539..61f265e  master     -> origin/master
Updating 3975539..61f265e
Fast-forward
 DeepRL_For_HPE/FC_RNN_Evaluater/Stateful_FC_RNN_Configuration.py         |    28 +-
 DeepRL_For_HPE/FC_RNN_Evaluater/continueFC_RNN_Experiment.py             |     6 +-
 .../scrollback_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.txt        | 14971 -------------------------------
 .../results/Exp2019-01-25_20-19-34/output_Exp2019-01-25_20-19-34.txt     |   146 -
 .../results/Exp2019-01-25_20-19-34/subject14_Exp2019-01-25_20-19-34.png  |   Bin 216571 -> 0 bytes
 .../results/Exp2019-01-25_20-19-34/subject3_Exp2019-01-25_20-19-34.png   |   Bin 158911 -> 0 bytes
 .../results/Exp2019-01-25_20-19-34/subject5_Exp2019-01-25_20-19-34.png   |   Bin 192661 -> 0 bytes
 .../results/Exp2019-01-25_20-19-34/subject9_Exp2019-01-25_20-19-34.png   |   Bin 189849 -> 0 bytes
 .../results/Exp2019-01-25_23-51-56/output_Exp2019-01-25_23-51-56.txt     |   104 -
 .../Exp2019-01-25_23-51-56/subject14_Exp2019-01-25_23-51-56_part2.png    |   Bin 169815 -> 0 bytes
 .../Exp2019-01-25_23-51-56/subject3_Exp2019-01-25_23-51-56_part2.png     |   Bin 125288 -> 0 bytes
 .../Exp2019-01-25_23-51-56/subject5_Exp2019-01-25_23-51-56_part2.png     |   Bin 176853 -> 0 bytes
 .../Exp2019-01-25_23-51-56/subject9_Exp2019-01-25_23-51-56_part2.png     |   Bin 142883 -> 0 bytes
 .../results/Exp2019-01-26_04-10-16/output_Exp2019-01-26_04-10-16.txt     |   108 -
 .../results/Exp2019-01-26_04-10-16/subject9_Exp2019-01-26_04-10-16.png   |   Bin 190226 -> 0 bytes
 .../results/Exp2019-01-26_04-28-49/scrollback_Exp2019-01-26_04-28-49.txt | 14971 +++++++++++++++++++++++++++++++
 .../results/Exp2019-01-26_13-37-43/output_Exp2019-01-26_13-37-43.txt     |   116 -
 .../results/Exp2019-01-26_13-37-43/subject14_Exp2019-01-26_13-37-43.png  |   Bin 186127 -> 0 bytes
 .../results/Exp2019-01-26_13-37-43/subject3_Exp2019-01-26_13-37-43.png   |   Bin 167262 -> 0 bytes
 .../results/Exp2019-01-26_13-37-43/subject5_Exp2019-01-26_13-37-43.png   |   Bin 170909 -> 0 bytes
 .../results/Exp2019-01-26_13-37-43/subject9_Exp2019-01-26_13-37-43.png   |   Bin 163913 -> 0 bytes
 .../results/Exp2019-01-27_11-59-18/output_Exp2019-01-27_11-59-18.txt     |   108 -
 .../results/Exp2019-01-27_11-59-18/subject9_Exp2019-01-27_11-59-18.png   |   Bin 133212 -> 0 bytes
 .../results/Exp2019-01-27_13-14-25/output_Exp2019-01-27_13-14-25.txt     |   108 -
 .../results/Exp2019-01-27_13-14-25/subject9_Exp2019-01-27_13-14-25.png   |   Bin 120821 -> 0 bytes
 .../results/Exp2019-01-27_13-25-50/output_Exp2019-01-27_13-25-50.txt     |   102 -
 .../results/Exp2019-01-27_13-25-50/subject9_Exp2019-01-27_13-25-50.png   |   Bin 121272 -> 0 bytes
 .../results/Exp2019-01-27_13-35-44/output_Exp2019-01-27_13-35-44.txt     |   102 -
 .../results/Exp2019-01-27_13-35-44/subject9_Exp2019-01-27_13-35-44.png   |   Bin 163758 -> 0 bytes
 .../results/Exp2019-01-27_13-36-52/output_Exp2019-01-27_13-36-52.txt     |   102 -
 .../results/Exp2019-01-27_13-36-52/subject9_Exp2019-01-27_13-36-52.png   |   Bin 116079 -> 0 bytes
 .../results/Exp2019-01-27_13-39-33/output_Exp2019-01-27_13-39-33.txt     |   108 -
 .../results/Exp2019-01-27_13-39-33/subject9_Exp2019-01-27_13-39-33.png   |   Bin 156638 -> 0 bytes
 .../results/Exp2019-01-27_13-41-30/output_Exp2019-01-27_13-41-30.txt     |   108 -
 .../results/Exp2019-01-27_13-41-30/subject9_Exp2019-01-27_13-41-30.png   |   Bin 165208 -> 0 bytes
 .../results/Exp2019-01-27_13-43-20/subject9_Exp2019-01-27_13-43-20.png   |   Bin 149959 -> 0 bytes
 .../results/Exp2019-01-27_13-44-42/output_Exp2019-01-27_13-44-42.txt     |   108 -
 .../results/Exp2019-01-27_13-44-42/subject9_Exp2019-01-27_13-44-42.png   |   Bin 165501 -> 0 bytes
 .../results/Exp2019-01-27_13-46-18/output_Exp2019-01-27_13-46-18.txt     |   102 -
 .../results/Exp2019-01-27_13-46-18/subject9_Exp2019-01-27_13-46-18.png   |   Bin 165599 -> 0 bytes
 .../results/Exp2019-01-27_13-54-23/output_Exp2019-01-27_13-54-23.txt     |   122 -
 .../results/Exp2019-01-27_13-54-23/subject9_Exp2019-01-27_13-54-23.png   |   Bin 164019 -> 0 bytes
 .../results/Exp2019-01-27_13-55-59/output_Exp2019-01-27_13-55-59.txt     |   122 -
 .../results/Exp2019-01-27_13-55-59/subject9_Exp2019-01-27_13-55-59.png   |   Bin 178380 -> 0 bytes
 .../results/Exp2019-01-27_13-57-56/output_Exp2019-01-27_13-57-56.txt     |   257 -
 .../results/Exp2019-01-27_13-57-56/subject9_Exp2019-01-27_13-57-56.png   |   Bin 171099 -> 0 bytes
 .../results/Exp2019-01-27_14-12-09/output_Exp2019-01-27_14-12-09.txt     |   249 -
 .../results/Exp2019-01-27_14-12-09/subject9_Exp2019-01-27_14-12-09.png   |   Bin 165182 -> 0 bytes
 .../results/Exp2019-01-27_14-22-51/output_Exp2019-01-27_14-22-51.txt     |   251 -
 .../results/Exp2019-01-27_14-22-51/subject9_Exp2019-01-27_14-22-51.png   |   Bin 181891 -> 0 bytes
 .../results/Exp2019-01-27_14-37-00/output_Exp2019-01-27_14-37-00.txt     |   144 -
 .../results/Exp2019-01-27_14-37-00/subject9_Exp2019-01-27_14-37-00.png   |   Bin 165564 -> 0 bytes
 .../results/Exp2019-01-27_14-44-55/output_Exp2019-01-27_14-44-55.txt     |   144 -
 .../results/Exp2019-01-27_14-44-55/subject9_Exp2019-01-27_14-44-55.png   |   Bin 204995 -> 0 bytes
 .../results/Exp2019-01-27_17-06-27/output_Exp2019-01-27_17-06-27.txt     |   144 -
 .../results/Exp2019-01-27_17-06-27/subject9_Exp2019-01-27_17-06-27.png   |   Bin 178214 -> 0 bytes
 .../results/Exp2019-01-27_17-11-08/output_Exp2019-01-27_17-11-08.txt     |   144 -
 .../results/Exp2019-01-27_17-11-08/subject9_Exp2019-01-27_17-11-08.png   |   Bin 110677 -> 0 bytes
 .../results/Exp2019-01-27_17-13-59/output_Exp2019-01-27_17-13-59.txt     |   144 -
 .../results/Exp2019-01-27_17-13-59/subject9_Exp2019-01-27_17-13-59.png   |   Bin 217042 -> 0 bytes
 .../results/Exp2019-01-27_17-37-52/output_Exp2019-01-27_17-37-52.txt     |   144 -
 .../results/Exp2019-01-27_17-37-52/subject9_Exp2019-01-27_17-37-52.png   |   Bin 208585 -> 0 bytes
 .../results/Exp2019-01-27_17-41-45/output_Exp2019-01-27_17-41-45.txt     |   129 -
 .../results/Exp2019-01-27_17-41-45/subject9_Exp2019-01-27_17-41-45.png   |   Bin 130391 -> 0 bytes
 .../results/Exp2019-01-27_17-44-39/output_Exp2019-01-27_17-44-39.txt     |   129 -
 .../results/Exp2019-01-27_17-44-39/subject9_Exp2019-01-27_17-44-39.png   |   Bin 73700 -> 0 bytes
 .../results/Exp2019-01-27_17-47-12/output_Exp2019-01-27_17-47-12.txt     |   129 -
 .../results/Exp2019-01-27_17-47-12/subject9_Exp2019-01-27_17-47-12.png   |   Bin 167706 -> 0 bytes
 .../output_Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51.txt            |   159 -
 .../subject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51.png          |   Bin 160505 -> 0 bytes
 .../output_Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26.txt            |    61 -
 .../subject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26.png          |   Bin 157826 -> 0 bytes
 .../Exp2019-01-27_18-18-37_part1/output_Exp2019-01-27_18-18-37_part1.txt |    99 -
 .../Exp2019-01-27_18-38-18_part1/output_Exp2019-01-27_18-38-18_part1.txt |    98 -
 .../Exp2019-01-27_18-39-25_part1/output_Exp2019-01-27_18-39-25_part1.txt |   101 -
 .../Exp2019-01-27_18-42-47_part4/output_Exp2019-01-27_18-42-47_part4.txt |   489 -
 .../output_Exp2019-01-27_20-07-11.txt}                                   |    55 +-
 .../results/Exp2019-01-27_20-07-11/subject9_Exp2019-01-27_20-07-11.png   |   Bin 0 -> 243803 bytes
 .../Exp2019-01-27_20-46-46_part1/output_Exp2019-01-27_20-46-46_part1.txt |   713 ++
 .../results/Exp2019-01-27_20-58-30/output_Exp2019-01-27_20-58-30.txt     |   728 ++
 .../results/Exp2019-01-27_20-58-30/subject9_Exp2019-01-27_20-58-30.png   |   Bin 0 -> 204463 bytes
 .../results/Exp2019-01-27_21-01-37/output_Exp2019-01-27_21-01-37.txt     |   728 ++
 .../results/Exp2019-01-27_21-01-37/subject9_Exp2019-01-27_21-01-37.png   |   Bin 0 -> 184138 bytes
 .../results/Exp2019-01-27_21-05-02/output_Exp2019-01-27_21-05-02.txt     |   756 ++
 .../results/Exp2019-01-27_21-05-02/subject9_Exp2019-01-27_21-05-02.png   |   Bin 0 -> 181642 bytes
 .../FC_RNN_Evaluater/results/Last_Model_/output_Last_Model.txt           |   713 +-
 .../FC_RNN_Evaluater/results/Last_Model___/output_Last_Model.txt         |    97 -
 .../FC_RNN_Evaluater/results/Last_Model____/output_Last_Model.txt        |    97 -
 .../FC_RNN_Evaluater/results/Last_Model_____/output_Last_Model.txt       |     1 -
 .../FC_RNN_Evaluater/results/Last_Model______/output_Last_Model.txt      |    96 -
 DeepRL_For_HPE/FC_RNN_Evaluater/runFC_RNN_Experiment.py                  |     2 +-
 DeepRL_For_HPE/Note_Files/commands.txt                                   |     2 +-
 92 files changed, 18655 insertions(+), 19990 deletions(-)
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-25_20-19-34/output_Exp2019-01-25_20-19-34.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-25_20-19-34/subject14_Exp2019-01-25_20-19-34.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-25_20-19-34/subject3_Exp2019-01-25_20-19-34.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-25_20-19-34/subject5_Exp2019-01-25_20-19-34.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-25_20-19-34/subject9_Exp2019-01-25_20-19-34.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-25_23-51-56/output_Exp2019-01-25_23-51-56.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-25_23-51-56/subject14_Exp2019-01-25_23-51-56_part
2.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-25_23-51-56/subject3_Exp2019-01-25_23-51-56_part2
.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-25_23-51-56/subject5_Exp2019-01-25_23-51-56_part2
.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-25_23-51-56/subject9_Exp2019-01-25_23-51-56_part2
.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-10-16/output_Exp2019-01-26_04-10-16.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-10-16/subject9_Exp2019-01-26_04-10-16.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/scrollback_Exp2019-01-26_04-28-49.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/output_Exp2019-01-26_13-37-43.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject14_Exp2019-01-26_13-37-43.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject3_Exp2019-01-26_13-37-43.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject5_Exp2019-01-26_13-37-43.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject9_Exp2019-01-26_13-37-43.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_11-59-18/output_Exp2019-01-27_11-59-18.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_11-59-18/subject9_Exp2019-01-27_11-59-18.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-14-25/output_Exp2019-01-27_13-14-25.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-14-25/subject9_Exp2019-01-27_13-14-25.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-25-50/output_Exp2019-01-27_13-25-50.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-25-50/subject9_Exp2019-01-27_13-25-50.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-35-44/output_Exp2019-01-27_13-35-44.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-35-44/subject9_Exp2019-01-27_13-35-44.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-36-52/output_Exp2019-01-27_13-36-52.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-36-52/subject9_Exp2019-01-27_13-36-52.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-39-33/output_Exp2019-01-27_13-39-33.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-39-33/subject9_Exp2019-01-27_13-39-33.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-41-30/output_Exp2019-01-27_13-41-30.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-41-30/subject9_Exp2019-01-27_13-41-30.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-43-20/subject9_Exp2019-01-27_13-43-20.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-44-42/output_Exp2019-01-27_13-44-42.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-44-42/subject9_Exp2019-01-27_13-44-42.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-46-18/output_Exp2019-01-27_13-46-18.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-46-18/subject9_Exp2019-01-27_13-46-18.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-54-23/output_Exp2019-01-27_13-54-23.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-54-23/subject9_Exp2019-01-27_13-54-23.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-55-59/output_Exp2019-01-27_13-55-59.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-55-59/subject9_Exp2019-01-27_13-55-59.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-57-56/output_Exp2019-01-27_13-57-56.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-57-56/subject9_Exp2019-01-27_13-57-56.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-12-09/output_Exp2019-01-27_14-12-09.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-12-09/subject9_Exp2019-01-27_14-12-09.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-22-51/output_Exp2019-01-27_14-22-51.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-22-51/subject9_Exp2019-01-27_14-22-51.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-37-00/output_Exp2019-01-27_14-37-00.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-37-00/subject9_Exp2019-01-27_14-37-00.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-44-55/output_Exp2019-01-27_14-44-55.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-44-55/subject9_Exp2019-01-27_14-44-55.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-06-27/output_Exp2019-01-27_17-06-27.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-06-27/subject9_Exp2019-01-27_17-06-27.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-11-08/output_Exp2019-01-27_17-11-08.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-11-08/subject9_Exp2019-01-27_17-11-08.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-13-59/output_Exp2019-01-27_17-13-59.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-13-59/subject9_Exp2019-01-27_17-13-59.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-37-52/output_Exp2019-01-27_17-37-52.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-37-52/subject9_Exp2019-01-27_17-37-52.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-41-45/output_Exp2019-01-27_17-41-45.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-41-45/subject9_Exp2019-01-27_17-41-45.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-44-39/output_Exp2019-01-27_17-44-39.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-44-39/subject9_Exp2019-01-27_17-44-39.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12/output_Exp2019-01-27_17-47-12.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12/subject9_Exp2019-01-27_17-47-12.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51/output_Exp201
9-01-27_17-47-12_and_2019-01-27_17-58-51.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51/subject9_Exp2
019-01-27_17-47-12_and_2019-01-27_17-58-51.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26/output_Exp201
9-01-27_17-47-12_and_2019-01-27_17-59-26.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26/subject9_Exp2
019-01-27_17-47-12_and_2019-01-27_17-59-26.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_18-18-37_part1/output_Exp2019-01-27_18-18-37_p
art1.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_18-38-18_part1/output_Exp2019-01-27_18-38-18_p
art1.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_18-39-25_part1/output_Exp2019-01-27_18-39-25_p
art1.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_18-42-47_part4/output_Exp2019-01-27_18-42-47_p
art4.txt
 rename DeepRL_For_HPE/FC_RNN_Evaluater/results/{Exp2019-01-27_13-43-20/output_Exp2019-01-27_13-43-20.txt => Exp2019-01-
27_20-07-11/output_Exp2019-01-27_20-07-11.txt} (72%)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_20-07-11/subject9_Exp2019-01-27_20-07-11.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_20-46-46_part1/output_Exp2019-01-27_20-46-46_p
art1.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_20-58-30/output_Exp2019-01-27_20-58-30.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_20-58-30/subject9_Exp2019-01-27_20-58-30.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_21-01-37/output_Exp2019-01-27_21-01-37.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_21-01-37/subject9_Exp2019-01-27_21-01-37.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_21-05-02/output_Exp2019-01-27_21-05-02.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_21-05-02/subject9_Exp2019-01-27_21-05-02.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model___/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model____/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_____/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model______/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 21:21:04.710150: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 21:21:04.807253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 21:21:04.807509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 21:21:04.807522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 21:21:04.964805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 21:21:04.964832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 21:21:04.964837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 21:21:04.964976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_21-21-21 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
time_distributed_1 (TimeDist (1, 1, 2048)              0
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   165520
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 21,968,367
Trainable params: 165,583
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-21-21
All frames and annotations from 1 datasets have been read by 2019-01-27 21:21:22.417923
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 21:21:31.601171!
Epoch 1/1
882/882 [==============================] - 27s 31ms/step - loss: 0.2035
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-21-21
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:22:01.395860
For the Subject 9 (M03):
882/882 [==============================] - 22s 25ms/step
        The absolute mean error on Pitch angle estimation: 28.30 Degree
        The absolute mean error on Yaw angle estimation: 30.98 Degree
        The absolute mean error on Roll angle estimation: 11.63 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 28.30 Degree
        The absolute mean error on Yaw angle estimations: 30.98 Degree
        The absolute mean error on Roll angle estimations: 11.63 Degree
Exp2019-01-27_21-21-21_part1 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-21-21
All frames and annotations from 1 datasets have been read by 2019-01-27 21:22:33.385174
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 21:22:42.556946!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1840
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-21-21
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:23:06.529625
For the Subject 9 (M03):
882/882 [==============================] - 21s 23ms/step
        The absolute mean error on Pitch angle estimation: 27.11 Degree
        The absolute mean error on Yaw angle estimation: 25.30 Degree
        The absolute mean error on Roll angle estimation: 9.11 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 27.11 Degree
        The absolute mean error on Yaw angle estimations: 25.30 Degree
        The absolute mean error on Roll angle estimations: 9.11 Degree
Exp2019-01-27_21-21-21_part2 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-21-21
All frames and annotations from 1 datasets have been read by 2019-01-27 21:23:37.301618
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 21:23:46.470864!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1800
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-21-21
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:24:10.517099
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 23.86 Degree
        The absolute mean error on Yaw angle estimation: 24.60 Degree
        The absolute mean error on Roll angle estimation: 8.98 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.86 Degree
        The absolute mean error on Yaw angle estimations: 24.60 Degree
        The absolute mean error on Roll angle estimations: 8.98 Degree
Exp2019-01-27_21-21-21_part3 completed!
Exp2019-01-27_21-21-21.h5 has been saved.
subject9_Exp2019-01-27_21-21-21.png has been saved by 2019-01-27 21:24:41.914539.
Model Exp2019-01-27_21-21-21 has been evaluated successfully.
Model Exp2019-01-27_21-21-21 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 10, in <module>
    from FC_RNN_Evaluater import *
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 106
    if num_testSubjects == return means
                                ^
SyntaxError: invalid syntax
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 21:28:48.395131: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 21:28:48.493025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 21:28:48.493333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 21:28:48.493345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 21:28:48.652369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 21:28:48.652396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 21:28:48.652401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 21:28:48.652579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_21-29-05 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
time_distributed_1 (TimeDist (1, 1, 2048)              0
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   165520
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 21,968,367
Trainable params: 165,583
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.000001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000001_2
019-01-27_21-29-05
All frames and annotations from 1 datasets have been read by 2019-01-27 21:29:06.164555
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 21:29:15.334876!
Epoch 1/1
882/882 [==============================] - 27s 31ms/step - loss: 0.2369
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000001
_2019-01-27_21-29-05
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:29:44.822612
For the Subject 9 (M03):
882/882 [==============================] - 22s 25ms/step
        The absolute mean error on Pitch angle estimation: 21.31 Degree
        The absolute mean error on Yaw angle estimation: 73.94 Degree
        The absolute mean error on Roll angle estimation: 44.26 Degree
Exp2019-01-27_21-29-05_part1 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000001_2
019-01-27_21-29-05
All frames and annotations from 1 datasets have been read by 2019-01-27 21:30:16.832844
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 21:30:26.008540!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1915
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000001
_2019-01-27_21-29-05
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:30:49.956935
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 22.61 Degree
        The absolute mean error on Yaw angle estimation: 74.20 Degree
        The absolute mean error on Roll angle estimation: 39.74 Degree
Exp2019-01-27_21-29-05_part2 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000001_2
019-01-27_21-29-05
All frames and annotations from 1 datasets have been read by 2019-01-27 21:31:20.835517
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 21:31:30.050934!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1864
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000001
_2019-01-27_21-29-05
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:31:54.146236
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 22.93 Degree
        The absolute mean error on Yaw angle estimation: 68.19 Degree
        The absolute mean error on Roll angle estimation: 33.01 Degree
Exp2019-01-27_21-29-05_part3 completed!
Exp2019-01-27_21-29-05.h5 has been saved.
subject9_Exp2019-01-27_21-29-05.png has been saved by 2019-01-27 21:32:25.457054.
Model Exp2019-01-27_21-29-05 has been evaluated successfully.
Model Exp2019-01-27_21-29-05 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 21:33:39.106477: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 21:33:39.203284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 21:33:39.203594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 21:33:39.203609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 21:33:39.362188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 21:33:39.362213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 21:33:39.362218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 21:33:39.362405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_21-33-55 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
time_distributed_1 (TimeDist (1, 1, 2048)              0
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   165520
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 21,968,367
Trainable params: 165,583
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000_2
019-01-27_21-33-55
All frames and annotations from 1 datasets have been read by 2019-01-27 21:33:56.665823
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 21:34:05.811112!
Epoch 1/1
882/882 [==============================] - 27s 31ms/step - loss: 0.1801
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000
_2019-01-27_21-33-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:34:35.725490
For the Subject 9 (M03):
882/882 [==============================] - 22s 25ms/step
        The absolute mean error on Pitch angle estimation: 18.28 Degree
        The absolute mean error on Yaw angle estimation: 25.78 Degree
        The absolute mean error on Roll angle estimation: 6.82 Degree
Exp2019-01-27_21-33-55_part1 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000_2
019-01-27_21-33-55
All frames and annotations from 1 datasets have been read by 2019-01-27 21:35:07.769094
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 21:35:16.938957!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1697
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000
_2019-01-27_21-33-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:35:40.906142
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 18.28 Degree
        The absolute mean error on Yaw angle estimation: 25.72 Degree
        The absolute mean error on Roll angle estimation: 6.82 Degree
Exp2019-01-27_21-33-55_part2 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000_2
019-01-27_21-33-55
All frames and annotations from 1 datasets have been read by 2019-01-27 21:36:11.721915
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 21:36:20.883768!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1695
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000
_2019-01-27_21-33-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:36:44.905703
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 18.27 Degree
        The absolute mean error on Yaw angle estimation: 25.73 Degree
        The absolute mean error on Roll angle estimation: 6.88 Degree
Exp2019-01-27_21-33-55_part3 completed!
Exp2019-01-27_21-33-55.h5 has been saved.
subject9_Exp2019-01-27_21-33-55.png has been saved by 2019-01-27 21:37:16.218541.
Model Exp2019-01-27_21-33-55 has been evaluated successfully.
Model Exp2019-01-27_21-33-55 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 21:38:08.304482: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 21:38:08.401862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 21:38:08.402173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 21:38:08.402188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 21:38:08.559900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 21:38:08.559927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 21:38:08.559932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 21:38:08.560109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_21-38-25 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
time_distributed_1 (TimeDist (1, 1, 2048)              0
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   165520
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 21,968,367
Trainable params: 165,583
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-38-25
All frames and annotations from 1 datasets have been read by 2019-01-27 21:38:26.051552
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 21:38:35.214065!
Epoch 1/1
882/882 [==============================] - 28s 31ms/step - loss: 0.1933
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-38-25
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:39:05.394096
For the Subject 9 (M03):
882/882 [==============================] - 22s 25ms/step
        The absolute mean error on Pitch angle estimation: 24.05 Degree
        The absolute mean error on Yaw angle estimation: 26.35 Degree
        The absolute mean error on Roll angle estimation: 13.81 Degree
Exp2019-01-27_21-38-25_part1 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-38-25
All frames and annotations from 1 datasets have been read by 2019-01-27 21:39:37.501025
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 21:39:46.686781!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1822
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-38-25
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:40:10.890406
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 19.81 Degree
        The absolute mean error on Yaw angle estimation: 26.18 Degree
        The absolute mean error on Roll angle estimation: 7.39 Degree
Exp2019-01-27_21-38-25_part2 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-38-25
All frames and annotations from 1 datasets have been read by 2019-01-27 21:40:41.744074
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 21:40:50.942046!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1751
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-38-25
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:41:15.050897
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 20.23 Degree
        The absolute mean error on Yaw angle estimation: 27.13 Degree
        The absolute mean error on Roll angle estimation: 7.30 Degree
Exp2019-01-27_21-38-25_part3 completed!
Exp2019-01-27_21-38-25.h5 has been saved.
subject9_Exp2019-01-27_21-38-25.png has been saved by 2019-01-27 21:41:46.396063.
Model Exp2019-01-27_21-38-25 has been evaluated successfully.
Model Exp2019-01-27_21-38-25 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 21:42:39.308695: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 21:42:39.410444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 21:42:39.410709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 21:42:39.410724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 21:42:39.569591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 21:42:39.569619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 21:42:39.569627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 21:42:39.569777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_21-42-56 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
time_distributed_1 (TimeDist (1, 1, 2048)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 6147
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 21,810,914
Trainable params: 8,130
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-42-56
All frames and annotations from 1 datasets have been read by 2019-01-27 21:42:57.003264
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 21:43:06.151150!
Epoch 1/1
882/882 [==============================] - 28s 32ms/step - loss: 0.1832
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-42-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:43:36.950542
For the Subject 9 (M03):
882/882 [==============================] - 22s 25ms/step
        The absolute mean error on Pitch angle estimation: 19.40 Degree
        The absolute mean error on Yaw angle estimation: 31.08 Degree
        The absolute mean error on Roll angle estimation: 20.77 Degree
Exp2019-01-27_21-42-56_part1 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-42-56
All frames and annotations from 1 datasets have been read by 2019-01-27 21:44:09.031962
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 21:44:18.203302!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 0.1755
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-42-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:44:42.614880
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 18.22 Degree
        The absolute mean error on Yaw angle estimation: 30.34 Degree
        The absolute mean error on Roll angle estimation: 19.10 Degree
Exp2019-01-27_21-42-56_part2 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-42-56
All frames and annotations from 1 datasets have been read by 2019-01-27 21:45:13.490720
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 21:45:22.669561!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.1736
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-42-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:45:47.182146
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 18.04 Degree
        The absolute mean error on Yaw angle estimation: 31.20 Degree
        The absolute mean error on Roll angle estimation: 17.52 Degree
Exp2019-01-27_21-42-56_part3 completed!
Exp2019-01-27_21-42-56.h5 has been saved.
subject9_Exp2019-01-27_21-42-56.png has been saved by 2019-01-27 21:46:18.652157.
Model Exp2019-01-27_21-42-56 has been evaluated successfully.
Model Exp2019-01-27_21-42-56 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 21:47:56.128548: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 21:47:56.225159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 21:47:56.225466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 21:47:56.225480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 21:47:56.383502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 21:47:56.383527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 21:47:56.383532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 21:47:56.383709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_21-48-12 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 6147
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 21,810,914
Trainable params: 8,130
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-48-12
All frames and annotations from 1 datasets have been read by 2019-01-27 21:48:13.837446
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 21:48:22.984419!
Epoch 1/1
882/882 [==============================] - 28s 32ms/step - loss: 0.1774
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-48-12
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:48:53.326306
For the Subject 9 (M03):
882/882 [==============================] - 22s 25ms/step
        The absolute mean error on Pitch angle estimation: 20.04 Degree
        The absolute mean error on Yaw angle estimation: 27.30 Degree
        The absolute mean error on Roll angle estimation: 7.96 Degree
Exp2019-01-27_21-48-12_part1 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-48-12
All frames and annotations from 1 datasets have been read by 2019-01-27 21:49:25.542573
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 21:49:34.760693!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.1748
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-48-12
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:49:59.357724
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 20.44 Degree
        The absolute mean error on Yaw angle estimation: 26.80 Degree
        The absolute mean error on Roll angle estimation: 8.07 Degree
Exp2019-01-27_21-48-12_part2 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_21-48-12
All frames and annotations from 1 datasets have been read by 2019-01-27 21:50:30.258284
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 21:50:39.423366!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.1725
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_21-48-12
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:51:04.009763
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 21.61 Degree
        The absolute mean error on Yaw angle estimation: 26.67 Degree
        The absolute mean error on Roll angle estimation: 7.81 Degree
Exp2019-01-27_21-48-12_part3 completed!
Exp2019-01-27_21-48-12.h5 has been saved.
subject9_Exp2019-01-27_21-48-12.png has been saved by 2019-01-27 21:51:35.529175.
Model Exp2019-01-27_21-48-12 has been evaluated successfully.
Model Exp2019-01-27_21-48-12 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 21:51:45.568994: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 21:51:45.649593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 21:51:45.649908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 21:51:45.649922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 21:51:45.808547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 21:51:45.808574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 21:51:45.808578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 21:51:45.808756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_di
m_ordering_tf_kernels_notop.h5
87916544/87910968 [==============================] - 2s 0us/step
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 37, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateful_FC_RNN_Configuration.py
", line 92, in getFinalModel
    rnn.add(LSTM(lstm_nodes, dropout=lstm_dropout, recurrent_dropout=lstm_recurrent_dropout, stateful=True))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py", line 500, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 575, in __call__
    self.assert_input_compatibility(inputs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 474, in assert_input_compatib
ility
    str(K.ndim(x)))
ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=5
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 21:52:57.715311: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 21:52:57.813073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 21:52:57.813333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 21:52:57.813343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 21:52:57.971986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 21:52:57.972013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 21:52:57.972018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 21:52:57.972152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_21-53-14 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 8, 8, 2048)        21802784
_________________________________________________________________
time_distributed_1 (TimeDist (1, 1, 131072)            0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 393219
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 22,197,986
Trainable params: 395,202
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = False # True #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01-2
7_21-53-14
All frames and annotations from 1 datasets have been read by 2019-01-27 21:53:15.303831
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 21:53:24.449070!
Epoch 1/1
882/882 [==============================] - 29s 32ms/step - loss: 0.1880
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-27_21-53-14
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:53:55.798508
For the Subject 9 (M03):
882/882 [==============================] - 23s 26ms/step
        The absolute mean error on Pitch angle estimation: 41.17 Degree
        The absolute mean error on Yaw angle estimation: 24.44 Degree
        The absolute mean error on Roll angle estimation: 15.83 Degree
Exp2019-01-27_21-53-14_part1 completed!
Training model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01-2
7_21-53-14
All frames and annotations from 1 datasets have been read by 2019-01-27 21:54:28.482170
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 21:54:37.707652!
Epoch 1/1
882/882 [==============================] - 24s 28ms/step - loss: 0.1689
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-27_21-53-14
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:55:02.861489
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 22.82 Degree
        The absolute mean error on Yaw angle estimation: 29.36 Degree
        The absolute mean error on Roll angle estimation: 18.85 Degree
Exp2019-01-27_21-53-14_part2 completed!
Training model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01-2
7_21-53-14
All frames and annotations from 1 datasets have been read by 2019-01-27 21:55:34.266185
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 21:55:43.420893!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.1693
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-27_21-53-14
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:56:08.520882
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 32.76 Degree
        The absolute mean error on Yaw angle estimation: 26.01 Degree
        The absolute mean error on Roll angle estimation: 22.09 Degree
Exp2019-01-27_21-53-14_part3 completed!
Exp2019-01-27_21-53-14.h5 has been saved.
subject9_Exp2019-01-27_21-53-14.png has been saved by 2019-01-27 21:56:40.488215.
Model Exp2019-01-27_21-53-14 has been evaluated successfully.
Model Exp2019-01-27_21-53-14 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 21:57:30.579329: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 21:57:30.662251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 21:57:30.662509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 21:57:30.662521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 21:57:30.820846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 21:57:30.820872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 21:57:30.820877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 21:57:30.821015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_21-57-47 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 8, 8, 2048)        21802784
_________________________________________________________________
time_distributed_1 (TimeDist (1, 1, 131072)            0
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   10487440
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 32,290,287
Trainable params: 10,487,503
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = False # True #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01-2
7_21-57-47
All frames and annotations from 1 datasets have been read by 2019-01-27 21:57:48.107862
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 21:57:57.256446!
Epoch 1/1
882/882 [==============================] - 37s 42ms/step - loss: 0.0977
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-27_21-57-47
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:58:36.710891
For the Subject 9 (M03):
882/882 [==============================] - 24s 28ms/step
        The absolute mean error on Pitch angle estimation: 36.76 Degree
        The absolute mean error on Yaw angle estimation: 24.49 Degree
        The absolute mean error on Roll angle estimation: 14.84 Degree
Exp2019-01-27_21-57-47_part1 completed!
Training model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01-2
7_21-57-47
All frames and annotations from 1 datasets have been read by 2019-01-27 21:59:11.109605
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 21:59:20.284675!
Epoch 1/1
882/882 [==============================] - 32s 37ms/step - loss: 0.0646
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-27_21-57-47
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 21:59:53.672718
For the Subject 9 (M03):
882/882 [==============================] - 23s 26ms/step
        The absolute mean error on Pitch angle estimation: 32.33 Degree
        The absolute mean error on Yaw angle estimation: 25.25 Degree
        The absolute mean error on Roll angle estimation: 20.85 Degree
Exp2019-01-27_21-57-47_part2 completed!
Training model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01-2
7_21-57-47
All frames and annotations from 1 datasets have been read by 2019-01-27 22:00:26.802229
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 22:00:35.967197!
Epoch 1/1
882/882 [==============================] - 32s 37ms/step - loss: 0.0559
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-27_21-57-47
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 22:01:09.284093
For the Subject 9 (M03):
882/882 [==============================] - 23s 26ms/step
        The absolute mean error on Pitch angle estimation: 32.13 Degree
        The absolute mean error on Yaw angle estimation: 24.83 Degree
        The absolute mean error on Roll angle estimation: 8.78 Degree
Exp2019-01-27_21-57-47_part3 completed!
Exp2019-01-27_21-57-47.h5 has been saved.
subject9_Exp2019-01-27_21-57-47.png has been saved by 2019-01-27 22:01:42.889291.
Model Exp2019-01-27_21-57-47 has been evaluated successfully.
Model Exp2019-01-27_21-57-47 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 22:03:14.123330: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 22:03:14.221610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 22:03:14.221878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 22:03:14.221893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 22:03:14.381181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 22:03:14.381207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 22:03:14.381215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 22:03:14.381357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_22-03-30 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
time_distributed_1 (TimeDist (1, 1, 2048)              0
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   165520
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 21,968,367
Trainable params: 165,583
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_22-03-30
All frames and annotations from 1 datasets have been read by 2019-01-27 22:03:31.802530
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 22:03:40.946646!
Epoch 1/1
882/882 [==============================] - 27s 31ms/step - loss: 0.1955
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_22-03-30
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 22:04:10.815873
For the Subject 9 (M03):
882/882 [==============================] - 22s 25ms/step
        The absolute mean error on Pitch angle estimation: 31.77 Degree
        The absolute mean error on Yaw angle estimation: 34.23 Degree
        The absolute mean error on Roll angle estimation: 11.85 Degree
Exp2019-01-27_22-03-30_part1 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_22-03-30
All frames and annotations from 1 datasets have been read by 2019-01-27 22:04:42.953087
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 22:04:52.126479!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1834
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_22-03-30
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 22:05:16.109708
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 29.92 Degree
        The absolute mean error on Yaw angle estimation: 36.11 Degree
        The absolute mean error on Roll angle estimation: 10.16 Degree
Exp2019-01-27_22-03-30_part2 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2
019-01-27_22-03-30
All frames and annotations from 1 datasets have been read by 2019-01-27 22:05:46.968521
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 22:05:56.151496!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1799
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010
_2019-01-27_22-03-30
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 22:06:20.193646
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 26.82 Degree
        The absolute mean error on Yaw angle estimation: 33.76 Degree
        The absolute mean error on Roll angle estimation: 9.05 Degree
Exp2019-01-27_22-03-30_part3 completed!
Exp2019-01-27_22-03-30.h5 has been saved.
subject9_Exp2019-01-27_22-03-30.png has been saved by 2019-01-27 22:06:51.508187.
Model Exp2019-01-27_22-03-30 has been evaluated successfully.
Model Exp2019-01-27_22-03-30 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 22:09:13.802141: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 22:09:13.898977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 22:09:13.899232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 22:09:13.899245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 22:09:14.058048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 22:09:14.058073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 22:09:14.058081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 22:09:14.058222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 37, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateful_FC_RNN_Configuration.py
", line 92, in getFinalModel
    rnn.add(LSTM(lstm_nodes, dropout=lstm_dropout, recurrent_dropout=lstm_recurrent_dropout, stateful=True))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/recurrent.py", line 500, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 575, in __call__
    self.assert_input_compatibility(inputs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 474, in assert_input_compatib
ility
    str(K.ndim(x)))
ValueError: Input 0 is incompatible with layer lstm_1: expected ndim=3, found ndim=5
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 22:10:16.860793: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 22:10:16.957521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 22:10:16.957837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 22:10:16.957851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 22:10:17.116234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 22:10:17.116260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 22:10:17.116265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 22:10:17.116444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_22-10-33 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 2048)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              2098176
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 32)                   4608
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    99
=================================================================
Total params: 23,908,742
Trainable params: 2,105,958
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 32
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm32_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2
019-01-27_22-10-33
All frames and annotations from 1 datasets have been read by 2019-01-27 22:10:34.500468
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 22:10:43.640065!
Epoch 1/1
882/882 [==============================] - 31s 35ms/step - loss: 0.2086
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm32_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100
_2019-01-27_22-10-33
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 22:11:17.557980
For the Subject 9 (M03):
882/882 [==============================] - 26s 29ms/step
        The absolute mean error on Pitch angle estimation: 18.29 Degree
        The absolute mean error on Yaw angle estimation: 36.97 Degree
        The absolute mean error on Roll angle estimation: 11.92 Degree
Exp2019-01-27_22-10-33_part1 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm32_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2
019-01-27_22-10-33
All frames and annotations from 1 datasets have been read by 2019-01-27 22:11:53.279985
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 22:12:02.484617!
Epoch 1/1
882/882 [==============================] - 27s 30ms/step - loss: 0.1796
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm32_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100
_2019-01-27_22-10-33
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 22:12:30.081122
For the Subject 9 (M03):
882/882 [==============================] - 24s 28ms/step
        The absolute mean error on Pitch angle estimation: 18.37 Degree
        The absolute mean error on Yaw angle estimation: 27.27 Degree
        The absolute mean error on Roll angle estimation: 7.32 Degree
Exp2019-01-27_22-10-33_part2 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm32_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2
019-01-27_22-10-33
All frames and annotations from 1 datasets have been read by 2019-01-27 22:13:04.569382
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 22:13:13.760431!
Epoch 1/1
882/882 [==============================] - 27s 30ms/step - loss: 0.1762
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm32_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100
_2019-01-27_22-10-33
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 22:13:41.215003
For the Subject 9 (M03):
882/882 [==============================] - 24s 28ms/step
        The absolute mean error on Pitch angle estimation: 18.46 Degree
        The absolute mean error on Yaw angle estimation: 25.79 Degree
        The absolute mean error on Roll angle estimation: 7.09 Degree
Exp2019-01-27_22-10-33_part3 completed!
Exp2019-01-27_22-10-33.h5 has been saved.
subject9_Exp2019-01-27_22-10-33.png has been saved by 2019-01-27 22:14:16.348765.
Model Exp2019-01-27_22-10-33 has been evaluated successfully.
Model Exp2019-01-27_22-10-33 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 22:20:25.162724: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 22:20:25.259799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 22:20:25.260108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 22:20:25.260122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 22:20:25.418646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 22:20:25.418672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 22:20:25.418677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 22:20:25.418854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_22-20-41 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   165520
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 21,968,367
Trainable params: 165,583
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2
019-01-27_22-20-41
All frames and annotations from 1 datasets have been read by 2019-01-27 22:20:42.700144
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 22:20:51.842081!
Epoch 1/1
882/882 [==============================] - 27s 31ms/step - loss: 0.1867
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100
_2019-01-27_22-20-41
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 22:21:21.321080
For the Subject 9 (M03):
882/882 [==============================] - 22s 25ms/step
        The absolute mean error on Pitch angle estimation: 17.48 Degree
        The absolute mean error on Yaw angle estimation: 27.28 Degree
        The absolute mean error on Roll angle estimation: 8.25 Degree
Exp2019-01-27_22-20-41_part1 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2
019-01-27_22-20-41
All frames and annotations from 1 datasets have been read by 2019-01-27 22:21:53.253791
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 22:22:02.405546!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1747
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100
_2019-01-27_22-20-41
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 22:22:26.129782
For the Subject 9 (M03):
882/882 [==============================] - 21s 23ms/step
        The absolute mean error on Pitch angle estimation: 18.15 Degree
        The absolute mean error on Yaw angle estimation: 27.07 Degree
        The absolute mean error on Roll angle estimation: 7.63 Degree
Exp2019-01-27_22-20-41_part2 completed!
Training model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2
019-01-27_22-20-41
All frames and annotations from 1 datasets have been read by 2019-01-27 22:22:56.853484
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 22:23:06.031386!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1746
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model InceptionV3_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100
_2019-01-27_22-20-41
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 22:23:29.946947
For the Subject 9 (M03):
882/882 [==============================] - 21s 24ms/step
        The absolute mean error on Pitch angle estimation: 16.98 Degree
        The absolute mean error on Yaw angle estimation: 26.08 Degree
        The absolute mean error on Roll angle estimation: 9.20 Degree
Exp2019-01-27_22-20-41_part3 completed!
Exp2019-01-27_22-20-41.h5 has been saved.
subject9_Exp2019-01-27_22-20-41.png has been saved by 2019-01-27 22:24:01.256565.
Model Exp2019-01-27_22-20-41 has been evaluated successfully.
Model Exp2019-01-27_22-20-41 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 23:46:47.597017: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 23:46:47.694961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 23:46:47.695225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 23:46:47.695239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 23:46:47.851163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 23:46:47.851191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 23:46:47.851199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 23:46:47.851344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_23-46-48 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_23-46-48
All frames and annotations from 1 datasets have been read by 2019-01-27 23:46:49.412793
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 23:46:58.291933!
Epoch 1/1
882/882 [==============================] - 17s 20ms/step - loss: 0.1340
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_23-46-48
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 23:47:16.929517
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.00 Degree
        The absolute mean error on Yaw angle estimation: 11.67 Degree
        The absolute mean error on Roll angle estimation: 4.60 Degree
Exp2019-01-27_23-46-48_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_23-46-48
All frames and annotations from 1 datasets have been read by 2019-01-27 23:47:34.920718
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 23:47:43.800152!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0738
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_23-46-48
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 23:48:00.581457
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.73 Degree
        The absolute mean error on Yaw angle estimation: 9.40 Degree
        The absolute mean error on Roll angle estimation: 3.10 Degree
Exp2019-01-27_23-46-48_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_23-46-48
All frames and annotations from 1 datasets have been read by 2019-01-27 23:48:18.545383
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 23:48:27.431177!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0632
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_23-46-48
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 23:48:44.451403
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.78 Degree
        The absolute mean error on Yaw angle estimation: 5.66 Degree
        The absolute mean error on Roll angle estimation: 3.04 Degree
Exp2019-01-27_23-46-48_part3 completed!
Exp2019-01-27_23-46-48.h5 has been saved.
subject9_Exp2019-01-27_23-46-48.png has been saved by 2019-01-27 23:49:02.142964.
Model Exp2019-01-27_23-46-48 has been evaluated successfully.
Model Exp2019-01-27_23-46-48 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 23:51:26.876881: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 23:51:26.974716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 23:51:26.975024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 23:51:26.975038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 23:51:27.131244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 23:51:27.131267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 23:51:27.131271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 23:51:27.131452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_23-51-27 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_23-51-27
All frames and annotations from 1 datasets have been read by 2019-01-27 23:51:28.669896
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 23:51:37.553369!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 17.9429
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_23-51-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 23:51:55.590014
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 19.36 Degree
        The absolute mean error on Yaw angle estimation: 27.38 Degree
        The absolute mean error on Roll angle estimation: 6.42 Degree
Exp2019-01-27_23-51-27_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_23-51-27
All frames and annotations from 1 datasets have been read by 2019-01-27 23:52:13.612200
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 23:52:22.480668!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 17.5834
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_23-51-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 23:52:39.428328
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.72 Degree
        The absolute mean error on Yaw angle estimation: 27.29 Degree
        The absolute mean error on Roll angle estimation: 6.27 Degree
Exp2019-01-27_23-51-27_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_23-51-27
All frames and annotations from 1 datasets have been read by 2019-01-27 23:52:57.402426
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-27 23:53:06.271728!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 17.3585
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_23-51-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 23:53:22.918099
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.34 Degree
        The absolute mean error on Yaw angle estimation: 27.23 Degree
        The absolute mean error on Roll angle estimation: 6.10 Degree
Exp2019-01-27_23-51-27_part3 completed!
Exp2019-01-27_23-51-27.h5 has been saved.
subject9_Exp2019-01-27_23-51-27.png has been saved by 2019-01-27 23:53:40.586140.
Model Exp2019-01-27_23-51-27 has been evaluated successfully.
Model Exp2019-01-27_23-51-27 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 23:58:41.352732: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 23:58:41.450352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-27 23:58:41.450608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 23:58:41.450620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-27 23:58:41.606476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-27 23:58:41.606502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 23:58:41.606507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 23:58:41.606643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_23-58-42 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_23-58-42
All frames and annotations from 1 datasets have been read by 2019-01-27 23:58:43.147615
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-27 23:58:52.035838!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 17.3942
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_23-58-42
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 23:59:10.269290
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 38.68 Degree
        The absolute mean error on Yaw angle estimation: 27.06 Degree
        The absolute mean error on Roll angle estimation: 13.28 Degree
Exp2019-01-27_23-58-42_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_23-58-42
All frames and annotations from 1 datasets have been read by 2019-01-27 23:59:28.314285
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-27 23:59:37.190254!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 15.8178
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_23-58-42
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 23:59:54.102530
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 23.86 Degree
        The absolute mean error on Yaw angle estimation: 42.08 Degree
        The absolute mean error on Roll angle estimation: 11.34 Degree
Exp2019-01-27_23-58-42_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-27_23-58-42
All frames and annotations from 1 datasets have been read by 2019-01-28 00:00:12.083932
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 00:00:20.964738!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 14.5711
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-27_23-58-42
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:00:37.675203
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.43 Degree
        The absolute mean error on Yaw angle estimation: 28.21 Degree
        The absolute mean error on Roll angle estimation: 7.01 Degree
Exp2019-01-27_23-58-42_part3 completed!
Exp2019-01-27_23-58-42.h5 has been saved.
subject9_Exp2019-01-27_23-58-42.png has been saved by 2019-01-28 00:00:55.373270.
Model Exp2019-01-27_23-58-42 has been evaluated successfully.
Model Exp2019-01-27_23-58-42 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 00:03:18.254245: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 00:03:18.352492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 00:03:18.352751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 00:03:18.352762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 00:03:18.508080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 00:03:18.508107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 00:03:18.508112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 00:03:18.508253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_00-03-19 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-03-19
All frames and annotations from 1 datasets have been read by 2019-01-28 00:03:20.045931
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 00:03:28.943872!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 114.7045
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-03-19
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:03:47.136548
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 224.72 Degree
        The absolute mean error on Yaw angle estimation: 97.70 Degree
        The absolute mean error on Roll angle estimation: 129.13 Degree
Exp2019-01-28_00-03-19_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-03-19
All frames and annotations from 1 datasets have been read by 2019-01-28 00:04:05.198403
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 00:04:14.085601!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 80.2034
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-03-19
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:04:30.636145
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 314.19 Degree
        The absolute mean error on Yaw angle estimation: 195.76 Degree
        The absolute mean error on Roll angle estimation: 12.55 Degree
Exp2019-01-28_00-03-19_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-03-19
All frames and annotations from 1 datasets have been read by 2019-01-28 00:04:48.645507
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 00:04:57.521981!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 53.4133
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-03-19
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:05:14.152534
^[[AFor the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 22.01 Degree
        The absolute mean error on Yaw angle estimation: 81.50 Degree
        The absolute mean error on Roll angle estimation: 10.55 Degree
Exp2019-01-28_00-03-19_part3 completed!
Exp2019-01-28_00-03-19.h5 has been saved.
subject9_Exp2019-01-28_00-03-19.png has been saved by 2019-01-28 00:05:31.900445.
Model Exp2019-01-28_00-03-19 has been evaluated successfully.
Model Exp2019-01-28_00-03-19 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 00:06:54.459635: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 00:06:54.557530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 00:06:54.557799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 00:06:54.557818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 00:06:54.713464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 00:06:54.713490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 00:06:54.713495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 00:06:54.713632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_00-06-55 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-06-55
All frames and annotations from 1 datasets have been read by 2019-01-28 00:06:56.242183
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 00:07:05.166234!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 8690.2567 - mean_absolute_error: 65.9073
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-06-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:07:23.542899
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 62.34 Degree
        The absolute mean error on Yaw angle estimation: 31.87 Degree
        The absolute mean error on Roll angle estimation: 54.10 Degree
Exp2019-01-28_00-06-55_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-06-55
All frames and annotations from 1 datasets have been read by 2019-01-28 00:07:41.653244
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 00:07:50.529802!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 3872.5014 - mean_absolute_error: 48.8574
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-06-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:08:06.896553
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 99.32 Degree
        The absolute mean error on Yaw angle estimation: 47.98 Degree
        The absolute mean error on Roll angle estimation: 72.79 Degree
Exp2019-01-28_00-06-55_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-06-55
All frames and annotations from 1 datasets have been read by 2019-01-28 00:08:24.928045
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 00:08:33.810775!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 4372.5749 - mean_absolute_error: 46.8414
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-06-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:08:50.777928
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.80 Degree
        The absolute mean error on Yaw angle estimation: 48.01 Degree
        The absolute mean error on Roll angle estimation: 33.04 Degree
Exp2019-01-28_00-06-55_part3 completed!
Exp2019-01-28_00-06-55.h5 has been saved.
subject9_Exp2019-01-28_00-06-55.png has been saved by 2019-01-28 00:09:08.488340.
Model Exp2019-01-28_00-06-55 has been evaluated successfully.
Model Exp2019-01-28_00-06-55 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 5, in <module>
    from Stateful_FC_RNN_Configuration import *
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateful_FC_RNN_Configuration.py
", line 92
    rnn.add(LSTM(lstm_nodes, dropout=lstm_dropout, recurrent_dropout=lstm_recurrent_dropout, stateful=True)), activation
='relu'
    ^
SyntaxError: can't assign to function call
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 00:11:42.200226: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 00:11:42.296482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 00:11:42.296738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 00:11:42.296749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 00:11:42.452651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 00:11:42.452676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 00:11:42.452682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 00:11:42.452818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_00-11-43 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-11-43
All frames and annotations from 1 datasets have been read by 2019-01-28 00:11:43.950286
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 00:11:52.854187!
Epoch 1/1
836/882 [===========================>..] - ETA: 0s - loss: 99.2547^C
Model Exp2019-01-28_00-11-43_part1 has been interrupted.
Exp2019-01-28_00-11-43_part1.h5 has been saved.
Model Exp2019-01-28_00-11-43_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 00:13:12.223328: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 00:13:12.321235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 00:13:12.321490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 00:13:12.321501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 00:13:12.477270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 00:13:12.477297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 00:13:12.477302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 00:13:12.477439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_00-13-13 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-13-13
All frames and annotations from 1 datasets have been read by 2019-01-28 00:13:14.005474
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 00:13:22.910360!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1511
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-13-13
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:13:41.049860
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.84 Degree
        The absolute mean error on Yaw angle estimation: 10.41 Degree
        The absolute mean error on Roll angle estimation: 6.72 Degree
Exp2019-01-28_00-13-13_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-13-13
All frames and annotations from 1 datasets have been read by 2019-01-28 00:13:59.164451
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 00:14:08.044459!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0896
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-13-13
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:14:24.839555
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.86 Degree
        The absolute mean error on Yaw angle estimation: 7.79 Degree
        The absolute mean error on Roll angle estimation: 5.65 Degree
Exp2019-01-28_00-13-13_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-13-13
All frames and annotations from 1 datasets have been read by 2019-01-28 00:14:42.858522
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 00:14:51.732310!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0700
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-13-13
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:15:08.805959
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.28 Degree
        The absolute mean error on Yaw angle estimation: 7.27 Degree
        The absolute mean error on Roll angle estimation: 4.96 Degree
Exp2019-01-28_00-13-13_part3 completed!
Exp2019-01-28_00-13-13.h5 has been saved.
subject9_Exp2019-01-28_00-13-13.png has been saved by 2019-01-28 00:15:26.502437.
Model Exp2019-01-28_00-13-13 has been evaluated successfully.
Model Exp2019-01-28_00-13-13 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 00:19:21.250961: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 00:19:21.348336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 00:19:21.348593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 00:19:21.348608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 00:19:21.504010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 00:19:21.504036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 00:19:21.504041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 00:19:21.504183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_00-19-22 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                329360
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-28_00-19-22
All frames and annotations from 1 datasets have been read by 2019-01-28 00:19:23.013977
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 00:19:31.899058!
Epoch 1/1
866/866 [==============================] - 100s 115ms/step - loss: 0.1314
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-28_00-19-22
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:21:12.710115
For the Subject 9 (M03):
882/882 [==============================] - 74s 84ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 8.01 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 12.45 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 6.02 Degree
Exp2019-01-28_00-19-22_part1 completed!
Training model VGG16_inc_top_seqLen16_lstm20_output3_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-28_00-19-22
All frames and annotations from 1 datasets have been read by 2019-01-28 00:22:36.617510
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 00:22:45.504250!
Epoch 1/1
866/866 [==============================] - 100s 115ms/step - loss: 0.0785
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-28_00-19-22
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:24:26.159539
For the Subject 9 (M03):
882/882 [==============================] - 73s 83ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 6.10 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 11.06 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 4.19 Degree
Exp2019-01-28_00-19-22_part2 completed!
Training model VGG16_inc_top_seqLen16_lstm20_output3_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-28_00-19-22
All frames and annotations from 1 datasets have been read by 2019-01-28 00:25:49.238667
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 00:25:58.104511!
Epoch 1/1
866/866 [==============================] - 100s 115ms/step - loss: 0.0642
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-28_00-19-22
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:27:38.587981
For the Subject 9 (M03):
882/882 [==============================] - 80s 91ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 7.13 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 11.62 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 4.21 Degree
Exp2019-01-28_00-19-22_part3 completed!
Exp2019-01-28_00-19-22.h5 has been saved.
subject9_Exp2019-01-28_00-19-22.png has been saved by 2019-01-28 00:29:08.076484.
Model Exp2019-01-28_00-19-22 has been evaluated successfully.
Model Exp2019-01-28_00-19-22 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 00:34:55.181441: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 00:34:55.280680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 00:34:55.280943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 00:34:55.280955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 00:34:55.436515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 00:34:55.436542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 00:34:55.436547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 00:34:55.436687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_00-34-56 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                329360
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 20
test_batch_size = 20

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-28_00-34-56
All frames and annotations from 1 datasets have been read by 2019-01-28 00:34:56.920351
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 00:35:05.807641!
Epoch 1/1
2019-01-28 00:35:07.500787: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memo
ry trying to allocate 1.61GiB. The caller indicates that this is not a failure, but may mean that there could be perform
ance gains if more memory were available.
2019-01-28 00:35:17.556189: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memo
ry trying to allocate 3.83GiB.  Current allocation summary follows.
2019-01-28 00:35:17.556289: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 93, Chunks
 in use: 93. 23.2KiB allocated for chunks. 23.2KiB in use in bin. 3.2KiB client-requested in use in bin.
2019-01-28 00:35:17.556332: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 10, Chunks
 in use: 10. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 3.7KiB client-requested in use in bin.
2019-01-28 00:35:17.556372: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 13, Chunks
 in use: 13. 19.2KiB allocated for chunks. 19.2KiB in use in bin. 17.5KiB client-requested in use in bin.
2019-01-28 00:35:17.556409: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 7, Chunks
in use: 7. 14.0KiB allocated for chunks. 14.0KiB in use in bin. 14.0KiB client-requested in use in bin.
2019-01-28 00:35:17.556446: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 11, Chunks
 in use: 10. 65.2KiB allocated for chunks. 58.5KiB in use in bin. 58.3KiB client-requested in use in bin.
2019-01-28 00:35:17.556478: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chunks
in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-28 00:35:17.556515: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks: 3,
 Chunks in use: 3. 48.0KiB allocated for chunks. 48.0KiB in use in bin. 48.0KiB client-requested in use in bin.
2019-01-28 00:35:17.556547: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks: 0,
 Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-28 00:35:17.556578: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks: 1,
 Chunks in use: 0. 65.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-28 00:35:17.556615: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks: 1,
 Chunks in use: 1. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.
2019-01-28 00:35:17.556647: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks: 8,
 Chunks in use: 8. 2.43MiB allocated for chunks. 2.43MiB in use in bin. 2.30MiB client-requested in use in bin.
2019-01-28 00:35:17.556680: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks: 3,
 Chunks in use: 3. 1.62MiB allocated for chunks. 1.62MiB in use in bin. 1.19MiB client-requested in use in bin.
2019-01-28 00:35:17.556712: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks: 5,
 Chunks in use: 5. 6.12MiB allocated for chunks. 6.12MiB in use in bin. 6.12MiB client-requested in use in bin.
2019-01-28 00:35:17.556745: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks: 4,
 Chunks in use: 4. 8.75MiB allocated for chunks. 8.75MiB in use in bin. 7.00MiB client-requested in use in bin.
2019-01-28 00:35:17.556778: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks: 2,
 Chunks in use: 1. 9.61MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-28 00:35:17.556813: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks: 7,
 Chunks in use: 6. 76.25MiB allocated for chunks. 60.62MiB in use in bin. 60.62MiB client-requested in use in bin.
2019-01-28 00:35:17.556843: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks: 0,
 Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-28 00:35:17.556876: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks: 1,
 Chunks in use: 0. 34.88MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-28 00:35:17.556911: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks: 1,
 Chunks in use: 1. 64.00MiB allocated for chunks. 64.00MiB in use in bin. 64.00MiB client-requested in use in bin.
2019-01-28 00:35:17.556942: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks: 0,
 Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-28 00:35:17.556976: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks: 5,
 Chunks in use: 3. 10.03GiB allocated for chunks. 8.04GiB in use in bin. 8.04GiB client-requested in use in bin.
2019-01-28 00:35:17.557006: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 3.83GiB was 256.00MiB, Chunk
State:
2019-01-28 00:35:17.557050: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 392.00MiB | Requested Size: 1
83.75MiB | in_use: 0, prev:   Size: 2.0KiB | Requested Size: 2.0KiB | in_use: 1, next:   Size: 16.0KiB | Requested Size:
 16.0KiB | in_use: 1
2019-01-28 00:35:17.557086: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 1.60GiB | Requested Size: 0B
| in_use: 0, prev:   Size: 3.83GiB | Requested Size: 3.83GiB | in_use: 1
2019-01-28 00:35:17.557115: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500000 of size 1280
2019-01-28 00:35:17.557139: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500500 of size 256
2019-01-28 00:35:17.557163: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500600 of size 256
2019-01-28 00:35:17.557187: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500700 of size 256
2019-01-28 00:35:17.557210: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500800 of size 256
2019-01-28 00:35:17.557235: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500900 of size 256
2019-01-28 00:35:17.557260: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500a00 of size 16384
2019-01-28 00:35:17.557283: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504a00 of size 256
2019-01-28 00:35:17.557307: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504b00 of size 256
2019-01-28 00:35:17.557332: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504c00 of size 512
2019-01-28 00:35:17.557355: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504e00 of size 256
2019-01-28 00:35:17.557379: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a504f00 of size 256
2019-01-28 00:35:17.557402: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505000 of size 256
2019-01-28 00:35:17.557430: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505100 of size 256
2019-01-28 00:35:17.557454: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505200 of size 1024
2019-01-28 00:35:17.557477: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505600 of size 256
2019-01-28 00:35:17.557501: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505700 of size 256
2019-01-28 00:35:17.557524: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505800 of size 256
2019-01-28 00:35:17.557548: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505900 of size 256
2019-01-28 00:35:17.557571: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505a00 of size 256
2019-01-28 00:35:17.557596: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a505b00 of size 2048
2019-01-28 00:35:17.557619: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506300 of size 256
2019-01-28 00:35:17.557643: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506400 of size 256
2019-01-28 00:35:17.557666: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506500 of size 256
2019-01-28 00:35:17.557721: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506600 of size 256
2019-01-28 00:35:17.557742: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506700 of size 256
2019-01-28 00:35:17.557759: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a506800 of size 4096
2019-01-28 00:35:17.557777: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507800 of size 256
2019-01-28 00:35:17.557795: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507900 of size 256
2019-01-28 00:35:17.557811: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a507a00 of size 6912
2019-01-28 00:35:17.557830: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a509500 of size 288000
2019-01-28 00:35:17.557849: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a54fa00 of size 512
2019-01-28 00:35:17.557868: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a54fc00 of size 589824
2019-01-28 00:35:17.557887: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5dfc00 of size 512
2019-01-28 00:35:17.557906: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a5dfe00 of size 327680
2019-01-28 00:35:17.557924: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a62fe00 of size 327680
2019-01-28 00:35:17.557943: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a67fe00 of size 524288
2019-01-28 00:35:17.557963: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a6ffe00 of size 1024
2019-01-28 00:35:17.557982: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a700200 of size 2359296
2019-01-28 00:35:17.558004: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a940200 of size 2359296
2019-01-28 00:35:17.558028: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ab80200 of size 1024
2019-01-28 00:35:17.558053: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ab80600 of size 4718592
2019-01-28 00:35:17.558076: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b000600 of size 9437184
2019-01-28 00:35:17.558099: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b900600 of size 16384000
2019-01-28 00:35:17.558122: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0c8a0600 of size 36569088
2019-01-28 00:35:17.558145: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0eb80600 of size 1024
2019-01-28 00:35:17.558169: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0eb80a00 of size 1310720
2019-01-28 00:35:17.558191: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ecc0a00 of size 1310720
2019-01-28 00:35:17.558215: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ee00a00 of size 2097152
2019-01-28 00:35:17.558238: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0f000a00 of size 2048
2019-01-28 00:35:17.558260: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0f001200 of size 9437184
2019-01-28 00:35:17.558283: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0f901200 of size 9437184
2019-01-28 00:35:17.558305: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb10201200 of size 9437184
2019-01-28 00:35:17.558328: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb10b01200 of size 9437184
2019-01-28 00:35:17.558351: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11401200 of size 1310720
2019-01-28 00:35:17.558373: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11541200 of size 1310720
2019-01-28 00:35:17.558396: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11681200 of size 327680
2019-01-28 00:35:17.558419: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb116d1200 of size 327680
2019-01-28 00:35:17.558442: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11721200 of size 327680
2019-01-28 00:35:17.558465: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11771200 of size 327680
2019-01-28 00:35:17.558488: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb117c1200 of size 147456
2019-01-28 00:35:17.558511: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb117e5200 of size 5357568
2019-01-28 00:35:17.558534: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d01200 of size 2048
2019-01-28 00:35:17.558557: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d01a00 of size 16384
2019-01-28 00:35:17.558580: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb11d05a00 of size 2048
2019-01-28 00:35:17.558603: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb11d06200 of size 16384000
2019-01-28 00:35:17.558625: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ca6200 of size 2048
2019-01-28 00:35:17.558647: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ca6a00 of size 2048
2019-01-28 00:35:17.558670: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ca7200 of size 4096
2019-01-28 00:35:17.558693: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb12ca8200 of size 2048
2019-01-28 00:35:17.558716: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb12ca8a00 of size 41104179
2
2019-01-28 00:35:17.558738: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4a8a00 of size 16384
2019-01-28 00:35:17.558761: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4aca00 of size 256
2019-01-28 00:35:17.558784: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4acb00 of size 256
2019-01-28 00:35:17.558808: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4acc00 of size 6400
2019-01-28 00:35:17.558831: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ae500 of size 256
2019-01-28 00:35:17.558853: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ae600 of size 512
2019-01-28 00:35:17.558876: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ae800 of size 256
2019-01-28 00:35:17.558898: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ae900 of size 256
2019-01-28 00:35:17.558920: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4aea00 of size 256
2019-01-28 00:35:17.558942: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4aeb00 of size 256
2019-01-28 00:35:17.558965: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4aec00 of size 256
2019-01-28 00:35:17.558987: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4aed00 of size 256
2019-01-28 00:35:17.559014: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4aee00 of size 256
2019-01-28 00:35:17.559037: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4aef00 of size 256
2019-01-28 00:35:17.559065: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4af000 of size 6400
2019-01-28 00:35:17.559087: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0900 of size 512
2019-01-28 00:35:17.559109: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0b00 of size 256
2019-01-28 00:35:17.559132: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0c00 of size 256
2019-01-28 00:35:17.559154: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0d00 of size 256
2019-01-28 00:35:17.559177: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b0e00 of size 6400
2019-01-28 00:35:17.559200: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2700 of size 512
2019-01-28 00:35:17.559222: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2900 of size 256
2019-01-28 00:35:17.559245: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2a00 of size 256
2019-01-28 00:35:17.559267: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2b00 of size 256
2019-01-28 00:35:17.559289: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2c00 of size 256
2019-01-28 00:35:17.559311: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2d00 of size 256
2019-01-28 00:35:17.559334: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2e00 of size 256
2019-01-28 00:35:17.559356: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b2f00 of size 256
2019-01-28 00:35:17.559378: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b3000 of size 6400
2019-01-28 00:35:17.559401: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b4900 of size 512
2019-01-28 00:35:17.559423: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b4b00 of size 256
2019-01-28 00:35:17.559445: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b4c00 of size 256
2019-01-28 00:35:17.559467: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b4d00 of size 6400
2019-01-28 00:35:17.559490: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6600 of size 512
2019-01-28 00:35:17.559513: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6800 of size 256
2019-01-28 00:35:17.559535: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6900 of size 256
2019-01-28 00:35:17.559557: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6a00 of size 256
2019-01-28 00:35:17.559579: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6b00 of size 256
2019-01-28 00:35:17.559601: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6c00 of size 256
2019-01-28 00:35:17.559623: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6d00 of size 256
2019-01-28 00:35:17.559645: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6e00 of size 256
2019-01-28 00:35:17.559668: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b6f00 of size 256
2019-01-28 00:35:17.559690: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b7000 of size 256
2019-01-28 00:35:17.559712: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b7100 of size 256
2019-01-28 00:35:17.559734: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b7200 of size 256
2019-01-28 00:35:17.559757: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b7300 of size 256
2019-01-28 00:35:17.559780: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b7400 of size 1792
2019-01-28 00:35:17.559803: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b7b00 of size 1792
2019-01-28 00:35:17.559825: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b8200 of size 1792
2019-01-28 00:35:17.559848: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b8900 of size 1792
2019-01-28 00:35:17.559871: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9000 of size 256
2019-01-28 00:35:17.559893: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9100 of size 256
2019-01-28 00:35:17.559915: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9200 of size 256
2019-01-28 00:35:17.559937: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9300 of size 256
2019-01-28 00:35:17.559960: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9400 of size 256
2019-01-28 00:35:17.559982: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9500 of size 256
2019-01-28 00:35:17.560004: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9600 of size 256
2019-01-28 00:35:17.560026: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9700 of size 256
2019-01-28 00:35:17.560049: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9800 of size 256
2019-01-28 00:35:17.560071: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9900 of size 256
2019-01-28 00:35:17.560093: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9a00 of size 256
2019-01-28 00:35:17.560115: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9b00 of size 256
2019-01-28 00:35:17.579846: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9c00 of size 256
2019-01-28 00:35:17.579898: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9d00 of size 256
2019-01-28 00:35:17.579920: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9e00 of size 256
2019-01-28 00:35:17.579939: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4b9f00 of size 256
2019-01-28 00:35:17.579961: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ba000 of size 256
2019-01-28 00:35:17.579979: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ba100 of size 256
2019-01-28 00:35:17.579997: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ba200 of size 256
2019-01-28 00:35:17.580015: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ba300 of size 256
2019-01-28 00:35:17.580034: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ba400 of size 256
2019-01-28 00:35:17.580052: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ba500 of size 256
2019-01-28 00:35:17.580067: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ba600 of size 256
2019-01-28 00:35:17.580085: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ba700 of size 256
2019-01-28 00:35:17.580104: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ba800 of size 256
2019-01-28 00:35:17.580122: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4ba900 of size 256
2019-01-28 00:35:17.580140: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4baa00 of size 256
2019-01-28 00:35:17.580159: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bab00 of size 256
2019-01-28 00:35:17.580177: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bac00 of size 1792
2019-01-28 00:35:17.580196: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb2b4bb300 of size 6912
2019-01-28 00:35:17.580215: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bce00 of size 1792
2019-01-28 00:35:17.580233: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bd500 of size 1792
2019-01-28 00:35:17.580252: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bdc00 of size 1792
2019-01-28 00:35:17.580270: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4be300 of size 256
2019-01-28 00:35:17.580288: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4be400 of size 6400
2019-01-28 00:35:17.580307: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bfd00 of size 512
2019-01-28 00:35:17.580325: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4bff00 of size 256
2019-01-28 00:35:17.580344: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c0000 of size 256
2019-01-28 00:35:17.580362: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c0100 of size 256
2019-01-28 00:35:17.580381: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c0200 of size 6400
2019-01-28 00:35:17.580400: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c1b00 of size 512
2019-01-28 00:35:17.580418: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c1d00 of size 256
2019-01-28 00:35:17.580436: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4c1e00 of size 256
2019-01-28 00:35:17.580455: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb2b4c1f00 of size 67328
2019-01-28 00:35:17.580474: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4d2600 of size 256
2019-01-28 00:35:17.580494: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b4d2700 of size 294912
2019-01-28 00:35:17.580514: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b51a700 of size 589824
2019-01-28 00:35:17.580534: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b5aa700 of size 1179648
2019-01-28 00:35:17.580553: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b6ca700 of size 2359296
2019-01-28 00:35:17.580573: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2b90a700 of size 67108864
2019-01-28 00:35:17.580594: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2f90a700 of size 41104179
2
2019-01-28 00:35:17.580617: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb4810a700 of size 41104179
20
2019-01-28 00:35:17.580638: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xc3d10a700 of size 41104179
20
2019-01-28 00:35:17.580657: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xd3210a700 of size 17225382
40
2019-01-28 00:35:17.580676: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size
:
2019-01-28 00:35:17.580707: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 93 Chunks of size 256 totalling 23.2K
iB
2019-01-28 00:35:17.580730: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 10 Chunks of size 512 totalling 5.0Ki
B
2019-01-28 00:35:17.580752: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 1024 totalling 4.0Ki
B
2019-01-28 00:35:17.580773: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.2Ki
B
2019-01-28 00:35:17.580794: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 8 Chunks of size 1792 totalling 14.0K
iB
2019-01-28 00:35:17.580816: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 7 Chunks of size 2048 totalling 14.0K
iB
2019-01-28 00:35:17.580837: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 4096 totalling 8.0Ki
B
2019-01-28 00:35:17.580856: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 7 Chunks of size 6400 totalling 43.8K
iB
2019-01-28 00:35:17.580877: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 6912 totalling 6.8Ki
B
2019-01-28 00:35:17.580899: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 16384 totalling 48.0
KiB
2019-01-28 00:35:17.580922: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 147456 totalling 144
.0KiB
2019-01-28 00:35:17.580944: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 288000 totalling 281
.2KiB
2019-01-28 00:35:17.580965: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 294912 totalling 288
.0KiB
2019-01-28 00:35:17.580987: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 6 Chunks of size 327680 totalling 1.8
8MiB
2019-01-28 00:35:17.581009: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 524288 totalling 512
.0KiB
2019-01-28 00:35:17.581029: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 589824 totalling 1.1
2MiB
2019-01-28 00:35:17.581050: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1179648 totalling 1.
12MiB
2019-01-28 00:35:17.581071: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 1310720 totalling 5.
00MiB
2019-01-28 00:35:17.581092: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 2097152 totalling 2.
00MiB
2019-01-28 00:35:17.581112: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 2359296 totalling 6.
75MiB
2019-01-28 00:35:17.581132: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 4718592 totalling 4.
50MiB
2019-01-28 00:35:17.581154: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 9437184 totalling 45
.00MiB
2019-01-28 00:35:17.581176: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 16384000 totalling 1
5.62MiB
2019-01-28 00:35:17.581198: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 67108864 totalling 6
4.00MiB
2019-01-28 00:35:17.581220: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 411041792 totalling
392.00MiB
2019-01-28 00:35:17.581241: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 4110417920 totalling
 7.66GiB
2019-01-28 00:35:17.581261: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 8.18GiB
2019-01-28 00:35:17.581288: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                 10979409920
InUse:                  8787444992
MaxInUse:               9005971712
NumAllocs:                     228
MaxAllocSize:           4110417920

2019-01-28 00:35:17.581353: W tensorflow/core/common_runtime/bfc_allocator.cc:279] **___********************************
************************************************_______________
2019-01-28 00:35:17.581400: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_ops.cc:672 : Resou
rce exhausted: OOM when allocating tensor with shape[320,64,224,224] and type float on /job:localhost/replica:0/task:0/d
evice:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_tf_
sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in __e
xit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[320,64,224,224] an
d type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], paddi
ng="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](tdVGG16/
block1_conv1/Relu, block1_conv2/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.

         [[Node: loss/mul/_295 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:
0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_2181_loss/m
ul", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record, preprocess_input = preprocess_input)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, preprocess_in
put = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 50, i
n trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, exp = exp, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 40, i
n trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_outputs, batch_
size, in_epochs, exp = exp, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 30, i
n trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_generator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482, in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[320,64,224,224] an
d type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], paddi
ng="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](tdVGG16/
block1_conv1/Relu, block1_conv2/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.

         [[Node: loss/mul/_295 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:
0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_2181_loss/m
ul", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.


Caused by op 'tdVGG16/block1_conv2/convolution', defined at:
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 37, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateless_FC_RNN_Configuration.p
y", line 82, in getFinalModel
    rnn.add(TimeDistributed(cnn_model, input_shape=(timesteps, inp[0], inp[1], inp[2]), name = 'tdVGG16'))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 497, in add
    layer(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 213, in call
    y = self.layer.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2085, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2235, in run_internal_graph
    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py", line 168, in call
    dilation_rate=self.dilation_rate)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 3341, in conv2d
    data_format=tf_data_format)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 782, in convolution
    return op(input, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 870, in __call__
    return self.conv_op(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 522, in __call__
    return self.call(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 206, in __call__
    name=self.name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 953, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in
_apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, in create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[320,64,224,224] and type float o
n /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], paddi
ng="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](tdVGG16/
block1_conv1/Relu, block1_conv2/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.

         [[Node: loss/mul/_295 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:
0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_2181_loss/m
ul", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOpti
ons for current allocation info.


mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 00:36:11.718456: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 00:36:11.816496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 00:36:11.816755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 00:36:11.816766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 00:36:11.972665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 00:36:11.972692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 00:36:11.972699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 00:36:11.972835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_00-36-12 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 20)                329360
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateless_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 15
test_batch_size = 15

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen16_lstm20_output3_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-28_00-36-12
All frames and annotations from 1 datasets have been read by 2019-01-28 00:36:13.437745
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 00:36:22.355487!
Epoch 1/1
58/58 [==============================] - 65s 1s/step - loss: 0.2385
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen16_lstm20_output3_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01-28_00-36-12
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:37:28.531182
For the Subject 9 (M03):
58/58 [==============================] - 57s 979ms/step
(882, 1) (866, 1)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, exp = exp, re
cord = record, preprocess_input = preprocess_input)
  File "runFC_RNN_Experiment.py", line 32, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record = record, prep
rocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 119,
in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, ba
tch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py", line 92, i
n evaluateSubject
    matrix = numpy.concatenate((test_labels[:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 00:41:17.331983: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 00:41:17.429616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 00:41:17.429878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 00:41:17.429890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 00:41:17.585659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 00:41:17.585692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 00:41:17.585700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 00:41:17.585838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_00-41-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 320)                  5653760
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    963
=================================================================
Total params: 139,915,267
Trainable params: 5,654,723
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 320
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-0
1-28_00-41-18
All frames and annotations from 1 datasets have been read by 2019-01-28 00:41:19.211088
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 00:41:28.116728!
Epoch 1/1
882/882 [==============================] - 19s 22ms/step - loss: 0.1672
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019
-01-28_00-41-18
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:41:48.586500
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 13.94 Degree
        The absolute mean error on Yaw angle estimation: 9.48 Degree
        The absolute mean error on Roll angle estimation: 7.15 Degree
Exp2019-01-28_00-41-18_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-0
1-28_00-41-18
All frames and annotations from 1 datasets have been read by 2019-01-28 00:42:06.816072
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 00:42:15.748480!
Epoch 1/1
882/882 [==============================] - 18s 21ms/step - loss: 0.0950
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019
-01-28_00-41-18
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:42:35.070992
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 4.61 Degree
        The absolute mean error on Yaw angle estimation: 6.97 Degree
        The absolute mean error on Roll angle estimation: 3.03 Degree
Exp2019-01-28_00-41-18_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-0
1-28_00-41-18
All frames and annotations from 1 datasets have been read by 2019-01-28 00:42:53.276164
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 00:43:02.170116!
Epoch 1/1
882/882 [==============================] - 18s 21ms/step - loss: 0.0730
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019
-01-28_00-41-18
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:43:21.420171
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 9.84 Degree
        The absolute mean error on Yaw angle estimation: 29.38 Degree
        The absolute mean error on Roll angle estimation: 3.65 Degree
Exp2019-01-28_00-41-18_part3 completed!
Exp2019-01-28_00-41-18.h5 has been saved.
subject9_Exp2019-01-28_00-41-18.png has been saved by 2019-01-28 00:43:39.384804.
Model Exp2019-01-28_00-41-18 has been evaluated successfully.
Model Exp2019-01-28_00-41-18 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.

2019-01-28 00:49:04.524023: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 00:49:04.621941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 00:49:04.622246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 00:49:04.622260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 00:49:04.778486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 00:49:04.778511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 00:49:04.778515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 00:49:04.778707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_00-49-05 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   1065216
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    195
=================================================================
Total params: 135,325,955
Trainable params: 1,065,411
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-49-05
All frames and annotations from 1 datasets have been read by 2019-01-28 00:49:06.328580
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 00:49:15.237026!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1337
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-49-05
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:49:33.552807
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.68 Degree
        The absolute mean error on Yaw angle estimation: 17.39 Degree
        The absolute mean error on Roll angle estimation: 2.96 Degree
Exp2019-01-28_00-49-05_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-49-05
All frames and annotations from 1 datasets have been read by 2019-01-28 00:49:51.623165
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 00:50:00.535235!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0798
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-49-05
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:50:17.084445
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 6.79 Degree
        The absolute mean error on Yaw angle estimation: 19.83 Degree
        The absolute mean error on Roll angle estimation: 3.18 Degree
Exp2019-01-28_00-49-05_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_00-49-05
All frames and annotations from 1 datasets have been read by 2019-01-28 00:50:35.175226
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 00:50:44.089192!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0658
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_00-49-05
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 00:51:00.956479
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 14.13 Degree
        The absolute mean error on Yaw angle estimation: 17.78 Degree
        The absolute mean error on Roll angle estimation: 14.19 Degree
Exp2019-01-28_00-49-05_part3 completed!
Exp2019-01-28_00-49-05.h5 has been saved.
subject9_Exp2019-01-28_00-49-05.png has been saved by 2019-01-28 00:51:18.733490.
Model Exp2019-01-28_00-49-05 has been evaluated successfully.
Model Exp2019-01-28_00-49-05 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 01:07:00.732500: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 01:07:00.829038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 01:07:00.829294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 01:07:00.829305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 01:07:00.984983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 01:07:00.985011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 01:07:00.985019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 01:07:00.985158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_01-07-01 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   1065216
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    195
=================================================================
Total params: 135,325,955
Trainable params: 1,065,411
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_01-07-01
All frames and annotations from 1 datasets have been read by 2019-01-28 01:07:02.522491
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:07:11.427650!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 1.3611
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_01-07-01
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:07:29.544349
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.99 Degree
        The absolute mean error on Yaw angle estimation: 8.02 Degree
        The absolute mean error on Roll angle estimation: 3.60 Degree
Exp2019-01-28_01-07-01_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_01-07-01
All frames and annotations from 1 datasets have been read by 2019-01-28 01:07:47.571283
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:07:56.458218!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.3832
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_01-07-01
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:08:13.240737
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.45 Degree
        The absolute mean error on Yaw angle estimation: 6.59 Degree
        The absolute mean error on Roll angle estimation: 3.56 Degree
Exp2019-01-28_01-07-01_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_01-07-01
All frames and annotations from 1 datasets have been read by 2019-01-28 01:08:31.204885
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:08:40.077673!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.2346
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_01-07-01
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:08:56.972972
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 6.75 Degree
        The absolute mean error on Yaw angle estimation: 7.51 Degree
        The absolute mean error on Roll angle estimation: 3.45 Degree
Exp2019-01-28_01-07-01_part3 completed!
Exp2019-01-28_01-07-01.h5 has been saved.
subject9_Exp2019-01-28_01-07-01.png has been saved by 2019-01-28 01:09:14.645464.
Model Exp2019-01-28_01-07-01 has been evaluated successfully.
Model Exp2019-01-28_01-07-01 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 01:12:18.774490: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 01:12:18.871119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 01:12:18.871430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 01:12:18.871444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 01:12:19.026499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 01:12:19.026525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 01:12:19.026530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 01:12:19.026707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_01-12-19 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   1065216
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    195
=================================================================
Total params: 135,325,955
Trainable params: 1,065,411
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000_2019-01
-28_01-12-19
All frames and annotations from 1 datasets have been read by 2019-01-28 01:12:20.572961
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:12:29.471775!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.3464
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000_2019-
01-28_01-12-19
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:12:47.794191
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.81 Degree
        The absolute mean error on Yaw angle estimation: 25.90 Degree
        The absolute mean error on Roll angle estimation: 7.14 Degree
Exp2019-01-28_01-12-19_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000_2019-01
-28_01-12-19
All frames and annotations from 1 datasets have been read by 2019-01-28 01:13:05.886525
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:13:14.773853!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.2514
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000_2019-
01-28_01-12-19
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:13:31.747324
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.69 Degree
        The absolute mean error on Yaw angle estimation: 28.75 Degree
        The absolute mean error on Roll angle estimation: 13.03 Degree
Exp2019-01-28_01-12-19_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000_2019-01
-28_01-12-19
All frames and annotations from 1 datasets have been read by 2019-01-28 01:13:49.787499
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:13:58.677299!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.2493
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.001000_2019-
01-28_01-12-19
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:14:15.873057
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.89 Degree
        The absolute mean error on Yaw angle estimation: 30.43 Degree
        The absolute mean error on Roll angle estimation: 18.35 Degree
Exp2019-01-28_01-12-19_part3 completed!
Exp2019-01-28_01-12-19.h5 has been saved.
subject9_Exp2019-01-28_01-12-19.png has been saved by 2019-01-28 01:14:33.577238.
Model Exp2019-01-28_01-12-19 has been evaluated successfully.
Model Exp2019-01-28_01-12-19 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 01:16:22.634189: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 01:16:22.731688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 01:16:22.731945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 01:16:22.731957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 01:16:22.887352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 01:16:22.887378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 01:16:22.887383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 01:16:22.887523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_01-16-23 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   1065216
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    195
=================================================================
Total params: 135,325,955
Trainable params: 1,065,411
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-28_01-16-23
All frames and annotations from 1 datasets have been read by 2019-01-28 01:16:24.391195
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:16:33.295430!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 3.9949
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-
01-28_01-16-23
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:16:51.727035
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.12 Degree
        The absolute mean error on Yaw angle estimation: 13.70 Degree
        The absolute mean error on Roll angle estimation: 7.33 Degree
Exp2019-01-28_01-16-23_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-28_01-16-23
All frames and annotations from 1 datasets have been read by 2019-01-28 01:17:09.839067
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:17:18.747853!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 2.5242
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-
01-28_01-16-23
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:17:35.060393
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.94 Degree
        The absolute mean error on Yaw angle estimation: 9.21 Degree
        The absolute mean error on Roll angle estimation: 7.68 Degree
Exp2019-01-28_01-16-23_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-28_01-16-23
All frames and annotations from 1 datasets have been read by 2019-01-28 01:17:53.060295
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:18:01.946015!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 1.7290
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-
01-28_01-16-23
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:18:18.944800
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.85 Degree
        The absolute mean error on Yaw angle estimation: 7.56 Degree
        The absolute mean error on Roll angle estimation: 4.51 Degree
Exp2019-01-28_01-16-23_part3 completed!
Exp2019-01-28_01-16-23.h5 has been saved.
subject9_Exp2019-01-28_01-16-23.png has been saved by 2019-01-28 01:18:36.662178.
Model Exp2019-01-28_01-16-23 has been evaluated successfully.
Model Exp2019-01-28_01-16-23 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 01:20:26.036151: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 01:20:26.133400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 01:20:26.133661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 01:20:26.133675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 01:20:26.288412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 01:20:26.288437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 01:20:26.288442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 01:20:26.288578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_01-20-26 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   1065216
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    195
=================================================================
Total params: 135,325,955
Trainable params: 1,065,411
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-28_01-20-26
All frames and annotations from 1 datasets have been read by 2019-01-28 01:20:27.853235
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:20:36.756751!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.4601 - mean_absolute_error: 0.2058
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-
01-28_01-20-26
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:20:54.730422
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.38 Degree
        The absolute mean error on Yaw angle estimation: 12.48 Degree
        The absolute mean error on Roll angle estimation: 12.97 Degree
Exp2019-01-28_01-20-26_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-28_01-20-26
All frames and annotations from 1 datasets have been read by 2019-01-28 01:21:12.823397
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:21:21.694230!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.3021 - mean_absolute_error: 0.1522
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-
01-28_01-20-26
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:21:45.849173
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 6.92 Degree
        The absolute mean error on Yaw angle estimation: 11.01 Degree
        The absolute mean error on Roll angle estimation: 6.05 Degree
Exp2019-01-28_01-20-26_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-01
-28_01-20-26
All frames and annotations from 1 datasets have been read by 2019-01-28 01:22:03.879721
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:22:12.762374!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.2215 - mean_absolute_error: 0.1260
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000010_2019-
01-28_01-20-26
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:22:29.523484
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.01 Degree
        The absolute mean error on Yaw angle estimation: 7.27 Degree
        The absolute mean error on Roll angle estimation: 4.18 Degree
Exp2019-01-28_01-20-26_part3 completed!
Exp2019-01-28_01-20-26.h5 has been saved.
subject9_Exp2019-01-28_01-20-26.png has been saved by 2019-01-28 01:22:47.244484.
Model Exp2019-01-28_01-20-26 has been evaluated successfully.
Model Exp2019-01-28_01-20-26 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 01:24:02.240759: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 01:24:02.338708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 01:24:02.338967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 01:24:02.338978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 01:24:02.493883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 01:24:02.493909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 01:24:02.493914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 01:24:02.494052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_01-24-03 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   1065216
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    195
=================================================================
Total params: 135,325,955
Trainable params: 1,065,411
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_01-24-03
All frames and annotations from 1 datasets have been read by 2019-01-28 01:24:04.036977
1. set (Dataset 9) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:24:12.938743!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.3330
Epoch 1 for Experiment 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_01-24-03
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:24:31.324323
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 11.55 Degree
        The absolute mean error on Yaw angle estimation: 7.26 Degree
        The absolute mean error on Roll angle estimation: 3.95 Degree
Exp2019-01-28_01-24-03_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_01-24-03
All frames and annotations from 1 datasets have been read by 2019-01-28 01:24:49.400464
1. set (Dataset 9) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:24:58.274258!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1889
Epoch 1 for Experiment 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_01-24-03
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:25:15.413609
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.53 Degree
        The absolute mean error on Yaw angle estimation: 8.13 Degree
        The absolute mean error on Roll angle estimation: 22.84 Degree
Exp2019-01-28_01-24-03_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_01-24-03
All frames and annotations from 1 datasets have been read by 2019-01-28 01:25:33.418539
1. set (Dataset 9) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:25:42.292870!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1465
Epoch 1 for Experiment 3 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_01-24-03
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-28 01:25:59.034788
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 13.40 Degree
        The absolute mean error on Yaw angle estimation: 18.16 Degree
        The absolute mean error on Roll angle estimation: 38.70 Degree
Exp2019-01-28_01-24-03_part3 completed!
Exp2019-01-28_01-24-03.h5 has been saved.
subject9_Exp2019-01-28_01-24-03.png has been saved by 2019-01-28 01:26:16.872203.
Model Exp2019-01-28_01-24-03 has been evaluated successfully.
Model Exp2019-01-28_01-24-03 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 01:31:31.958455: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 01:31:32.057065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 01:31:32.057376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 01:31:32.057390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 01:31:32.212125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 01:31:32.212155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 01:31:32.212159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 01:31:32.212343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_01-31-32 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   1065216
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    195
=================================================================
Total params: 135,325,955
Trainable params: 1,065,411
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_01-31-32
All frames and annotations from 20 datasets have been read by 2019-01-28 01:31:37.264675
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:31:43.647380!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.4062
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:32:01.521772!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.2763
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:32:16.694660!
Epoch 1/1
654/654 [==============================] - 11s 17ms/step - loss: 0.2486
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:32:32.860586!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.2062
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:32:49.657142!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.2101
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:33:09.332018!
Epoch 1/1
569/569 [==============================] - 11s 19ms/step - loss: 0.2140
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:33:25.952531!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.1900
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:33:46.235083!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.1332
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:34:10.216669!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.1292
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:34:31.185726!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.1200
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:34:51.732015!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.1307
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:35:09.716769!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.1477
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:35:24.302496!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1473
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:35:40.492642!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.1172
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:35:57.276614!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.1200
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:36:15.982979!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.1083
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:36:29.905395!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.1029
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:36:42.307268!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1465
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:36:56.825459!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0881
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:37:11.056176!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0817
Epoch 1 for Experiment 1 completed!
Exp2019-01-28_01-31-32_part1.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21, 'F02'), (16
, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_01-31-32
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 01:37:23.398216
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 11.97 Degree
        The absolute mean error on Yaw angle estimation: 39.67 Degree
        The absolute mean error on Roll angle estimation: 20.56 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 9.81 Degree
        The absolute mean error on Yaw angle estimation: 29.28 Degree
        The absolute mean error on Roll angle estimation: 4.12 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 12.17 Degree
        The absolute mean error on Yaw angle estimation: 39.98 Degree
        The absolute mean error on Roll angle estimation: 7.08 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 14.83 Degree
        The absolute mean error on Yaw angle estimation: 36.56 Degree
        The absolute mean error on Roll angle estimation: 14.21 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 12.19 Degree
        The absolute mean error on Yaw angle estimations: 36.37 Degree
        The absolute mean error on Roll angle estimations: 11.49 Degree
Exp2019-01-28_01-31-32_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_01-31-32
All frames and annotations from 20 datasets have been read by 2019-01-28 01:38:33.477872
1. set (Dataset 6) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:38:38.587185!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1180
2. set (Dataset 11) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:38:54.034694!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0628
3. set (Dataset 10) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:39:11.574769!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0998
4. set (Dataset 4) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:39:32.310872!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0948
5. set (Dataset 23) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:39:51.373719!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1368
6. set (Dataset 13) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:40:06.156659!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0791
7. set (Dataset 17) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:40:18.753065!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0878
8. set (Dataset 1) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:40:31.052937!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.1270
9. set (Dataset 8) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:40:47.837089!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.1033
10. set (Dataset 7) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:41:09.385586!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0986
11. set (Dataset 21) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:41:29.110145!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1300
12. set (Dataset 22) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:41:46.978914!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0741
13. set (Dataset 2) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:42:03.840191!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0997
14. set (Dataset 24) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:42:17.909998!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0760
15. set (Dataset 15) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:42:33.160554!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.1010
16. set (Dataset 20) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:42:50.120275!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0949
17. set (Dataset 18) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:43:06.063542!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1361
18. set (Dataset 19) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:43:21.921674!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0924
19. set (Dataset 12) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:43:38.235547!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0788
20. set (Dataset 16) being trained for epoch 1 in Experiment 2 by 2019-01-28 01:44:00.043515!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0763
Epoch 1 for Experiment 2 completed!
Exp2019-01-28_01-31-32_part2.h5 has been saved.
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (23, 'M13'), (13, 'M07'), (17, 'M10'), (1,
'F01'), (8, 'M02'), (7, 'M01'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'), (20, 'M12'), (18, 'F05')
, (19, 'M11'), (12, 'M06'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_01-31-32
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 01:44:19.194944
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 12.64 Degree
        The absolute mean error on Yaw angle estimation: 30.29 Degree
        The absolute mean error on Roll angle estimation: 13.73 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.41 Degree
        The absolute mean error on Yaw angle estimation: 28.21 Degree
        The absolute mean error on Roll angle estimation: 3.49 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 23.06 Degree
        The absolute mean error on Yaw angle estimation: 37.38 Degree
        The absolute mean error on Roll angle estimation: 6.70 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 16.68 Degree
        The absolute mean error on Yaw angle estimation: 31.58 Degree
        The absolute mean error on Roll angle estimation: 13.52 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.95 Degree
        The absolute mean error on Yaw angle estimations: 31.87 Degree
        The absolute mean error on Roll angle estimations: 9.36 Degree
Exp2019-01-28_01-31-32_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-01
-28_01-31-32
All frames and annotations from 20 datasets have been read by 2019-01-28 01:45:28.915418
1. set (Dataset 19) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:45:33.736627!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0785
2. set (Dataset 16) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:45:51.666079!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0621
3. set (Dataset 21) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:46:13.880336!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1208
4. set (Dataset 15) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:46:31.505037!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0839
5. set (Dataset 13) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:46:48.013607!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0711
6. set (Dataset 12) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:47:03.840170!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0717
7. set (Dataset 18) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:47:22.798825!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1075
8. set (Dataset 22) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:47:40.348806!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0625
9. set (Dataset 23) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:47:57.521508!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1253
10. set (Dataset 8) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:48:15.543115!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.1046
11. set (Dataset 17) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:48:33.308332!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0798
12. set (Dataset 6) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:48:45.525640!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1409
13. set (Dataset 24) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:49:00.088372!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0745
14. set (Dataset 11) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:49:14.595338!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0954
15. set (Dataset 10) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:49:31.892782!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.1036
16. set (Dataset 20) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:49:49.861175!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0872
17. set (Dataset 2) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:50:05.079613!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0952
18. set (Dataset 4) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:50:21.675798!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.1138
19. set (Dataset 7) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:50:42.278975!
Epoch 1/1
745/745 [==============================] - 14s 19ms/step - loss: 0.0825
20. set (Dataset 1) being trained for epoch 1 in Experiment 3 by 2019-01-28 01:51:01.105416!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.1296
Epoch 1 for Experiment 3 completed!
Exp2019-01-28_01-31-32_part3.h5 has been saved.
The subjects are trained: [(19, 'M11'), (16, 'M09'), (21, 'F02'), (15, 'F03'), (13, 'M07'), (12, 'M06'), (18, 'F05'), (2
2, 'M01'), (23, 'M13'), (8, 'M02'), (17, 'M10'), (6, 'F06'), (24, 'M14'), (11, 'M05'), (10, 'M04'), (20, 'M12'), (2, 'F0
2'), (4, 'F04'), (7, 'M01'), (1, 'F01')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.000100_2019-
01-28_01-31-32
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 01:51:12.546216
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.24 Degree
        The absolute mean error on Yaw angle estimation: 45.66 Degree
        The absolute mean error on Roll angle estimation: 17.70 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.54 Degree
        The absolute mean error on Yaw angle estimation: 27.44 Degree
        The absolute mean error on Roll angle estimation: 7.14 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 22.05 Degree
        The absolute mean error on Yaw angle estimation: 33.61 Degree
        The absolute mean error on Roll angle estimation: 8.87 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 10.77 Degree
        The absolute mean error on Yaw angle estimation: 51.87 Degree
        The absolute mean error on Roll angle estimation: 15.01 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 12.15 Degree
        The absolute mean error on Yaw angle estimations: 39.64 Degree
        The absolute mean error on Roll angle estimations: 12.18 Degree
Exp2019-01-28_01-31-32_part3 completed!
Exp2019-01-28_01-31-32.h5 has been saved.
subject3_Exp2019-01-28_01-31-32.png has been saved by 2019-01-28 01:52:18.270148.
subject5_Exp2019-01-28_01-31-32.png has been saved by 2019-01-28 01:52:18.471129.
subject9_Exp2019-01-28_01-31-32.png has been saved by 2019-01-28 01:52:18.670936.
subject14_Exp2019-01-28_01-31-32.png has been saved by 2019-01-28 01:52:18.892865.
Model Exp2019-01-28_01-31-32 has been evaluated successfully.
Model Exp2019-01-28_01-31-32 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 01:59:18.084584: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 01:59:18.182273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 01:59:18.182532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 01:59:18.182545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 01:59:18.338882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 01:59:18.338910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 01:59:18.338915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 01:59:18.339052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_01-59-19 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   1065216
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    195
=================================================================
Total params: 135,325,955
Trainable params: 1,065,411
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 50
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000010_2019-0
1-28_01-59-19
All frames and annotations from 20 datasets have been read by 2019-01-28 01:59:23.441430
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:59:29.819999!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.6194
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-28 01:59:47.639170!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.5258
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:00:02.655544!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.4940
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:00:19.216889!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.4438
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:00:35.868720!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.4541
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:00:55.367251!
Epoch 1/1
569/569 [==============================] - 10s 17ms/step - loss: 0.4460
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:01:11.177186!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.4253
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:01:31.416177!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.3639
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:01:55.707370!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.3968
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:02:16.184918!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.3452
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:02:36.558829!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.3499
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:02:54.711800!
Epoch 1/1
218/498 [============>.................] - ETA: 5s - loss: 0.3791^C
Model Exp2019-01-28_01-59-19_part1 has been interrupted.
Exp2019-01-28_01-59-19_part1.h5 has been saved.
Model Exp2019-01-28_01-59-19_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Experiment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(flo
at).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-28 02:03:56.486153: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that thi
s TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-28 02:03:56.583276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from S
ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-28 02:03:56.583532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-28 02:03:56.583544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-28 02:03:56.738724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor w
ith strength 1 edge matrix:
2019-01-28 02:03:56.738749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-28 02:03:56.738754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-28 02:03:56.738893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:loc
alhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bu
s id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-28_02-03-57 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   1065216
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    195
=================================================================
Total params: 135,325,955
Trainable params: 1,065,411
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 50
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True # False #

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True # False #
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019-0
1-28_02-03-57
All frames and annotations from 20 datasets have been read by 2019-01-28 02:04:01.784912
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:04:08.189304!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.1271
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:04:26.258578!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0883
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:04:41.453020!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.1019
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:04:58.441447!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0875
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:05:15.077954!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.1079
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:05:34.256534!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1332
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:05:50.568761!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1315
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:06:10.537802!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0765
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:06:34.826586!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0922
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:06:55.624339!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0822
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:07:16.281853!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0992
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:07:34.404277!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.1166
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:07:49.397469!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1171
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:08:05.413886!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.1050
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:08:21.849640!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0987
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:08:40.634947!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0790
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:08:54.369683!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.0775
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:09:07.013808!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1263
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:09:21.586771!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0676
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-28 02:09:35.956481!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0810
Epoch 1 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 02:09:50.667418
1. set (Dataset 6) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:09:55.852240!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0983
2. set (Dataset 11) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:10:11.128190!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0576
3. set (Dataset 10) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:10:28.876650!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0721
4. set (Dataset 4) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:10:49.386971!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0740
5. set (Dataset 23) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:11:08.566569!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1118
6. set (Dataset 13) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:11:23.607480!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0602
7. set (Dataset 17) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:11:36.096185!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0692
8. set (Dataset 1) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:11:48.123464!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.1011
9. set (Dataset 8) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:12:05.001443!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0824
10. set (Dataset 7) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:12:26.397072!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0598
11. set (Dataset 21) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:12:45.908380!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.1037
12. set (Dataset 22) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:13:03.859294!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0555
13. set (Dataset 2) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:13:20.990440!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0676
14. set (Dataset 24) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:13:34.729345!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0612
15. set (Dataset 15) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:13:49.736169!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0739
16. set (Dataset 20) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:14:07.260694!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0600
17. set (Dataset 18) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:14:23.002006!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0915
18. set (Dataset 19) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:14:39.255521!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0774
19. set (Dataset 12) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:14:55.594656!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0655
20. set (Dataset 16) being trained for epoch 2 in Experiment 1 by 2019-01-28 02:15:17.694403!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0606
Epoch 2 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 02:15:38.409112
1. set (Dataset 19) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:15:43.279111!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0659
2. set (Dataset 16) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:16:01.145987!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0514
3. set (Dataset 21) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:16:23.482593!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0966
4. set (Dataset 15) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:16:41.001748!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0650
5. set (Dataset 13) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:16:57.571575!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0568
6. set (Dataset 12) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:17:13.762168!
Epoch 1/1
732/732 [==============================] - 14s 19ms/step - loss: 0.0520
7. set (Dataset 18) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:17:33.301833!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0805
8. set (Dataset 22) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:17:50.855067!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0520
9. set (Dataset 23) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:18:08.654256!
Epoch 1/1
569/569 [==============================] - 11s 19ms/step - loss: 0.0945
10. set (Dataset 8) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:18:27.063644!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0734
11. set (Dataset 17) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:18:44.972753!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0593
12. set (Dataset 6) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:18:57.205177!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0967
13. set (Dataset 24) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:19:11.935063!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0532
14. set (Dataset 11) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:19:26.613201!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0517
15. set (Dataset 10) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:19:44.296439!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0614
16. set (Dataset 20) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:20:02.421026!
Epoch 1/1
556/556 [==============================] - 10s 17ms/step - loss: 0.0534
17. set (Dataset 2) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:20:17.208777!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0554
18. set (Dataset 4) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:20:33.634643!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0673
19. set (Dataset 7) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:20:54.748283!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0516
20. set (Dataset 1) being trained for epoch 3 in Experiment 1 by 2019-01-28 02:21:12.918707!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0861
Epoch 3 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 02:21:26.321860
1. set (Dataset 4) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:21:33.694051!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0562
2. set (Dataset 1) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:21:52.442536!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0732
3. set (Dataset 17) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:22:05.345182!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0604
4. set (Dataset 10) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:22:19.784341!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0550
5. set (Dataset 12) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:22:40.179934!
Epoch 1/1
732/732 [==============================] - 14s 19ms/step - loss: 0.0502
6. set (Dataset 7) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:23:01.488179!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0468
7. set (Dataset 2) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:23:20.281140!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0479
8. set (Dataset 6) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:23:34.948522!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0756
9. set (Dataset 13) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:23:49.615988!
Epoch 1/1
485/485 [==============================] - 9s 19ms/step - loss: 0.0518
10. set (Dataset 23) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:24:04.141751!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0881
11. set (Dataset 18) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:24:20.177433!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0767
12. set (Dataset 19) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:24:36.263546!
Epoch 1/1
502/502 [==============================] - 9s 17ms/step - loss: 0.0760
13. set (Dataset 11) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:24:50.734948!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0420
14. set (Dataset 16) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:25:09.777733!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0532
15. set (Dataset 21) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:25:32.097965!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0820
16. set (Dataset 20) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:25:48.868875!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0520
17. set (Dataset 24) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:26:03.786607!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0473
18. set (Dataset 15) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:26:18.937301!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0603
19. set (Dataset 8) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:26:38.545062!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0593
20. set (Dataset 22) being trained for epoch 4 in Experiment 1 by 2019-01-28 02:26:58.649784!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0491
Epoch 4 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 02:27:15.012037
1. set (Dataset 15) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:27:21.351336!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0550
2. set (Dataset 22) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:27:39.576486!
Epoch 1/1
665/665 [==============================] - 12s 17ms/step - loss: 0.0422
3. set (Dataset 18) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:27:57.098646!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0738
4. set (Dataset 21) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:28:14.321267!
Epoch 1/1
634/634 [==============================] - 12s 19ms/step - loss: 0.0759
5. set (Dataset 7) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:28:33.693632!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0477
6. set (Dataset 8) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:28:54.917133!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0555
7. set (Dataset 24) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:29:13.537098!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0448
8. set (Dataset 19) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:29:27.145410!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0677
9. set (Dataset 12) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:29:43.355693!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0496
10. set (Dataset 13) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:30:01.444262!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0522
11. set (Dataset 2) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:30:15.454160!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0460
12. set (Dataset 4) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:30:32.073060!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0578
13. set (Dataset 16) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:30:54.417946!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0514
14. set (Dataset 1) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:31:16.048723!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0736
15. set (Dataset 17) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:31:28.696833!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.0585
16. set (Dataset 20) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:31:41.402782!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0456
17. set (Dataset 11) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:31:57.426076!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0406
18. set (Dataset 10) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:32:15.111137!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0551
19. set (Dataset 23) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:32:33.707599!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0862
20. set (Dataset 6) being trained for epoch 5 in Experiment 1 by 2019-01-28 02:32:48.943235!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0799
Epoch 5 for Experiment 1 completed!
Exp2019-01-28_02-03-57_part1.h5 has been saved.
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24, 'M14'), (19,
 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'), (20, 'M12'), (11, 'M05'
), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019
-01-28_02-03-57
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 02:33:01.115711
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 14.43 Degree
        The absolute mean error on Yaw angle estimation: 27.77 Degree
        The absolute mean error on Roll angle estimation: 12.71 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 6.34 Degree
        The absolute mean error on Yaw angle estimation: 27.52 Degree
        The absolute mean error on Roll angle estimation: 3.86 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 35.05 Degree
        The absolute mean error on Yaw angle estimation: 27.70 Degree
        The absolute mean error on Roll angle estimation: 9.10 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 17.44 Degree
        The absolute mean error on Yaw angle estimation: 24.35 Degree
        The absolute mean error on Roll angle estimation: 13.86 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.31 Degree
        The absolute mean error on Yaw angle estimations: 26.84 Degree
        The absolute mean error on Roll angle estimations: 9.88 Degree
Exp2019-01-28_02-03-57_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019-0
1-28_02-03-57
All frames and annotations from 20 datasets have been read by 2019-01-28 02:34:11.148835
1. set (Dataset 10) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:34:18.369514!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0479
2. set (Dataset 6) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:34:36.765699!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0641
3. set (Dataset 2) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:34:51.709599!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0456
4. set (Dataset 17) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:35:04.775470!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0525
5. set (Dataset 8) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:35:19.573356!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0542
6. set (Dataset 23) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:35:39.107454!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0736
7. set (Dataset 11) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:35:55.042562!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0382
8. set (Dataset 4) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:36:12.673223!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0535
9. set (Dataset 7) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:36:34.001798!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0446
10. set (Dataset 12) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:36:54.673958!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0445
11. set (Dataset 24) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:37:12.592322!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0468
12. set (Dataset 15) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:37:28.071868!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0573
13. set (Dataset 1) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:37:44.867954!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0733
14. set (Dataset 22) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:38:00.530852!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0430
15. set (Dataset 18) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:38:18.128255!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0724
16. set (Dataset 20) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:38:34.699453!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0447
17. set (Dataset 16) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:38:53.614806!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0487
18. set (Dataset 21) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:39:16.320713!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0850
19. set (Dataset 13) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:39:32.549526!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0518
20. set (Dataset 19) being trained for epoch 1 in Experiment 2 by 2019-01-28 02:39:46.003149!
Epoch 1/1
502/502 [==============================] - 9s 17ms/step - loss: 0.0652
Epoch 1 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 02:39:59.223030
1. set (Dataset 21) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:40:05.230268!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0722
2. set (Dataset 19) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:40:21.566476!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0613
3. set (Dataset 24) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:40:35.437855!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0471
4. set (Dataset 18) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:40:50.261950!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0796
5. set (Dataset 23) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:41:06.933758!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0814
6. set (Dataset 13) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:41:22.095510!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0449
7. set (Dataset 16) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:41:39.850665!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0529
8. set (Dataset 15) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:42:02.506188!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0558
9. set (Dataset 8) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:42:22.173959!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0521
10. set (Dataset 7) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:42:43.959847!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0441
11. set (Dataset 11) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:43:03.241448!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0362
12. set (Dataset 10) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:43:20.834527!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0512
13. set (Dataset 22) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:43:40.491147!
Epoch 1/1
665/665 [==============================] - 12s 19ms/step - loss: 0.0418
14. set (Dataset 6) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:43:58.070812!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0793
15. set (Dataset 2) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:44:12.933758!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0434
16. set (Dataset 20) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:44:27.745646!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0516
17. set (Dataset 1) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:44:43.101485!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0700
18. set (Dataset 17) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:44:56.195263!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0526
19. set (Dataset 12) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:45:10.486152!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0457
20. set (Dataset 4) being trained for epoch 2 in Experiment 2 by 2019-01-28 02:45:30.717478!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0559
Epoch 2 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 02:45:48.201169
1. set (Dataset 17) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:45:51.945202!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0509
2. set (Dataset 4) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:46:06.660615!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0477
3. set (Dataset 11) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:46:26.106441!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0385
4. set (Dataset 2) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:46:41.578206!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0442
5. set (Dataset 13) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:46:55.558568!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0450
6. set (Dataset 12) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:47:11.523256!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0424
7. set (Dataset 1) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:47:29.721905!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0652
8. set (Dataset 10) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:47:45.905777!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0480
9. set (Dataset 23) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:48:04.353716!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0741
10. set (Dataset 8) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:48:22.496898!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0509
11. set (Dataset 16) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:48:44.954613!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0486
12. set (Dataset 21) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:49:07.418077!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0715
13. set (Dataset 6) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:49:24.229688!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0698
14. set (Dataset 19) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:49:39.044241!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0606
15. set (Dataset 24) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:49:52.873934!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0438
16. set (Dataset 20) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:50:07.281088!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0480
17. set (Dataset 22) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:50:23.585210!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0402
18. set (Dataset 18) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:50:41.465847!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0739
19. set (Dataset 7) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:50:59.990401!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0445
20. set (Dataset 15) being trained for epoch 3 in Experiment 2 by 2019-01-28 02:51:19.486829!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0556
Epoch 3 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 02:51:35.763374
1. set (Dataset 18) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:51:41.655810!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0694
2. set (Dataset 15) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:51:59.239922!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0518
3. set (Dataset 16) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:52:19.857304!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0476
4. set (Dataset 24) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:52:41.097481!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0418
5. set (Dataset 12) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:52:57.311363!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0431
6. set (Dataset 7) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:53:18.112009!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0413
7. set (Dataset 22) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:53:37.447246!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0395
8. set (Dataset 21) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:53:55.682223!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0709
9. set (Dataset 13) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:54:12.066066!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0444
10. set (Dataset 23) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:54:26.449939!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0689
11. set (Dataset 1) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:54:41.643323!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0649
12. set (Dataset 17) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:54:54.334186!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0490
13. set (Dataset 19) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:55:06.544221!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0556
14. set (Dataset 4) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:55:23.187125!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0541
15. set (Dataset 11) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:55:42.317592!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0387
16. set (Dataset 20) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:55:58.205967!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0464
17. set (Dataset 6) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:56:13.502521!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0657
18. set (Dataset 2) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:56:28.342319!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0424
19. set (Dataset 8) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:56:45.424821!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0508
20. set (Dataset 10) being trained for epoch 4 in Experiment 2 by 2019-01-28 02:57:06.489014!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0475
Epoch 4 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 02:57:23.723482
1. set (Dataset 2) being trained for epoch 5 in Experiment 2 by 2019-01-28 02:57:28.804364!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0403
2. set (Dataset 10) being trained for epoch 5 in Experiment 2 by 2019-01-28 02:57:45.448325!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0448
3. set (Dataset 1) being trained for epoch 5 in Experiment 2 by 2019-01-28 02:58:03.603076!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0585
4. set (Dataset 11) being trained for epoch 5 in Experiment 2 by 2019-01-28 02:58:18.298219!
Epoch 1/1
572/572 [==============================] - 11s 18ms/step - loss: 0.0354
5. set (Dataset 7) being trained for epoch 5 in Experiment 2 by 2019-01-28 02:58:36.472232!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0415
6. set (Dataset 8) being trained for epoch 5 in Experiment 2 by 2019-01-28 02:58:58.018014!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0473
7. set (Dataset 6) being trained for epoch 5 in Experiment 2 by 2019-01-28 02:59:17.071972!
Epoch 1/1
542/542 [==============================] - 10s 19ms/step - loss: 0.0586
8. set (Dataset 17) being trained for epoch 5 in Experiment 2 by 2019-01-28 02:59:30.906741!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0503
9. set (Dataset 12) being trained for epoch 5 in Experiment 2 by 2019-01-28 02:59:45.107962!
Epoch 1/1
732/732 [==============================] - 14s 18ms/step - loss: 0.0417
10. set (Dataset 13) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:00:03.517262!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0428
11. set (Dataset 22) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:00:18.939400!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0405
12. set (Dataset 18) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:00:36.662579!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0698
13. set (Dataset 4) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:00:55.147926!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0481
14. set (Dataset 15) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:01:14.943447!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0528
15. set (Dataset 16) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:01:35.604702!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0476
16. set (Dataset 20) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:01:57.387712!
Epoch 1/1
556/556 [==============================] - 10s 17ms/step - loss: 0.0439
17. set (Dataset 19) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:02:12.051684!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0579
18. set (Dataset 24) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:02:25.926291!
Epoch 1/1
492/492 [==============================] - 9s 19ms/step - loss: 0.0387
19. set (Dataset 23) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:02:40.537945!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0717
20. set (Dataset 21) being trained for epoch 5 in Experiment 2 by 2019-01-28 03:02:56.988110!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0710
Epoch 5 for Experiment 2 completed!
Exp2019-01-28_02-03-57_part2.h5 has been saved.
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F06'), (17, 'M
10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'), (20, 'M12'), (19, 'M11')
, (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019
-01-28_02-03-57
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 03:03:10.736593
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 16.29 Degree
        The absolute mean error on Yaw angle estimation: 32.74 Degree
        The absolute mean error on Roll angle estimation: 17.73 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.40 Degree
        The absolute mean error on Yaw angle estimation: 24.83 Degree
        The absolute mean error on Roll angle estimation: 5.32 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 37.75 Degree
        The absolute mean error on Yaw angle estimation: 29.97 Degree
        The absolute mean error on Roll angle estimation: 10.38 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.10 Degree
        The absolute mean error on Yaw angle estimation: 30.85 Degree
        The absolute mean error on Roll angle estimation: 14.28 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 20.63 Degree
        The absolute mean error on Yaw angle estimations: 29.60 Degree
        The absolute mean error on Roll angle estimations: 11.93 Degree
Exp2019-01-28_02-03-57_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019-0
1-28_02-03-57
All frames and annotations from 20 datasets have been read by 2019-01-28 03:04:20.789996
1. set (Dataset 24) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:04:25.485260!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0386
2. set (Dataset 21) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:04:40.461445!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0648
3. set (Dataset 22) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:04:58.325685!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0353
4. set (Dataset 16) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:05:19.423870!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0442
5. set (Dataset 8) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:05:44.098940!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0486
6. set (Dataset 23) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:06:03.286740!
Epoch 1/1
569/569 [==============================] - 10s 17ms/step - loss: 0.0673
7. set (Dataset 19) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:06:18.056929!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0569
8. set (Dataset 18) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:06:33.126314!
Epoch 1/1
614/614 [==============================] - 11s 17ms/step - loss: 0.0668
9. set (Dataset 7) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:06:51.470029!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0474
10. set (Dataset 12) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:07:12.113006!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0430
11. set (Dataset 6) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:07:30.203059!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0602
12. set (Dataset 2) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:07:45.118638!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0409
13. set (Dataset 15) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:08:00.718168!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0531
14. set (Dataset 10) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:08:20.114786!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0488
15. set (Dataset 1) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:08:38.332929!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0616
16. set (Dataset 20) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:08:52.952044!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0463
17. set (Dataset 4) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:09:10.460374!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0479
18. set (Dataset 11) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:09:29.467685!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0391
19. set (Dataset 13) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:09:44.346075!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0446
20. set (Dataset 17) being trained for epoch 1 in Experiment 3 by 2019-01-28 03:09:56.746552!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0498
Epoch 1 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 03:10:08.322757
1. set (Dataset 11) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:10:14.031230!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0347
2. set (Dataset 17) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:10:28.039694!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0468
3. set (Dataset 6) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:10:40.468916!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0586
4. set (Dataset 1) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:10:55.371534!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0607
5. set (Dataset 23) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:11:10.016194!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0675
6. set (Dataset 13) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:11:25.325603!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0419
7. set (Dataset 4) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:11:41.473919!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0463
8. set (Dataset 2) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:12:00.277937!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0404
9. set (Dataset 8) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:12:17.123987!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.0555
10. set (Dataset 7) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:12:38.257467!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0432
11. set (Dataset 19) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:12:56.475783!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0542
12. set (Dataset 24) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:13:10.087384!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0421
13. set (Dataset 10) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:13:26.143056!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0442
14. set (Dataset 21) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:13:45.325862!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0692
15. set (Dataset 22) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:14:03.274333!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0386
16. set (Dataset 20) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:14:20.408269!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0454
17. set (Dataset 15) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:14:36.647655!
Epoch 1/1
654/654 [==============================] - 11s 17ms/step - loss: 0.0508
18. set (Dataset 16) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:14:56.828505!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0431
19. set (Dataset 12) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:15:20.565831!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0439
20. set (Dataset 18) being trained for epoch 2 in Experiment 3 by 2019-01-28 03:15:39.558247!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0650
Epoch 2 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 03:15:55.299131
1. set (Dataset 16) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:16:04.023744!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0414
2. set (Dataset 18) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:16:26.457672!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0583
3. set (Dataset 19) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:16:42.659968!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0521
4. set (Dataset 22) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:16:57.994901!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0347
5. set (Dataset 13) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:17:15.121969!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0416
6. set (Dataset 12) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:17:31.389356!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0408
7. set (Dataset 15) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:17:51.008375!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0500
8. set (Dataset 24) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:18:07.672720!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0381
9. set (Dataset 23) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:18:22.107313!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0677
10. set (Dataset 8) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:18:40.232820!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0474
11. set (Dataset 4) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:19:01.293526!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0472
12. set (Dataset 11) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:19:20.138651!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0355
13. set (Dataset 21) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:19:36.525868!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0660
14. set (Dataset 17) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:19:51.757814!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0528
15. set (Dataset 6) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:20:04.171093!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0594
16. set (Dataset 20) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:20:19.333560!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0441
17. set (Dataset 10) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:20:36.566866!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0444
18. set (Dataset 1) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:20:54.409091!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0605
19. set (Dataset 7) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:21:11.078047!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0434
20. set (Dataset 2) being trained for epoch 3 in Experiment 3 by 2019-01-28 03:21:29.186104!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0412
Epoch 3 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 03:21:42.913562
1. set (Dataset 1) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:21:47.959826!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0548
2. set (Dataset 2) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:22:02.407350!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0390
3. set (Dataset 4) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:22:19.073187!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0433
4. set (Dataset 6) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:22:37.832284!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0546
5. set (Dataset 12) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:22:54.834427!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0409
6. set (Dataset 7) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:23:15.472524!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0415
7. set (Dataset 10) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:23:36.064806!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0420
8. set (Dataset 11) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:23:54.860607!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0344
9. set (Dataset 13) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:24:09.851006!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0404
10. set (Dataset 23) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:24:24.228979!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0661
11. set (Dataset 15) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:24:40.612285!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0490
12. set (Dataset 16) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:25:01.174732!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0424
13. set (Dataset 17) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:25:21.467004!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0472
14. set (Dataset 18) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:25:34.339186!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0625
15. set (Dataset 19) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:25:50.036329!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0532
16. set (Dataset 20) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:26:04.319602!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0431
17. set (Dataset 21) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:26:20.288485!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0683
18. set (Dataset 22) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:26:37.963298!
Epoch 1/1
665/665 [==============================] - 9s 14ms/step - loss: 0.0375
19. set (Dataset 8) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:26:55.096383!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0485
20. set (Dataset 24) being trained for epoch 4 in Experiment 3 by 2019-01-28 03:27:13.489581!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0429
Epoch 4 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 03:27:26.560758
1. set (Dataset 22) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:27:32.950844!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0339
2. set (Dataset 24) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:27:49.599970!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0363
3. set (Dataset 15) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:28:04.974773!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0482
4. set (Dataset 19) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:28:21.933879!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0503
5. set (Dataset 7) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:28:38.442497!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0409
6. set (Dataset 8) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:28:59.669775!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0452
7. set (Dataset 21) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:29:19.629366!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0656
8. set (Dataset 16) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:29:39.943391!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0417
9. set (Dataset 12) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:30:03.725050!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0395
10. set (Dataset 13) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:30:21.697699!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0398
11. set (Dataset 10) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:30:37.734376!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0437
12. set (Dataset 1) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:30:55.686215!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0601
13. set (Dataset 18) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:31:10.492407!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0640
14. set (Dataset 2) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:31:26.568877!
Epoch 1/1
511/511 [==============================] - 9s 17ms/step - loss: 0.0400
15. set (Dataset 4) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:31:42.854633!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0434
16. set (Dataset 20) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:32:01.641202!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0433
17. set (Dataset 17) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:32:15.877810!
Epoch 1/1
395/395 [==============================] - 8s 19ms/step - loss: 0.0481
18. set (Dataset 6) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:32:28.617109!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0558
19. set (Dataset 23) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:32:43.979150!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0635
20. set (Dataset 11) being trained for epoch 5 in Experiment 3 by 2019-01-28 03:33:00.166362!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0401
Epoch 5 for Experiment 3 completed!
Exp2019-01-28_02-03-57_part3.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21, 'F02'), (16,
 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'), (20, 'M12'), (17, 'M10'
), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019
-01-28_02-03-57
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 03:33:12.704056
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 9.27 Degree
        The absolute mean error on Yaw angle estimation: 28.86 Degree
        The absolute mean error on Roll angle estimation: 13.80 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.47 Degree
        The absolute mean error on Yaw angle estimation: 23.85 Degree
        The absolute mean error on Roll angle estimation: 4.27 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 38.81 Degree
        The absolute mean error on Yaw angle estimation: 25.14 Degree
        The absolute mean error on Roll angle estimation: 9.61 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 22.11 Degree
        The absolute mean error on Yaw angle estimation: 31.60 Degree
        The absolute mean error on Roll angle estimation: 14.36 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.67 Degree
        The absolute mean error on Yaw angle estimations: 27.36 Degree
        The absolute mean error on Roll angle estimations: 10.51 Degree
Exp2019-01-28_02-03-57_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019-0
1-28_02-03-57
All frames and annotations from 20 datasets have been read by 2019-01-28 03:34:23.042931
1. set (Dataset 6) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:34:28.222675!
Epoch 1/1
542/542 [==============================] - 9s 17ms/step - loss: 0.0529
2. set (Dataset 11) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:34:43.129055!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0345
3. set (Dataset 10) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:35:00.853932!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0435
4. set (Dataset 4) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:35:21.372256!
Epoch 1/1
744/744 [==============================] - 13s 17ms/step - loss: 0.0425
5. set (Dataset 8) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:35:42.132853!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0466
6. set (Dataset 23) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:36:01.419429!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0618
7. set (Dataset 17) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:36:15.428648!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0447
8. set (Dataset 1) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:36:27.511118!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0568
9. set (Dataset 7) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:36:43.908430!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0432
10. set (Dataset 12) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:37:04.369420!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0392
11. set (Dataset 21) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:37:23.427029!
Epoch 1/1
634/634 [==============================] - 12s 19ms/step - loss: 0.0664
12. set (Dataset 22) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:37:41.747720!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0370
13. set (Dataset 2) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:37:58.919127!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0410
14. set (Dataset 24) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:38:12.818399!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0387
15. set (Dataset 15) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:38:28.080991!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0488
16. set (Dataset 20) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:38:45.238038!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0422
17. set (Dataset 18) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:39:01.190841!
Epoch 1/1
614/614 [==============================] - 11s 17ms/step - loss: 0.0619
18. set (Dataset 19) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:39:16.922887!
Epoch 1/1
502/502 [==============================] - 9s 19ms/step - loss: 0.0512
19. set (Dataset 13) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:39:31.175843!
Epoch 1/1
485/485 [==============================] - 8s 17ms/step - loss: 0.0420
20. set (Dataset 16) being trained for epoch 1 in Experiment 4 by 2019-01-28 03:39:48.486562!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0427
Epoch 1 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 03:40:09.307318
1. set (Dataset 19) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:40:14.200941!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0494
2. set (Dataset 16) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:40:31.952101!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0388
3. set (Dataset 21) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:40:54.454496!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0651
4. set (Dataset 15) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:41:12.455936!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0462
5. set (Dataset 23) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:41:29.845592!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0602
6. set (Dataset 13) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:41:44.819369!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0380
7. set (Dataset 18) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:41:59.589175!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0600
8. set (Dataset 22) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:42:17.213154!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0362
9. set (Dataset 8) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:42:37.272407!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0460
10. set (Dataset 7) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:42:58.603242!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0412
11. set (Dataset 17) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:43:15.820252!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0492
12. set (Dataset 6) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:43:28.161017!
Epoch 1/1
542/542 [==============================] - 10s 19ms/step - loss: 0.0537
13. set (Dataset 24) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:43:43.081375!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0389
14. set (Dataset 11) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:43:57.646361!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0332
15. set (Dataset 10) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:44:15.218074!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0439
16. set (Dataset 20) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:44:33.578983!
Epoch 1/1
556/556 [==============================] - 10s 17ms/step - loss: 0.0434
17. set (Dataset 2) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:44:48.278708!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0410
18. set (Dataset 4) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:45:04.925703!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0425
19. set (Dataset 12) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:45:25.535038!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0400
20. set (Dataset 1) being trained for epoch 2 in Experiment 4 by 2019-01-28 03:45:43.776667!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0573
Epoch 2 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 03:45:57.021731
1. set (Dataset 4) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:46:04.407624!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0409
2. set (Dataset 1) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:46:23.056793!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0539
3. set (Dataset 17) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:46:36.108989!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.0508
4. set (Dataset 10) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:46:50.709643!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0417
5. set (Dataset 13) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:47:08.757087!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0402
6. set (Dataset 12) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:47:24.881715!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0377
7. set (Dataset 2) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:47:43.156074!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0397
8. set (Dataset 6) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:47:57.606790!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0568
9. set (Dataset 23) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:48:12.817410!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0624
10. set (Dataset 8) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:48:30.882652!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0457
11. set (Dataset 18) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:48:50.690397!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0609
12. set (Dataset 19) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:49:06.960405!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0508
13. set (Dataset 11) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:49:21.799929!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0359
14. set (Dataset 16) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:49:40.733865!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0414
15. set (Dataset 21) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:50:02.856795!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0660
16. set (Dataset 20) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:50:19.427353!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0407
17. set (Dataset 24) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:50:33.999877!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0391
18. set (Dataset 15) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:50:49.272861!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0471
19. set (Dataset 7) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:51:08.773360!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0425
20. set (Dataset 22) being trained for epoch 3 in Experiment 4 by 2019-01-28 03:51:28.797161!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0345
Epoch 3 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 03:51:45.058857
1. set (Dataset 15) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:51:51.399651!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0469
2. set (Dataset 22) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:52:09.461886!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0341
3. set (Dataset 18) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:52:27.482240!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0588
4. set (Dataset 21) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:52:44.432556!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0646
5. set (Dataset 12) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:53:03.242356!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0382
6. set (Dataset 7) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:53:24.339242!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0414
7. set (Dataset 24) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:53:42.724382!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0372
8. set (Dataset 19) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:53:56.533188!
Epoch 1/1
502/502 [==============================] - 9s 19ms/step - loss: 0.0512
9. set (Dataset 13) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:54:10.695416!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0389
10. set (Dataset 23) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:54:25.025565!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0617
11. set (Dataset 2) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:54:40.195719!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0390
12. set (Dataset 4) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:54:56.733429!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0443
13. set (Dataset 16) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:55:19.230889!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0402
14. set (Dataset 1) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:55:40.703987!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0586
15. set (Dataset 17) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:55:53.539286!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0480
16. set (Dataset 20) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:56:06.189222!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0408
17. set (Dataset 11) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:56:22.194863!
Epoch 1/1
572/572 [==============================] - 11s 19ms/step - loss: 0.0348
18. set (Dataset 10) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:56:40.202809!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0422
19. set (Dataset 8) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:57:01.042773!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0466
20. set (Dataset 6) being trained for epoch 4 in Experiment 4 by 2019-01-28 03:57:20.001276!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0559
Epoch 4 for Experiment 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 03:57:34.214622
1. set (Dataset 10) being trained for epoch 5 in Experiment 4 by 2019-01-28 03:57:41.445258!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0407
2. set (Dataset 6) being trained for epoch 5 in Experiment 4 by 2019-01-28 03:57:59.906382!
Epoch 1/1
542/542 [==============================] - 10s 19ms/step - loss: 0.0518
3. set (Dataset 2) being trained for epoch 5 in Experiment 4 by 2019-01-28 03:58:15.159059!
Epoch 1/1
511/511 [==============================] - 9s 19ms/step - loss: 0.0392
4. set (Dataset 17) being trained for epoch 5 in Experiment 4 by 2019-01-28 03:58:28.434346!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0467
5. set (Dataset 7) being trained for epoch 5 in Experiment 4 by 2019-01-28 03:58:43.097499!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0411
6. set (Dataset 8) being trained for epoch 5 in Experiment 4 by 2019-01-28 03:59:04.351054!
Epoch 1/1
772/772 [==============================] - 14s 19ms/step - loss: 0.0446
7. set (Dataset 11) being trained for epoch 5 in Experiment 4 by 2019-01-28 03:59:24.473099!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0335
8. set (Dataset 4) being trained for epoch 5 in Experiment 4 by 2019-01-28 03:59:42.200251!
Epoch 1/1
744/744 [==============================] - 14s 19ms/step - loss: 0.0437
9. set (Dataset 12) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:00:03.305260!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0389
10. set (Dataset 13) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:00:21.525260!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0378
11. set (Dataset 24) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:00:34.805826!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0371
12. set (Dataset 15) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:00:50.132841!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0466
13. set (Dataset 1) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:01:07.352403!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0595
14. set (Dataset 22) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:01:23.018533!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0349
15. set (Dataset 18) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:01:40.851319!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0580
16. set (Dataset 20) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:01:57.488379!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0387
17. set (Dataset 16) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:02:16.346465!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0380
18. set (Dataset 21) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:02:38.612947!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0639
19. set (Dataset 23) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:02:55.636897!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0627
20. set (Dataset 19) being trained for epoch 5 in Experiment 4 by 2019-01-28 04:03:10.982081!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0501
Epoch 5 for Experiment 4 completed!
Exp2019-01-28_02-03-57_part4.h5 has been saved.
The subjects are trained: [(10, 'M04'), (6, 'F06'), (2, 'F02'), (17, 'M10'), (7, 'M01'), (8, 'M02'), (11, 'M05'), (4, 'F
04'), (12, 'M06'), (13, 'M07'), (24, 'M14'), (15, 'F03'), (1, 'F01'), (22, 'M01'), (18, 'F05'), (20, 'M12'), (16, 'M09')
, (21, 'F02'), (23, 'M13'), (19, 'M11')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019
-01-28_02-03-57
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 04:03:22.400693
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 11.67 Degree
        The absolute mean error on Yaw angle estimation: 26.25 Degree
        The absolute mean error on Roll angle estimation: 12.04 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.39 Degree
        The absolute mean error on Yaw angle estimation: 24.88 Degree
        The absolute mean error on Roll angle estimation: 3.40 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 40.55 Degree
        The absolute mean error on Yaw angle estimation: 26.09 Degree
        The absolute mean error on Roll angle estimation: 8.35 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.52 Degree
        The absolute mean error on Yaw angle estimation: 31.03 Degree
        The absolute mean error on Roll angle estimation: 13.24 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 20.03 Degree
        The absolute mean error on Yaw angle estimations: 27.06 Degree
        The absolute mean error on Roll angle estimations: 9.25 Degree
Exp2019-01-28_02-03-57_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019-0
1-28_02-03-57
All frames and annotations from 20 datasets have been read by 2019-01-28 04:04:32.238887
1. set (Dataset 21) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:04:38.280525!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0596
2. set (Dataset 19) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:04:54.749675!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0488
3. set (Dataset 24) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:05:08.537251!
Epoch 1/1
492/492 [==============================] - 9s 19ms/step - loss: 0.0364
4. set (Dataset 18) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:05:23.662567!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0584
5. set (Dataset 8) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:05:42.518237!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0456
6. set (Dataset 23) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:06:02.148880!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0606
7. set (Dataset 16) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:06:21.136369!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0389
8. set (Dataset 15) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:06:44.362919!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0461
9. set (Dataset 7) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:07:03.803031!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0411
10. set (Dataset 12) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:07:24.812404!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0378
11. set (Dataset 11) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:07:43.676246!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0332
12. set (Dataset 10) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:08:01.268259!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0407
13. set (Dataset 22) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:08:20.866094!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0356
14. set (Dataset 6) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:08:38.404266!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0554
15. set (Dataset 2) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:08:53.126479!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0385
16. set (Dataset 20) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:09:07.896575!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0389
17. set (Dataset 1) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:09:22.775226!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0568
18. set (Dataset 17) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:09:35.517660!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0470
19. set (Dataset 13) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:09:47.617652!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0391
20. set (Dataset 4) being trained for epoch 1 in Experiment 5 by 2019-01-28 04:10:03.882034!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0424
Epoch 1 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 04:10:21.747215
1. set (Dataset 17) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:10:25.487489!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0425
2. set (Dataset 4) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:10:40.125320!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0382
3. set (Dataset 11) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:10:59.307735!
Epoch 1/1
572/572 [==============================] - 11s 18ms/step - loss: 0.0350
4. set (Dataset 2) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:11:14.963301!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0400
5. set (Dataset 23) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:11:29.834498!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0616
6. set (Dataset 13) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:11:45.123788!
Epoch 1/1
485/485 [==============================] - 9s 19ms/step - loss: 0.0372
7. set (Dataset 1) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:11:59.226612!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0533
8. set (Dataset 10) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:12:15.458963!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0416
9. set (Dataset 8) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:12:36.486976!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0451
10. set (Dataset 7) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:12:58.094402!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0408
11. set (Dataset 16) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:13:19.747618!
Epoch 1/1
914/914 [==============================] - 17s 19ms/step - loss: 0.0394
12. set (Dataset 21) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:13:42.781738!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0635
13. set (Dataset 6) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:13:59.588370!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0517
14. set (Dataset 19) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:14:14.190162!
Epoch 1/1
502/502 [==============================] - 9s 17ms/step - loss: 0.0475
15. set (Dataset 24) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:14:27.694770!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0355
16. set (Dataset 20) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:14:41.780743!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0392
17. set (Dataset 22) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:14:58.559745!
Epoch 1/1
665/665 [==============================] - 12s 19ms/step - loss: 0.0348
18. set (Dataset 18) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:15:16.804840!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0588
19. set (Dataset 12) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:15:35.280830!
Epoch 1/1
732/732 [==============================] - 14s 19ms/step - loss: 0.0383
20. set (Dataset 15) being trained for epoch 2 in Experiment 5 by 2019-01-28 04:15:55.234348!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0485
Epoch 2 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 04:16:11.793743
1. set (Dataset 18) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:16:17.660963!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0558
2. set (Dataset 15) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:16:34.955685!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0444
3. set (Dataset 16) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:16:55.294454!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0382
4. set (Dataset 24) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:17:16.340829!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0370
5. set (Dataset 13) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:17:29.826611!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0383
6. set (Dataset 12) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:17:46.009743!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0372
7. set (Dataset 22) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:18:05.771770!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0332
8. set (Dataset 21) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:18:23.845668!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0613
9. set (Dataset 23) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:18:40.639123!
Epoch 1/1
569/569 [==============================] - 11s 19ms/step - loss: 0.0610
10. set (Dataset 8) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:18:59.034774!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0445
11. set (Dataset 1) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:19:18.175027!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0574
12. set (Dataset 17) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:19:31.072178!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0454
13. set (Dataset 19) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:19:43.181566!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0501
14. set (Dataset 4) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:19:59.659375!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0420
15. set (Dataset 11) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:20:18.923606!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0364
16. set (Dataset 20) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:20:34.705393!
Epoch 1/1
556/556 [==============================] - 10s 17ms/step - loss: 0.0386
17. set (Dataset 6) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:20:49.641384!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0526
18. set (Dataset 2) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:21:04.554520!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0396
19. set (Dataset 7) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:21:21.586396!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0425
20. set (Dataset 10) being trained for epoch 3 in Experiment 5 by 2019-01-28 04:21:42.277040!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0425
Epoch 3 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 04:21:59.824836
1. set (Dataset 2) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:22:04.907082!
Epoch 1/1
511/511 [==============================] - 10s 19ms/step - loss: 0.0365
2. set (Dataset 10) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:22:21.779584!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0415
3. set (Dataset 1) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:22:40.180662!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0519
4. set (Dataset 11) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:22:55.038857!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0341
5. set (Dataset 12) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:23:12.754196!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0354
6. set (Dataset 7) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:23:33.700861!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0401
7. set (Dataset 6) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:23:52.490340!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0519
8. set (Dataset 17) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:24:05.921374!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0450
9. set (Dataset 13) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:24:18.023171!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0364
10. set (Dataset 23) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:24:32.415948!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0590
11. set (Dataset 22) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:24:48.937659!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0354
12. set (Dataset 18) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:25:06.541768!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0564
13. set (Dataset 4) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:25:25.038135!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0402
14. set (Dataset 15) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:25:44.779008!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0454
15. set (Dataset 16) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:26:05.300376!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0388
16. set (Dataset 20) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:26:27.199606!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0409
17. set (Dataset 19) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:26:42.146515!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0478
18. set (Dataset 24) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:26:55.838527!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0364
19. set (Dataset 8) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:27:12.584329!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0457
20. set (Dataset 21) being trained for epoch 4 in Experiment 5 by 2019-01-28 04:27:32.653157!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0635
Epoch 4 for Experiment 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 04:27:48.597019
1. set (Dataset 24) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:27:53.264293!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0352
2. set (Dataset 21) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:28:08.069955!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0601
3. set (Dataset 22) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:28:25.681271!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0346
4. set (Dataset 16) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:28:46.422575!
Epoch 1/1
914/914 [==============================] - 17s 19ms/step - loss: 0.0389
5. set (Dataset 7) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:29:11.441798!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0422
6. set (Dataset 8) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:29:32.700305!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0420
7. set (Dataset 19) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:29:51.824836!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0470
8. set (Dataset 18) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:30:07.004460!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0551
9. set (Dataset 12) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:30:25.421350!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0380
10. set (Dataset 13) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:30:43.558032!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0363
11. set (Dataset 6) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:30:57.642416!
Epoch 1/1
542/542 [==============================] - 9s 18ms/step - loss: 0.0525
12. set (Dataset 2) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:31:12.269607!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0392
13. set (Dataset 15) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:31:27.787559!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0462
14. set (Dataset 10) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:31:46.969323!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0430
15. set (Dataset 1) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:32:05.184530!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0571
16. set (Dataset 20) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:32:19.350248!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0406
17. set (Dataset 4) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:32:36.991389!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0407
18. set (Dataset 11) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:32:56.243415!
Epoch 1/1
572/572 [==============================] - 11s 18ms/step - loss: 0.0359
19. set (Dataset 23) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:33:12.256686!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0608
20. set (Dataset 17) being trained for epoch 5 in Experiment 5 by 2019-01-28 04:33:26.170358!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0461
Epoch 5 for Experiment 5 completed!
Exp2019-01-28_02-03-57_part5.h5 has been saved.
The subjects are trained: [(24, 'M14'), (21, 'F02'), (22, 'M01'), (16, 'M09'), (7, 'M01'), (8, 'M02'), (19, 'M11'), (18,
 'F05'), (12, 'M06'), (13, 'M07'), (6, 'F06'), (2, 'F02'), (15, 'F03'), (10, 'M04'), (1, 'F01'), (20, 'M12'), (4, 'F04')
, (11, 'M05'), (23, 'M13'), (17, 'M10')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019
-01-28_02-03-57
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 04:33:35.706552
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.69 Degree
        The absolute mean error on Yaw angle estimation: 26.97 Degree
        The absolute mean error on Roll angle estimation: 11.24 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.38 Degree
        The absolute mean error on Yaw angle estimation: 25.73 Degree
        The absolute mean error on Roll angle estimation: 3.57 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 38.75 Degree
        The absolute mean error on Yaw angle estimation: 25.78 Degree
        The absolute mean error on Roll angle estimation: 8.44 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 21.36 Degree
        The absolute mean error on Yaw angle estimation: 33.08 Degree
        The absolute mean error on Roll angle estimation: 14.05 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.05 Degree
        The absolute mean error on Yaw angle estimations: 27.89 Degree
        The absolute mean error on Roll angle estimations: 9.33 Degree
Exp2019-01-28_02-03-57_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019-0
1-28_02-03-57
All frames and annotations from 20 datasets have been read by 2019-01-28 04:34:46.055936
1. set (Dataset 11) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:34:51.756103!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0338
2. set (Dataset 17) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:35:05.630270!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0428
3. set (Dataset 6) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:35:17.764649!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0504
4. set (Dataset 1) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:35:32.680660!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0538
5. set (Dataset 8) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:35:49.437619!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0450
6. set (Dataset 23) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:36:08.903578!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0597
7. set (Dataset 4) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:36:26.752217!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0402
8. set (Dataset 2) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:36:45.506513!
Epoch 1/1
511/511 [==============================] - 10s 19ms/step - loss: 0.0383
9. set (Dataset 7) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:37:02.633121!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0419
10. set (Dataset 12) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:37:23.523651!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0365
11. set (Dataset 19) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:37:41.422593!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0468
12. set (Dataset 24) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:37:55.105591!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0384
13. set (Dataset 10) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:38:11.152280!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0415
14. set (Dataset 21) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:38:30.210126!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0604
15. set (Dataset 22) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:38:47.961856!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0346
16. set (Dataset 20) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:39:05.412645!
Epoch 1/1
556/556 [==============================] - 10s 17ms/step - loss: 0.0387
17. set (Dataset 15) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:39:21.495187!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0453
18. set (Dataset 16) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:39:42.291149!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0368
19. set (Dataset 13) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:40:03.669063!
Epoch 1/1
485/485 [==============================] - 8s 17ms/step - loss: 0.0415
20. set (Dataset 18) being trained for epoch 1 in Experiment 6 by 2019-01-28 04:40:18.012852!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0558
Epoch 1 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 04:40:33.566346
1. set (Dataset 16) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:40:42.287415!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0376
2. set (Dataset 18) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:41:04.856122!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0534
3. set (Dataset 19) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:41:20.744772!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0474
4. set (Dataset 22) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:41:36.200969!
Epoch 1/1
665/665 [==============================] - 12s 19ms/step - loss: 0.0344
5. set (Dataset 23) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:41:54.124634!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0593
6. set (Dataset 13) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:42:09.202388!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0372
7. set (Dataset 15) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:42:24.432128!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0441
8. set (Dataset 24) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:42:41.300453!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0354
9. set (Dataset 8) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:42:58.025298!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0432
10. set (Dataset 7) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:43:19.441265!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0399
11. set (Dataset 4) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:43:40.242923!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0413
12. set (Dataset 11) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:43:59.488915!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0337
13. set (Dataset 21) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:44:15.752797!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0626
14. set (Dataset 17) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:44:31.101272!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0479
15. set (Dataset 6) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:44:43.427677!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0515
16. set (Dataset 20) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:44:58.590483!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0371
17. set (Dataset 10) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:45:15.676893!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0425
18. set (Dataset 1) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:45:33.457643!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0563
19. set (Dataset 12) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:45:49.602383!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0364
20. set (Dataset 2) being trained for epoch 2 in Experiment 6 by 2019-01-28 04:46:07.956062!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0396
Epoch 2 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 04:46:21.490821
1. set (Dataset 1) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:46:26.533651!
Epoch 1/1
498/498 [==============================] - 9s 17ms/step - loss: 0.0507
2. set (Dataset 2) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:46:40.336594!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0360
3. set (Dataset 4) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:46:57.127772!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0393
4. set (Dataset 6) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:47:15.848133!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0508
5. set (Dataset 13) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:47:30.363892!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0397
6. set (Dataset 12) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:47:46.475718!
Epoch 1/1
732/732 [==============================] - 13s 17ms/step - loss: 0.0339
7. set (Dataset 10) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:48:06.445895!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0391
8. set (Dataset 11) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:48:25.333271!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0335
9. set (Dataset 23) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:48:41.127208!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0600
10. set (Dataset 8) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:48:59.155212!
Epoch 1/1
772/772 [==============================] - 14s 19ms/step - loss: 0.0442
11. set (Dataset 15) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:49:19.896867!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0456
12. set (Dataset 16) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:49:40.891176!
Epoch 1/1
914/914 [==============================] - 17s 19ms/step - loss: 0.0372
13. set (Dataset 17) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:50:01.690551!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0454
14. set (Dataset 18) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:50:14.739042!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0549
15. set (Dataset 19) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:50:30.936896!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0459
16. set (Dataset 20) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:50:45.238530!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0372
17. set (Dataset 21) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:51:01.220083!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0619
18. set (Dataset 22) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:51:19.384408!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0343
19. set (Dataset 7) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:51:38.889714!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0410
20. set (Dataset 24) being trained for epoch 3 in Experiment 6 by 2019-01-28 04:51:56.904022!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0362
Epoch 3 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 04:52:10.260956
1. set (Dataset 22) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:52:16.648930!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0334
2. set (Dataset 24) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:52:33.279774!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0351
3. set (Dataset 15) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:52:48.237939!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0449
4. set (Dataset 19) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:53:05.140512!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0453
5. set (Dataset 12) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:53:21.428503!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0370
6. set (Dataset 7) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:53:42.319612!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0391
7. set (Dataset 21) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:54:01.968850!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0601
8. set (Dataset 16) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:54:22.110785!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0379
9. set (Dataset 13) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:54:43.257852!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0355
10. set (Dataset 23) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:54:57.382701!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0579
11. set (Dataset 10) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:55:14.797676!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0417
12. set (Dataset 1) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:55:33.217438!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0543
13. set (Dataset 18) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:55:48.155410!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0555
14. set (Dataset 2) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:56:04.069061!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0381
15. set (Dataset 4) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:56:20.667356!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0405
16. set (Dataset 20) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:56:39.379148!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0389
17. set (Dataset 17) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:56:53.508278!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.0443
18. set (Dataset 6) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:57:06.163070!
Epoch 1/1
542/542 [==============================] - 10s 19ms/step - loss: 0.0522
19. set (Dataset 8) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:57:24.098760!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0449
20. set (Dataset 11) being trained for epoch 4 in Experiment 6 by 2019-01-28 04:57:43.959535!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0341
Epoch 4 for Experiment 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 04:57:58.578270
1. set (Dataset 6) being trained for epoch 5 in Experiment 6 by 2019-01-28 04:58:03.760283!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0491
2. set (Dataset 11) being trained for epoch 5 in Experiment 6 by 2019-01-28 04:58:19.229708!
Epoch 1/1
572/572 [==============================] - 11s 18ms/step - loss: 0.0336
3. set (Dataset 10) being trained for epoch 5 in Experiment 6 by 2019-01-28 04:58:37.025425!
Epoch 1/1
726/726 [==============================] - 14s 19ms/step - loss: 0.0399
4. set (Dataset 4) being trained for epoch 5 in Experiment 6 by 2019-01-28 04:58:57.963703!
Epoch 1/1
744/744 [==============================] - 14s 19ms/step - loss: 0.0415
5. set (Dataset 7) being trained for epoch 5 in Experiment 6 by 2019-01-28 04:59:19.534087!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0420
6. set (Dataset 8) being trained for epoch 5 in Experiment 6 by 2019-01-28 04:59:40.601578!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0425
7. set (Dataset 17) being trained for epoch 5 in Experiment 6 by 2019-01-28 04:59:58.506281!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.0479
8. set (Dataset 1) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:00:10.929844!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0541
9. set (Dataset 12) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:00:27.298331!
Epoch 1/1
732/732 [==============================] - 13s 17ms/step - loss: 0.0373
10. set (Dataset 13) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:00:44.881628!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0363
11. set (Dataset 21) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:00:59.880387!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0634
12. set (Dataset 22) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:01:17.991671!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0341
13. set (Dataset 2) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:01:35.389479!
Epoch 1/1
511/511 [==============================] - 10s 19ms/step - loss: 0.0383
14. set (Dataset 24) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:01:49.672423!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0375
15. set (Dataset 15) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:02:04.958538!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0459
16. set (Dataset 20) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:02:22.153507!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0367
17. set (Dataset 18) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:02:38.158538!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0551
18. set (Dataset 19) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:02:54.226186!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0448
19. set (Dataset 23) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:03:08.853348!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0601
20. set (Dataset 16) being trained for epoch 5 in Experiment 6 by 2019-01-28 05:03:28.065912!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0375
Epoch 5 for Experiment 6 completed!
Exp2019-01-28_02-03-57_part6.h5 has been saved.
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (7, 'M01'), (8, 'M02'), (17, 'M10'), (1, 'F
01'), (12, 'M06'), (13, 'M07'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'), (20, 'M12'), (18, 'F05')
, (19, 'M11'), (23, 'M13'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019
-01-28_02-03-57
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 05:03:46.815398
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 12.54 Degree
        The absolute mean error on Yaw angle estimation: 26.16 Degree
        The absolute mean error on Roll angle estimation: 14.24 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.21 Degree
        The absolute mean error on Yaw angle estimation: 26.73 Degree
        The absolute mean error on Roll angle estimation: 3.68 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 35.41 Degree
        The absolute mean error on Yaw angle estimation: 24.86 Degree
        The absolute mean error on Roll angle estimation: 9.14 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 18.62 Degree
        The absolute mean error on Yaw angle estimation: 30.33 Degree
        The absolute mean error on Roll angle estimation: 12.87 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.44 Degree
        The absolute mean error on Yaw angle estimations: 27.02 Degree
        The absolute mean error on Roll angle estimations: 9.98 Degree
Exp2019-01-28_02-03-57_part6 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019-0
1-28_02-03-57
All frames and annotations from 20 datasets have been read by 2019-01-28 05:04:56.901456
1. set (Dataset 19) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:05:01.776769!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0444
2. set (Dataset 16) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:05:19.645051!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0357
3. set (Dataset 21) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:05:42.221169!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0583
4. set (Dataset 15) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:05:59.979036!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0446
5. set (Dataset 8) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:06:19.769308!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0440
6. set (Dataset 23) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:06:39.129112!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0590
7. set (Dataset 18) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:06:55.423103!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0525
8. set (Dataset 22) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:07:13.077117!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0357
9. set (Dataset 7) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:07:32.839127!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0489
10. set (Dataset 12) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:07:53.242463!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0440
11. set (Dataset 17) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:08:10.249926!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0451
12. set (Dataset 6) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:08:22.657654!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0512
13. set (Dataset 24) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:08:37.147598!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0371
14. set (Dataset 11) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:08:51.803232!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0374
15. set (Dataset 10) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:09:09.416035!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0410
16. set (Dataset 20) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:09:28.008483!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0381
17. set (Dataset 2) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:09:43.067598!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0390
18. set (Dataset 4) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:09:59.763625!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0401
19. set (Dataset 13) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:10:17.919203!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0387
20. set (Dataset 1) being trained for epoch 1 in Experiment 7 by 2019-01-28 05:10:31.971626!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0534
Epoch 1 for Experiment 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 05:10:45.609572
1. set (Dataset 4) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:10:52.987972!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0390
2. set (Dataset 1) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:11:11.492779!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0524
3. set (Dataset 17) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:11:24.703000!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0452
4. set (Dataset 10) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:11:38.839387!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0393
5. set (Dataset 23) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:11:57.589285!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0602
6. set (Dataset 13) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:12:12.731187!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0385
7. set (Dataset 2) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:12:26.454686!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0373
8. set (Dataset 6) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:12:40.973396!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0520
9. set (Dataset 8) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:12:58.604338!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0440
10. set (Dataset 7) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:13:20.356124!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0450
11. set (Dataset 18) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:13:39.832268!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0555
12. set (Dataset 19) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:13:55.777535!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0447
13. set (Dataset 11) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:14:10.511951!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0335
14. set (Dataset 16) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:14:29.616475!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0368
15. set (Dataset 21) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:14:52.006705!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0607
16. set (Dataset 20) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:15:09.076667!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0363
17. set (Dataset 24) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:15:23.909551!
Epoch 1/1
492/492 [==============================] - 9s 19ms/step - loss: 0.0361
18. set (Dataset 15) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:15:39.448878!
Epoch 1/1
654/654 [==============================] - 11s 17ms/step - loss: 0.0443
19. set (Dataset 12) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:15:58.128048!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0371
20. set (Dataset 22) being trained for epoch 2 in Experiment 7 by 2019-01-28 05:16:17.513710!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0348
Epoch 2 for Experiment 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 05:16:34.190787
1. set (Dataset 15) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:16:40.533202!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0424
2. set (Dataset 22) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:16:58.472756!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0343
3. set (Dataset 18) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:17:16.633254!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0542
4. set (Dataset 21) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:17:33.880411!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0589
5. set (Dataset 13) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:17:50.419252!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0370
6. set (Dataset 12) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:18:06.541646!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0395
7. set (Dataset 24) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:18:24.388760!
Epoch 1/1
492/492 [==============================] - 9s 19ms/step - loss: 0.0378
8. set (Dataset 19) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:18:38.468237!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0454
9. set (Dataset 23) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:18:53.069084!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0585
10. set (Dataset 8) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:19:11.288573!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0449
11. set (Dataset 2) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:19:30.552956!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0404
12. set (Dataset 4) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:19:47.306946!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0411
13. set (Dataset 16) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:20:09.547680!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0383
14. set (Dataset 1) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:20:31.266278!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0549
15. set (Dataset 17) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:20:44.101127!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0437
16. set (Dataset 20) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:20:56.697241!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0367
17. set (Dataset 11) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:21:12.608592!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0370
18. set (Dataset 10) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:21:30.098481!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0401
19. set (Dataset 7) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:21:50.950341!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0419
20. set (Dataset 6) being trained for epoch 3 in Experiment 7 by 2019-01-28 05:22:09.833891!
Epoch 1/1
542/542 [==============================] - 10s 19ms/step - loss: 0.0539
Epoch 3 for Experiment 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 05:22:24.285743
1. set (Dataset 10) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:22:31.496721!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0386
2. set (Dataset 6) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:22:49.773986!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0494
3. set (Dataset 2) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:23:04.685199!
Epoch 1/1
511/511 [==============================] - 10s 19ms/step - loss: 0.0370
4. set (Dataset 17) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:23:17.978276!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0446
5. set (Dataset 12) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:23:32.269129!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0364
6. set (Dataset 7) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:23:53.177375!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0390
7. set (Dataset 11) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:24:12.249876!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0337
8. set (Dataset 4) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:24:30.083812!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0417
9. set (Dataset 13) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:24:48.051099!
Epoch 1/1
485/485 [==============================] - 9s 19ms/step - loss: 0.0376
10. set (Dataset 23) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:25:02.615798!
Epoch 1/1
569/569 [==============================] - 10s 17ms/step - loss: 0.0595
11. set (Dataset 24) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:25:17.274376!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0367
12. set (Dataset 15) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:25:32.384485!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0444
13. set (Dataset 1) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:25:49.134923!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0566
14. set (Dataset 22) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:26:04.785769!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0339
15. set (Dataset 18) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:26:22.947302!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0530
16. set (Dataset 20) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:26:39.501380!
Epoch 1/1
556/556 [==============================] - 10s 17ms/step - loss: 0.0378
17. set (Dataset 16) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:26:58.006222!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0379
18. set (Dataset 21) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:27:20.221548!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0609
19. set (Dataset 8) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:27:39.583631!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0452
20. set (Dataset 19) being trained for epoch 4 in Experiment 7 by 2019-01-28 05:27:58.436895!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0461
Epoch 4 for Experiment 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 05:28:11.974998
1. set (Dataset 21) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:28:17.985017!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0578
2. set (Dataset 19) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:28:34.508090!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0448
3. set (Dataset 24) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:28:48.339271!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0360
4. set (Dataset 18) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:29:02.821127!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0538
5. set (Dataset 7) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:29:21.485443!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0412
6. set (Dataset 8) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:29:42.370249!
Epoch 1/1
772/772 [==============================] - 14s 19ms/step - loss: 0.0429
7. set (Dataset 16) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:30:05.623344!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0364
8. set (Dataset 15) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:30:28.633619!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0441
9. set (Dataset 12) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:30:47.786379!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0357
10. set (Dataset 13) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:31:05.942852!
Epoch 1/1
485/485 [==============================] - 9s 19ms/step - loss: 0.0340
11. set (Dataset 11) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:31:20.865600!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0321
12. set (Dataset 10) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:31:38.305405!
Epoch 1/1
726/726 [==============================] - 13s 19ms/step - loss: 0.0402
13. set (Dataset 22) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:31:58.164224!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0354
14. set (Dataset 6) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:32:15.619621!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0509
15. set (Dataset 2) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:32:30.403322!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0373
16. set (Dataset 20) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:32:45.232858!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0397
17. set (Dataset 1) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:33:00.395025!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0561
18. set (Dataset 17) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:33:13.009904!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.0427
19. set (Dataset 23) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:33:25.845622!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0587
20. set (Dataset 4) being trained for epoch 5 in Experiment 7 by 2019-01-28 05:33:43.592755!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0406
Epoch 5 for Experiment 7 completed!
Exp2019-01-28_02-03-57_part7.h5 has been saved.
The subjects are trained: [(21, 'F02'), (19, 'M11'), (24, 'M14'), (18, 'F05'), (7, 'M01'), (8, 'M02'), (16, 'M09'), (15,
 'F03'), (12, 'M06'), (13, 'M07'), (11, 'M05'), (10, 'M04'), (22, 'M01'), (6, 'F06'), (2, 'F02'), (20, 'M12'), (1, 'F01'
), (17, 'M10'), (23, 'M13'), (4, 'F04')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019
-01-28_02-03-57
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 05:33:59.256343
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.64 Degree
        The absolute mean error on Yaw angle estimation: 26.62 Degree
        The absolute mean error on Roll angle estimation: 6.07 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 6.81 Degree
        The absolute mean error on Yaw angle estimation: 24.32 Degree
        The absolute mean error on Roll angle estimation: 4.54 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 35.22 Degree
        The absolute mean error on Yaw angle estimation: 29.93 Degree
        The absolute mean error on Roll angle estimation: 8.10 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.77 Degree
        The absolute mean error on Yaw angle estimation: 33.09 Degree
        The absolute mean error on Roll angle estimation: 14.16 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.86 Degree
        The absolute mean error on Yaw angle estimations: 28.49 Degree
        The absolute mean error on Roll angle estimations: 8.22 Degree
Exp2019-01-28_02-03-57_part7 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019-0
1-28_02-03-57
All frames and annotations from 20 datasets have been read by 2019-01-28 05:35:09.425221
1. set (Dataset 17) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:35:13.166660!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0415
2. set (Dataset 4) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:35:27.761030!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0373
3. set (Dataset 11) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:35:47.126964!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0358
4. set (Dataset 2) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:36:02.625387!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0388
5. set (Dataset 8) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:36:19.701176!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0440
6. set (Dataset 23) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:36:39.431330!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0563
7. set (Dataset 1) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:36:54.826544!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0540
8. set (Dataset 10) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:37:11.049537!
Epoch 1/1
726/726 [==============================] - 14s 19ms/step - loss: 0.0389
9. set (Dataset 7) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:37:32.229585!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0412
10. set (Dataset 12) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:37:53.160635!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0355
11. set (Dataset 16) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:38:15.165108!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0382
12. set (Dataset 21) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:38:37.717865!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0613
13. set (Dataset 6) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:38:54.409578!
Epoch 1/1
542/542 [==============================] - 10s 19ms/step - loss: 0.0500
14. set (Dataset 19) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:39:09.423353!
Epoch 1/1
502/502 [==============================] - 9s 19ms/step - loss: 0.0456
15. set (Dataset 24) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:39:23.548557!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0361
16. set (Dataset 20) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:39:37.739443!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0372
17. set (Dataset 22) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:39:54.224254!
Epoch 1/1
665/665 [==============================] - 12s 19ms/step - loss: 0.0334
18. set (Dataset 18) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:40:12.545384!
Epoch 1/1
614/614 [==============================] - 11s 17ms/step - loss: 0.0544
19. set (Dataset 13) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:40:28.089474!
Epoch 1/1
485/485 [==============================] - 9s 19ms/step - loss: 0.0358
20. set (Dataset 15) being trained for epoch 1 in Experiment 8 by 2019-01-28 05:40:43.596265!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0439
Epoch 1 for Experiment 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 05:41:00.215063
1. set (Dataset 18) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:41:06.088326!
Epoch 1/1
614/614 [==============================] - 11s 19ms/step - loss: 0.0517
2. set (Dataset 15) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:41:23.875790!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0407
3. set (Dataset 16) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:41:44.574123!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0361
4. set (Dataset 24) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:42:05.788634!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0347
5. set (Dataset 23) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:42:20.126239!
Epoch 1/1
569/569 [==============================] - 11s 18ms/step - loss: 0.0574
6. set (Dataset 13) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:42:35.512432!
Epoch 1/1
485/485 [==============================] - 9s 19ms/step - loss: 0.0350
7. set (Dataset 22) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:42:50.934250!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0339
8. set (Dataset 21) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:43:08.824430!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0596
9. set (Dataset 8) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:43:28.059217!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0432
10. set (Dataset 7) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:43:49.488802!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0405
11. set (Dataset 1) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:44:07.960307!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0541
12. set (Dataset 17) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:44:20.555117!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0456
13. set (Dataset 19) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:44:32.798755!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0453
14. set (Dataset 4) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:44:49.378342!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0417
15. set (Dataset 11) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:45:08.871072!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0340
16. set (Dataset 20) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:45:24.494050!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0386
17. set (Dataset 6) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:45:39.536866!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0509
18. set (Dataset 2) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:45:54.350919!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0378
19. set (Dataset 12) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:46:10.795561!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0364
20. set (Dataset 10) being trained for epoch 2 in Experiment 8 by 2019-01-28 05:46:31.362429!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0418
Epoch 2 for Experiment 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 05:46:48.525396
1. set (Dataset 2) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:46:53.599802!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0363
2. set (Dataset 10) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:47:10.024861!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0396
3. set (Dataset 1) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:47:28.437532!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0521
4. set (Dataset 11) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:47:43.011138!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0347
5. set (Dataset 13) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:47:58.021235!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0365
6. set (Dataset 12) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:48:14.070554!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0339
7. set (Dataset 6) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:48:32.434858!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0497
8. set (Dataset 17) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:48:46.118588!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0419
9. set (Dataset 23) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:48:58.903447!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0575
10. set (Dataset 8) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:49:16.977779!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0428
11. set (Dataset 22) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:49:37.324320!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0342
12. set (Dataset 18) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:49:55.351352!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0543
13. set (Dataset 4) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:50:13.853995!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0421
14. set (Dataset 15) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:50:33.426837!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0444
15. set (Dataset 16) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:50:54.140733!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0355
16. set (Dataset 20) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:51:16.008967!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0361
17. set (Dataset 19) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:51:30.933138!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0464
18. set (Dataset 24) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:51:44.668778!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0361
19. set (Dataset 7) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:52:01.027508!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0415
20. set (Dataset 21) being trained for epoch 3 in Experiment 8 by 2019-01-28 05:52:20.642845!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0581
Epoch 3 for Experiment 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 05:52:36.640502
1. set (Dataset 24) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:52:41.312396!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0346
2. set (Dataset 21) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:52:56.401206!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0563
3. set (Dataset 22) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:53:14.463348!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0337
4. set (Dataset 16) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:53:35.335240!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0372
5. set (Dataset 12) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:53:59.481580!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0354
6. set (Dataset 7) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:54:20.301687!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0399
7. set (Dataset 19) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:54:38.680711!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0462
8. set (Dataset 18) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:54:53.584211!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0525
9. set (Dataset 13) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:55:09.747990!
Epoch 1/1
485/485 [==============================] - 9s 19ms/step - loss: 0.0344
10. set (Dataset 23) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:55:24.382381!
Epoch 1/1
569/569 [==============================] - 11s 19ms/step - loss: 0.0560
11. set (Dataset 6) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:55:40.198684!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0486
12. set (Dataset 2) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:55:55.220209!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0370
13. set (Dataset 15) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:56:10.865740!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0437
14. set (Dataset 10) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:56:29.922689!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0421
15. set (Dataset 1) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:56:48.043862!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0537
16. set (Dataset 20) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:57:02.324684!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0366
17. set (Dataset 4) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:57:19.833564!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0400
18. set (Dataset 11) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:57:39.171202!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0360
19. set (Dataset 8) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:57:56.910227!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0442
20. set (Dataset 17) being trained for epoch 4 in Experiment 8 by 2019-01-28 05:58:14.606256!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0454
Epoch 4 for Experiment 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 05:58:26.159350
1. set (Dataset 11) being trained for epoch 5 in Experiment 8 by 2019-01-28 05:58:31.852839!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0317
2. set (Dataset 17) being trained for epoch 5 in Experiment 8 by 2019-01-28 05:58:45.585369!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0422
3. set (Dataset 6) being trained for epoch 5 in Experiment 8 by 2019-01-28 05:58:57.867093!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0490
4. set (Dataset 1) being trained for epoch 5 in Experiment 8 by 2019-01-28 05:59:12.788351!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0548
5. set (Dataset 7) being trained for epoch 5 in Experiment 8 by 2019-01-28 05:59:29.534383!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0423
6. set (Dataset 8) being trained for epoch 5 in Experiment 8 by 2019-01-28 05:59:50.744338!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0422
7. set (Dataset 4) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:00:12.036454!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0410
8. set (Dataset 2) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:00:30.520068!
Epoch 1/1
511/511 [==============================] - 10s 19ms/step - loss: 0.0366
9. set (Dataset 12) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:00:47.339541!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0340
10. set (Dataset 13) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:01:05.581745!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0364
11. set (Dataset 19) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:01:19.292769!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0456
12. set (Dataset 24) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:01:33.106789!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0361
13. set (Dataset 10) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:01:49.239194!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0401
14. set (Dataset 21) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:02:08.527909!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0587
15. set (Dataset 22) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:02:26.504092!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0346
16. set (Dataset 20) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:02:44.005784!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0371
17. set (Dataset 15) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:03:00.364602!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0433
18. set (Dataset 16) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:03:21.190816!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0383
19. set (Dataset 23) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:03:43.339891!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0559
20. set (Dataset 18) being trained for epoch 5 in Experiment 8 by 2019-01-28 06:03:59.598259!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0530
Epoch 5 for Experiment 8 completed!
Exp2019-01-28_02-03-57_part8.h5 has been saved.
The subjects are trained: [(11, 'M05'), (17, 'M10'), (6, 'F06'), (1, 'F01'), (7, 'M01'), (8, 'M02'), (4, 'F04'), (2, 'F0
2'), (12, 'M06'), (13, 'M07'), (19, 'M11'), (24, 'M14'), (10, 'M04'), (21, 'F02'), (22, 'M01'), (20, 'M12'), (15, 'F03')
, (16, 'M09'), (23, 'M13'), (18, 'F05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019
-01-28_02-03-57
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 06:04:12.724246
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.76 Degree
        The absolute mean error on Yaw angle estimation: 24.07 Degree
        The absolute mean error on Roll angle estimation: 8.87 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 9.40 Degree
        The absolute mean error on Yaw angle estimation: 27.00 Degree
        The absolute mean error on Roll angle estimation: 4.84 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 34.93 Degree
        The absolute mean error on Yaw angle estimation: 32.04 Degree
        The absolute mean error on Roll angle estimation: 7.91 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 18.99 Degree
        The absolute mean error on Yaw angle estimation: 28.32 Degree
        The absolute mean error on Roll angle estimation: 13.14 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.52 Degree
        The absolute mean error on Yaw angle estimations: 27.86 Degree
        The absolute mean error on Roll angle estimations: 8.69 Degree
Exp2019-01-28_02-03-57_part8 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019-0
1-28_02-03-57
All frames and annotations from 20 datasets have been read by 2019-01-28 06:05:22.658107
1. set (Dataset 16) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:05:31.392351!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0349
2. set (Dataset 18) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:05:53.816687!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0497
3. set (Dataset 19) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:06:09.948553!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0479
4. set (Dataset 22) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:06:25.251512!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0337
5. set (Dataset 8) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:06:44.931127!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0422
6. set (Dataset 23) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:07:04.038169!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0555
7. set (Dataset 15) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:07:20.873252!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0439
8. set (Dataset 24) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:07:37.454886!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0348
9. set (Dataset 7) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:07:53.950143!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0406
10. set (Dataset 12) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:08:14.792281!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0345
11. set (Dataset 4) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:08:35.511183!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0415
12. set (Dataset 11) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:08:54.526552!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0324
13. set (Dataset 21) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:09:11.003174!
Epoch 1/1
634/634 [==============================] - 12s 19ms/step - loss: 0.0589
14. set (Dataset 17) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:09:26.621834!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0449
15. set (Dataset 6) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:09:39.039275!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0485
16. set (Dataset 20) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:09:54.317341!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0379
17. set (Dataset 10) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:10:11.806486!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0415
18. set (Dataset 1) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:10:30.171298!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0541
19. set (Dataset 13) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:10:43.954880!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0362
20. set (Dataset 2) being trained for epoch 1 in Experiment 9 by 2019-01-28 06:10:57.855874!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0372
Epoch 1 for Experiment 9 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 06:11:11.423540
1. set (Dataset 1) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:11:16.457592!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0509
2. set (Dataset 2) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:11:30.450407!
Epoch 1/1
511/511 [==============================] - 9s 19ms/step - loss: 0.0355
3. set (Dataset 4) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:11:47.340906!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0393
4. set (Dataset 6) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:12:05.993442!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0491
5. set (Dataset 23) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:12:21.333965!
Epoch 1/1
569/569 [==============================] - 11s 19ms/step - loss: 0.0561
6. set (Dataset 13) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:12:37.051680!
Epoch 1/1
485/485 [==============================] - 7s 15ms/step - loss: 0.0379
7. set (Dataset 10) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:12:51.662096!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0391
8. set (Dataset 11) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:13:10.587050!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0326
9. set (Dataset 8) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:13:28.712042!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0421
10. set (Dataset 7) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:13:50.417575!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0404
11. set (Dataset 15) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:14:10.552064!
Epoch 1/1
654/654 [==============================] - 11s 18ms/step - loss: 0.0432
12. set (Dataset 16) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:14:30.851521!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0376
13. set (Dataset 17) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:14:51.013873!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0452
14. set (Dataset 18) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:15:04.199824!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0514
15. set (Dataset 19) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:15:20.350494!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0449
16. set (Dataset 20) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:15:34.921447!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0373
17. set (Dataset 21) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:15:51.394237!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0603
18. set (Dataset 22) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:16:09.063835!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0333
19. set (Dataset 12) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:16:28.455655!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0363
20. set (Dataset 24) being trained for epoch 2 in Experiment 9 by 2019-01-28 06:16:46.345558!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0354
Epoch 2 for Experiment 9 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 06:16:59.722192
1. set (Dataset 22) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:17:06.110400!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0321
2. set (Dataset 24) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:17:22.597582!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0333
3. set (Dataset 15) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:17:37.808227!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0439
4. set (Dataset 19) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:17:54.491366!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0433
5. set (Dataset 13) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:18:08.434478!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0359
6. set (Dataset 12) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:18:24.537489!
Epoch 1/1
732/732 [==============================] - 14s 19ms/step - loss: 0.0337
7. set (Dataset 21) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:18:44.164947!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0571
8. set (Dataset 16) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:19:04.308152!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0363
9. set (Dataset 23) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:19:26.325816!
Epoch 1/1
569/569 [==============================] - 10s 17ms/step - loss: 0.0553
10. set (Dataset 8) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:19:44.107978!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.0417
11. set (Dataset 10) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:20:04.896100!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0414
12. set (Dataset 1) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:20:23.167961!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0537
13. set (Dataset 18) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:20:38.353899!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0512
14. set (Dataset 2) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:20:54.381492!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0378
15. set (Dataset 4) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:21:11.112521!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0417
16. set (Dataset 20) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:21:29.988040!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0372
17. set (Dataset 17) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:21:44.093155!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0448
18. set (Dataset 6) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:21:56.278700!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0499
19. set (Dataset 7) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:22:13.463798!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0436
20. set (Dataset 11) being trained for epoch 3 in Experiment 9 by 2019-01-28 06:22:32.938921!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0326
Epoch 3 for Experiment 9 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 06:22:47.657262
1. set (Dataset 6) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:22:52.848197!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0482
2. set (Dataset 11) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:23:08.453254!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0314
3. set (Dataset 10) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:23:25.959800!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0395
4. set (Dataset 4) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:23:46.556447!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0391
5. set (Dataset 12) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:24:07.171969!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0365
6. set (Dataset 7) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:24:27.937033!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0411
7. set (Dataset 17) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:24:44.969661!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0432
8. set (Dataset 1) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:24:57.290196!
Epoch 1/1
498/498 [==============================] - 9s 17ms/step - loss: 0.0529
9. set (Dataset 13) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:25:10.729108!
Epoch 1/1
485/485 [==============================] - 8s 17ms/step - loss: 0.0362
10. set (Dataset 23) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:25:24.676855!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0560
11. set (Dataset 21) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:25:40.723224!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0575
12. set (Dataset 22) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:25:58.735690!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0365
13. set (Dataset 2) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:26:15.814584!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0376
14. set (Dataset 24) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:26:29.771504!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0356
15. set (Dataset 15) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:26:45.080649!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0430
16. set (Dataset 20) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:27:02.184868!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0368
17. set (Dataset 18) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:27:18.158314!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0519
18. set (Dataset 19) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:27:34.157357!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0453
19. set (Dataset 8) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:27:50.945146!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0440
20. set (Dataset 16) being trained for epoch 4 in Experiment 9 by 2019-01-28 06:28:13.605731!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0379
Epoch 4 for Experiment 9 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 06:28:34.734441
1. set (Dataset 19) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:28:39.608922!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0441
2. set (Dataset 16) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:28:57.387035!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0343
3. set (Dataset 21) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:29:20.098225!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0572
4. set (Dataset 15) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:29:37.966818!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0425
5. set (Dataset 7) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:29:57.218894!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0416
6. set (Dataset 8) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:30:18.112837!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0409
7. set (Dataset 18) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:30:37.585173!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0505
8. set (Dataset 22) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:30:55.375297!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0358
9. set (Dataset 12) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:31:14.623273!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0347
10. set (Dataset 13) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:31:32.772627!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0338
11. set (Dataset 17) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:31:45.425967!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0459
12. set (Dataset 6) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:31:57.751284!
Epoch 1/1
542/542 [==============================] - 9s 18ms/step - loss: 0.0489
13. set (Dataset 24) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:32:12.004218!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0355
14. set (Dataset 11) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:32:26.550223!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0340
15. set (Dataset 10) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:32:44.207136!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0409
16. set (Dataset 20) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:33:02.650694!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0374
17. set (Dataset 2) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:33:17.708069!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0377
18. set (Dataset 4) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:33:34.515333!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0407
19. set (Dataset 23) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:33:53.062821!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0570
20. set (Dataset 1) being trained for epoch 5 in Experiment 9 by 2019-01-28 06:34:08.508384!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0546
Epoch 5 for Experiment 9 completed!
Exp2019-01-28_02-03-57_part9.h5 has been saved.
The subjects are trained: [(19, 'M11'), (16, 'M09'), (21, 'F02'), (15, 'F03'), (7, 'M01'), (8, 'M02'), (18, 'F05'), (22,
 'M01'), (12, 'M06'), (13, 'M07'), (17, 'M10'), (6, 'F06'), (24, 'M14'), (11, 'M05'), (10, 'M04'), (20, 'M12'), (2, 'F02
'), (4, 'F04'), (23, 'M13'), (1, 'F01')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019
-01-28_02-03-57
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 06:34:19.762009
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 9.11 Degree
        The absolute mean error on Yaw angle estimation: 24.93 Degree
        The absolute mean error on Roll angle estimation: 9.06 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 6.92 Degree
        The absolute mean error on Yaw angle estimation: 24.23 Degree
        The absolute mean error on Roll angle estimation: 3.52 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 37.66 Degree
        The absolute mean error on Yaw angle estimation: 27.78 Degree
        The absolute mean error on Roll angle estimation: 9.19 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 19.06 Degree
        The absolute mean error on Yaw angle estimation: 32.10 Degree
        The absolute mean error on Roll angle estimation: 12.70 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.19 Degree
        The absolute mean error on Yaw angle estimations: 27.26 Degree
        The absolute mean error on Roll angle estimations: 8.62 Degree
Exp2019-01-28_02-03-57_part9 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019-0
1-28_02-03-57
All frames and annotations from 20 datasets have been read by 2019-01-28 06:35:30.072562
1. set (Dataset 4) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:35:37.448629!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0387
2. set (Dataset 1) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:35:55.645265!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0504
3. set (Dataset 17) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:36:08.503230!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0420
4. set (Dataset 10) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:36:22.834542!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0390
5. set (Dataset 8) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:36:43.517165!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0434
6. set (Dataset 23) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:37:03.166150!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0544
7. set (Dataset 2) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:37:18.568475!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0374
8. set (Dataset 6) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:37:33.042785!
Epoch 1/1
542/542 [==============================] - 9s 17ms/step - loss: 0.0495
9. set (Dataset 7) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:37:50.063733!
Epoch 1/1
745/745 [==============================] - 11s 15ms/step - loss: 0.0419
10. set (Dataset 12) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:38:08.758403!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0352
11. set (Dataset 18) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:38:27.878063!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0524
12. set (Dataset 19) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:38:43.916037!
Epoch 1/1
502/502 [==============================] - 9s 19ms/step - loss: 0.0454
13. set (Dataset 11) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:38:59.158079!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0330
14. set (Dataset 16) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:39:18.248537!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0362
15. set (Dataset 21) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:39:40.819387!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0588
16. set (Dataset 20) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:39:57.741068!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0356
17. set (Dataset 24) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:40:12.461749!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0348
18. set (Dataset 15) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:40:27.534850!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0427
19. set (Dataset 13) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:40:44.747277!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0360
20. set (Dataset 22) being trained for epoch 1 in Experiment 10 by 2019-01-28 06:40:59.963630!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0341
Epoch 1 for Experiment 10 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 06:41:16.519808
1. set (Dataset 15) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:41:22.859937!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0410
2. set (Dataset 22) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:41:40.986900!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0331
3. set (Dataset 18) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:41:59.054038!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0524
4. set (Dataset 21) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:42:16.129888!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0571
5. set (Dataset 23) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:42:33.261856!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0551
6. set (Dataset 13) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:42:48.247025!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0349
7. set (Dataset 24) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:43:01.796332!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0354
8. set (Dataset 19) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:43:15.558009!
Epoch 1/1
502/502 [==============================] - 9s 19ms/step - loss: 0.0437
9. set (Dataset 8) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:43:32.617449!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0415
10. set (Dataset 7) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:43:54.178606!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0391
11. set (Dataset 2) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:44:12.243022!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0380
12. set (Dataset 4) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:44:28.889531!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0409
13. set (Dataset 16) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:44:50.927406!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0377
14. set (Dataset 1) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:45:12.411545!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0527
15. set (Dataset 17) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:45:25.019880!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0434
16. set (Dataset 20) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:45:37.501755!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0368
17. set (Dataset 11) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:45:53.444788!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0333
18. set (Dataset 10) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:46:11.130526!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0396
19. set (Dataset 12) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:46:31.686034!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0335
20. set (Dataset 6) being trained for epoch 2 in Experiment 10 by 2019-01-28 06:46:50.239741!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0499
Epoch 2 for Experiment 10 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 06:47:04.605559
1. set (Dataset 10) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:47:11.810363!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0390
2. set (Dataset 6) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:47:30.098889!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0464
3. set (Dataset 2) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:47:45.046232!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0373
4. set (Dataset 17) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:47:58.270256!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0448
5. set (Dataset 13) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:48:10.092965!
Epoch 1/1
485/485 [==============================] - 8s 17ms/step - loss: 0.0356
6. set (Dataset 12) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:48:25.853698!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0326
7. set (Dataset 11) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:48:44.915612!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0328
8. set (Dataset 4) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:49:02.449494!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0391
9. set (Dataset 23) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:49:21.171520!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0595
10. set (Dataset 8) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:49:38.972895!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0436
11. set (Dataset 24) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:49:57.575242!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0380
12. set (Dataset 15) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:50:12.701693!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0429
13. set (Dataset 1) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:50:29.636930!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0554
14. set (Dataset 22) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:50:44.973626!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0349
15. set (Dataset 18) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:51:02.971684!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0519
16. set (Dataset 20) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:51:19.505346!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0360
17. set (Dataset 16) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:51:38.461257!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0349
18. set (Dataset 21) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:52:01.255941!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0594
19. set (Dataset 7) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:52:20.345890!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0423
20. set (Dataset 19) being trained for epoch 3 in Experiment 10 by 2019-01-28 06:52:38.992365!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0458
Epoch 3 for Experiment 10 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 06:52:52.580162
1. set (Dataset 21) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:52:58.605011!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0569
2. set (Dataset 19) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:53:15.137472!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0426
3. set (Dataset 24) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:53:28.865303!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0365
4. set (Dataset 18) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:53:43.735949!
Epoch 1/1
614/614 [==============================] - 11s 19ms/step - loss: 0.0508
5. set (Dataset 12) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:54:02.425793!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0361
6. set (Dataset 7) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:54:23.233361!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0400
7. set (Dataset 16) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:54:45.010915!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0357
8. set (Dataset 15) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:55:07.690144!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0422
9. set (Dataset 13) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:55:24.606818!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0365
10. set (Dataset 23) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:55:38.927934!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0541
11. set (Dataset 11) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:55:54.936758!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0319
12. set (Dataset 10) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:56:12.476279!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0402
13. set (Dataset 22) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:56:32.216042!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0341
14. set (Dataset 6) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:56:49.505819!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0504
15. set (Dataset 2) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:57:04.422911!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0354
16. set (Dataset 20) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:57:19.042889!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0373
17. set (Dataset 1) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:57:34.260957!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0565
18. set (Dataset 17) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:57:46.992940!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0427
19. set (Dataset 8) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:58:01.940434!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0419
20. set (Dataset 4) being trained for epoch 4 in Experiment 10 by 2019-01-28 06:58:23.225689!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0392
Epoch 4 for Experiment 10 completed!
All frames and annotations from 20 datasets have been read by 2019-01-28 06:58:40.871927
1. set (Dataset 17) being trained for epoch 5 in Experiment 10 by 2019-01-28 06:58:44.611961!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0447
2. set (Dataset 4) being trained for epoch 5 in Experiment 10 by 2019-01-28 06:58:59.295228!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0359
3. set (Dataset 11) being trained for epoch 5 in Experiment 10 by 2019-01-28 06:59:18.389951!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0346
4. set (Dataset 2) being trained for epoch 5 in Experiment 10 by 2019-01-28 06:59:33.892462!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0358
5. set (Dataset 7) being trained for epoch 5 in Experiment 10 by 2019-01-28 06:59:50.658388!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0410
6. set (Dataset 8) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:00:12.176256!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0406
7. set (Dataset 1) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:00:30.972533!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0545
8. set (Dataset 10) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:00:47.368096!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0402
9. set (Dataset 12) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:01:08.069199!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0360
10. set (Dataset 13) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:01:26.217223!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0342
11. set (Dataset 16) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:01:43.744512!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0369
12. set (Dataset 21) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:02:06.137942!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0603
13. set (Dataset 6) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:02:22.989815!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0511
14. set (Dataset 19) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:02:37.809714!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0454
15. set (Dataset 24) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:02:51.711005!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0350
16. set (Dataset 20) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:03:06.218002!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0357
17. set (Dataset 22) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:03:23.057548!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0329
18. set (Dataset 18) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:03:41.165916!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0512
19. set (Dataset 23) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:03:57.758375!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0546
20. set (Dataset 15) being trained for epoch 5 in Experiment 10 by 2019-01-28 07:04:14.468966!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0420
Epoch 5 for Experiment 10 completed!
Exp2019-01-28_02-03-57_part10.h5 has been saved.
The subjects are trained: [(17, 'M10'), (4, 'F04'), (11, 'M05'), (2, 'F02'), (7, 'M01'), (8, 'M02'), (1, 'F01'), (10, 'M
04'), (12, 'M06'), (13, 'M07'), (16, 'M09'), (21, 'F02'), (6, 'F06'), (19, 'M11'), (24, 'M14'), (20, 'M12'), (22, 'M01')
, (18, 'F05'), (23, 'M13'), (15, 'F03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs50_AdamOpt_lr-0.000100_2019
-01-28_02-03-57
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-28 07:04:28.775608
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.82 Degree
        The absolute mean error on Yaw angle estimation: 22.81 Degree
        The absolute mean error on Roll angle estimation: 10.69 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.98 Degree
        The absolute mean error on Yaw angle estimation: 26.23 Degree
        The absolute mean error on Roll angle estimation: 3.53 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 37.14 Degree
        The absolute mean error on Yaw angle estimation: 27.28 Degree
        The absolute mean error on Roll angle estimation: 8.00 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 19.12 Degree
        The absolute mean error on Yaw angle estimation: 30.75 Degree
        The absolute mean error on Roll angle estimation: 12.47 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.02 Degree
        The absolute mean error on Yaw angle estimations: 26.77 Degree
        The absolute mean error on Roll angle estimations: 8.67 Degree
Exp2019-01-28_02-03-57_part10 completed!
Exp2019-01-28_02-03-57.h5 has been saved.
subject3_Exp2019-01-28_02-03-57.png has been saved by 2019-01-28 07:05:34.786461.
subject5_Exp2019-01-28_02-03-57.png has been saved by 2019-01-28 07:05:34.989697.
subject9_Exp2019-01-28_02-03-57.png has been saved by 2019-01-28 07:05:35.187602.
subject14_Exp2019-01-28_02-03-57.png has been saved by 2019-01-28 07:05:35.406948.
Model Exp2019-01-28_02-03-57 has been evaluated successfully.
Model Exp2019-01-28_02-03-57 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$
