2019-01-25 18:17:15.532336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 18:17:15.532594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 18:17:15.532612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 18:17:15.687658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 18:17:15.687684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 18:17:15.687689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 18:17:15.687822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_03-13-09.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 10)                164280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 33
=================================================================
Total params: 134,424,857
Trainable params: 164,313
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate = 0.0001
in_epochs = 1
out_epochs = 30
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04
'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'),
(20, 'M12'), (21, 'F02'), (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-25_03-13-09_and_2019-01-25_18-17-16
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 18:17:18.623197
For the Subject 3 (F03):
730/730 [==============================] - 58s 80ms/step
(714, 1) (730, 1)
Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 64, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 91,
in evaluateCNN_LSTM
    outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, ba
tch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 64,
in evaluateSubject
    matrix = numpy.concatenate((test_labels[timesteps:, i:i+1], predictions[:, i:i+1]), axis=1)
ValueError: all the input array dimensions except for the concatenation axis must match exactly
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_03-13-09
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 18:19:48.786476: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 18:19:48.882389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 18:19:48.882692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 18:19:48.882707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 18:19:49.038317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 18:19:49.038344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 18:19:49.038349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 18:19:49.038525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_03-13-09.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 10)                164280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 33
=================================================================
Total params: 134,424,857
Trainable params: 164,313
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate = 0.0001
in_epochs = 1
out_epochs = 30
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04
'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'),
(20, 'M12'), (21, 'F02'), (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 18:19:51.984968
For the Subject 3 (F03):
730/730 [==============================] - 59s 81ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 8.64 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 27.16 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 5.80 Degree
For the Subject 5 (F05):
946/946 [==============================] - 78s 82ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 5.74 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 19.72 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 5.28 Degree
For the Subject 9 (M03):
882/882 [==============================] - 73s 83ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 27.54 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 36.32 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 9.94 Degree
For the Subject 14 (M08):
797/797 [==============================] - 66s 83ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 18.87 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 42.55 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 34.30 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.20 Degree
        The absolute mean error on Yaw angle estimations: 31.44 Degree
        The absolute mean error on Roll angle estimations: 13.83 Degree
subject3_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png has been saved by 2019-01-25 18:25:01.643703.
subject5_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png has been saved by 2019-01-25 18:25:01.840532.
subject9_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png has been saved by 2019-01-25 18:25:02.040890.
subject14_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png has been saved by 2019-01-25 18:25:02.257325.
Model Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49 has been evaluated successfully.
Model Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_03-13-09
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 18:50:03.057010: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 18:50:03.152630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 18:50:03.152886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 18:50:03.152900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 18:50:03.308227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 18:50:03.308254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 18:50:03.308259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 18:50:03.308396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_03-13-09.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 10)                164280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 33
=================================================================
Total params: 134,424,857
Trainable params: 164,313
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate = 0.0001
in_epochs = 1
out_epochs = 30
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04
'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'),
(20, 'M12'), (21, 'F02'), (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-25_03-13-09_and_2019-01-25_18-50-04
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 18:50:06.283436
For the Subject 3 (F03):
Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 64, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 92,
in evaluateCNN_LSTM
    outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, ba
tch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 52,
in evaluateSubject
    model.reset_states()
NameError: name 'model' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_03-13-09
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 18:51:24.032334: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 18:51:24.129107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 18:51:24.129369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 18:51:24.129384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 18:51:24.285845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 18:51:24.285872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 18:51:24.285879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 18:51:24.286020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_03-13-09.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 4096)          134260544
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 10)                164280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 33
=================================================================
Total params: 134,424,857
Trainable params: 164,313
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 16 # TimeseriesGenerator Handles overlapping
learning_rate = 0.0001
in_epochs = 1
out_epochs = 30
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04
'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'),
(20, 'M12'), (21, 'F02'), (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 18:51:27.299800
For the Subject 3 (F03):
730/730 [==============================] - 58s 80ms/step
(730, 1) (730, 1)
        The absolute mean error on Pitch angle estimation: 8.64 Degree
(730, 1) (730, 1)
        The absolute mean error on Yaw angle estimation: 27.16 Degree
(730, 1) (730, 1)
        The absolute mean error on Roll angle estimation: 5.80 Degree
For the Subject 5 (F05):
946/946 [==============================] - 77s 82ms/step
(946, 1) (946, 1)
        The absolute mean error on Pitch angle estimation: 5.74 Degree
(946, 1) (946, 1)
        The absolute mean error on Yaw angle estimation: 19.72 Degree
(946, 1) (946, 1)
        The absolute mean error on Roll angle estimation: 5.28 Degree
For the Subject 9 (M03):
882/882 [==============================] - 72s 82ms/step
(882, 1) (882, 1)
        The absolute mean error on Pitch angle estimation: 27.54 Degree
(882, 1) (882, 1)
        The absolute mean error on Yaw angle estimation: 36.32 Degree
(882, 1) (882, 1)
        The absolute mean error on Roll angle estimation: 9.94 Degree
For the Subject 14 (M08):
797/797 [==============================] - 65s 82ms/step
(797, 1) (797, 1)
        The absolute mean error on Pitch angle estimation: 18.87 Degree
(797, 1) (797, 1)
        The absolute mean error on Yaw angle estimation: 42.55 Degree
(797, 1) (797, 1)
        The absolute mean error on Roll angle estimation: 34.30 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.20 Degree
        The absolute mean error on Yaw angle estimations: 31.44 Degree
        The absolute mean error on Roll angle estimations: 13.83 Degree
subject3_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png has been saved by 2019-01-25 18:56:34.782819.
subject5_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png has been saved by 2019-01-25 18:56:34.978974.
subject9_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png has been saved by 2019-01-25 18:56:35.178593.
subject14_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png has been saved by 2019-01-25 18:56:35.394346.
Model Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25 has been evaluated successfully.
Model Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit -m "stateless 30
 epochs"
[master 03447ea] stateless 30 epochs
 43 files changed, 27888 insertions(+), 18 deletions(-)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26_and_2019-01-25_01-00-50/output_
Exp2019-01-24_23-58-26_and_2019-01-25_01-00-50.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/output_
Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
14_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
3_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
5_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
9_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-12-13/output_Exp2019-01-25_02-12-13.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-12-13/subject9_Exp2019-01-25_02-12-13
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-13-30/output_Exp2019-01-25_02-13-30.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-13-30/subject1_Exp2019-01-25_02-13-30
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-17-13/output_Exp2019-01-25_02-17-13.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-17-13/subject1_Exp2019-01-25_02-17-13
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/output_Exp2019-01-25_02-20-23.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject14_Exp2019-01-25_02-20-2
3.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject3_Exp2019-01-25_02-20-23
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject5_Exp2019-01-25_02-20-23
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject9_Exp2019-01-25_02-20-23
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/output_Exp2019-01-25_02-37-37.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject14_Exp2019-01-25_02-37-3
7.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject3_Exp2019-01-25_02-37-37
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject5_Exp2019-01-25_02-37-37
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject9_Exp2019-01-25_02-37-37
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/output_Exp2019-01-25_03-00-06.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject14_Exp2019-01-25_03-00-0
6.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject3_Exp2019-01-25_03-00-06
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject5_Exp2019-01-25_03-00-06
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject9_Exp2019-01-25_03-00-06
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/output_
Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
14_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
3_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
5_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
9_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/output_
Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/scrollb
ack_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
14_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
3_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
5_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
9_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 59, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (58/58), done.
Writing objects: 100% (59/59), 3.38 MiB | 945.00 KiB/s, done.
Total 59 (delta 17), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   c8143d6..03447ea  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 19:22:55.671462: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 19:22:55.769163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 19:22:55.769423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 19:22:55.769435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 19:22:55.924424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 19:22:55.924451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 19:22:55.924456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 19:22:55.924596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_19-22-56 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 320)                  5653760
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    963
=================================================================
Total params: 139,915,267
Trainable params: 5,654,723
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 320
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.0
00010_2019-01-25_19-22-56
All frames and annotations from 20 datasets have been read by 2019-01-25 19:23:00.967199
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 19:23:07.377758!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0298 - mean_absolute_error: 0.1315
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 19:23:27.335422!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.0203 - mean_absolute_error: 0.1100
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 19:23:43.862988!
Epoch 1/1
654/654 [==============================] - 14s 21ms/step - loss: 0.0242 - mean_absolute_error: 0.1121
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 19:24:02.392879!
Epoch 1/1
502/502 [==============================] - 10s 21ms/step - loss: 0.0241 - mean_absolute_error: 0.1135
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 19:24:20.547182!
Epoch 1/1
772/772 [==============================] - 16s 21ms/step - loss: 0.0269 - mean_absolute_error: 0.1068
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 19:24:42.020915!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.0343 - mean_absolute_error: 0.1387
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 19:24:59.921230!
Epoch 1/1
634/634 [==============================] - 13s 20ms/step - loss: 0.0282 - mean_absolute_error: 0.1261
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 19:25:21.750818!
Epoch 1/1
914/914 [==============================] - 19s 21ms/step - loss: 0.0146 - mean_absolute_error: 0.0874
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 19:25:48.493508!
Epoch 1/1
745/745 [==============================] - 15s 21ms/step - loss: 0.0138 - mean_absolute_error: 0.0817
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 19:26:11.240707!
Epoch 1/1
732/732 [==============================] - 15s 21ms/step - loss: 0.0129 - mean_absolute_error: 0.0776
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 19:26:33.642862!
Epoch 1/1
726/726 [==============================] - 15s 21ms/step - loss: 0.0190 - mean_absolute_error: 0.0900
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 19:26:53.730193!
Epoch 1/1
498/498 [==============================] - 10s 21ms/step - loss: 0.0220 - mean_absolute_error: 0.1016
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 19:27:09.951435!
Epoch 1/1
614/614 [==============================] - 13s 21ms/step - loss: 0.0219 - mean_absolute_error: 0.1093
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 19:27:27.956547!
Epoch 1/1
511/511 [==============================] - 11s 21ms/step - loss: 0.0200 - mean_absolute_error: 0.0967
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 19:27:46.016054!
Epoch 1/1
744/744 [==============================] - 15s 21ms/step - loss: 0.0163 - mean_absolute_error: 0.0888
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 19:28:06.923049!
Epoch 1/1
556/556 [==============================] - 11s 21ms/step - loss: 0.0232 - mean_absolute_error: 0.1052
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 19:28:22.119816!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0113 - mean_absolute_error: 0.0784
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 19:28:35.524195!
Epoch 1/1
542/542 [==============================] - 11s 21ms/step - loss: 0.0388 - mean_absolute_error: 0.1397
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 19:28:51.830547!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0102 - mean_absolute_error: 0.0733
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 19:29:07.692549!
Epoch 1/1
572/572 [==============================] - 12s 21ms/step - loss: 0.0122 - mean_absolute_error: 0.0713
Epoch 1 completed!
Exp2019-01-25_19-22-56.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0
.000010_2019-01-25_19-22-56
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 19:29:21.851676
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 12.84 Degree
        The absolute mean error on Yaw angle estimation: 103.06 Degree
        The absolute mean error on Roll angle estimation: 16.31 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 10.49 Degree
        The absolute mean error on Yaw angle estimation: 31.10 Degree
        The absolute mean error on Roll angle estimation: 5.07 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 12.84 Degree
        The absolute mean error on Yaw angle estimation: 29.57 Degree
        The absolute mean error on Roll angle estimation: 35.91 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 17.41 Degree
        The absolute mean error on Yaw angle estimation: 54.23 Degree
        The absolute mean error on Roll angle estimation: 27.75 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 13.39 Degree
        The absolute mean error on Yaw angle estimations: 54.49 Degree
        The absolute mean error on Roll angle estimations: 21.26 Degree
subject3_Exp2019-01-25_19-22-56.png has been saved by 2019-01-25 19:30:28.871980.
subject5_Exp2019-01-25_19-22-56.png has been saved by 2019-01-25 19:30:29.076140.
subject9_Exp2019-01-25_19-22-56.png has been saved by 2019-01-25 19:30:29.277677.
subject14_Exp2019-01-25_19-22-56.png has been saved by 2019-01-25 19:30:29.499349.
Model Exp2019-01-25_19-22-56 has been evaluated successfully.
Model Exp2019-01-25_19-22-56 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 19:31:27.238735: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 19:31:27.335759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 19:31:27.336064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 19:31:27.336078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 19:31:27.492029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 19:31:27.492054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 19:31:27.492059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 19:31:27.492236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_19-31-28 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 320)                  5653760
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    963
=================================================================
Total params: 139,915,267
Trainable params: 5,654,723
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 320
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.0
00100_2019-01-25_19-31-28
All frames and annotations from 20 datasets have been read by 2019-01-25 19:31:32.605126
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 19:31:39.011387!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0293 - mean_absolute_error: 0.1096
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 19:31:58.887011!
Epoch 1/1
492/492 [==============================] - 10s 21ms/step - loss: 0.0114 - mean_absolute_error: 0.0746
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 19:32:15.623966!
Epoch 1/1
654/654 [==============================] - 14s 21ms/step - loss: 0.0140 - mean_absolute_error: 0.0862
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 19:32:34.229226!
Epoch 1/1
502/502 [==============================] - 10s 21ms/step - loss: 0.0159 - mean_absolute_error: 0.0911
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 19:32:52.511540!
Epoch 1/1
772/772 [==============================] - 16s 21ms/step - loss: 0.0129 - mean_absolute_error: 0.0718
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 19:33:13.995964!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.0228 - mean_absolute_error: 0.1116
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 19:33:31.807473!
Epoch 1/1
634/634 [==============================] - 13s 21ms/step - loss: 0.0169 - mean_absolute_error: 0.0977
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 19:33:53.883514!
Epoch 1/1
914/914 [==============================] - 19s 21ms/step - loss: 0.0092 - mean_absolute_error: 0.0676
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 19:34:20.524197!
Epoch 1/1
745/745 [==============================] - 15s 21ms/step - loss: 0.0076 - mean_absolute_error: 0.0594
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 19:34:43.224167!
Epoch 1/1
732/732 [==============================] - 15s 21ms/step - loss: 0.0101 - mean_absolute_error: 0.0675
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 19:35:05.614513!
Epoch 1/1
726/726 [==============================] - 15s 21ms/step - loss: 0.0146 - mean_absolute_error: 0.0800
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 19:35:25.854676!
Epoch 1/1
498/498 [==============================] - 10s 21ms/step - loss: 0.0173 - mean_absolute_error: 0.0832
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 19:35:42.076243!
Epoch 1/1
614/614 [==============================] - 13s 21ms/step - loss: 0.0179 - mean_absolute_error: 0.1016
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 19:35:59.864252!
Epoch 1/1
511/511 [==============================] - 11s 21ms/step - loss: 0.0142 - mean_absolute_error: 0.0746
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 19:36:18.044680!
Epoch 1/1
744/744 [==============================] - 15s 20ms/step - loss: 0.0137 - mean_absolute_error: 0.0768
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 19:36:38.375221!
Epoch 1/1
556/556 [==============================] - 12s 21ms/step - loss: 0.0127 - mean_absolute_error: 0.0723
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 19:36:53.839500!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0115 - mean_absolute_error: 0.0742
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 19:37:07.330325!
Epoch 1/1
542/542 [==============================] - 11s 21ms/step - loss: 0.0368 - mean_absolute_error: 0.1306
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 19:37:23.458181!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0084 - mean_absolute_error: 0.0639
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 19:37:39.385376!
Epoch 1/1
572/572 [==============================] - 12s 20ms/step - loss: 0.0103 - mean_absolute_error: 0.0635
Epoch 1 completed!
Exp2019-01-25_19-31-28.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm320_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0
.000100_2019-01-25_19-31-28
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 19:37:53.493730
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 26.15 Degree
        The absolute mean error on Yaw angle estimation: 90.21 Degree
        The absolute mean error on Roll angle estimation: 29.46 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 7.16 Degree
        The absolute mean error on Yaw angle estimation: 25.04 Degree
        The absolute mean error on Roll angle estimation: 6.84 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 26.52 Degree
        The absolute mean error on Yaw angle estimation: 26.45 Degree
        The absolute mean error on Roll angle estimation: 10.82 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 13.55 Degree
        The absolute mean error on Yaw angle estimation: 45.58 Degree
        The absolute mean error on Roll angle estimation: 17.28 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.34 Degree
        The absolute mean error on Yaw angle estimations: 46.82 Degree
        The absolute mean error on Roll angle estimations: 16.10 Degree
subject3_Exp2019-01-25_19-31-28.png has been saved by 2019-01-25 19:39:00.520167.
subject5_Exp2019-01-25_19-31-28.png has been saved by 2019-01-25 19:39:00.721563.
subject9_Exp2019-01-25_19-31-28.png has been saved by 2019-01-25 19:39:00.920002.
subject14_Exp2019-01-25_19-31-28.png has been saved by 2019-01-25 19:39:01.138635.
Model Exp2019-01-25_19-31-28 has been evaluated successfully.
Model Exp2019-01-25_19-31-28 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_19-31-28 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 19:41:00.790660: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 19:41:00.887793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 19:41:00.888051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 19:41:00.888063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 19:41:01.043755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 19:41:01.043781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 19:41:01.043788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 19:41:01.043929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_19-31-28.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 320)                  5653760
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    963
=================================================================
Total params: 139,915,267
Trainable params: 5,654,723
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 320
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02
All frames and annotations from 20 datasets have been read by 2019-01-25 19:41:06.566689
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 19:41:12.967112!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0076 - mean_absolute_error: 0.0576
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 19:41:32.436436!
Epoch 1/1
492/492 [==============================] - 10s 21ms/step - loss: 0.0067 - mean_absolute_error: 0.0571
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 19:41:49.138914!
Epoch 1/1
654/654 [==============================] - 14s 21ms/step - loss: 0.0091 - mean_absolute_error: 0.0671
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 19:42:07.732692!
Epoch 1/1
502/502 [==============================] - 10s 20ms/step - loss: 0.0126 - mean_absolute_error: 0.0791
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 19:42:25.872676!
Epoch 1/1
772/772 [==============================] - 16s 21ms/step - loss: 0.0070 - mean_absolute_error: 0.0571
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 19:42:47.557152!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.0160 - mean_absolute_error: 0.0924
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 19:43:05.470606!
Epoch 1/1
634/634 [==============================] - 13s 21ms/step - loss: 0.0164 - mean_absolute_error: 0.0968
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 19:43:27.326531!
Epoch 1/1
914/914 [==============================] - 19s 21ms/step - loss: 0.0063 - mean_absolute_error: 0.0547
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 19:43:53.945284!
Epoch 1/1
745/745 [==============================] - 15s 21ms/step - loss: 0.0035 - mean_absolute_error: 0.0432
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 19:44:16.789719!
Epoch 1/1
732/732 [==============================] - 15s 21ms/step - loss: 0.0035 - mean_absolute_error: 0.0419
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 19:44:39.283436!
Epoch 1/1
726/726 [==============================] - 15s 21ms/step - loss: 0.0052 - mean_absolute_error: 0.0501
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 19:44:59.429129!
Epoch 1/1
498/498 [==============================] - 10s 21ms/step - loss: 0.0079 - mean_absolute_error: 0.0575
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 19:45:15.815033!
Epoch 1/1
614/614 [==============================] - 13s 21ms/step - loss: 0.0109 - mean_absolute_error: 0.0790
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 19:45:33.709925!
Epoch 1/1
511/511 [==============================] - 11s 21ms/step - loss: 0.0043 - mean_absolute_error: 0.0465
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 19:45:51.647740!
Epoch 1/1
744/744 [==============================] - 16s 21ms/step - loss: 0.0067 - mean_absolute_error: 0.0574
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 19:46:12.636868!
Epoch 1/1
556/556 [==============================] - 11s 20ms/step - loss: 0.0075 - mean_absolute_error: 0.0618
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 19:46:27.785545!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0083 - mean_absolute_error: 0.0662
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 19:46:41.269235!
Epoch 1/1
542/542 [==============================] - 11s 21ms/step - loss: 0.0172 - mean_absolute_error: 0.0908
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 19:46:57.544770!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0059 - mean_absolute_error: 0.0529
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 19:47:13.419765!
Epoch 1/1
572/572 [==============================] - 12s 21ms/step - loss: 0.0037 - mean_absolute_error: 0.0422
Epoch 1 completed!
Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 19:47:27.710249
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 23.32 Degree
        The absolute mean error on Yaw angle estimation: 71.51 Degree
        The absolute mean error on Roll angle estimation: 38.54 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 5.54 Degree
        The absolute mean error on Yaw angle estimation: 26.87 Degree
        The absolute mean error on Roll angle estimation: 6.51 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 24.33 Degree
        The absolute mean error on Yaw angle estimation: 20.80 Degree
        The absolute mean error on Roll angle estimation: 14.47 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 13.23 Degree
        The absolute mean error on Yaw angle estimation: 41.72 Degree
        The absolute mean error on Roll angle estimation: 24.13 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.61 Degree
        The absolute mean error on Yaw angle estimations: 40.23 Degree
        The absolute mean error on Roll angle estimations: 20.91 Degree
subject3_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png has been saved by 2019-01-25 19:48:34.708354.
subject5_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png has been saved by 2019-01-25 19:48:34.905422.
subject9_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png has been saved by 2019-01-25 19:48:35.103156.
subject14_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png has been saved by 2019-01-25 19:48:35.319176.
Model Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02 has been evaluated successfully.
Model Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py

/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 19:51:43.217483: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 19:51:43.314361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 19:51:43.314624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 19:51:43.314638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 19:51:43.471013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 19:51:43.471040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 19:51:43.471050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 19:51:43.471190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_19-51-44 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_19-51-44
All frames and annotations from 20 datasets have been read by 2019-01-25 19:51:48.441954
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 19:51:54.843805!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.0189 - mean_absolute_error: 0.0961
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 19:52:12.880277!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0125 - mean_absolute_error: 0.0784
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 19:52:28.335169!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0162 - mean_absolute_error: 0.0883
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 19:52:44.847695!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0154 - mean_absolute_error: 0.0869
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 19:53:01.668156!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0181 - mean_absolute_error: 0.0876
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 19:53:21.109962!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0239 - mean_absolute_error: 0.1149
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 19:53:37.539858!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0307 - mean_absolute_error: 0.1283
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 19:53:58.002635!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0165 - mean_absolute_error: 0.0902
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 19:54:22.350679!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0098 - mean_absolute_error: 0.0694
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 19:54:42.679066!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0144 - mean_absolute_error: 0.0819
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 19:55:03.352900!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0183 - mean_absolute_error: 0.0877
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 19:55:21.675636!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0206 - mean_absolute_error: 0.0967
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 19:55:36.322902!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0234 - mean_absolute_error: 0.1141
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 19:55:52.604706!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0190 - mean_absolute_error: 0.0932
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 19:56:09.301802!
Epoch 1/1
744/744 [==============================] - 14s 19ms/step - loss: 0.0164 - mean_absolute_error: 0.0841
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 19:56:28.516558!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0197 - mean_absolute_error: 0.0964
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 19:56:42.317798!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0141 - mean_absolute_error: 0.0923
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 19:56:54.773881!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0415 - mean_absolute_error: 0.1457
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 19:57:09.257387!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0087 - mean_absolute_error: 0.0669
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 19:57:23.603258!
Epoch 1/1
572/572 [==============================] - 11s 19ms/step - loss: 0.0134 - mean_absolute_error: 0.0735
Epoch 1 completed!
Exp2019-01-25_19-51-44.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_19-51-44
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 19:57:36.559735
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 10.15 Degree
        The absolute mean error on Yaw angle estimation: 71.16 Degree
        The absolute mean error on Roll angle estimation: 34.03 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.64 Degree
        The absolute mean error on Yaw angle estimation: 25.72 Degree
        The absolute mean error on Roll angle estimation: 4.15 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 27.77 Degree
        The absolute mean error on Yaw angle estimation: 49.33 Degree
        The absolute mean error on Roll angle estimation: 8.36 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 17.46 Degree
        The absolute mean error on Yaw angle estimation: 59.36 Degree
        The absolute mean error on Roll angle estimation: 25.39 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.00 Degree
        The absolute mean error on Yaw angle estimations: 51.39 Degree
        The absolute mean error on Roll angle estimations: 17.98 Degree
subject3_Exp2019-01-25_19-51-44.png has been saved by 2019-01-25 19:58:42.447388.
subject5_Exp2019-01-25_19-51-44.png has been saved by 2019-01-25 19:58:42.649336.
subject9_Exp2019-01-25_19-51-44.png has been saved by 2019-01-25 19:58:42.849330.
subject14_Exp2019-01-25_19-51-44.png has been saved by 2019-01-25 19:58:43.071652.
Model Exp2019-01-25_19-51-44 has been evaluated successfully.
Model Exp2019-01-25_19-51-44 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 20:09:27.874083: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 20:09:27.971935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 20:09:27.972236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 20:09:27.972250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 20:09:28.128474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 20:09:28.128502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 20:09:28.128507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 20:09:28.128686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_20-09-28 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_20-09-28
All frames and annotations from 20 datasets have been read by 2019-01-25 20:09:33.213658
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 20:09:39.627556!
Epoch 1/1
665/665 [==============================] - 14s 22ms/step - loss: 0.0242 - mean_absolute_error: 0.1071
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 20:09:59.294695!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.0188 - mean_absolute_error: 0.0994
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 20:10:15.661798!
Epoch 1/1
654/654 [==============================] - 13s 20ms/step - loss: 0.0274 - mean_absolute_error: 0.1217
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 20:10:33.648571!
Epoch 1/1
502/502 [==============================] - 10s 20ms/step - loss: 0.0173 - mean_absolute_error: 0.0958
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 20:10:51.483142!
Epoch 1/1
772/772 [==============================] - 16s 20ms/step - loss: 0.0424 - mean_absolute_error: 0.1472
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 20:11:12.621354!
Epoch 1/1
569/569 [==============================] - 11s 20ms/step - loss: 0.0406 - mean_absolute_error: 0.1498
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 20:11:30.196915!
Epoch 1/1
634/634 [==============================] - 13s 20ms/step - loss: 0.0342 - mean_absolute_error: 0.1388
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 20:11:51.545036!
Epoch 1/1
914/914 [==============================] - 19s 20ms/step - loss: 0.0157 - mean_absolute_error: 0.0891
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 20:12:17.794842!
Epoch 1/1
745/745 [==============================] - 16s 21ms/step - loss: 0.0358 - mean_absolute_error: 0.1344
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 20:12:40.655678!
Epoch 1/1
732/732 [==============================] - 15s 20ms/step - loss: 0.0171 - mean_absolute_error: 0.0934
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 20:13:02.838804!
Epoch 1/1
726/726 [==============================] - 15s 20ms/step - loss: 0.0189 - mean_absolute_error: 0.0943
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 20:13:22.506862!
Epoch 1/1
498/498 [==============================] - 10s 20ms/step - loss: 0.0207 - mean_absolute_error: 0.0970
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 20:13:38.463144!
Epoch 1/1
614/614 [==============================] - 12s 20ms/step - loss: 0.0264 - mean_absolute_error: 0.1233
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 20:13:55.990582!
Epoch 1/1
511/511 [==============================] - 10s 20ms/step - loss: 0.0162 - mean_absolute_error: 0.0881
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 20:14:13.689405!
Epoch 1/1
744/744 [==============================] - 15s 20ms/step - loss: 0.0136 - mean_absolute_error: 0.0839
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 20:14:34.347482!
Epoch 1/1
556/556 [==============================] - 11s 20ms/step - loss: 0.0204 - mean_absolute_error: 0.0967
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 20:14:49.180138!
Epoch 1/1
395/395 [==============================] - 8s 20ms/step - loss: 0.0143 - mean_absolute_error: 0.0890
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 20:15:02.239528!
Epoch 1/1
542/542 [==============================] - 11s 21ms/step - loss: 0.0353 - mean_absolute_error: 0.1295
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 20:15:18.234422!
Epoch 1/1
485/485 [==============================] - 10s 20ms/step - loss: 0.0083 - mean_absolute_error: 0.0674
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 20:15:33.833666!
Epoch 1/1
572/572 [==============================] - 12s 20ms/step - loss: 0.0095 - mean_absolute_error: 0.0634
Epoch 1 completed!
Exp2019-01-25_20-09-28.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_20-09-28
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 20:15:47.777946
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 16.11 Degree
        The absolute mean error on Yaw angle estimation: 68.87 Degree
        The absolute mean error on Roll angle estimation: 25.63 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.95 Degree
        The absolute mean error on Yaw angle estimation: 25.49 Degree
        The absolute mean error on Roll angle estimation: 4.62 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 19.19 Degree
        The absolute mean error on Yaw angle estimation: 27.35 Degree
        The absolute mean error on Roll angle estimation: 9.17 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 13.25 Degree
        The absolute mean error on Yaw angle estimation: 39.89 Degree
        The absolute mean error on Roll angle estimation: 22.82 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.87 Degree
        The absolute mean error on Yaw angle estimations: 40.40 Degree
        The absolute mean error on Roll angle estimations: 15.56 Degree
subject3_Exp2019-01-25_20-09-28.png has been saved by 2019-01-25 20:16:54.329623.
subject5_Exp2019-01-25_20-09-28.png has been saved by 2019-01-25 20:16:54.529163.
subject9_Exp2019-01-25_20-09-28.png has been saved by 2019-01-25 20:16:54.725852.
subject14_Exp2019-01-25_20-09-28.png has been saved by 2019-01-25 20:16:54.949242.
Model Exp2019-01-25_20-09-28 has been evaluated successfully.
Model Exp2019-01-25_20-09-28 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 20:19:33.758066: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 20:19:33.856273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 20:19:33.856537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 20:19:33.856551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 20:19:34.013544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 20:19:34.013572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 20:19:34.013580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 20:19:34.013723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_20-19-34 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.0
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_20-19-34
All frames and annotations from 20 datasets have been read by 2019-01-25 20:19:39.274677
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 20:19:45.684010!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.0305 - mean_absolute_error: 0.1266
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 20:20:04.147402!
Epoch 1/1
492/492 [==============================] - 9s 19ms/step - loss: 0.0210 - mean_absolute_error: 0.1041
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 20:20:19.808759!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0321 - mean_absolute_error: 0.1254
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 20:20:36.848703!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0217 - mean_absolute_error: 0.1056
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 20:20:53.791070!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0459 - mean_absolute_error: 0.1417
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 20:21:13.307989!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0415 - mean_absolute_error: 0.1530
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 20:21:29.801170!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0415 - mean_absolute_error: 0.1543
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 20:21:50.069800!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0193 - mean_absolute_error: 0.1002
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 20:22:14.299166!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0223 - mean_absolute_error: 0.1059
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 20:22:34.910522!
Epoch 1/1
732/732 [==============================] - 14s 19ms/step - loss: 0.0169 - mean_absolute_error: 0.0926
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 20:22:55.861404!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0243 - mean_absolute_error: 0.1038
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 20:23:14.127551!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0301 - mean_absolute_error: 0.1138
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 20:23:28.923407!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0306 - mean_absolute_error: 0.1343
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 20:23:45.259308!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0351 - mean_absolute_error: 0.1228
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 20:24:02.063833!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0219 - mean_absolute_error: 0.1071
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 20:24:21.062521!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0300 - mean_absolute_error: 0.1231
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 20:24:35.192200!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0163 - mean_absolute_error: 0.0991
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 20:24:47.598678!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0531 - mean_absolute_error: 0.1582
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 20:25:02.260663!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0112 - mean_absolute_error: 0.0780
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 20:25:16.924093!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0142 - mean_absolute_error: 0.0800
Epoch 1 completed!
Exp2019-01-25_20-19-34.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_20-19-34
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 20:25:29.536691
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 9.63 Degree
        The absolute mean error on Yaw angle estimation: 98.50 Degree
        The absolute mean error on Roll angle estimation: 38.93 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.51 Degree
        The absolute mean error on Yaw angle estimation: 28.40 Degree
        The absolute mean error on Roll angle estimation: 8.99 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 18.39 Degree
        The absolute mean error on Yaw angle estimation: 38.39 Degree
        The absolute mean error on Roll angle estimation: 8.05 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 8.87 Degree
        The absolute mean error on Yaw angle estimation: 56.54 Degree
        The absolute mean error on Roll angle estimation: 22.12 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 13.10 Degree
        The absolute mean error on Yaw angle estimations: 55.46 Degree
        The absolute mean error on Roll angle estimations: 19.52 Degree
subject3_Exp2019-01-25_20-19-34.png has been saved by 2019-01-25 20:26:36.924086.
subject5_Exp2019-01-25_20-19-34.png has been saved by 2019-01-25 20:26:37.126513.
subject9_Exp2019-01-25_20-19-34.png has been saved by 2019-01-25 20:26:37.366218.
subject14_Exp2019-01-25_20-19-34.png has been saved by 2019-01-25 20:26:37.588083.
Model Exp2019-01-25_20-19-34 has been evaluated successfully.
Model Exp2019-01-25_20-19-34 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 20:27:44.121939: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 20:27:44.219804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 20:27:44.220067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 20:27:44.220084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 20:27:44.374676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 20:27:44.374703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 20:27:44.374708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 20:27:44.374843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_20-27-45 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_20-27-45
All frames and annotations from 20 datasets have been read by 2019-01-25 20:27:49.586105
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 20:27:55.998416!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.0676 - mean_absolute_error: 0.1948
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 20:28:19.459493!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0409 - mean_absolute_error: 0.1533
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 20:28:37.945893!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0594 - mean_absolute_error: 0.1769
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 20:28:58.985975!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0373 - mean_absolute_error: 0.1458
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 20:29:19.311381!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0719 - mean_absolute_error: 0.2041
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 20:29:43.796026!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0668 - mean_absolute_error: 0.2000
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 20:30:04.111513!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0610 - mean_absolute_error: 0.1866
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 20:30:28.890812!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0339 - mean_absolute_error: 0.1381
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 20:30:59.964428!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0499 - mean_absolute_error: 0.1702
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 20:31:26.046263!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0364 - mean_absolute_error: 0.1445
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 20:31:51.723456!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0413 - mean_absolute_error: 0.1455
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 20:32:15.109685!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0606 - mean_absolute_error: 0.1784
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 20:32:33.498093!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0482 - mean_absolute_error: 0.1705
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 20:32:53.953105!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0461 - mean_absolute_error: 0.1624
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 20:33:14.503022!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0433 - mean_absolute_error: 0.1511
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 20:33:38.451712!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0324 - mean_absolute_error: 0.1318
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 20:33:56.742021!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0213 - mean_absolute_error: 0.1108
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 20:34:11.869948!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0621 - mean_absolute_error: 0.1761
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 20:34:30.132069!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0213 - mean_absolute_error: 0.1062
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 20:34:47.594655!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0231 - mean_absolute_error: 0.1044
Epoch 1 completed!
Exp2019-01-25_20-27-45.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_20-27-45
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 20:35:04.718573
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.14 Degree
        The absolute mean error on Yaw angle estimation: 53.58 Degree
        The absolute mean error on Roll angle estimation: 17.27 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.75 Degree
        The absolute mean error on Yaw angle estimation: 28.73 Degree
        The absolute mean error on Roll angle estimation: 4.68 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.36 Degree
        The absolute mean error on Yaw angle estimation: 32.03 Degree
        The absolute mean error on Roll angle estimation: 11.26 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.80 Degree
        The absolute mean error on Yaw angle estimation: 37.89 Degree
        The absolute mean error on Roll angle estimation: 14.19 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.51 Degree
        The absolute mean error on Yaw angle estimations: 38.06 Degree
        The absolute mean error on Roll angle estimations: 11.85 Degree
subject3_Exp2019-01-25_20-27-45.png has been saved by 2019-01-25 20:36:29.256036.
subject5_Exp2019-01-25_20-27-45.png has been saved by 2019-01-25 20:36:29.453853.
subject9_Exp2019-01-25_20-27-45.png has been saved by 2019-01-25 20:36:29.649442.
subject14_Exp2019-01-25_20-27-45.png has been saved by 2019-01-25 20:36:29.867309.
Model Exp2019-01-25_20-27-45 has been evaluated successfully.
Model Exp2019-01-25_20-27-45 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 20:37:41.375782: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 20:37:41.472472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 20:37:41.472731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 20:37:41.472750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 20:37:41.628167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 20:37:41.628192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 20:37:41.628197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 20:37:41.628334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_20-37-42 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_20-37-42
All frames and annotations from 20 datasets have been read by 2019-01-25 20:37:46.800402
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 20:37:53.203522!
Epoch 1/1
665/665 [==============================] - 18s 28ms/step - loss: 0.0973 - mean_absolute_error: 0.2330
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 20:38:16.923838!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.1001 - mean_absolute_error: 0.2253
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 20:38:35.182750!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.1154 - mean_absolute_error: 0.2538
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 20:38:56.353424!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0889 - mean_absolute_error: 0.2309
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 20:39:16.889442!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1669 - mean_absolute_error: 0.3127
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 20:39:41.592178!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1041 - mean_absolute_error: 0.2435
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 20:40:01.521372!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1048 - mean_absolute_error: 0.2440
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 20:40:26.071813!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0738 - mean_absolute_error: 0.1993
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 20:40:56.567390!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1167 - mean_absolute_error: 0.2563
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 20:41:22.775703!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.1156 - mean_absolute_error: 0.2607
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 20:41:48.018957!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0916 - mean_absolute_error: 0.2260
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 20:42:10.767807!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1026 - mean_absolute_error: 0.2359
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 20:42:29.131067!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0881 - mean_absolute_error: 0.2294
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 20:42:49.466898!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0951 - mean_absolute_error: 0.2355
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 20:43:09.757143!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0884 - mean_absolute_error: 0.2269
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 20:43:34.135177!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0614 - mean_absolute_error: 0.1786
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 20:43:51.627728!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0369 - mean_absolute_error: 0.1372
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 20:44:06.635602!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0902 - mean_absolute_error: 0.2156
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 20:44:25.078467!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0485 - mean_absolute_error: 0.1662
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 20:44:42.697012!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0498 - mean_absolute_error: 0.1533
Epoch 1 completed!
Exp2019-01-25_20-37-42.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_20-37-42
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 20:44:59.405719
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 36.38 Degree
        The absolute mean error on Yaw angle estimation: 33.15 Degree
        The absolute mean error on Roll angle estimation: 7.97 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 29.30 Degree
        The absolute mean error on Yaw angle estimation: 35.05 Degree
        The absolute mean error on Roll angle estimation: 15.87 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 35.26 Degree
        The absolute mean error on Yaw angle estimation: 31.52 Degree
        The absolute mean error on Roll angle estimation: 15.96 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 53.41 Degree
        The absolute mean error on Yaw angle estimation: 38.89 Degree
        The absolute mean error on Roll angle estimation: 17.76 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 38.59 Degree
        The absolute mean error on Yaw angle estimations: 34.65 Degree
        The absolute mean error on Roll angle estimations: 14.39 Degree
subject3_Exp2019-01-25_20-37-42.png has been saved by 2019-01-25 20:46:24.157936.
subject5_Exp2019-01-25_20-37-42.png has been saved by 2019-01-25 20:46:24.349229.
subject9_Exp2019-01-25_20-37-42.png has been saved by 2019-01-25 20:46:24.540379.
subject14_Exp2019-01-25_20-37-42.png has been saved by 2019-01-25 20:46:24.755958.
Model Exp2019-01-25_20-37-42 has been evaluated successfully.
Model Exp2019-01-25_20-37-42 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 20:48:21.256389: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 20:48:21.354729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 20:48:21.354998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 20:48:21.355012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 20:48:21.511423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 20:48:21.511449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 20:48:21.511458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 20:48:21.511600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_20-48-22 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-25_20-48-22
All frames and annotations from 20 datasets have been read by 2019-01-25 20:48:26.700343
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 20:48:33.090651!
Epoch 1/1
665/665 [==============================] - 19s 28ms/step - loss: 0.0343 - mean_absolute_error: 0.1232
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 20:48:57.192287!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0307 - mean_absolute_error: 0.1180
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 20:49:15.344403!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0562 - mean_absolute_error: 0.1538
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 20:49:36.941772!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0339 - mean_absolute_error: 0.1350
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 20:49:56.674498!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0700 - mean_absolute_error: 0.1930
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 20:50:20.694320!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0745 - mean_absolute_error: 0.2002
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 20:50:41.014758!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0703 - mean_absolute_error: 0.1879
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 20:51:05.743434!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0496 - mean_absolute_error: 0.1503
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 20:51:36.390077!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0839 - mean_absolute_error: 0.2141
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 20:52:01.783879!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0507 - mean_absolute_error: 0.1675
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 20:52:27.642991!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0595 - mean_absolute_error: 0.1708
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 20:52:51.617848!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0906 - mean_absolute_error: 0.2167
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 20:53:10.589965!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0675 - mean_absolute_error: 0.1962
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 20:53:31.253237!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0880 - mean_absolute_error: 0.2218
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 20:53:52.053837!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0858 - mean_absolute_error: 0.2174
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 20:54:16.169458!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0436 - mean_absolute_error: 0.1448
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 20:54:33.926989!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0254 - mean_absolute_error: 0.1028
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 20:54:48.858286!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0728 - mean_absolute_error: 0.1816
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 20:55:07.477788!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0415 - mean_absolute_error: 0.1464
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 20:55:25.158345!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0449 - mean_absolute_error: 0.1391
Epoch 1 completed!
Exp2019-01-25_20-48-22.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-25_20-48-22
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 20:55:42.159894
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.65 Degree
        The absolute mean error on Yaw angle estimation: 32.30 Degree
        The absolute mean error on Roll angle estimation: 9.62 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 21.83 Degree
        The absolute mean error on Yaw angle estimation: 29.76 Degree
        The absolute mean error on Roll angle estimation: 4.77 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.23 Degree
        The absolute mean error on Yaw angle estimation: 26.27 Degree
        The absolute mean error on Roll angle estimation: 8.83 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.14 Degree
        The absolute mean error on Yaw angle estimation: 34.71 Degree
        The absolute mean error on Roll angle estimation: 14.13 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.96 Degree
        The absolute mean error on Yaw angle estimations: 30.76 Degree
        The absolute mean error on Roll angle estimations: 9.34 Degree
subject3_Exp2019-01-25_20-48-22.png has been saved by 2019-01-25 20:57:06.854349.
subject5_Exp2019-01-25_20-48-22.png has been saved by 2019-01-25 20:57:07.044592.
subject9_Exp2019-01-25_20-48-22.png has been saved by 2019-01-25 20:57:07.233937.
subject14_Exp2019-01-25_20-48-22.png has been saved by 2019-01-25 20:57:07.443411.
Model Exp2019-01-25_20-48-22 has been evaluated successfully.
Model Exp2019-01-25_20-48-22 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_20-48-22 trainLSTM_VGG16.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Incorrect argument for method. Try again...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_20-48-22 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:01:27.730948: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:01:27.828494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:01:27.828797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:01:27.828811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:01:27.985223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:01:27.985251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:01:27.985256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:01:27.985442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_20-48-22.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model Exp2019-01-25_20-48-22_and_2019-01-25_21-01-29
All frames and annotations from 20 datasets have been read by 2019-01-25 21:01:33.741987
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:01:40.149632!
Epoch 1/1
2019-01-25 21:01:41.407892: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_arra
y_ops.cc:121 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does no
t exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327,
in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312,
in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420,
in _call_tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line
 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 44,
in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 56, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 46, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on
_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482,
in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, i
n run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140,
in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321,
in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340,
in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGra
dV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_tra
in_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519,
in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 48
8, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 37
9, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line
 104, in _TensorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line
 6221, in tensor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", l
ine 787, in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, i
n create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, i
n __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserializ
e
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deser
ialize_keras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771,
in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754,
in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Tenso
rArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_20-48-22 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:02:47.698628: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:02:47.796456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:02:47.796719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:02:47.796737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:02:47.952566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:02:47.952587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:02:47.952591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:02:47.952727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_20-48-22.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model Exp2019-01-25_20-48-22_and_2019-01-25_21-02-49
All frames and annotations from 20 datasets have been read by 2019-01-25 21:02:53.716250
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:03:00.118834!
Epoch 1/1
2019-01-25 21:03:01.352415: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_arra
y_ops.cc:121 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does no
t exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327,
in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312,
in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420,
in _call_tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line
 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 44,
in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 56, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 46, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on
_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482,
in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, i
n run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140,
in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321,
in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340,
in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGra
dV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_tra
in_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519,
in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 48
8, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 37
9, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line
 104, in _TensorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line
 6221, in tensor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", l
ine 787, in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, i
n create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, i
n __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserializ
e
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deser
ialize_keras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771,
in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754,
in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Tenso
rArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py

/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:04:24.358133: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:04:24.454825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:04:24.455080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:04:24.455092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:04:24.611086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:04:24.611111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:04:24.611116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:04:24.611251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_21-04-25 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 220)                  197120
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    663
=================================================================
Total params: 138,656,730
Trainable params: 4,396,186
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 220
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm220_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.0
00100_2019-01-25_21-04-25
All frames and annotations from 20 datasets have been read by 2019-01-25 21:04:29.818114
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:04:36.228989!
Epoch 1/1
665/665 [==============================] - 18s 26ms/step - loss: 0.0317 - mean_absolute_error: 0.1183
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 21:04:59.175922!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0297 - mean_absolute_error: 0.1155
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 21:05:17.686704!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0561 - mean_absolute_error: 0.1528
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 21:05:38.901843!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0321 - mean_absolute_error: 0.1302
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 21:05:58.963960!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0668 - mean_absolute_error: 0.1849
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 21:06:23.763213!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0695 - mean_absolute_error: 0.1892
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 21:06:43.983929!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0697 - mean_absolute_error: 0.1872
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 21:07:08.665075!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0494 - mean_absolute_error: 0.1496
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 21:07:39.304678!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0823 - mean_absolute_error: 0.2134
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 21:08:05.353688!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0496 - mean_absolute_error: 0.1663
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 21:08:31.059163!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0597 - mean_absolute_error: 0.1715
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 21:08:54.264154!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0898 - mean_absolute_error: 0.2168
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 21:09:12.544788!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0626 - mean_absolute_error: 0.1845
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 21:09:32.908943!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0760 - mean_absolute_error: 0.2055
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 21:09:52.748137!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0794 - mean_absolute_error: 0.2082
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 21:10:16.639813!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0417 - mean_absolute_error: 0.1364
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 21:10:34.054182!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0249 - mean_absolute_error: 0.1051
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 21:10:49.135734!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0727 - mean_absolute_error: 0.1828
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 21:11:07.080386!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0389 - mean_absolute_error: 0.1425
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 21:11:25.067093!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0440 - mean_absolute_error: 0.1366
Epoch 1 completed!
Exp2019-01-25_21-04-25.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm220_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0
.000100_2019-01-25_21-04-25
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 21:11:41.802172
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.63 Degree
        The absolute mean error on Yaw angle estimation: 33.09 Degree
        The absolute mean error on Roll angle estimation: 10.98 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 21.47 Degree
        The absolute mean error on Yaw angle estimation: 29.85 Degree
        The absolute mean error on Roll angle estimation: 5.83 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.26 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 9.63 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.37 Degree
        The absolute mean error on Yaw angle estimation: 35.02 Degree
        The absolute mean error on Roll angle estimation: 14.20 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.93 Degree
        The absolute mean error on Yaw angle estimations: 31.16 Degree
        The absolute mean error on Roll angle estimations: 10.16 Degree
subject3_Exp2019-01-25_21-04-25.png has been saved by 2019-01-25 21:13:06.164778.
subject5_Exp2019-01-25_21-04-25.png has been saved by 2019-01-25 21:13:06.355577.
subject9_Exp2019-01-25_21-04-25.png has been saved by 2019-01-25 21:13:06.546277.
subject14_Exp2019-01-25_21-04-25.png has been saved by 2019-01-25 21:13:06.755282.
Model Exp2019-01-25_21-04-25 has been evaluated successfully.
Model Exp2019-01-25_21-04-25 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_21-04-25 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:14:07.900861: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:14:07.999181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:14:07.999439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:14:07.999451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:14:08.155522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:14:08.155544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:14:08.155549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:14:08.155685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_21-04-25.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 220)                  197120
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    663
=================================================================
Total params: 138,656,730
Trainable params: 4,396,186
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 220
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model Exp2019-01-25_21-04-25_and_2019-01-25_21-14-09
All frames and annotations from 20 datasets have been read by 2019-01-25 21:14:13.956025
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:14:20.351969!
Epoch 1/1
2019-01-25 21:14:21.612943: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_arra
y_ops.cc:121 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does no
t exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327,
in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312,
in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420,
in _call_tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line
 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 44,
in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 56, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 46, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on
_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482,
in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, i
n run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140,
in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321,
in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340,
in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGra
dV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_tra
in_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519,
in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 48
8, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 37
9, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line
 104, in _TensorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line
 6221, in tensor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", l
ine 787, in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, i
n create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, i
n __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserializ
e
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deser
ialize_keras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771,
in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754,
in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Tenso
rArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_L
STM.py Exp2019-01-25_21-04-25 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:14:50.039352: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:14:50.135763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:14:50.136021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:14:50.136033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:14:50.291171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:14:50.291198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:14:50.291203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:14:50.291341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-25_21-04-25.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 220)                  197120
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    663
=================================================================
Total params: 138,656,730
Trainable params: 4,396,186
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model Exp2019-01-25_21-04-25_and_2019-01-25_21-14-51
All frames and annotations from 20 datasets have been read by 2019-01-25 21:14:56.103022
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:15:02.541440!
Epoch 1/1
2019-01-25 21:15:03.808742: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_arra
y_ops.cc:121 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does no
t exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327,
in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312,
in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420,
in _call_tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line
 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 44,
in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 56, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 46, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on
_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482,
in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, i
n run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140,
in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321,
in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340,
in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGra
dV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_tra
in_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519,
in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 48
8, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 37
9, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line
 104, in _TensorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line
 6221, in tensor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", l
ine 787, in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, i
n create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, i
n __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueTrainigCNN_LSTM.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", li
ne 57, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserializ
e
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deser
ialize_keras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771,
in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754,
in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Tenso
rArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_575, ^_cloopConstantFolding/training/Adam/gradients/dropout025/while/cond/drop
out/div_grad/RealDiv_recip/_481)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py

/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:15:23.280117: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:15:23.363371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:15:23.363630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:15:23.363642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:15:23.519061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:15:23.519085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:15:23.519090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:15:23.519226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_21-15-24 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0010_2019-01-25_21-15-24
All frames and annotations from 20 datasets have been read by 2019-01-25 21:15:28.725173
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:15:35.121011!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.0307 - mean_absolute_error: 0.1202
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 21:15:58.484969!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0327 - mean_absolute_error: 0.1286
^C
Model Exp2019-01-25_21-15-24 has been interrupted.
Exp2019-01-25_21-15-24.h5 has been saved.
Model Exp2019-01-25_21-15-24 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:16:35.809219: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:16:35.907161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:16:35.907422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:16:35.907434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:16:36.063488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:16:36.063512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:16:36.063517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:16:36.063653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_21-16-36 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 3
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0010_2019-01-25_21-16-36
All frames and annotations from 20 datasets have been read by 2019-01-25 21:16:41.203046
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:16:47.604799!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.0432 - mean_absolute_error: 0.1479
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 21:17:10.789603!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0527 - mean_absolute_error: 0.1658
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 21:17:29.207964!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0786 - mean_absolute_error: 0.2051
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 21:17:50.533495!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0638 - mean_absolute_error: 0.1986
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 21:18:10.963529!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0759 - mean_absolute_error: 0.1950
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 21:18:35.733518!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0707 - mean_absolute_error: 0.1944
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 21:18:56.109214!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0808 - mean_absolute_error: 0.2077
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 21:19:20.477033!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0619 - mean_absolute_error: 0.1729
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 21:19:51.150242!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0839 - mean_absolute_error: 0.2161
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 21:20:17.277450!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0573 - mean_absolute_error: 0.1755
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 21:20:43.045608!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0618 - mean_absolute_error: 0.1781
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 21:21:06.316051!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0943 - mean_absolute_error: 0.2188
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 21:21:24.713538!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0704 - mean_absolute_error: 0.2033
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 21:21:45.400497!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0955 - mean_absolute_error: 0.2320
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 21:22:05.549894!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0959 - mean_absolute_error: 0.2308
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 21:22:29.605328!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0442 - mean_absolute_error: 0.1439
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 21:22:47.750536!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0341 - mean_absolute_error: 0.1221
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 21:23:02.904529!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0837 - mean_absolute_error: 0.1900
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 21:23:21.384257!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0410 - mean_absolute_error: 0.1455
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 21:23:39.488155!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0438 - mean_absolute_error: 0.1387
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-25 21:23:57.951911
1. set (Dataset 6) being trained for epoch 2 by 2019-01-25 21:24:03.130997!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0819 - mean_absolute_error: 0.1872
2. set (Dataset 11) being trained for epoch 2 by 2019-01-25 21:24:22.665649!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0431 - mean_absolute_error: 0.1348
3. set (Dataset 10) being trained for epoch 2 by 2019-01-25 21:24:44.297650!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0632 - mean_absolute_error: 0.1804
4. set (Dataset 4) being trained for epoch 2 by 2019-01-25 21:25:10.482302!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0931 - mean_absolute_error: 0.2309
5. set (Dataset 23) being trained for epoch 2 by 2019-01-25 21:25:34.804892!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0690 - mean_absolute_error: 0.1899
6. set (Dataset 13) being trained for epoch 2 by 2019-01-25 21:25:53.776504!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0406 - mean_absolute_error: 0.1435
7. set (Dataset 17) being trained for epoch 2 by 2019-01-25 21:26:10.332711!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0309 - mean_absolute_error: 0.1136
8. set (Dataset 1) being trained for epoch 2 by 2019-01-25 21:26:25.098666!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0981 - mean_absolute_error: 0.2207
9. set (Dataset 8) being trained for epoch 2 by 2019-01-25 21:26:45.617723!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0741 - mean_absolute_error: 0.1982
10. set (Dataset 7) being trained for epoch 2 by 2019-01-25 21:27:12.652208!
Epoch 1/1
745/745 [==============================] - 287s 385ms/step - loss: 0.0814 - mean_absolute_error: 0.2108
11. set (Dataset 21) being trained for epoch 2 by 2019-01-25 21:32:05.538803!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0800 - mean_absolute_error: 0.2080
12. set (Dataset 22) being trained for epoch 2 by 2019-01-25 21:32:27.815278!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0313 - mean_absolute_error: 0.1098
13. set (Dataset 2) being trained for epoch 2 by 2019-01-25 21:32:49.340874!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0988 - mean_absolute_error: 0.2351
14. set (Dataset 24) being trained for epoch 2 by 2019-01-25 21:33:07.149618!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0411 - mean_absolute_error: 0.1414
15. set (Dataset 15) being trained for epoch 2 by 2019-01-25 21:33:26.324381!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0676 - mean_absolute_error: 0.1827
16. set (Dataset 20) being trained for epoch 2 by 2019-01-25 21:33:48.015391!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0406 - mean_absolute_error: 0.1324
17. set (Dataset 18) being trained for epoch 2 by 2019-01-25 21:34:08.304271!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.0660 - mean_absolute_error: 0.1990
18. set (Dataset 19) being trained for epoch 2 by 2019-01-25 21:34:27.976188!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0459 - mean_absolute_error: 0.1615
19. set (Dataset 12) being trained for epoch 2 by 2019-01-25 21:34:47.731945!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0616 - mean_absolute_error: 0.1806
20. set (Dataset 16) being trained for epoch 2 by 2019-01-25 21:35:14.863557!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0571 - mean_absolute_error: 0.1665
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-25 21:35:42.203045
1. set (Dataset 19) being trained for epoch 3 by 2019-01-25 21:35:47.081226!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0443 - mean_absolute_error: 0.1581
2. set (Dataset 16) being trained for epoch 3 by 2019-01-25 21:36:08.492940!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0545 - mean_absolute_error: 0.1614
3. set (Dataset 21) being trained for epoch 3 by 2019-01-25 21:36:37.252151!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0723 - mean_absolute_error: 0.1925
4. set (Dataset 15) being trained for epoch 3 by 2019-01-25 21:36:59.577983!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0595 - mean_absolute_error: 0.1649
5. set (Dataset 13) being trained for epoch 3 by 2019-01-25 21:37:21.047623!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0475 - mean_absolute_error: 0.1615
6. set (Dataset 12) being trained for epoch 3 by 2019-01-25 21:37:40.922420!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0661 - mean_absolute_error: 0.1860
7. set (Dataset 18) being trained for epoch 3 by 2019-01-25 21:38:05.315966!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0635 - mean_absolute_error: 0.1937
8. set (Dataset 22) being trained for epoch 3 by 2019-01-25 21:38:27.350732!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0296 - mean_absolute_error: 0.1048
9. set (Dataset 23) being trained for epoch 3 by 2019-01-25 21:38:49.306848!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0683 - mean_absolute_error: 0.1863
10. set (Dataset 8) being trained for epoch 3 by 2019-01-25 21:39:11.388628!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0787 - mean_absolute_error: 0.2070
11. set (Dataset 17) being trained for epoch 3 by 2019-01-25 21:39:35.061663!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0277 - mean_absolute_error: 0.1047
12. set (Dataset 6) being trained for epoch 3 by 2019-01-25 21:39:50.505633!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0774 - mean_absolute_error: 0.1805
13. set (Dataset 24) being trained for epoch 3 by 2019-01-25 21:40:08.536982!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0326 - mean_absolute_error: 0.1227
14. set (Dataset 11) being trained for epoch 3 by 2019-01-25 21:40:27.274785!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0433 - mean_absolute_error: 0.1340
15. set (Dataset 10) being trained for epoch 3 by 2019-01-25 21:40:49.139868!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0670 - mean_absolute_error: 0.1860
16. set (Dataset 20) being trained for epoch 3 by 2019-01-25 21:41:12.575648!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0395 - mean_absolute_error: 0.1276
17. set (Dataset 2) being trained for epoch 3 by 2019-01-25 21:41:31.985709!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.1081 - mean_absolute_error: 0.2426
18. set (Dataset 4) being trained for epoch 3 by 2019-01-25 21:41:52.610471!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0901 - mean_absolute_error: 0.2275
19. set (Dataset 7) being trained for epoch 3 by 2019-01-25 21:42:18.663241!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0823 - mean_absolute_error: 0.2121
20. set (Dataset 1) being trained for epoch 3 by 2019-01-25 21:42:42.760509!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0992 - mean_absolute_error: 0.2222
Epoch 3 completed!
Exp2019-01-25_21-16-36.h5 has been saved.
The subjects are trained: [(19, 'M11'), (16, 'M09'), (21, 'F02'), (15, 'F03'), (13, 'M07'), (12, 'M06'), (18
, 'F05'), (22, 'M01'), (23, 'M13'), (8, 'M02'), (17, 'M10'), (6, 'F06'), (24, 'M14'), (11, 'M05'), (10, 'M04
'), (20, 'M12'), (2, 'F02'), (4, 'F04'), (7, 'M01'), (1, 'F01')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000010_2019-01-25_21-16-36
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 21:42:57.325885
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.47 Degree
        The absolute mean error on Yaw angle estimation: 32.07 Degree
        The absolute mean error on Roll angle estimation: 6.68 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 19.86 Degree
        The absolute mean error on Yaw angle estimation: 29.75 Degree
        The absolute mean error on Roll angle estimation: 3.34 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.48 Degree
        The absolute mean error on Yaw angle estimation: 26.16 Degree
        The absolute mean error on Roll angle estimation: 7.43 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.46 Degree
        The absolute mean error on Yaw angle estimation: 34.62 Degree
        The absolute mean error on Roll angle estimation: 14.31 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.82 Degree
        The absolute mean error on Yaw angle estimations: 30.65 Degree
        The absolute mean error on Roll angle estimations: 7.94 Degree
subject3_Exp2019-01-25_21-16-36.png has been saved by 2019-01-25 21:44:21.848924.
subject5_Exp2019-01-25_21-16-36.png has been saved by 2019-01-25 21:44:22.038883.
subject9_Exp2019-01-25_21-16-36.png has been saved by 2019-01-25 21:44:22.228643.
subject14_Exp2019-01-25_21-16-36.png has been saved by 2019-01-25 21:44:22.437772.
Model Exp2019-01-25_21-16-36 has been evaluated successfully.
Model Exp2019-01-25_21-16-36 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 21:47:30.345755: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 21:47:30.442350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 21:47:30.442613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 21:47:30.442627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 21:47:30.597299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 21:47:30.597326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 21:47:30.597334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 21:47:30.597495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_21-47-31 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-25_21-47-31
All frames and annotations from 20 datasets have been read by 2019-01-25 21:47:35.716582
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 21:47:42.127845!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.0338 - mean_absolute_error: 0.1227
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 21:48:05.672260!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0325 - mean_absolute_error: 0.1241
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 21:48:24.851210!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0567 - mean_absolute_error: 0.1554
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 21:48:46.512061!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0351 - mean_absolute_error: 0.1373
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 21:49:06.891789!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0711 - mean_absolute_error: 0.1965
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 21:49:31.711141!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0741 - mean_absolute_error: 0.2008
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 21:49:52.129148!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0705 - mean_absolute_error: 0.1881
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 21:50:17.031585!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0498 - mean_absolute_error: 0.1522
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 21:50:47.753576!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0847 - mean_absolute_error: 0.2154
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 21:51:14.169974!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0516 - mean_absolute_error: 0.1700
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 21:51:40.355781!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0594 - mean_absolute_error: 0.1707
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 21:52:04.022981!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0907 - mean_absolute_error: 0.2163
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 21:52:22.369723!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0688 - mean_absolute_error: 0.1998
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 21:52:43.091718!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0882 - mean_absolute_error: 0.2231
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 21:53:03.348247!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0880 - mean_absolute_error: 0.2202
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 21:53:27.682484!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0440 - mean_absolute_error: 0.1479
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 21:53:45.131146!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0260 - mean_absolute_error: 0.1050
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 21:54:00.020712!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0728 - mean_absolute_error: 0.1818
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 21:54:17.726008!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0427 - mean_absolute_error: 0.1462
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 21:54:35.986823!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0446 - mean_absolute_error: 0.1399
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-25 21:54:55.114967
1. set (Dataset 6) being trained for epoch 2 by 2019-01-25 21:55:00.336519!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0738 - mean_absolute_error: 0.1796
2. set (Dataset 11) being trained for epoch 2 by 2019-01-25 21:55:19.517829!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0441 - mean_absolute_error: 0.1361
3. set (Dataset 10) being trained for epoch 2 by 2019-01-25 21:55:40.935609!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0613 - mean_absolute_error: 0.1759
4. set (Dataset 4) being trained for epoch 2 by 2019-01-25 21:56:07.043105!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0820 - mean_absolute_error: 0.2127
5. set (Dataset 23) being trained for epoch 2 by 2019-01-25 21:56:31.035354!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0739 - mean_absolute_error: 0.1982
6. set (Dataset 13) being trained for epoch 2 by 2019-01-25 21:56:50.488569!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0386 - mean_absolute_error: 0.1399
7. set (Dataset 17) being trained for epoch 2 by 2019-01-25 21:57:06.237974!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0297 - mean_absolute_error: 0.1145
8. set (Dataset 1) being trained for epoch 2 by 2019-01-25 21:57:21.145009!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0985 - mean_absolute_error: 0.2227
9. set (Dataset 8) being trained for epoch 2 by 2019-01-25 21:57:41.616639!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0665 - mean_absolute_error: 0.1854
10. set (Dataset 7) being trained for epoch 2 by 2019-01-25 21:58:09.077665!
Epoch 1/1
745/745 [==============================] - 20s 27ms/step - loss: 0.0804 - mean_absolute_error: 0.2125
11. set (Dataset 21) being trained for epoch 2 by 2019-01-25 21:58:35.027346!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.0747 - mean_absolute_error: 0.1960
12. set (Dataset 22) being trained for epoch 2 by 2019-01-25 21:58:58.012074!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0295 - mean_absolute_error: 0.1110
13. set (Dataset 2) being trained for epoch 2 by 2019-01-25 21:59:19.396022!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0830 - mean_absolute_error: 0.2154
14. set (Dataset 24) being trained for epoch 2 by 2019-01-25 21:59:36.850312!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0497 - mean_absolute_error: 0.1558
15. set (Dataset 15) being trained for epoch 2 by 2019-01-25 21:59:55.735098!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0577 - mean_absolute_error: 0.1577
16. set (Dataset 20) being trained for epoch 2 by 2019-01-25 22:00:16.926903!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0398 - mean_absolute_error: 0.1308
17. set (Dataset 18) being trained for epoch 2 by 2019-01-25 22:00:36.719189!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0581 - mean_absolute_error: 0.1789
18. set (Dataset 19) being trained for epoch 2 by 2019-01-25 22:00:56.908603!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0383 - mean_absolute_error: 0.1431
19. set (Dataset 12) being trained for epoch 2 by 2019-01-25 22:01:16.758834!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0573 - mean_absolute_error: 0.1743
20. set (Dataset 16) being trained for epoch 2 by 2019-01-25 22:01:44.383620!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0551 - mean_absolute_error: 0.1612
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-25 22:02:11.547986
1. set (Dataset 19) being trained for epoch 3 by 2019-01-25 22:02:16.425644!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0368 - mean_absolute_error: 0.1389
2. set (Dataset 16) being trained for epoch 3 by 2019-01-25 22:02:37.497128!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0509 - mean_absolute_error: 0.1572
3. set (Dataset 21) being trained for epoch 3 by 2019-01-25 22:03:06.680904!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0699 - mean_absolute_error: 0.1867
4. set (Dataset 15) being trained for epoch 3 by 2019-01-25 22:03:28.811506!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0567 - mean_absolute_error: 0.1556
5. set (Dataset 13) being trained for epoch 3 by 2019-01-25 22:03:50.112380!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0418 - mean_absolute_error: 0.1456
6. set (Dataset 12) being trained for epoch 3 by 2019-01-25 22:04:09.586942!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0509 - mean_absolute_error: 0.1682
7. set (Dataset 18) being trained for epoch 3 by 2019-01-25 22:04:33.888146!
Epoch 1/1
614/614 [==============================] - 14s 24ms/step - loss: 0.0661 - mean_absolute_error: 0.1946
8. set (Dataset 22) being trained for epoch 3 by 2019-01-25 22:04:54.862167!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0295 - mean_absolute_error: 0.1102
9. set (Dataset 23) being trained for epoch 3 by 2019-01-25 22:05:17.409987!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0665 - mean_absolute_error: 0.1810
10. set (Dataset 8) being trained for epoch 3 by 2019-01-25 22:05:39.363474!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0702 - mean_absolute_error: 0.1920
11. set (Dataset 17) being trained for epoch 3 by 2019-01-25 22:06:03.228587!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0334 - mean_absolute_error: 0.1246
12. set (Dataset 6) being trained for epoch 3 by 2019-01-25 22:06:18.566099!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0733 - mean_absolute_error: 0.1805
13. set (Dataset 24) being trained for epoch 3 by 2019-01-25 22:06:36.997875!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0282 - mean_absolute_error: 0.1100
14. set (Dataset 11) being trained for epoch 3 by 2019-01-25 22:06:55.263270!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0447 - mean_absolute_error: 0.1384
15. set (Dataset 10) being trained for epoch 3 by 2019-01-25 22:07:17.480090!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0610 - mean_absolute_error: 0.1749
16. set (Dataset 20) being trained for epoch 3 by 2019-01-25 22:07:41.197798!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0413 - mean_absolute_error: 0.1345
17. set (Dataset 2) being trained for epoch 3 by 2019-01-25 22:08:00.202170!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0789 - mean_absolute_error: 0.2122
18. set (Dataset 4) being trained for epoch 3 by 2019-01-25 22:08:20.514494!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0861 - mean_absolute_error: 0.2172
19. set (Dataset 7) being trained for epoch 3 by 2019-01-25 22:08:46.941678!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0812 - mean_absolute_error: 0.2114
20. set (Dataset 1) being trained for epoch 3 by 2019-01-25 22:09:11.292430!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0945 - mean_absolute_error: 0.2180
Epoch 3 completed!
Exp2019-01-25_21-47-31.h5 has been saved.
The subjects are trained: [(19, 'M11'), (16, 'M09'), (21, 'F02'), (15, 'F03'), (13, 'M07'), (12, 'M06'), (18
, 'F05'), (22, 'M01'), (23, 'M13'), (8, 'M02'), (17, 'M10'), (6, 'F06'), (24, 'M14'), (11, 'M05'), (10, 'M04
'), (20, 'M12'), (2, 'F02'), (4, 'F04'), (7, 'M01'), (1, 'F01')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-25_21-47-31
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 22:09:26.458440
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.59 Degree
        The absolute mean error on Yaw angle estimation: 31.54 Degree
        The absolute mean error on Roll angle estimation: 8.00 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 18.98 Degree
        The absolute mean error on Yaw angle estimation: 29.77 Degree
        The absolute mean error on Roll angle estimation: 3.78 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 20.78 Degree
        The absolute mean error on Yaw angle estimation: 25.91 Degree
        The absolute mean error on Roll angle estimation: 8.00 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 29.81 Degree
        The absolute mean error on Yaw angle estimation: 34.52 Degree
        The absolute mean error on Roll angle estimation: 14.20 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 22.04 Degree
        The absolute mean error on Yaw angle estimations: 30.44 Degree
        The absolute mean error on Roll angle estimations: 8.49 Degree
subject3_Exp2019-01-25_21-47-31.png has been saved by 2019-01-25 22:10:51.023742.
subject5_Exp2019-01-25_21-47-31.png has been saved by 2019-01-25 22:10:51.212333.
subject9_Exp2019-01-25_21-47-31.png has been saved by 2019-01-25 22:10:51.400980.
subject14_Exp2019-01-25_21-47-31.png has been saved by 2019-01-25 22:10:51.609004.
Model Exp2019-01-25_21-47-31 has been evaluated successfully.
Model Exp2019-01-25_21-47-31 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit -m "still 30 deg
ree on yaw"
[master 70b8c9e] still 30 degree on yaw
 66 files changed, 2133 insertions(+), 2316 deletions(-)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/output_Exp2019-01-25_19-22-56.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject14_Exp2019-01-25_19-22-5
6.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject3_Exp2019-01-25_19-22-56
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject5_Exp2019-01-25_19-22-56
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject9_Exp2019-01-25_19-22-56
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/output_Exp2019-01-25_19-31-28.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject14_Exp2019-01-25_19-31-2
8.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject3_Exp2019-01-25_19-31-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject5_Exp2019-01-25_19-31-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject9_Exp2019-01-25_19-31-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/output_
Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
14_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
3_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
5_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
9_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/output_Exp2019-01-25_19-51-44.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject14_Exp2019-01-25_19-51-4
4.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject3_Exp2019-01-25_19-51-44
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject5_Exp2019-01-25_19-51-44
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject9_Exp2019-01-25_19-51-44
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/output_Exp2019-01-25_20-09-28.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject14_Exp2019-01-25_20-09-2
8.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject3_Exp2019-01-25_20-09-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject5_Exp2019-01-25_20-09-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject9_Exp2019-01-25_20-09-28
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-19-34/output_Exp2019-01-25_20-19-34.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-19-34/subject14_Exp2019-01-25_20-19-3
4.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-19-34/subject3_Exp2019-01-25_20-19-34
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-19-34/subject5_Exp2019-01-25_20-19-34
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-19-34/subject9_Exp2019-01-25_20-19-34
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/output_Exp2019-01-25_20-27-45.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject14_Exp2019-01-25_20-27-4
5.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject3_Exp2019-01-25_20-27-45
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject5_Exp2019-01-25_20-27-45
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject9_Exp2019-01-25_20-27-45
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/output_Exp2019-01-25_20-37-42.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject14_Exp2019-01-25_20-37-4
2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject3_Exp2019-01-25_20-37-42
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject5_Exp2019-01-25_20-37-42
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject9_Exp2019-01-25_20-37-42
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/output_Exp2019-01-25_20-48-22.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject14_Exp2019-01-25_20-48-2
2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject3_Exp2019-01-25_20-48-22
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject5_Exp2019-01-25_20-48-22
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject9_Exp2019-01-25_20-48-22
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/output_Exp2019-01-25_21-04-25.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject14_Exp2019-01-25_21-04-2
5.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject3_Exp2019-01-25_21-04-25
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject5_Exp2019-01-25_21-04-25
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject9_Exp2019-01-25_21-04-25
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-15-24/output_Exp2019-01-25_21-15-24.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/output_Exp2019-01-25_21-16-36.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject14_Exp2019-01-25_21-16-3
6.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject3_Exp2019-01-25_21-16-36
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject5_Exp2019-01-25_21-16-36
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject9_Exp2019-01-25_21-16-36
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/output_Exp2019-01-25_21-47-31.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject14_Exp2019-01-25_21-47-3
1.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject3_Exp2019-01-25_21-47-31
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject5_Exp2019-01-25_21-47-31
.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject9_Exp2019-01-25_21-47-31
.png
 rewrite DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_/output_Last_Model.txt (100%)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model___/output_Last_Model.txt
 copy DeepRL_For_HPE/LSTM_VGG16/results/{Last_Model_ => Last_Model____}/output_Last_Model.txt (100%)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 86, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (82/82), done.
Writing objects: 100% (86/86), 6.94 MiB | 978.00 KiB/s, done.
Total 86 (delta 17), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   03447ea..70b8c9e  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
remote: Counting objects: 16, done.
remote: Compressing objects: 100% (16/16), done.
remote: Total 16 (delta 11), reused 0 (delta 0)
Unpacking objects: 100% (16/16), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   70b8c9e..dc6659e  master     -> origin/master
Updating 70b8c9e..dc6659e
Fast-forward
 DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py                      |  43 +++++++++++++
 DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16.pyproj                         |  15 +++--
 DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py                       | 126 ++++++++++++++++++---------------
---
 DeepRL_For_HPE/LSTM_VGG16/Quick_Scripts/LSTM_VGG16Helper.py         | 142 +++++++++++++++++++++++++++++++++
++++++++
 DeepRL_For_HPE/LSTM_VGG16/{ => Quick_Scripts}/tf_trainLSTM_VGG16.py |   0
 DeepRL_For_HPE/LSTM_VGG16/{ => Quick_Scripts}/trainLSTM_VGG16.py    |   0
 DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py                            | 137 ++++++++-------------------------
------
 DeepRL_For_HPE/Note_Files/commands.txt                              |   2 +-
 8 files changed, 287 insertions(+), 178 deletions(-)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/Quick_Scripts/LSTM_VGG16Helper.py
 rename DeepRL_For_HPE/LSTM_VGG16/{ => Quick_Scripts}/tf_trainLSTM_VGG16.py (100%)
 rename DeepRL_For_HPE/LSTM_VGG16/{ => Quick_Scripts}/trainLSTM_VGG16.py (100%)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 22:43:58.294630: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 22:43:58.391558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 22:43:58.391820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 22:43:58.391837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 22:43:58.547272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 22:43:58.547299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 22:43:58.547304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 22:43:58.547446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_22-43-59 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_22-43-59
All frames and annotations from 1 datasets have been read by 2019-01-25 22:44:00.155786
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 22:44:09.060969!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.0663 - mean_absolute_error: 0.1985
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 22:44:34.646643
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 22:44:43.529028!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0631 - mean_absolute_error: 0.1925
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_22-43-59
The subjects will be tested: [(1, 'F01')]
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 75, in <module>
    main()
  File "runCNN_LSTM.py", line 72, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 62, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runCNN_LSTM.py", line 45, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, record = record)
TypeError: evaluateCNN_LSTM() missing 1 required positional argument: 'angles'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 22:47:49.568788: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 22:47:49.664608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 22:47:49.664870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 22:47:49.664884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 22:47:49.820352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 22:47:49.820378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 22:47:49.820386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 22:47:49.820529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_22-47-50 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
1000_2019-01-25_22-47-50
All frames and annotations from 1 datasets have been read by 2019-01-25 22:47:51.435681
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 22:48:00.345485!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.0636 - mean_absolute_error: 0.1922
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 22:48:26.090188
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 22:48:34.981978!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0688 - mean_absolute_error: 0.1982
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
001000_2019-01-25_22-47-50
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 22:48:58.002920
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.33 Degree
        The absolute mean error on Yaw angle estimation: 31.67 Degree
        The absolute mean error on Roll angle estimation: 24.33 Degree
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 75, in <module>
    main()
  File "runCNN_LSTM.py", line 72, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 62, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runCNN_LSTM.py", line 45, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record =
record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 142, in evaluateCNN_LSTM
    means = evaluateAverage(results, angles, num_outputs, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 125, in evaluateAverage
    for an, (matrix, absolute_mean_error) in enumerate(outputs):
TypeError: 'Sequential' object is not iterable
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 22:52:43.242250: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 22:52:43.339347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 22:52:43.339645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 22:52:43.339660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 22:52:43.494786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 22:52:43.494813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 22:52:43.494818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 22:52:43.495000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_22-52-44 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
1000_2019-01-25_22-52-44
All frames and annotations from 1 datasets have been read by 2019-01-25 22:52:45.109564
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 22:52:53.994251!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.1062 - mean_absolute_error: 0.2506
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 22:53:18.521622
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 22:53:27.286714!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0629 - mean_absolute_error: 0.1905
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
001000_2019-01-25_22-52-44
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 22:53:50.723118
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.70 Degree
        The absolute mean error on Yaw angle estimation: 31.72 Degree
        The absolute mean error on Roll angle estimation: 5.10 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.70 Degree
        The absolute mean error on Yaw angle estimations: 31.72 Degree
        The absolute mean error on Roll angle estimations: 5.10 Degree
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
1000_2019-01-25_22-52-44
All frames and annotations from 1 datasets have been read by 2019-01-25 22:54:04.208775
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 22:54:12.969114!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0633 - mean_absolute_error: 0.1910
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 22:54:35.730690
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 22:54:44.504656!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0633 - mean_absolute_error: 0.1912
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
001000_2019-01-25_22-52-44
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 22:55:07.704292
For the Subject 1 (F01):
498/498 [==============================] - 7s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.80 Degree
        The absolute mean error on Yaw angle estimation: 32.12 Degree
        The absolute mean error on Roll angle estimation: 6.22 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.80 Degree
        The absolute mean error on Yaw angle estimations: 32.12 Degree
        The absolute mean error on Roll angle estimations: 6.22 Degree
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 75, in <module>
    main()
  File "runCNN_LSTM.py", line 72, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 63, in runCNN_LSTM
    if out_epochs % eveva_epoch > 0:
NameError: name 'eveva_epoch' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 22:59:56.942864: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 22:59:57.040713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 22:59:57.040970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 22:59:57.040982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 22:59:57.197018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 22:59:57.197044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 22:59:57.197048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 22:59:57.197185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_22-59-57 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
1000_2019-01-25_22-59-57
All frames and annotations from 1 datasets have been read by 2019-01-25 22:59:58.854500
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:00:07.744985!
Epoch 1/1
596/882 [===================>..........] - ETA: 7s - loss: 0.1282 - mean_absolute_error: 0.2862^C
Model Exp2019-01-25_22-59-57_part1 has been interrupted.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:00:58.014848: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:00:58.110500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:00:58.110755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:00:58.110767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:00:58.265932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:00:58.265958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:00:58.265963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:00:58.266100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-00-59 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0010_2019-01-25_23-00-59
All frames and annotations from 1 datasets have been read by 2019-01-25 23:00:59.914507
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:01:08.814662!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 0.0718 - mean_absolute_error: 0.2086
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:01:33.858489
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:01:42.739566!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0705 - mean_absolute_error: 0.2120
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000010_2019-01-25_23-00-59
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:02:05.736254
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.29 Degree
        The absolute mean error on Yaw angle estimation: 31.85 Degree
        The absolute mean error on Roll angle estimation: 4.81 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.29 Degree
        The absolute mean error on Yaw angle estimations: 31.85 Degree
        The absolute mean error on Roll angle estimations: 4.81 Degree
Epoch 1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0010_2019-01-25_23-00-59
All frames and annotations from 1 datasets have been read by 2019-01-25 23:02:19.241711
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:02:28.113870!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0683 - mean_absolute_error: 0.2099
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:02:51.571476
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:03:00.459940!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0678 - mean_absolute_error: 0.2101
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000010_2019-01-25_23-00-59
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:03:23.838961
For the Subject 1 (F01):
498/498 [==============================] - 7s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.35 Degree
        The absolute mean error on Yaw angle estimation: 31.85 Degree
        The absolute mean error on Roll angle estimation: 6.07 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.35 Degree
        The absolute mean error on Yaw angle estimations: 31.85 Degree
        The absolute mean error on Roll angle estimations: 6.07 Degree
Epoch 2 completed!
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 67, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, out_epochs %
 eveva_epoch, record = False)
NameError: name 'eveva_epoch' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:06:19.237234: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:06:19.333499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:06:19.333768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:06:19.333781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:06:19.491005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:06:19.491033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:06:19.491041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:06:19.491185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-06-20 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-06-20
All frames and annotations from 1 datasets have been read by 2019-01-25 23:06:21.111882
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:06:30.014348!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 0.0667 - mean_absolute_error: 0.1991
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:06:55.158412
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:07:04.027475!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0630 - mean_absolute_error: 0.1920
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-06-20
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:07:26.705600
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.07 Degree
        The absolute mean error on Yaw angle estimation: 31.70 Degree
        The absolute mean error on Roll angle estimation: 5.59 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.07 Degree
        The absolute mean error on Yaw angle estimations: 31.70 Degree
        The absolute mean error on Roll angle estimations: 5.59 Degree
Epoch 1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-06-20
All frames and annotations from 1 datasets have been read by 2019-01-25 23:07:40.269742
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:07:49.140360!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0626 - mean_absolute_error: 0.1900
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:08:12.902864
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:08:21.776641!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0626 - mean_absolute_error: 0.1900
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-06-20
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:08:45.235755
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.17 Degree
        The absolute mean error on Yaw angle estimation: 31.84 Degree
        The absolute mean error on Roll angle estimation: 5.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.17 Degree
        The absolute mean error on Yaw angle estimations: 31.84 Degree
        The absolute mean error on Roll angle estimations: 5.30 Degree
Epoch 2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-06-20
All frames and annotations from 1 datasets have been read by 2019-01-25 23:08:58.721743
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:09:07.606753!
Epoch 1/1
882/882 [==============================] - 22s 24ms/step - loss: 0.0626 - mean_absolute_error: 0.1900
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-06-20
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:09:30.025457
For the Subject 1 (F01):
498/498 [==============================] - 7s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.16 Degree
        The absolute mean error on Yaw angle estimation: 31.82 Degree
        The absolute mean error on Roll angle estimation: 5.44 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.16 Degree
        The absolute mean error on Yaw angle estimations: 31.82 Degree
        The absolute mean error on Roll angle estimations: 5.44 Degree
Epoch 2 completed!
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 70, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py", lin
e 36, in drawResults
    f = drawPlotsForSubj(outputs, subject, BIWI_Subject_IDs[subject], modelStr, angles = angles)
TypeError: drawPlotsForSubj() missing 1 required positional argument: 'num_outputs'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 17, in <module>
    from EstimationPlotter import *
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py", lin
e 36
    f = drawPlotsForSubj(outputs, subject, BIWI_Subject_IDs[subject], modelStr num_outputs, angles = angles)
                                                                                         ^
SyntaxError: invalid syntax
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:12:13.543751: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:12:13.640583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:12:13.640838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:12:13.640850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:12:13.796263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:12:13.796288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:12:13.796292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:12:13.796429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-12-14 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   17408
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    195
=================================================================
Total params: 138,476,550
Trainable params: 4,216,006
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-12-14
All frames and annotations from 1 datasets have been read by 2019-01-25 23:12:15.423206
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:12:24.324229!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.0650 - mean_absolute_error: 0.1958
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:12:49.845610
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:12:58.708879!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.0631 - mean_absolute_error: 0.1903
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-12-14
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:13:22.308549
For the Subject 1 (F01):
498/498 [==============================] - 8s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.25 Degree
        The absolute mean error on Yaw angle estimation: 31.83 Degree
        The absolute mean error on Roll angle estimation: 5.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.25 Degree
        The absolute mean error on Yaw angle estimations: 31.83 Degree
        The absolute mean error on Roll angle estimations: 5.37 Degree
Epoch 1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-12-14
All frames and annotations from 1 datasets have been read by 2019-01-25 23:13:35.872580
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:13:44.759136!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0628 - mean_absolute_error: 0.1902
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:14:07.558166
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:14:16.432293!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0628 - mean_absolute_error: 0.1900
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-12-14
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:14:39.476042
For the Subject 1 (F01):
498/498 [==============================] - 7s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.16 Degree
        The absolute mean error on Yaw angle estimation: 31.81 Degree
        The absolute mean error on Roll angle estimation: 5.35 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.16 Degree
        The absolute mean error on Yaw angle estimations: 31.81 Degree
        The absolute mean error on Roll angle estimations: 5.35 Degree
Epoch 2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-12-14
All frames and annotations from 1 datasets have been read by 2019-01-25 23:14:52.903867
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:15:01.796306!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.0627 - mean_absolute_error: 0.1902
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-12-14
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:15:24.569932
For the Subject 1 (F01):
498/498 [==============================] - 7s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.45 Degree
        The absolute mean error on Yaw angle estimation: 31.93 Degree
        The absolute mean error on Roll angle estimation: 5.24 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.45 Degree
        The absolute mean error on Yaw angle estimations: 31.93 Degree
        The absolute mean error on Roll angle estimations: 5.24 Degree
Epoch 2 completed!
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 70, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py", lin
e 36, in drawResults
    f = drawPlotsForSubj(outputs, subject, BIWI_Subject_IDs[subject], modelStr, num_outputs, angles = angles
)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EstimationPlotter.py", lin
e 18, in drawPlotsForSubj
    f, rows = plt.subplots(num_outputs, 1, sharey=True, sharex=True, figsize=(16, 3*num_outputs))
NameError: name 'plt' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:18:17.536312: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:18:17.634453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:18:17.634751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:18:17.634765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:18:17.790338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:18:17.790366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:18:17.790373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:18:17.790555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-18-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   17408
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    195
=================================================================
Total params: 138,476,550
Trainable params: 4,216,006
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-18-18
All frames and annotations from 1 datasets have been read by 2019-01-25 23:18:19.375663
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:18:28.291877!
Epoch 1/1
882/882 [==============================] - 19s 22ms/step - loss: 0.0642 - mean_absolute_error: 0.1937
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:18:48.972863
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:18:57.875567!
Epoch 1/1
882/882 [==============================] - 18s 20ms/step - loss: 0.0629 - mean_absolute_error: 0.1905
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-18-18
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:19:16.507094
For the Subject 1 (F01):
498/498 [==============================] - 5s 10ms/step
        The absolute mean error on Pitch angle estimation: 23.98 Degree
        The absolute mean error on Yaw angle estimation: 31.75 Degree
        The absolute mean error on Roll angle estimation: 5.10 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 23.98 Degree
        The absolute mean error on Yaw angle estimations: 31.75 Degree
        The absolute mean error on Roll angle estimations: 5.10 Degree
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 63, in runCNN_LSTM
    printLog('%S completed!' % (modelID), record = record)
ValueError: unsupported format character 'S' (0x53) at index 1
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:20:21.120619: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:20:21.218282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:20:21.218540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:20:21.218552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:20:21.375862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:20:21.375888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:20:21.375893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:20:21.376030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-20-22 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   17408
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    195
=================================================================
Total params: 138,476,550
Trainable params: 4,216,006
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 5
eva_epoch = 2
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [1] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-20-22
All frames and annotations from 1 datasets have been read by 2019-01-25 23:20:22.935868
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:20:31.845298!
Epoch 1/1
882/882 [==============================] - 19s 22ms/step - loss: 0.0645 - mean_absolute_error: 0.1942
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:20:52.761842
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:21:01.644105!
Epoch 1/1
882/882 [==============================] - 18s 20ms/step - loss: 0.0629 - mean_absolute_error: 0.1907
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-20-22
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:21:20.553619
For the Subject 1 (F01):
498/498 [==============================] - 5s 10ms/step
        The absolute mean error on Pitch angle estimation: 24.19 Degree
        The absolute mean error on Yaw angle estimation: 31.60 Degree
        The absolute mean error on Roll angle estimation: 5.39 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.19 Degree
        The absolute mean error on Yaw angle estimations: 31.60 Degree
        The absolute mean error on Roll angle estimations: 5.39 Degree
Exp2019-01-25_23-20-22_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-20-22
All frames and annotations from 1 datasets have been read by 2019-01-25 23:21:31.254096
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:21:40.155659!
Epoch 1/1
882/882 [==============================] - 18s 20ms/step - loss: 0.0628 - mean_absolute_error: 0.1901
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-25 23:21:58.987009
1. set (Dataset 9) being trained for epoch 2 by 2019-01-25 23:22:07.894580!
Epoch 1/1
882/882 [==============================] - 18s 21ms/step - loss: 0.0629 - mean_absolute_error: 0.1903
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-20-22
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:22:27.014083
For the Subject 1 (F01):
498/498 [==============================] - 5s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.12 Degree
        The absolute mean error on Yaw angle estimation: 31.75 Degree
        The absolute mean error on Roll angle estimation: 5.67 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.12 Degree
        The absolute mean error on Yaw angle estimations: 31.75 Degree
        The absolute mean error on Roll angle estimations: 5.67 Degree
Exp2019-01-25_23-20-22_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.00
0100_2019-01-25_23-20-22
All frames and annotations from 1 datasets have been read by 2019-01-25 23:22:37.658062
1. set (Dataset 9) being trained for epoch 1 by 2019-01-25 23:22:46.575975!
Epoch 1/1
882/882 [==============================] - 18s 21ms/step - loss: 0.0628 - mean_absolute_error: 0.1903
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs5_AdamOpt_lr-0.
000100_2019-01-25_23-20-22
The subjects will be tested: [(1, 'F01')]
All frames and annotations from 1 datasets have been read by 2019-01-25 23:23:05.615915
For the Subject 1 (F01):
498/498 [==============================] - 5s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.08 Degree
        The absolute mean error on Yaw angle estimation: 31.73 Degree
        The absolute mean error on Roll angle estimation: 5.43 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.08 Degree
        The absolute mean error on Yaw angle estimations: 31.73 Degree
        The absolute mean error on Roll angle estimations: 5.43 Degree
Exp2019-01-25_23-20-22_part2 completed!
subject1_Exp2019-01-25_23-20-22_part2.png has been saved by 2019-01-25 23:23:15.593821.
Model Exp2019-01-25_23-20-22 has been evaluated successfully.
Model Exp2019-01-25_23-20-22 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:26:52.699524: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:26:52.795824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:26:52.796089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:26:52.796106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:26:52.951421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:26:52.951447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:26:52.951452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:26:52.951590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-26-53 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   17408
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    195
=================================================================
Total params: 138,476,550
Trainable params: 4,216,006
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-25_23-26-53
All frames and annotations from 20 datasets have been read by 2019-01-25 23:26:58.287309
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 23:27:04.698955!
Epoch 1/1
665/665 [==============================] - 15s 22ms/step - loss: 0.0317 - mean_absolute_error: 0.1148
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 23:27:24.838512!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.0287 - mean_absolute_error: 0.1120
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 23:27:41.096783!
Epoch 1/1
654/654 [==============================] - 13s 20ms/step - loss: 0.0562 - mean_absolute_error: 0.1518
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 23:27:59.319158!
Epoch 1/1
502/502 [==============================] - 10s 21ms/step - loss: 0.0327 - mean_absolute_error: 0.1324
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 23:28:17.424579!
Epoch 1/1
772/772 [==============================] - 16s 20ms/step - loss: 0.0673 - mean_absolute_error: 0.1857
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 23:28:38.465648!
Epoch 1/1
569/569 [==============================] - 11s 20ms/step - loss: 0.0716 - mean_absolute_error: 0.1945
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 23:28:56.086104!
Epoch 1/1
634/634 [==============================] - 13s 20ms/step - loss: 0.0695 - mean_absolute_error: 0.1862
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 23:29:17.906106!
Epoch 1/1
914/914 [==============================] - 18s 20ms/step - loss: 0.0494 - mean_absolute_error: 0.1494
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 23:29:43.894356!
Epoch 1/1
745/745 [==============================] - 15s 21ms/step - loss: 0.0827 - mean_absolute_error: 0.2127
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 23:30:06.685890!
Epoch 1/1
732/732 [==============================] - 15s 20ms/step - loss: 0.0495 - mean_absolute_error: 0.1657
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 23:30:28.890471!
Epoch 1/1
726/726 [==============================] - 15s 20ms/step - loss: 0.0597 - mean_absolute_error: 0.1712
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 23:30:48.931493!
Epoch 1/1
498/498 [==============================] - 10s 20ms/step - loss: 0.0899 - mean_absolute_error: 0.2170
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 23:31:05.156695!
Epoch 1/1
614/614 [==============================] - 13s 20ms/step - loss: 0.0648 - mean_absolute_error: 0.1906
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 23:31:22.881400!
Epoch 1/1
511/511 [==============================] - 10s 20ms/step - loss: 0.0834 - mean_absolute_error: 0.2166
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 23:31:40.776040!
Epoch 1/1
744/744 [==============================] - 15s 20ms/step - loss: 0.0813 - mean_absolute_error: 0.2117
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 23:32:01.398077!
Epoch 1/1
556/556 [==============================] - 11s 21ms/step - loss: 0.0420 - mean_absolute_error: 0.1393
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 23:32:16.730034!
Epoch 1/1
395/395 [==============================] - 8s 21ms/step - loss: 0.0250 - mean_absolute_error: 0.1037
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 23:32:30.118816!
Epoch 1/1
542/542 [==============================] - 11s 20ms/step - loss: 0.0726 - mean_absolute_error: 0.1820
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 23:32:45.971645!
Epoch 1/1
485/485 [==============================] - 10s 21ms/step - loss: 0.0407 - mean_absolute_error: 0.1458
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 23:33:01.749333!
Epoch 1/1
572/572 [==============================] - 12s 21ms/step - loss: 0.0447 - mean_absolute_error: 0.1389
Epoch 1 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-25_23-26-53
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 23:33:15.725960
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 16.64 Degree
        The absolute mean error on Yaw angle estimation: 32.31 Degree
        The absolute mean error on Roll angle estimation: 10.70 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 21.65 Degree
        The absolute mean error on Yaw angle estimation: 29.76 Degree
        The absolute mean error on Roll angle estimation: 5.60 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.25 Degree
        The absolute mean error on Yaw angle estimation: 26.27 Degree
        The absolute mean error on Roll angle estimation: 9.45 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 23.25 Degree
        The absolute mean error on Yaw angle estimation: 34.71 Degree
        The absolute mean error on Roll angle estimation: 14.17 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.95 Degree
        The absolute mean error on Yaw angle estimations: 30.76 Degree
        The absolute mean error on Roll angle estimations: 9.98 Degree
Exp2019-01-25_23-26-53_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-25_23-26-53
All frames and annotations from 20 datasets have been read by 2019-01-25 23:34:25.937931
1. set (Dataset 6) being trained for epoch 1 by 2019-01-25 23:34:31.128874!
Epoch 1/1
542/542 [==============================] - 11s 20ms/step - loss: 0.0735 - mean_absolute_error: 0.1812
2. set (Dataset 11) being trained for epoch 1 by 2019-01-25 23:34:48.020397!
Epoch 1/1
572/572 [==============================] - 12s 21ms/step - loss: 0.0433 - mean_absolute_error: 0.1339
3. set (Dataset 10) being trained for epoch 1 by 2019-01-25 23:35:07.063689!
Epoch 1/1
726/726 [==============================] - 15s 21ms/step - loss: 0.0608 - mean_absolute_error: 0.1744
4. set (Dataset 4) being trained for epoch 1 by 2019-01-25 23:35:29.498193!
Epoch 1/1
744/744 [==============================] - 15s 20ms/step - loss: 0.0783 - mean_absolute_error: 0.2084
5. set (Dataset 23) being trained for epoch 1 by 2019-01-25 23:35:50.268723!
Epoch 1/1
569/569 [==============================] - 12s 21ms/step - loss: 0.0710 - mean_absolute_error: 0.1916
6. set (Dataset 13) being trained for epoch 1 by 2019-01-25 23:36:06.874644!
Epoch 1/1
485/485 [==============================] - 10s 20ms/step - loss: 0.0385 - mean_absolute_error: 0.1398
7. set (Dataset 17) being trained for epoch 1 by 2019-01-25 23:36:20.559054!
Epoch 1/1
395/395 [==============================] - 8s 20ms/step - loss: 0.0288 - mean_absolute_error: 0.1132
8. set (Dataset 1) being trained for epoch 1 by 2019-01-25 23:36:33.691170!
Epoch 1/1
498/498 [==============================] - 10s 21ms/step - loss: 0.0952 - mean_absolute_error: 0.2196
9. set (Dataset 8) being trained for epoch 1 by 2019-01-25 23:36:51.821482!
Epoch 1/1
772/772 [==============================] - 16s 21ms/step - loss: 0.0659 - mean_absolute_error: 0.1820
10. set (Dataset 7) being trained for epoch 1 by 2019-01-25 23:37:15.286184!
Epoch 1/1
745/745 [==============================] - 15s 21ms/step - loss: 0.0805 - mean_absolute_error: 0.2113
11. set (Dataset 21) being trained for epoch 1 by 2019-01-25 23:37:36.700779!
Epoch 1/1
634/634 [==============================] - 13s 20ms/step - loss: 0.0735 - mean_absolute_error: 0.1942
12. set (Dataset 22) being trained for epoch 1 by 2019-01-25 23:37:55.902630!
Epoch 1/1
665/665 [==============================] - 14s 20ms/step - loss: 0.0294 - mean_absolute_error: 0.1088
13. set (Dataset 2) being trained for epoch 1 by 2019-01-25 23:38:14.529027!
Epoch 1/1
511/511 [==============================] - 10s 20ms/step - loss: 0.0752 - mean_absolute_error: 0.2036
14. set (Dataset 24) being trained for epoch 1 by 2019-01-25 23:38:29.534510!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.0462 - mean_absolute_error: 0.1460
15. set (Dataset 15) being trained for epoch 1 by 2019-01-25 23:38:45.876440!
Epoch 1/1
654/654 [==============================] - 13s 20ms/step - loss: 0.0559 - mean_absolute_error: 0.1518
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 23:39:04.482983!
Epoch 1/1
556/556 [==============================] - 11s 20ms/step - loss: 0.0395 - mean_absolute_error: 0.1294
17. set (Dataset 18) being trained for epoch 1 by 2019-01-25 23:39:21.751709!
Epoch 1/1
614/614 [==============================] - 13s 20ms/step - loss: 0.0578 - mean_absolute_error: 0.1765
18. set (Dataset 19) being trained for epoch 1 by 2019-01-25 23:39:39.296934!
Epoch 1/1
502/502 [==============================] - 10s 20ms/step - loss: 0.0350 - mean_absolute_error: 0.1368
19. set (Dataset 12) being trained for epoch 1 by 2019-01-25 23:39:56.817267!
Epoch 1/1
732/732 [==============================] - 15s 20ms/step - loss: 0.0525 - mean_absolute_error: 0.1688
20. set (Dataset 16) being trained for epoch 1 by 2019-01-25 23:40:20.599993!
Epoch 1/1
914/914 [==============================] - 19s 20ms/step - loss: 0.0540 - mean_absolute_error: 0.1578
Epoch 1 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (23, 'M13'), (13, 'M07'), (17,
'M10'), (1, 'F01'), (8, 'M02'), (7, 'M01'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'),
(20, 'M12'), (18, 'F05'), (19, 'M11'), (12, 'M06'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-25_23-26-53
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 23:40:41.373262
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 19.66 Degree
        The absolute mean error on Yaw angle estimation: 32.57 Degree
        The absolute mean error on Roll angle estimation: 7.82 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 30.65 Degree
        The absolute mean error on Yaw angle estimation: 29.77 Degree
        The absolute mean error on Roll angle estimation: 3.69 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.64 Degree
        The absolute mean error on Yaw angle estimation: 26.40 Degree
        The absolute mean error on Roll angle estimation: 7.91 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 21.48 Degree
        The absolute mean error on Yaw angle estimation: 34.80 Degree
        The absolute mean error on Roll angle estimation: 14.20 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 23.11 Degree
        The absolute mean error on Yaw angle estimations: 30.89 Degree
        The absolute mean error on Roll angle estimations: 8.40 Degree
Exp2019-01-25_23-26-53_part2 completed!
subject3_Exp2019-01-25_23-26-53_part2.png has been saved by 2019-01-25 23:41:47.432863.
subject5_Exp2019-01-25_23-26-53_part2.png has been saved by 2019-01-25 23:41:47.622836.
subject9_Exp2019-01-25_23-26-53_part2.png has been saved by 2019-01-25 23:41:47.812736.
subject14_Exp2019-01-25_23-26-53_part2.png has been saved by 2019-01-25 23:41:48.021442.
Model Exp2019-01-25_23-26-53 has been evaluated successfully.
Model Exp2019-01-25_23-26-53 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-25 23:51:55.846169: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-25 23:51:55.942397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-25 23:51:55.942700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-25 23:51:55.942715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-25 23:51:56.098169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-25 23:51:56.098196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-25 23:51:56.098201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-25 23:51:56.098387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-25_23-51-56 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 64)                   17408
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    195
=================================================================
Total params: 134,290,438
Trainable params: 29,894
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 64
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-25_23-51-56
All frames and annotations from 20 datasets have been read by 2019-01-25 23:52:01.163672
1. set (Dataset 22) being trained for epoch 1 by 2019-01-25 23:52:07.567505!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.0284 - mean_absolute_error: 0.1103
2. set (Dataset 24) being trained for epoch 1 by 2019-01-25 23:52:26.035945!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0256 - mean_absolute_error: 0.1092
3. set (Dataset 15) being trained for epoch 1 by 2019-01-25 23:52:41.257450!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0517 - mean_absolute_error: 0.1528
4. set (Dataset 19) being trained for epoch 1 by 2019-01-25 23:52:57.946292!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0336 - mean_absolute_error: 0.1341
5. set (Dataset 8) being trained for epoch 1 by 2019-01-25 23:53:14.725493!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0670 - mean_absolute_error: 0.1859
6. set (Dataset 23) being trained for epoch 1 by 2019-01-25 23:53:33.854577!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0581 - mean_absolute_error: 0.1812
7. set (Dataset 21) being trained for epoch 1 by 2019-01-25 23:53:50.002752!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0618 - mean_absolute_error: 0.1828
8. set (Dataset 16) being trained for epoch 1 by 2019-01-25 23:54:10.427192!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0384 - mean_absolute_error: 0.1441
9. set (Dataset 7) being trained for epoch 1 by 2019-01-25 23:54:34.469446!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0805 - mean_absolute_error: 0.2126
10. set (Dataset 12) being trained for epoch 1 by 2019-01-25 23:54:55.400152!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0499 - mean_absolute_error: 0.1657
11. set (Dataset 10) being trained for epoch 1 by 2019-01-25 23:55:15.823954!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0597 - mean_absolute_error: 0.1710
12. set (Dataset 1) being trained for epoch 1 by 2019-01-25 23:55:33.785566!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0900 - mean_absolute_error: 0.2169
13. set (Dataset 18) being trained for epoch 1 by 2019-01-25 23:55:48.664525!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0493 - mean_absolute_error: 0.1713
14. set (Dataset 2) being trained for epoch 1 by 2019-01-25 23:56:04.716952!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0626 - mean_absolute_error: 0.1856
15. set (Dataset 4) being trained for epoch 1 by 2019-01-25 23:56:21.456084!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0569 - mean_absolute_error: 0.1728
16. set (Dataset 20) being trained for epoch 1 by 2019-01-25 23:56:40.338628!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0442 - mean_absolute_error: 0.1523
17. set (Dataset 17) being trained for epoch 1 by 2019-01-25 23:56:54.275994!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0238 - mean_absolute_error: 0.1115
18. set (Dataset 6) being trained for epoch 1 by 2019-01-25 23:57:06.709594!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0627 - mean_absolute_error: 0.1732
19. set (Dataset 13) being trained for epoch 1 by 2019-01-25 23:57:21.261694!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0305 - mean_absolute_error: 0.1219
20. set (Dataset 11) being trained for epoch 1 by 2019-01-25 23:57:35.894987!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0392 - mean_absolute_error: 0.1316
Epoch 1 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-25_23-51-56
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-25 23:57:48.100614
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.56 Degree
        The absolute mean error on Yaw angle estimation: 31.71 Degree
        The absolute mean error on Roll angle estimation: 9.63 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.73 Degree
        The absolute mean error on Yaw angle estimation: 30.31 Degree
        The absolute mean error on Roll angle estimation: 5.46 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.55 Degree
        The absolute mean error on Yaw angle estimation: 33.82 Degree
        The absolute mean error on Roll angle estimation: 8.28 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 29.63 Degree
        The absolute mean error on Yaw angle estimation: 34.54 Degree
        The absolute mean error on Roll angle estimation: 14.15 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 21.12 Degree
        The absolute mean error on Yaw angle estimations: 32.60 Degree
        The absolute mean error on Roll angle estimations: 9.38 Degree
Exp2019-01-25_23-51-56_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-25_23-51-56
All frames and annotations from 20 datasets have been read by 2019-01-25 23:58:57.637212
1. set (Dataset 6) being trained for epoch 1 by 2019-01-25 23:59:02.824821!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0573 - mean_absolute_error: 0.1679
2. set (Dataset 11) being trained for epoch 1 by 2019-01-25 23:59:18.230182!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0326 - mean_absolute_error: 0.1248
3. set (Dataset 10) being trained for epoch 1 by 2019-01-25 23:59:35.946602!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0596 - mean_absolute_error: 0.1714
4. set (Dataset 4) being trained for epoch 1 by 2019-01-25 23:59:56.431037!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0429 - mean_absolute_error: 0.1518
5. set (Dataset 23) being trained for epoch 1 by 2019-01-26 00:00:15.467587!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0611 - mean_absolute_error: 0.1873
6. set (Dataset 13) being trained for epoch 1 by 2019-01-26 00:00:30.678576!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0279 - mean_absolute_error: 0.1214
7. set (Dataset 17) being trained for epoch 1 by 2019-01-26 00:00:43.155736!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0253 - mean_absolute_error: 0.1174
8. set (Dataset 1) being trained for epoch 1 by 2019-01-26 00:00:55.210216!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0881 - mean_absolute_error: 0.2113
9. set (Dataset 8) being trained for epoch 1 by 2019-01-26 00:01:11.839785!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0485 - mean_absolute_error: 0.1553
10. set (Dataset 7) being trained for epoch 1 by 2019-01-26 00:01:33.587933!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0526 - mean_absolute_error: 0.1677
11. set (Dataset 21) being trained for epoch 1 by 2019-01-26 00:01:52.876115!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0551 - mean_absolute_error: 0.1807
12. set (Dataset 22) being trained for epoch 1 by 2019-01-26 00:02:10.754297!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0214 - mean_absolute_error: 0.1009
13. set (Dataset 2) being trained for epoch 1 by 2019-01-26 00:02:27.903116!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0540 - mean_absolute_error: 0.1757
14. set (Dataset 24) being trained for epoch 1 by 2019-01-26 00:02:41.749679!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0225 - mean_absolute_error: 0.1089
15. set (Dataset 15) being trained for epoch 1 by 2019-01-26 00:02:56.762922!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0415 - mean_absolute_error: 0.1426
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 00:03:14.027861!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0366 - mean_absolute_error: 0.1306
17. set (Dataset 18) being trained for epoch 1 by 2019-01-26 00:03:30.160569!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0406 - mean_absolute_error: 0.1589
18. set (Dataset 19) being trained for epoch 1 by 2019-01-26 00:03:46.043747!
Epoch 1/1
502/502 [==============================] - 9s 19ms/step - loss: 0.0246 - mean_absolute_error: 0.1153
19. set (Dataset 12) being trained for epoch 1 by 2019-01-26 00:04:02.666426!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0426 - mean_absolute_error: 0.1500
20. set (Dataset 16) being trained for epoch 1 by 2019-01-26 00:04:24.565911!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0341 - mean_absolute_error: 0.1388
Epoch 1 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (23, 'M13'), (13, 'M07'), (17,
'M10'), (1, 'F01'), (8, 'M02'), (7, 'M01'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'),
(20, 'M12'), (18, 'F05'), (19, 'M11'), (12, 'M06'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm64_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-25_23-51-56
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 00:04:43.110208
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 16.97 Degree
        The absolute mean error on Yaw angle estimation: 31.64 Degree
        The absolute mean error on Roll angle estimation: 9.60 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.44 Degree
        The absolute mean error on Yaw angle estimation: 28.44 Degree
        The absolute mean error on Roll angle estimation: 5.36 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 19.78 Degree
        The absolute mean error on Yaw angle estimation: 25.99 Degree
        The absolute mean error on Roll angle estimation: 9.10 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 27.69 Degree
        The absolute mean error on Yaw angle estimation: 33.56 Degree
        The absolute mean error on Roll angle estimation: 14.23 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 22.22 Degree
        The absolute mean error on Yaw angle estimations: 29.91 Degree
        The absolute mean error on Roll angle estimations: 9.57 Degree
Exp2019-01-25_23-51-56_part2 completed!
subject3_Exp2019-01-25_23-51-56_part2.png has been saved by 2019-01-26 00:05:48.797204.
subject5_Exp2019-01-25_23-51-56_part2.png has been saved by 2019-01-26 00:05:48.996840.
subject9_Exp2019-01-25_23-51-56_part2.png has been saved by 2019-01-26 00:05:49.189573.
subject14_Exp2019-01-25_23-51-56_part2.png has been saved by 2019-01-26 00:05:49.402624.
Model Exp2019-01-25_23-51-56 has been evaluated successfully.
Model Exp2019-01-25_23-51-56 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit -m "don't use fc
1024"
[master 87e1e1b] don't use fc1024
 28 files changed, 1256 insertions(+), 2419 deletions(-)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-20-22/output_Exp2019-01-25_23-20-22.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-20-22/subject1_Exp2019-01-25_23-20-22
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/output_Exp2019-01-25_23-26-53.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject14_Exp2019-01-25_23-26-5
3_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject3_Exp2019-01-25_23-26-53
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject5_Exp2019-01-25_23-26-53
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject9_Exp2019-01-25_23-26-53
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-51-56/output_Exp2019-01-25_23-51-56.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-51-56/subject14_Exp2019-01-25_23-51-5
6_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-51-56/subject3_Exp2019-01-25_23-51-56
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-51-56/subject5_Exp2019-01-25_23-51-56
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-51-56/subject9_Exp2019-01-25_23-51-56
_part2.png
 rewrite DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_/output_Last_Model.txt (100%)
 rewrite DeepRL_For_HPE/LSTM_VGG16/results/Last_Model____/output_Last_Model.txt (98%)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_____/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model______/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_______/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model________/output_Last_Model.txt
 copy DeepRL_For_HPE/LSTM_VGG16/results/{Last_Model_ => Last_Model_________}/output_Last_Model.txt (100%)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__________/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model___________/output_Last_Model.txt
 copy DeepRL_For_HPE/LSTM_VGG16/results/{Last_Model____ => Last_Model____________}/output_Last_Model.txt (10
0%)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 40, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (32/32), done.
Writing objects: 100% (40/40), 1.09 MiB | 816.00 KiB/s, done.
Total 40 (delta 18), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   dc6659e..87e1e1b  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
Already up-to-date.
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 00:12:41.927566: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 00:12:42.024262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 00:12:42.024563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 00:12:42.024576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 00:12:42.179922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 00:12:42.179948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 00:12:42.179952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 00:12:42.180131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_00-12-42 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-26_00-12-42
All frames and annotations from 20 datasets have been read by 2019-01-26 00:12:47.248707
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 00:12:53.677350!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.1278
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 00:13:12.371593!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.1189
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 00:13:27.688705!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.1498
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 00:13:44.492984!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.1282
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 00:14:01.341359!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.1931
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 00:14:20.931527!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1783
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 00:14:37.216535!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.1835
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 00:14:57.528343!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.1339
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 00:15:21.678287!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.1813
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 00:15:42.379065!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.1422
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 00:16:02.988077!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.1465
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 00:16:21.231374!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.1714
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 00:16:36.366602!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1798
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 00:16:52.773770!
Epoch 1/1
511/511 [==============================] - 10s 19ms/step - loss: 0.1880
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 00:17:09.738784!
Epoch 1/1
744/744 [==============================] - 14s 19ms/step - loss: 0.1499
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 00:17:29.049705!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.1323
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 00:17:42.834604!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.1156
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 00:17:55.400458!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1845
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 00:18:10.234248!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.1023
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 00:18:24.577182!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.1148
Epoch 1 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-26_00-12-42
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 00:18:36.843726
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 7.74 Degree
        The absolute mean error on Yaw angle estimation: 52.97 Degree
        The absolute mean error on Roll angle estimation: 13.95 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.23 Degree
        The absolute mean error on Yaw angle estimation: 27.69 Degree
        The absolute mean error on Roll angle estimation: 10.22 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 31.61 Degree
        The absolute mean error on Yaw angle estimation: 53.48 Degree
        The absolute mean error on Roll angle estimation: 15.95 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 17.83 Degree
        The absolute mean error on Yaw angle estimation: 26.55 Degree
        The absolute mean error on Roll angle estimation: 18.35 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.35 Degree
        The absolute mean error on Yaw angle estimations: 40.17 Degree
        The absolute mean error on Roll angle estimations: 14.62 Degree
Exp2019-01-26_00-12-42_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-26_00-12-42
All frames and annotations from 20 datasets have been read by 2019-01-26 00:19:46.752737
1. set (Dataset 6) being trained for epoch 1 by 2019-01-26 00:19:51.932797!
Epoch 1/1
542/542 [==============================] - 10s 19ms/step - loss: 0.1566
2. set (Dataset 11) being trained for epoch 1 by 2019-01-26 00:20:08.029365!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0954
3. set (Dataset 10) being trained for epoch 1 by 2019-01-26 00:20:25.706267!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.1265
4. set (Dataset 4) being trained for epoch 1 by 2019-01-26 00:20:46.000190!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.1311
5. set (Dataset 23) being trained for epoch 1 by 2019-01-26 00:21:04.791036!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1574
6. set (Dataset 13) being trained for epoch 1 by 2019-01-26 00:21:20.059721!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0925
7. set (Dataset 17) being trained for epoch 1 by 2019-01-26 00:21:32.605965!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.1113
8. set (Dataset 1) being trained for epoch 1 by 2019-01-26 00:21:44.967098!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.1491
9. set (Dataset 8) being trained for epoch 1 by 2019-01-26 00:22:01.463451!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.1194
10. set (Dataset 7) being trained for epoch 1 by 2019-01-26 00:22:23.102969!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.1101
11. set (Dataset 21) being trained for epoch 1 by 2019-01-26 00:22:42.784328!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1594
12. set (Dataset 22) being trained for epoch 1 by 2019-01-26 00:23:00.718334!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.1040
13. set (Dataset 2) being trained for epoch 1 by 2019-01-26 00:23:18.061942!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.1505
14. set (Dataset 24) being trained for epoch 1 by 2019-01-26 00:23:32.049163!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0963
15. set (Dataset 15) being trained for epoch 1 by 2019-01-26 00:23:47.429706!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.1198
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 00:24:04.665255!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.1137
17. set (Dataset 18) being trained for epoch 1 by 2019-01-26 00:24:21.052891!
Epoch 1/1
614/614 [==============================] - 11s 19ms/step - loss: 0.1340
18. set (Dataset 19) being trained for epoch 1 by 2019-01-26 00:24:37.367086!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.1061
19. set (Dataset 12) being trained for epoch 1 by 2019-01-26 00:24:53.797321!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.1087
20. set (Dataset 16) being trained for epoch 1 by 2019-01-26 00:25:15.892507!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.1001
Epoch 1 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (23, 'M13'), (13, 'M07'), (17,
'M10'), (1, 'F01'), (8, 'M02'), (7, 'M01'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'),
(20, 'M12'), (18, 'F05'), (19, 'M11'), (12, 'M06'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-26_00-12-42
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 00:25:34.534668
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.25 Degree
        The absolute mean error on Yaw angle estimation: 37.73 Degree
        The absolute mean error on Roll angle estimation: 16.26 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.21 Degree
        The absolute mean error on Yaw angle estimation: 25.89 Degree
        The absolute mean error on Roll angle estimation: 4.29 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.08 Degree
        The absolute mean error on Yaw angle estimation: 40.89 Degree
        The absolute mean error on Roll angle estimation: 16.39 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 15.42 Degree
        The absolute mean error on Yaw angle estimation: 28.46 Degree
        The absolute mean error on Roll angle estimation: 16.12 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 12.74 Degree
        The absolute mean error on Yaw angle estimations: 33.25 Degree
        The absolute mean error on Roll angle estimations: 13.27 Degree
Exp2019-01-26_00-12-42_part2 completed!
subject3_Exp2019-01-26_00-12-42_part2.png has been saved by 2019-01-26 00:26:40.313523.
subject5_Exp2019-01-26_00-12-42_part2.png has been saved by 2019-01-26 00:26:40.512352.
subject9_Exp2019-01-26_00-12-42_part2.png has been saved by 2019-01-26 00:26:40.711535.
subject14_Exp2019-01-26_00-12-42_part2.png has been saved by 2019-01-26 00:26:40.929039.
Model Exp2019-01-26_00-12-42 has been evaluated successfully.
Model Exp2019-01-26_00-12-42 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
remote: Counting objects: 5, done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 5 (delta 4), reused 0 (delta 0)
Unpacking objects: 100% (5/5), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   87e1e1b..256039d  master     -> origin/master
Updating 87e1e1b..256039d
Fast-forward
 .../output_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.txt          |   464 -
 .../subject14_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png       |   Bin 228286 -> 0 bytes
 .../subject3_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png        |   Bin 184926 -> 0 bytes
 .../subject5_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png        |   Bin 196018 -> 0 bytes
 .../subject9_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png        |   Bin 197645 -> 0 bytes
 .../output_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.txt          |    44 -
 .../subject14_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png       |   Bin 203701 -> 0 bytes
 .../subject3_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png        |   Bin 174982 -> 0 bytes
 .../subject5_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png        |   Bin 175522 -> 0 bytes
 .../subject9_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png        |   Bin 163637 -> 0 bytes
 .../results/Exp2019-01-24_23-58-26/output_Exp2019-01-24_23-58-26.txt   |   144 -
 .../Exp2019-01-24_23-58-26/subject14_Exp2019-01-24_23-58-26.png        |   Bin 202556 -> 0 bytes
 .../results/Exp2019-01-24_23-58-26/subject3_Exp2019-01-24_23-58-26.png |   Bin 152763 -> 0 bytes
 .../results/Exp2019-01-24_23-58-26/subject5_Exp2019-01-24_23-58-26.png |   Bin 184182 -> 0 bytes
 .../results/Exp2019-01-24_23-58-26/subject9_Exp2019-01-24_23-58-26.png |   Bin 178958 -> 0 bytes
 .../output_Exp2019-01-24_23-58-26_and_2019-01-25_01-00-50.txt          |   197 -
 .../output_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.txt          |    71 -
 .../subject14_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png       |   Bin 199709 -> 0 bytes
 .../subject3_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png        |   Bin 178917 -> 0 bytes
 .../subject5_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png        |   Bin 179552 -> 0 bytes
 .../subject9_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png        |   Bin 184311 -> 0 bytes
 .../results/Exp2019-01-25_02-12-13/output_Exp2019-01-25_02-12-13.txt   |   105 -
 .../results/Exp2019-01-25_02-12-13/subject9_Exp2019-01-25_02-12-13.png |   Bin 86502 -> 0 bytes
 .../results/Exp2019-01-25_02-13-30/output_Exp2019-01-25_02-13-30.txt   |   107 -
 .../results/Exp2019-01-25_02-13-30/subject1_Exp2019-01-25_02-13-30.png |   Bin 79968 -> 0 bytes
 .../results/Exp2019-01-25_02-17-13/output_Exp2019-01-25_02-17-13.txt   |   113 -
 .../results/Exp2019-01-25_02-17-13/subject1_Exp2019-01-25_02-17-13.png |   Bin 74133 -> 0 bytes
 .../results/Exp2019-01-25_02-20-23/output_Exp2019-01-25_02-20-23.txt   |   134 -
 .../Exp2019-01-25_02-20-23/subject14_Exp2019-01-25_02-20-23.png        |   Bin 88887 -> 0 bytes
 .../results/Exp2019-01-25_02-20-23/subject3_Exp2019-01-25_02-20-23.png |   Bin 72555 -> 0 bytes
 .../results/Exp2019-01-25_02-20-23/subject5_Exp2019-01-25_02-20-23.png |   Bin 75021 -> 0 bytes
 .../results/Exp2019-01-25_02-20-23/subject9_Exp2019-01-25_02-20-23.png |   Bin 61798 -> 0 bytes
 .../results/Exp2019-01-25_02-37-37/output_Exp2019-01-25_02-37-37.txt   |   176 -
 .../Exp2019-01-25_02-37-37/subject14_Exp2019-01-25_02-37-37.png        |   Bin 99104 -> 0 bytes
 .../results/Exp2019-01-25_02-37-37/subject3_Exp2019-01-25_02-37-37.png |   Bin 91601 -> 0 bytes
 .../results/Exp2019-01-25_02-37-37/subject5_Exp2019-01-25_02-37-37.png |   Bin 97782 -> 0 bytes
 .../results/Exp2019-01-25_02-37-37/subject9_Exp2019-01-25_02-37-37.png |   Bin 92552 -> 0 bytes
 .../results/Exp2019-01-25_03-00-06/output_Exp2019-01-25_03-00-06.txt   |   134 -
 .../Exp2019-01-25_03-00-06/subject14_Exp2019-01-25_03-00-06.png        |   Bin 96855 -> 0 bytes
 .../results/Exp2019-01-25_03-00-06/subject3_Exp2019-01-25_03-00-06.png |   Bin 57903 -> 0 bytes
 .../results/Exp2019-01-25_03-00-06/subject5_Exp2019-01-25_03-00-06.png |   Bin 90819 -> 0 bytes
 .../results/Exp2019-01-25_03-00-06/subject9_Exp2019-01-25_03-00-06.png |   Bin 92457 -> 0 bytes
 .../output_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.txt          |   851 --
 .../subject14_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png       |   Bin 202153 -> 0 bytes
 .../subject3_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png        |   Bin 173973 -> 0 bytes
 .../subject5_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png        |   Bin 174975 -> 0 bytes
 .../subject9_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png        |   Bin 192854 -> 0 bytes
 .../output_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt          |   120 -
 .../scrollback_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt      | 23553 ----------------------------
--
 .../subject14_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png       |   Bin 202136 -> 0 bytes
 .../subject3_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png        |   Bin 173956 -> 0 bytes
 .../subject5_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png        |   Bin 174962 -> 0 bytes
 .../subject9_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png        |   Bin 192849 -> 0 bytes
 .../results/Exp2019-01-25_19-22-56/output_Exp2019-01-25_19-22-56.txt   |   144 -
 .../Exp2019-01-25_19-22-56/subject14_Exp2019-01-25_19-22-56.png        |   Bin 212119 -> 0 bytes
 .../results/Exp2019-01-25_19-22-56/subject3_Exp2019-01-25_19-22-56.png |   Bin 159378 -> 0 bytes
 .../results/Exp2019-01-25_19-22-56/subject5_Exp2019-01-25_19-22-56.png |   Bin 191241 -> 0 bytes
 .../results/Exp2019-01-25_19-22-56/subject9_Exp2019-01-25_19-22-56.png |   Bin 189925 -> 0 bytes
 .../results/Exp2019-01-25_19-31-28/output_Exp2019-01-25_19-31-28.txt   |   144 -
 .../Exp2019-01-25_19-31-28/subject14_Exp2019-01-25_19-31-28.png        |   Bin 202897 -> 0 bytes
 .../results/Exp2019-01-25_19-31-28/subject3_Exp2019-01-25_19-31-28.png |   Bin 156950 -> 0 bytes
 .../results/Exp2019-01-25_19-31-28/subject5_Exp2019-01-25_19-31-28.png |   Bin 181616 -> 0 bytes
 .../results/Exp2019-01-25_19-31-28/subject9_Exp2019-01-25_19-31-28.png |   Bin 177434 -> 0 bytes
 .../output_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.txt          |    92 -
 .../subject14_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png       |   Bin 199480 -> 0 bytes
 .../subject3_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png        |   Bin 162234 -> 0 bytes
 .../subject5_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png        |   Bin 171704 -> 0 bytes
 .../subject9_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png        |   Bin 178818 -> 0 bytes
 .../results/Exp2019-01-25_19-51-44/output_Exp2019-01-25_19-51-44.txt   |   144 -
 .../Exp2019-01-25_19-51-44/subject14_Exp2019-01-25_19-51-44.png        |   Bin 214307 -> 0 bytes
 .../results/Exp2019-01-25_19-51-44/subject3_Exp2019-01-25_19-51-44.png |   Bin 164378 -> 0 bytes
 .../results/Exp2019-01-25_19-51-44/subject5_Exp2019-01-25_19-51-44.png |   Bin 178148 -> 0 bytes
 .../results/Exp2019-01-25_19-51-44/subject9_Exp2019-01-25_19-51-44.png |   Bin 177293 -> 0 bytes
 .../results/Exp2019-01-25_20-09-28/output_Exp2019-01-25_20-09-28.txt   |   148 -
 .../Exp2019-01-25_20-09-28/subject14_Exp2019-01-25_20-09-28.png        |   Bin 221003 -> 0 bytes
 .../results/Exp2019-01-25_20-09-28/subject3_Exp2019-01-25_20-09-28.png |   Bin 171524 -> 0 bytes
 .../results/Exp2019-01-25_20-09-28/subject5_Exp2019-01-25_20-09-28.png |   Bin 174791 -> 0 bytes
 .../results/Exp2019-01-25_20-09-28/subject9_Exp2019-01-25_20-09-28.png |   Bin 167111 -> 0 bytes
 .../results/Exp2019-01-25_20-27-45/output_Exp2019-01-25_20-27-45.txt   |   152 -
 .../Exp2019-01-25_20-27-45/subject14_Exp2019-01-25_20-27-45.png        |   Bin 195529 -> 0 bytes
 .../results/Exp2019-01-25_20-27-45/subject3_Exp2019-01-25_20-27-45.png |   Bin 154760 -> 0 bytes
 .../results/Exp2019-01-25_20-27-45/subject5_Exp2019-01-25_20-27-45.png |   Bin 167809 -> 0 bytes
 .../results/Exp2019-01-25_20-27-45/subject9_Exp2019-01-25_20-27-45.png |   Bin 158979 -> 0 bytes
 .../results/Exp2019-01-25_20-37-42/output_Exp2019-01-25_20-37-42.txt   |   152 -
 .../Exp2019-01-25_20-37-42/subject14_Exp2019-01-25_20-37-42.png        |   Bin 182078 -> 0 bytes
 .../results/Exp2019-01-25_20-37-42/subject3_Exp2019-01-25_20-37-42.png |   Bin 141195 -> 0 bytes
 .../results/Exp2019-01-25_20-37-42/subject5_Exp2019-01-25_20-37-42.png |   Bin 130378 -> 0 bytes
 .../results/Exp2019-01-25_20-37-42/subject9_Exp2019-01-25_20-37-42.png |   Bin 136091 -> 0 bytes
 .../results/Exp2019-01-25_20-48-22/output_Exp2019-01-25_20-48-22.txt   |   152 -
 .../Exp2019-01-25_20-48-22/subject14_Exp2019-01-25_20-48-22.png        |   Bin 135837 -> 0 bytes
 .../results/Exp2019-01-25_20-48-22/subject3_Exp2019-01-25_20-48-22.png |   Bin 118094 -> 0 bytes
 .../results/Exp2019-01-25_20-48-22/subject5_Exp2019-01-25_20-48-22.png |   Bin 117076 -> 0 bytes
 .../results/Exp2019-01-25_20-48-22/subject9_Exp2019-01-25_20-48-22.png |   Bin 117670 -> 0 bytes
 .../results/Exp2019-01-25_21-04-25/output_Exp2019-01-25_21-04-25.txt   |   152 -
 .../Exp2019-01-25_21-04-25/subject14_Exp2019-01-25_21-04-25.png        |   Bin 136648 -> 0 bytes
 .../results/Exp2019-01-25_21-04-25/subject3_Exp2019-01-25_21-04-25.png |   Bin 118774 -> 0 bytes
 .../results/Exp2019-01-25_21-04-25/subject5_Exp2019-01-25_21-04-25.png |   Bin 117789 -> 0 bytes
 .../results/Exp2019-01-25_21-04-25/subject9_Exp2019-01-25_21-04-25.png |   Bin 118207 -> 0 bytes
 .../results/Exp2019-01-25_21-15-24/output_Exp2019-01-25_21-15-24.txt   |   106 -
 .../results/Exp2019-01-25_21-16-36/output_Exp2019-01-25_21-16-36.txt   |   194 -
 .../Exp2019-01-25_21-16-36/subject14_Exp2019-01-25_21-16-36.png        |   Bin 135992 -> 0 bytes
 .../results/Exp2019-01-25_21-16-36/subject3_Exp2019-01-25_21-16-36.png |   Bin 117254 -> 0 bytes
 .../results/Exp2019-01-25_21-16-36/subject5_Exp2019-01-25_21-16-36.png |   Bin 116207 -> 0 bytes
 .../results/Exp2019-01-25_21-16-36/subject9_Exp2019-01-25_21-16-36.png |   Bin 116941 -> 0 bytes
 .../results/Exp2019-01-25_21-47-31/output_Exp2019-01-25_21-47-31.txt   |   194 -
 .../Exp2019-01-25_21-47-31/subject14_Exp2019-01-25_21-47-31.png        |   Bin 136087 -> 0 bytes
 .../results/Exp2019-01-25_21-47-31/subject3_Exp2019-01-25_21-47-31.png |   Bin 117531 -> 0 bytes
 .../results/Exp2019-01-25_21-47-31/subject5_Exp2019-01-25_21-47-31.png |   Bin 116515 -> 0 bytes
 .../results/Exp2019-01-25_21-47-31/subject9_Exp2019-01-25_21-47-31.png |   Bin 117675 -> 0 bytes
 .../results/Exp2019-01-25_23-20-22/output_Exp2019-01-25_23-20-22.txt   |   104 -
 .../Exp2019-01-25_23-20-22/subject1_Exp2019-01-25_23-20-22_part2.png   |   Bin 107516 -> 0 bytes
 .../results/Exp2019-01-25_23-26-53/output_Exp2019-01-25_23-26-53.txt   |   106 -
 .../Exp2019-01-25_23-26-53/subject14_Exp2019-01-25_23-26-53_part2.png  |   Bin 137215 -> 0 bytes
 .../Exp2019-01-25_23-26-53/subject3_Exp2019-01-25_23-26-53_part2.png   |   Bin 118900 -> 0 bytes
 .../Exp2019-01-25_23-26-53/subject5_Exp2019-01-25_23-26-53_part2.png   |   Bin 117827 -> 0 bytes
 .../Exp2019-01-25_23-26-53/subject9_Exp2019-01-25_23-26-53_part2.png   |   Bin 118233 -> 0 bytes
 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_/output_Last_Model.txt    |    99 -
 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__/output_Last_Model.txt   |   106 -
 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model___/output_Last_Model.txt  |   106 -
 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model____/output_Last_Model.txt |   105 -
 .../LSTM_VGG16/results/Last_Model_____/output_Last_Model.txt           |   103 -
 .../LSTM_VGG16/results/Last_Model______/output_Last_Model.txt          |   103 -
 .../LSTM_VGG16/results/Last_Model_______/output_Last_Model.txt         |   103 -
 .../LSTM_VGG16/results/Last_Model________/output_Last_Model.txt        |   103 -
 .../LSTM_VGG16/results/Last_Model_________/output_Last_Model.txt       |     1 -
 .../LSTM_VGG16/results/Last_Model__________/output_Last_Model.txt      |   102 -
 .../LSTM_VGG16/results/Last_Model___________/output_Last_Model.txt     |   102 -
 .../LSTM_VGG16/results/Last_Model____________/output_Last_Model.txt    |  2309 ---
 128 files changed, 31539 deletions(-)
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45/output_
Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45/subject
14_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45/subject
3_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45/subject
5_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45/subject
9_Exp2019-01-24_19-28-43_and_2019-01-24_20-31-45.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09/output_
Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09/subject
14_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09/subject
3_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09/subject
5_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09/subject
9_Exp2019-01-24_22-38-46_and_2019-01-24_23-18-09.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26/output_Exp2019-01-24_23-58-26.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26/subject14_Exp2019-01-24_23-58-2
6.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26/subject3_Exp2019-01-24_23-58-26
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26/subject5_Exp2019-01-24_23-58-26
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26/subject9_Exp2019-01-24_23-58-26
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-24_23-58-26_and_2019-01-25_01-00-50/output_
Exp2019-01-24_23-58-26_and_2019-01-25_01-00-50.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/output_
Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
14_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
3_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
5_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37/subject
9_Exp2019-01-25_01-00-50_and_2019-01-25_02-01-37.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-12-13/output_Exp2019-01-25_02-12-13.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-12-13/subject9_Exp2019-01-25_02-12-13
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-13-30/output_Exp2019-01-25_02-13-30.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-13-30/subject1_Exp2019-01-25_02-13-30
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-17-13/output_Exp2019-01-25_02-17-13.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-17-13/subject1_Exp2019-01-25_02-17-13
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/output_Exp2019-01-25_02-20-23.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject14_Exp2019-01-25_02-20-2
3.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject3_Exp2019-01-25_02-20-23
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject5_Exp2019-01-25_02-20-23
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-20-23/subject9_Exp2019-01-25_02-20-23
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/output_Exp2019-01-25_02-37-37.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject14_Exp2019-01-25_02-37-3
7.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject3_Exp2019-01-25_02-37-37
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject5_Exp2019-01-25_02-37-37
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_02-37-37/subject9_Exp2019-01-25_02-37-37
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/output_Exp2019-01-25_03-00-06.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject14_Exp2019-01-25_03-00-0
6.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject3_Exp2019-01-25_03-00-06
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject5_Exp2019-01-25_03-00-06
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-00-06/subject9_Exp2019-01-25_03-00-06
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/output_
Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
14_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
3_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
5_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49/subject
9_Exp2019-01-25_03-13-09_and_2019-01-25_18-19-49.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/output_
Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/scrollb
ack_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
14_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
3_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
5_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25/subject
9_Exp2019-01-25_03-13-09_and_2019-01-25_18-51-25.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/output_Exp2019-01-25_19-22-56.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject14_Exp2019-01-25_19-22-5
6.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject3_Exp2019-01-25_19-22-56
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject5_Exp2019-01-25_19-22-56
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-22-56/subject9_Exp2019-01-25_19-22-56
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/output_Exp2019-01-25_19-31-28.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject14_Exp2019-01-25_19-31-2
8.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject3_Exp2019-01-25_19-31-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject5_Exp2019-01-25_19-31-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28/subject9_Exp2019-01-25_19-31-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/output_
Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
14_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
3_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
5_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02/subject
9_Exp2019-01-25_19-31-28_and_2019-01-25_19-41-02.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/output_Exp2019-01-25_19-51-44.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject14_Exp2019-01-25_19-51-4
4.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject3_Exp2019-01-25_19-51-44
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject5_Exp2019-01-25_19-51-44
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_19-51-44/subject9_Exp2019-01-25_19-51-44
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/output_Exp2019-01-25_20-09-28.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject14_Exp2019-01-25_20-09-2
8.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject3_Exp2019-01-25_20-09-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject5_Exp2019-01-25_20-09-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-09-28/subject9_Exp2019-01-25_20-09-28
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/output_Exp2019-01-25_20-27-45.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject14_Exp2019-01-25_20-27-4
5.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject3_Exp2019-01-25_20-27-45
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject5_Exp2019-01-25_20-27-45
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-27-45/subject9_Exp2019-01-25_20-27-45
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/output_Exp2019-01-25_20-37-42.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject14_Exp2019-01-25_20-37-4
2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject3_Exp2019-01-25_20-37-42
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject5_Exp2019-01-25_20-37-42
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-37-42/subject9_Exp2019-01-25_20-37-42
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/output_Exp2019-01-25_20-48-22.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject14_Exp2019-01-25_20-48-2
2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject3_Exp2019-01-25_20-48-22
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject5_Exp2019-01-25_20-48-22
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_20-48-22/subject9_Exp2019-01-25_20-48-22
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/output_Exp2019-01-25_21-04-25.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject14_Exp2019-01-25_21-04-2
5.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject3_Exp2019-01-25_21-04-25
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject5_Exp2019-01-25_21-04-25
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-04-25/subject9_Exp2019-01-25_21-04-25
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-15-24/output_Exp2019-01-25_21-15-24.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/output_Exp2019-01-25_21-16-36.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject14_Exp2019-01-25_21-16-3
6.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject3_Exp2019-01-25_21-16-36
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject5_Exp2019-01-25_21-16-36
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-16-36/subject9_Exp2019-01-25_21-16-36
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/output_Exp2019-01-25_21-47-31.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject14_Exp2019-01-25_21-47-3
1.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject3_Exp2019-01-25_21-47-31
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject5_Exp2019-01-25_21-47-31
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_21-47-31/subject9_Exp2019-01-25_21-47-31
.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-20-22/output_Exp2019-01-25_23-20-22.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-20-22/subject1_Exp2019-01-25_23-20-22
_part2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/output_Exp2019-01-25_23-26-53.t
xt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject14_Exp2019-01-25_23-26-5
3_part2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject3_Exp2019-01-25_23-26-53
_part2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject5_Exp2019-01-25_23-26-53
_part2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-25_23-26-53/subject9_Exp2019-01-25_23-26-53
_part2.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model___/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model____/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_____/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model______/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_______/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model_________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model___________/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model____________/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 00:51:05.057561: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 00:51:05.155880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 00:51:05.156146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 00:51:05.156160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 00:51:05.312125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 00:51:05.312148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 00:51:05.312155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 00:51:05.312296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 51, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/Stateful_CNN_LSTM_Configur
ation.py", line 83, in getFinalModel
    rnn.compile(optimizer=adam, loss='mean_absolute_error', loss_weights=[1.0,2.0,1.0]) #'mean_squared_error
', metrics=['mae']
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 863, in compile
    **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 677, in compile
    str(loss_weights))
ValueError: When passing a list as loss_weights, it should have one entry per model output. The model has 1
outputs, but you passed loss_weights=[1.0, 2.0, 1.0]
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 00:52:49.489444: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 00:52:49.587172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 00:52:49.587427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 00:52:49.587438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 00:52:49.743970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 00:52:49.743996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 00:52:49.744001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 00:52:49.744140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_00-52-50 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 1)                 4097
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   480
_________________________________________________________________
dense_2 (Dense)              (1, 1)                    11
=================================================================
Total params: 134,265,132
Trainable params: 4,588
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 4
num_outputs = 1

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output1_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_00-52-50
All frames and annotations from 20 datasets have been read by 2019-01-26 00:52:54.890715
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 00:53:01.289970!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.1430
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 00:53:19.734353!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.1171
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 00:53:35.066903!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.2065
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 00:53:51.770206!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.1807
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 00:54:08.631278!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.3079
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 00:54:27.857494!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.2141
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 00:54:43.904878!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1510
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 00:55:04.074129!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.1651
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 00:55:28.392161!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.2856
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 00:55:49.238753!
Epoch 1/1
732/732 [==============================] - 14s 19ms/step - loss: 0.2016
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 00:56:10.224210!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.2505
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 00:56:28.315862!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.2830
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 00:56:43.480760!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.2074
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 00:56:59.471396!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.2409
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 00:57:16.095988!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.2538
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 00:57:34.945441!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.2093
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 00:57:48.819318!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.1153
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 00:58:01.237185!
Epoch 1/1
542/542 [==============================] - 10s 19ms/step - loss: 0.1979
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 00:58:16.172935!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.1415
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 00:58:30.753657!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.2215
Epoch 1 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output1_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_00-52-50
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 00:58:43.195650
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Yaw angle estimation: 32.51 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Yaw angle estimation: 27.77 Degree
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Yaw angle estimation: 27.31 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Yaw angle estimation: 37.02 Degree
On average in 4 test subjects:
        The absolute mean error on Yaw angle estimations: 31.15 Degree
Exp2019-01-26_00-52-50_part1 completed!
subject3_Exp2019-01-26_00-52-50_part1.png has been saved by 2019-01-26 00:59:49.134975.
subject5_Exp2019-01-26_00-52-50_part1.png has been saved by 2019-01-26 00:59:49.216325.
subject9_Exp2019-01-26_00-52-50_part1.png has been saved by 2019-01-26 00:59:49.296179.
subject14_Exp2019-01-26_00-52-50_part1.png has been saved by 2019-01-26 00:59:49.387635.
Model Exp2019-01-26_00-52-50 has been evaluated successfully.
Model Exp2019-01-26_00-52-50 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 01:05:23.692171: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 01:05:23.788700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 01:05:23.789008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 01:05:23.789023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 01:05:23.946497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 01:05:23.946521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 01:05:23.946526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 01:05:23.946705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_01-05-24 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (10, 1, 4096)             134260544
_________________________________________________________________
fc3 (TimeDistributed)        (10, 1, 1)                4097
_________________________________________________________________
lstm_1 (LSTM)                (10, 10)                  480
_________________________________________________________________
dense_2 (Dense)              (10, 1)                   11
=================================================================
Total params: 134,265,132
Trainable params: 4,588
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_CNN_LSTM_Configuration.py'
RECORD = True # False #

output_begin = 4
num_outputs = 1

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 10
test_batch_size = 10

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [1] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output1_BatchSize10_inEpochs1_outEpochs1_AdamOpt_lr-0.0
00100_2019-01-26_01-05-24
All frames and annotations from 20 datasets have been read by 2019-01-26 01:05:29.126806
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 01:05:35.537744!
Epoch 1/1
66/66 [==============================] - 5s 80ms/step - loss: 0.1479
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 01:05:45.981153!
Epoch 1/1
49/49 [==============================] - 3s 59ms/step - loss: 0.1227
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 01:05:55.235414!
Epoch 1/1
65/65 [==============================] - 4s 60ms/step - loss: 0.2288
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 01:06:03.985440!
Epoch 1/1
50/50 [==============================] - 3s 60ms/step - loss: 0.2557
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 01:06:14.809869!
Epoch 1/1
77/77 [==============================] - 5s 60ms/step - loss: 0.3590
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 01:06:24.855336!
Epoch 1/1
56/56 [==============================] - 3s 60ms/step - loss: 0.2382
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 01:06:34.201865!
Epoch 1/1
63/63 [==============================] - 4s 60ms/step - loss: 0.1661
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 01:06:46.677050!
Epoch 1/1
91/91 [==============================] - 5s 60ms/step - loss: 0.1875
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 01:06:59.690941!
Epoch 1/1
74/74 [==============================] - 4s 60ms/step - loss: 0.3776
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 01:07:11.486219!
Epoch 1/1
73/73 [==============================] - 4s 60ms/step - loss: 0.2518
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 01:07:23.137220!
Epoch 1/1
72/72 [==============================] - 4s 60ms/step - loss: 0.2975
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 01:07:32.529139!
Epoch 1/1
49/49 [==============================] - 3s 60ms/step - loss: 0.3581
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 01:07:41.403071!
Epoch 1/1
61/61 [==============================] - 4s 60ms/step - loss: 0.2099
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 01:07:50.142890!
Epoch 1/1
51/51 [==============================] - 3s 60ms/step - loss: 0.2932
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 01:08:00.659596!
Epoch 1/1
74/74 [==============================] - 4s 60ms/step - loss: 0.4256
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 01:08:10.547297!
Epoch 1/1
55/55 [==============================] - 3s 60ms/step - loss: 0.1525
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 01:08:17.666403!
Epoch 1/1
39/39 [==============================] - 2s 60ms/step - loss: 0.0883
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 01:08:25.257406!
Epoch 1/1
54/54 [==============================] - 3s 60ms/step - loss: 0.1841
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 01:08:33.382369!
Epoch 1/1
48/48 [==============================] - 3s 60ms/step - loss: 0.2108
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 01:08:41.924109!
Epoch 1/1
57/57 [==============================] - 3s 60ms/step - loss: 0.2851
Epoch 1 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (8, 'M02'), (23, 'M13'), (21,
 'F02'), (16, 'M09'), (7, 'M01'), (12, 'M06'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (13, 'M07'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output1_BatchSize10_inEpochs1_outEpochs1_AdamOpt_lr-0
.000100_2019-01-26_01-05-24
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 01:08:47.500035
For the Subject 3 (F03):
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 78, in <module>
    main()
  File "runCNN_LSTM.py", line 75, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 62, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runCNN_LSTM.py", line 45, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record =
record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 140, in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs
, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line
 110, in evaluateSubject
    matrix = numpy.concatenate((test_labels[start_index:, i:i+1], predictions[:, i:i+1]), axis=1)
TypeError: list indices must be integers or slices, not tuple
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
remote: Counting objects: 17, done.
remote: Compressing objects: 100% (17/17), done.
remote: Total 17 (delta 8), reused 0 (delta 0)
Unpacking objects: 100% (17/17), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   256039d..8330c29  master     -> origin/master
Updating 256039d..8330c29
error: Your local changes to the following files would be overwritten by merge:
        DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py
        DeepRL_For_HPE/LSTM_VGG16/Stateful_CNN_LSTM_Configuration.py
Please, commit your changes or stash them before you can merge.
Aborting
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git reset --hard HEAD
HEAD is now at 256039d Emptying results folder
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
Updating 256039d..8330c29
Fast-forward
 DeepRL_For_HPE/DatasetHandler/BiwiBrowser.py                         |  51 +++++++++++----------
 DeepRL_For_HPE/DeepRL_For_HPE.sln                                    |   2 +-
 DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/EstimationPlotter.py |   6 +--
 .../{LSTM_VGG16 => FC_RNN_Evaluater}/EvaluationRecorder.py           |  74 ++++++++++++++++---------------
 .../LSTM_VGG16Helper.py => FC_RNN_Evaluater/FC_RNN_Evaluater.py}     |  51 +++++----------------
 .../LSTM_VGG16.pyproj => FC_RNN_Evaluater/FC_RNN_Evaluater.pyproj}   |  17 +++----
 .../{LSTM_VGG16 => FC_RNN_Evaluater}/NeighborFolderimporter.py       |   0
 .../Quick_Scripts/LSTM_VGG16Helper.py                                |   0
 .../Quick_Scripts/tf_trainLSTM_VGG16.py                              |   0
 .../Quick_Scripts/trainLSTM_VGG16.py                                 |   0
 .../Stateful_FC_RNN_Configuration.py}                                |  53 +++++++++++++++-------
 .../Stateless_FC_RNN_Configuration.py}                               |  14 +++---
 DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/VGG_model.txt        |   0
 DeepRL_For_HPE/FC_RNN_Evaluater/__init__.py                          |   5 +++
 .../continueFC_RNN_Experiment.py}                                    |  18 ++++----
 .../results/Exp2019-01-25_20-19-34/output_Exp2019-01-25_20-19-34.txt |   0
 .../Exp2019-01-25_20-19-34/subject14_Exp2019-01-25_20-19-34.png      | Bin
 .../Exp2019-01-25_20-19-34/subject3_Exp2019-01-25_20-19-34.png       | Bin
 .../Exp2019-01-25_20-19-34/subject5_Exp2019-01-25_20-19-34.png       | Bin
 .../Exp2019-01-25_20-19-34/subject9_Exp2019-01-25_20-19-34.png       | Bin
 .../results/Exp2019-01-25_23-51-56/output_Exp2019-01-25_23-51-56.txt |   0
 .../subject14_Exp2019-01-25_23-51-56_part2.png                       | Bin
 .../Exp2019-01-25_23-51-56/subject3_Exp2019-01-25_23-51-56_part2.png | Bin
 .../Exp2019-01-25_23-51-56/subject5_Exp2019-01-25_23-51-56_part2.png | Bin
 .../Exp2019-01-25_23-51-56/subject9_Exp2019-01-25_23-51-56_part2.png | Bin
 .../runCNN_LSTM.py => FC_RNN_Evaluater/runFC_RNN_Experiment.py}      |  38 +++++-----------
 DeepRL_For_HPE/Note_Files/commands.txt                               |   4 +-
 27 files changed, 158 insertions(+), 175 deletions(-)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/EstimationPlotter.py (92%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/EvaluationRecorder.py (89%)
 rename DeepRL_For_HPE/{LSTM_VGG16/LSTM_VGG16Helper.py => FC_RNN_Evaluater/FC_RNN_Evaluater.py} (78%)
 rename DeepRL_For_HPE/{LSTM_VGG16/LSTM_VGG16.pyproj => FC_RNN_Evaluater/FC_RNN_Evaluater.pyproj} (82%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/NeighborFolderimporter.py (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/Quick_Scripts/LSTM_VGG16Helper.py (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/Quick_Scripts/tf_trainLSTM_VGG16.py (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/Quick_Scripts/trainLSTM_VGG16.py (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16/Stateful_CNN_LSTM_Configuration.py => FC_RNN_Evaluater/Stateful_FC_RNN_Co
nfiguration.py} (61%)
 rename DeepRL_For_HPE/{LSTM_VGG16/CNN_LSTM_Configuration.py => FC_RNN_Evaluater/Stateless_FC_RNN_Configurat
ion.py} (91%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/VGG_model.txt (100%)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/__init__.py
 rename DeepRL_For_HPE/{LSTM_VGG16/continueTrainigCNN_LSTM.py => FC_RNN_Evaluater/continueFC_RNN_Experiment.
py} (83%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_20-19-34/output_Exp2019-01-25_
20-19-34.txt (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_20-19-34/subject14_Exp2019-01-
25_20-19-34.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_20-19-34/subject3_Exp2019-01-2
5_20-19-34.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_20-19-34/subject5_Exp2019-01-2
5_20-19-34.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_20-19-34/subject9_Exp2019-01-2
5_20-19-34.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_23-51-56/output_Exp2019-01-25_
23-51-56.txt (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_23-51-56/subject14_Exp2019-01-
25_23-51-56_part2.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_23-51-56/subject3_Exp2019-01-2
5_23-51-56_part2.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_23-51-56/subject5_Exp2019-01-2
5_23-51-56_part2.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16 => FC_RNN_Evaluater}/results/Exp2019-01-25_23-51-56/subject9_Exp2019-01-2
5_23-51-56_part2.png (100%)
 rename DeepRL_For_HPE/{LSTM_VGG16/runCNN_LSTM.py => FC_RNN_Evaluater/runFC_RNN_Experiment.py} (74%)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ cd ..
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE$ cd FC_RNN_Evaluater/
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:33:43.019413: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:33:43.117506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:33:43.117764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:33:43.117775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:33:43.272818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:33:43.272844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:33:43.272849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:33:43.272988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 37, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateful_FC_RNN_Conf
iguration.py", line 103, in getFinalModel
    modelID = modelID + '_%s' % now()[:-7].replace(' ', '_').replace(':', '-')
NameError: name 'now' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:41:46.350962: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:41:46.448568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:41:46.448825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:41:46.448837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:41:46.603651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:41:46.603677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:41:46.603682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:41:46.603820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 37, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateful_FC_RNN_Conf
iguration.py", line 104, in getFinalModel
    modelID = modelID + '_%s' % now()[:-7].replace(' ', '_').replace(':', '-')
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/Stateful_FC_RNN_Conf
iguration.py", line 8, in now
    def now(): return str(datetime.datetime.now())
NameError: name 'datetime' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:42:26.881057: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:42:26.977631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:42:26.977896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:42:26.977909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:42:27.133122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:42:27.133149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:42:27.133154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:42:27.133295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 37, in runCNN_LSTM
    num_outputs = num_outputs, lr = learning_rate, include_vgg_top = include_vgg_top)
ValueError: not enough values to unpack (expected 4, got 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:46:00.779877: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:46:00.877528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:46:00.877793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:46:00.877805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:46:01.032793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:46:01.032818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:46:01.032823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:46:01.032959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-46-01 has been started to be evaluated.
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 41, in runCNN_LSTM
    printLog(get_model_summary(vgg_model), record = record)
  File "runFC_RNN_Experiment.py", line 13, in get_model_summary
    stream = io.StringIO()
NameError: name 'io' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:47:06.392190: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:47:06.488823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:47:06.489082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:47:06.489095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:47:06.644446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:47:06.644472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:47:06.644477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:47:06.644614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-47-07 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 43, in runCNN_LSTM
    saveConfiguration(confFile = confFile, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.p
y", line 22, in saveConfiguration
    with open(confFile, 'r') as conf:
FileNotFoundError: [Errno 2] No such file or directory: 'Stateful_CNN_LSTM_Configuration.py'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:48:06.521179: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:48:06.617631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:48:06.617892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:48:06.617905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:48:06.773032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:48:06.773059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:48:06.773065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:48:06.773204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-48-07 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-48-07
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_i
nput = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 47, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 36, in trainImageModelForEpochs
    random.Random(4).shuffle(trainingSubjects)
NameError: name 'random' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:49:20.514872: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:49:20.611778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:49:20.612038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:49:20.612052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:49:20.766916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:49:20.766942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:49:20.766951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:49:20.767096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-49-21 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-49-21
All frames and annotations from 1 datasets have been read by 2019-01-26 03:49:22.299620
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_i
nput = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 48, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 39, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 22, in trainImageModelOnSets
    printLog('%d. set (Dataset %d) being trained for epoch %d by %s!' % (c+1, trainingSubjects[c], epoch+1,
now()), record = record)
NameError: name 'now' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:51:18.718144: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:51:18.816303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:51:18.816561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:51:18.816574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:51:18.973477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:51:18.973503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:51:18.973508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:51:18.973646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-51-19 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-51-19
All frames and annotations from 1 datasets have been read by 2019-01-26 03:51:20.506803
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 03:51:29.416103!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2063
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_03-51-19
The subjects will be tested: [(9, 'M03')]
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 30, in runCNN_LSTM_ExperimentWithModel
    full_model, means, results = evaluateCNN_LSTM(full_model, label_rescaling_factor = label_rescaling_facto
r,
NameError: name 'label_rescaling_factor' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:54:08.987256: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:54:09.084941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:54:09.085198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:54:09.085210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:54:09.240557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:54:09.240582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:54:09.240587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:54:09.240722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-54-09 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-54-09
All frames and annotations from 1 datasets have been read by 2019-01-26 03:54:10.801423
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 03:54:19.723145!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2129
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_03-54-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 03:54:38.189740
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 32, in runCNN_LSTM_ExperimentWithModel
    num_outputs = num_outputs, batch_size = test_batch_size, angles = angles, stateful = STATEFUL, record =
record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 114, in evaluateCNN_LSTM
    full_model, outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs
, angles, batch_size = batch_size, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 90, in evaluateSubject
    absolute_mean_error = np.abs(differences).mean()
NameError: name 'np' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:55:53.070851: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:55:53.168750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:55:53.169009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:55:53.169021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:55:53.324774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:55:53.324801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:55:53.324806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:55:53.324946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-55-54 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-55-54
All frames and annotations from 1 datasets have been read by 2019-01-26 03:55:54.883566
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 03:56:03.790556!
Epoch 1/1
882/882 [==============================] - 17s 20ms/step - loss: 0.2365
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_03-55-54
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 03:56:22.552461
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 14.25 Degree
        The absolute mean error on Yaw angle estimation: 17.34 Degree
        The absolute mean error on Roll angle estimation: 18.53 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 14.25 Degree
        The absolute mean error on Yaw angle estimations: 17.34 Degree
        The absolute mean error on Roll angle estimations: 18.53 Degree
Exp2019-01-26_03-55-54_part1 completed!
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EstimationPlotter.py
", line 37, in drawResults
    f = drawPlotsForSubj(outputs, subject, BIWI_Subject_IDs[subject], modelStr, num_outputs, angles = angles
)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EstimationPlotter.py
", line 29, in drawPlotsForSubj
    cell.set_ylim([-label_rescaling_factor, label_rescaling_factor])
NameError: name 'label_rescaling_factor' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 03:57:40.680883: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 03:57:40.778094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 03:57:40.778395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 03:57:40.778409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 03:57:40.934473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 03:57:40.934498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 03:57:40.934503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 03:57:40.934681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_03-57-41 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_03-57-41
All frames and annotations from 1 datasets have been read by 2019-01-26 03:57:42.576001
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 03:57:51.470764!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2710
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_03-57-41
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 03:58:16.281421
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 15.81 Degree
        The absolute mean error on Yaw angle estimation: 20.35 Degree
        The absolute mean error on Roll angle estimation: 8.50 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.81 Degree
        The absolute mean error on Yaw angle estimations: 20.35 Degree
        The absolute mean error on Roll angle estimations: 8.50 Degree
Exp2019-01-26_03-57-41_part1 completed!
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EstimationPlotter.py
", line 42, in drawResults
    f.savefig(addModelFolder(CURRENT_MODEL, fileName), bbox_inches='tight')
NameError: name 'addModelFolder' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:01:56.497215: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:01:56.594076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:01:56.594381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:01:56.594396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:01:56.749528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:01:56.749556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:01:56.749561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:01:56.749745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-01-57 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_04-01-57
All frames and annotations from 1 datasets have been read by 2019-01-26 04:01:58.299366
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:02:07.199021!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1852
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_04-01-57
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:02:25.869609
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 14.82 Degree
        The absolute mean error on Yaw angle estimation: 18.24 Degree
        The absolute mean error on Roll angle estimation: 4.23 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 14.82 Degree
        The absolute mean error on Yaw angle estimations: 18.24 Degree
        The absolute mean error on Roll angle estimations: 4.23 Degree
Exp2019-01-26_04-01-57_part1 completed!
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EstimationPlotter.py
", line 43, in drawResults
    printLog(fileName, 'has been saved by %s.' % now(), record = save)
NameError: name 'printLog' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 8, in <module>
    from EvaluationRecorder import *
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.p
y", line 9, in <module>
    import shutil, os, numpy as np, datatime
ModuleNotFoundError: No module named 'datatime'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:05:13.204314: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:05:13.300833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:05:13.301139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:05:13.301154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:05:13.456636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:05:13.456660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:05:13.456665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:05:13.456845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-05-14 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_04-05-14
All frames and annotations from 1 datasets have been read by 2019-01-26 04:05:14.973562
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:05:23.943874!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2299
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_04-05-14
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:05:42.509464
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 20.74 Degree
        The absolute mean error on Yaw angle estimation: 20.95 Degree
        The absolute mean error on Roll angle estimation: 8.36 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.74 Degree
        The absolute mean error on Yaw angle estimations: 20.95 Degree
        The absolute mean error on Roll angle estimations: 8.36 Degree
Exp2019-01-26_04-05-14_part1 completed!
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 56, in runCNN_LSTM
    figures = drawResults(results, modelStr, modelID, num_outputs = num_outputs, angles = angles, save = rec
ord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EstimationPlotter.py
", line 43, in drawResults
    printLog(fileName, 'has been saved by %s.' % now(), record = save)
NameError: name 'printLog' is not defined
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:06:28.629338: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:06:28.709179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:06:28.709476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:06:28.709490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:06:28.864187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:06:28.864214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:06:28.864218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:06:28.864405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-06-29 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    33
=================================================================
Total params: 134,273,428
Trainable params: 12,884
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-26_04-06-29
All frames and annotations from 1 datasets have been read by 2019-01-26 04:06:30.443397
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:06:39.332947!
Epoch 1/1
882/882 [==============================] - 17s 20ms/step - loss: 0.2223
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-26_04-06-29
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:06:58.244987
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.46 Degree
        The absolute mean error on Yaw angle estimation: 25.36 Degree
        The absolute mean error on Roll angle estimation: 7.35 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.46 Degree
        The absolute mean error on Yaw angle estimations: 25.36 Degree
        The absolute mean error on Roll angle estimations: 7.35 Degree
Exp2019-01-26_04-06-29_part1 completed!
subject9_Exp2019-01-26_04-06-29_part1.png has been saved by 2019-01-26 04:07:15.780947.
Model Exp2019-01-26_04-06-29 has been evaluated successfully.
Model Exp2019-01-26_04-06-29 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:10:15.867587: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:10:15.964455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:10:15.964711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:10:15.964722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:10:16.120441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:10:16.120468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:10:16.120473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:10:16.120615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-10-16 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-26_04-10-16
All frames and annotations from 1 datasets have been read by 2019-01-26 04:10:17.800543
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:10:26.686713!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2587
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-26_04-10-16
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:10:51.618520
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 19.45 Degree
        The absolute mean error on Yaw angle estimation: 22.98 Degree
        The absolute mean error on Roll angle estimation: 9.85 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 19.45 Degree
        The absolute mean error on Yaw angle estimations: 22.98 Degree
        The absolute mean error on Roll angle estimations: 9.85 Degree
Exp2019-01-26_04-10-16_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-26_04-10-16
All frames and annotations from 1 datasets have been read by 2019-01-26 04:11:14.690885
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:11:23.566395!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2025
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-26_04-10-16
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:11:47.332976
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.94 Degree
        The absolute mean error on Yaw angle estimation: 15.61 Degree
        The absolute mean error on Roll angle estimation: 16.81 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.94 Degree
        The absolute mean error on Yaw angle estimations: 15.61 Degree
        The absolute mean error on Roll angle estimations: 16.81 Degree
Exp2019-01-26_04-10-16_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-26_04-10-16
All frames and annotations from 1 datasets have been read by 2019-01-26 04:12:10.285470
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:12:19.151822!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1813
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-26_04-10-16
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-26 04:12:42.406155
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 15.44 Degree
        The absolute mean error on Yaw angle estimation: 16.99 Degree
        The absolute mean error on Roll angle estimation: 20.66 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.44 Degree
        The absolute mean error on Yaw angle estimations: 16.99 Degree
        The absolute mean error on Roll angle estimations: 20.66 Degree
Exp2019-01-26_04-10-16_part3 completed!
subject9_Exp2019-01-26_04-10-16.png has been saved by 2019-01-26 04:13:04.775866.
Model Exp2019-01-26_04-10-16 has been evaluated successfully.
Model Exp2019-01-26_04-10-16 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "fixing
 import errors"
[master ed724e7] fixing import errors
 18 files changed, 434 insertions(+), 17 deletions(-)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-10-16/output_Exp2019-01-26_04-1
0-16.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-10-16/subject9_Exp2019-01-26_04
-10-16.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-12-42/output_Exp2019-01-26_00-12-42.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-12-42/subject14_Exp2019-01-26_00-12-4
2_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-12-42/subject3_Exp2019-01-26_00-12-42
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-12-42/subject5_Exp2019-01-26_00-12-42
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-12-42/subject9_Exp2019-01-26_00-12-42
_part2.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-52-50/output_Exp2019-01-26_00-52-50.t
xt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-52-50/subject14_Exp2019-01-26_00-52-5
0_part1.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-52-50/subject3_Exp2019-01-26_00-52-50
_part1.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-52-50/subject5_Exp2019-01-26_00-52-50
_part1.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-26_00-52-50/subject9_Exp2019-01-26_00-52-50
_part1.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 29, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (27/27), done.
Writing objects: 100% (29/29), 1.12 MiB | 1.02 MiB/s, done.
Total 29 (delta 11), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   8330c29..ed724e7  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Everything up-to-date
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:18:08.957832: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:18:09.055721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:18:09.055984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:18:09.055999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:18:09.214162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:18:09.214190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:18:09.214198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:18:09.214341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_w
eights_tf_dim_ordering_tf_kernels.h5
96116736/96112376 [==============================] - 3s 0us/step
Model Exp2019-01-26_04-18-29 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 2048)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              2098176
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_l
r-0.000100_2019-01-26_04-18-29
All frames and annotations from 1 datasets have been read by 2019-01-26 04:18:30.404352
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:18:39.282865!
Epoch 1/1
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_i
nput = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 48, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 39, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 29, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1877, in train_on
_batch
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1476, in _standar
dize_user_data
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 123, in _standard
ize_input_data
    str(data_shape))
ValueError: Error when checking input: expected tdCNN_input to have shape (1, 299, 299, 3) but got array wit
h shape (1, 224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:23:24.718995: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:23:24.815953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:23:24.816211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:23:24.816230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:23:24.973638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:23:24.973665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:23:24.973669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:23:24.973816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-23-41 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 2048)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              2098176
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_l
r-0.000100_2019-01-26_04-23-41
All frames and annotations from 1 datasets have been read by 2019-01-26 04:23:42.383138
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:23:51.273039!
Epoch 1/1
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_i
nput = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 48, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 39, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 29, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1877, in train_on
_batch
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1476, in _standar
dize_user_data
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 123, in _standard
ize_input_data
    str(data_shape))
ValueError: Error when checking input: expected tdCNN_input to have shape (1, 299, 299, 3) but got array wit
h shape (1, 224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:25:17.160113: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:25:17.258330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:25:17.258632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:25:17.258646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:25:17.417035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:25:17.417061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:25:17.417066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:25:17.417245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-25-33 has been started to be evaluated.
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 299, 299, 3)  0
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]
                                                                 activation_8[0][0]
                                                                 activation_11[0][0]
                                                                 activation_12[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]
                                                                 activation_15[0][0]
                                                                 activation_18[0][0]
                                                                 activation_19[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]
                                                                 activation_22[0][0]
                                                                 activation_25[0][0]
                                                                 activation_26[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]
                                                                 activation_30[0][0]
                                                                 max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]
                                                                 activation_34[0][0]
                                                                 activation_39[0][0]
                                                                 activation_40[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]
                                                                 activation_44[0][0]
                                                                 activation_49[0][0]
                                                                 activation_50[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]
                                                                 activation_54[0][0]
                                                                 activation_59[0][0]
                                                                 activation_60[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]
                                                                 activation_64[0][0]
                                                                 activation_69[0][0]
                                                                 activation_70[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]
                                                                 activation_76[0][0]
                                                                 max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]
__________________________________________________________________________________________________
activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]
__________________________________________________________________________________________________
activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]
                                                                 activation_80[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]
                                                                 activation_84[0][0]
__________________________________________________________________________________________________
activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]
                                                                 mixed9_0[0][0]
                                                                 concatenate_1[0][0]
                                                                 activation_85[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]
__________________________________________________________________________________________________
activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]
__________________________________________________________________________________________________
activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]
__________________________________________________________________________________________________
activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]
__________________________________________________________________________________________________
activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]
__________________________________________________________________________________________________
activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]
__________________________________________________________________________________________________
activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]
__________________________________________________________________________________________________
activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]
__________________________________________________________________________________________________
activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]
                                                                 activation_89[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]
                                                                 activation_93[0][0]
__________________________________________________________________________________________________
activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]
                                                                 mixed9_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 activation_94[0][0]
__________________________________________________________________________________________________
avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]
==================================================================================================
Total params: 21,802,784
Trainable params: 0
Non-trainable params: 21,802,784
__________________________________________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 2048)              21802784
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 2048)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              2098176
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 23,904,628
Trainable params: 2,101,844
Non-trainable params: 21,802,784
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = False
######### CONF_ends_Here ###########
Training model InceptionV3_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_l
r-0.000100_2019-01-26_04-25-33
All frames and annotations from 1 datasets have been read by 2019-01-26 04:25:34.774293
1. set (Dataset 9) being trained for epoch 1 by 2019-01-26 04:25:43.666742!
Epoch 1/1
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 64, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 61, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = False)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record, preprocess_i
nput = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 48, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 39, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 29, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1877, in train_on
_batch
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1476, in _standar
dize_user_data
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 123, in _standard
ize_input_data
    str(data_shape))
ValueError: Error when checking input: expected tdCNN_input to have shape (1, 299, 299, 3) but got array wit
h shape (1, 224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 04:28:48.573417: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 04:28:48.670272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 04:28:48.670536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 04:28:48.670551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 04:28:48.826268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 04:28:48.826296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 04:28:48.826305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 04:28:48.826451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_04-28-49 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 10)                   560
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    33
=================================================================
Total params: 138,459,540
Trainable params: 4,198,996
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 10
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 04:28:54.047867
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 04:29:00.442153!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.2228
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 04:29:24.008025!
Epoch 1/1
492/492 [==============================] - 13s 25ms/step - loss: 0.1842
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 04:29:42.960016!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.2238
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 04:30:04.142948!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1564
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 04:30:24.700792!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.2204
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 04:30:50.176104!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.2073
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 04:31:10.019732!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.2010
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 04:31:35.182952!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1556
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 04:32:05.959504!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.2257
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 04:32:32.248763!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.1805
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 04:32:58.596110!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1888
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 04:33:21.691104!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.2175
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 04:33:40.065292!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.2058
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 04:34:00.696278!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.2369
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 04:34:21.055728!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.2100
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 04:34:45.127164!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.1549
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 04:35:02.980311!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.1113
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 04:35:17.873040!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1806
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 04:35:35.946084!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.1475
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 04:35:53.898483!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.1368
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:36:12.687372
1. set (Dataset 6) being trained for epoch 2 by 2019-01-26 04:36:17.863106!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1746
2. set (Dataset 11) being trained for epoch 2 by 2019-01-26 04:36:36.954885!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.1286
3. set (Dataset 10) being trained for epoch 2 by 2019-01-26 04:36:58.816780!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1656
4. set (Dataset 4) being trained for epoch 2 by 2019-01-26 04:37:24.215415!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.1856
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 04:37:48.097318!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1965
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 04:38:07.103853!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.1225
7. set (Dataset 17) being trained for epoch 2 by 2019-01-26 04:38:23.311534!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.1133
8. set (Dataset 1) being trained for epoch 2 by 2019-01-26 04:38:38.016834!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1979
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 04:38:58.596935!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1561
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 04:39:25.509463!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.1581
11. set (Dataset 21) being trained for epoch 2 by 2019-01-26 04:39:50.068361!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1873
12. set (Dataset 22) being trained for epoch 2 by 2019-01-26 04:40:12.266179!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.1034
13. set (Dataset 2) being trained for epoch 2 by 2019-01-26 04:40:34.357408!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.2106
14. set (Dataset 24) being trained for epoch 2 by 2019-01-26 04:40:51.781517!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.1173
15. set (Dataset 15) being trained for epoch 2 by 2019-01-26 04:41:10.889728!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.1358
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 04:41:33.479932!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.1172
17. set (Dataset 18) being trained for epoch 2 by 2019-01-26 04:41:53.051489!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1529
18. set (Dataset 19) being trained for epoch 2 by 2019-01-26 04:42:13.228258!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1125
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 04:42:33.319423!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.1461
20. set (Dataset 16) being trained for epoch 2 by 2019-01-26 04:43:00.063280!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1159
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:43:27.577297
1. set (Dataset 19) being trained for epoch 3 by 2019-01-26 04:43:32.450320!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.1012
2. set (Dataset 16) being trained for epoch 3 by 2019-01-26 04:43:53.600948!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1014
3. set (Dataset 21) being trained for epoch 3 by 2019-01-26 04:44:22.243162!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1541
4. set (Dataset 15) being trained for epoch 3 by 2019-01-26 04:44:44.480331!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.1154
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 04:45:05.510978!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.1072
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 04:45:25.607692!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.1200
7. set (Dataset 18) being trained for epoch 3 by 2019-01-26 04:45:49.765595!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1377
8. set (Dataset 22) being trained for epoch 3 by 2019-01-26 04:46:11.389184!
Epoch 1/1
665/665 [==============================] - 18s 26ms/step - loss: 0.0811
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 04:46:34.424081!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1434
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 04:46:56.362502!
Epoch 1/1
772/772 [==============================] - 21s 27ms/step - loss: 0.1282
11. set (Dataset 17) being trained for epoch 3 by 2019-01-26 04:47:20.757338!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0972
12. set (Dataset 6) being trained for epoch 3 by 2019-01-26 04:47:35.794624!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1520
13. set (Dataset 24) being trained for epoch 3 by 2019-01-26 04:47:53.916679!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0802
14. set (Dataset 11) being trained for epoch 3 by 2019-01-26 04:48:11.541992!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.1024
15. set (Dataset 10) being trained for epoch 3 by 2019-01-26 04:48:33.061723!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1253
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 04:48:56.679181!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0939
17. set (Dataset 2) being trained for epoch 3 by 2019-01-26 04:49:15.698703!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1578
18. set (Dataset 4) being trained for epoch 3 by 2019-01-26 04:49:35.819931!
Epoch 1/1
744/744 [==============================] - 20s 26ms/step - loss: 0.1303
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 04:50:03.140728!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.1213
20. set (Dataset 1) being trained for epoch 3 by 2019-01-26 04:50:26.528219!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1554
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:50:43.835969
1. set (Dataset 4) being trained for epoch 4 by 2019-01-26 04:50:51.196768!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.1126
2. set (Dataset 1) being trained for epoch 4 by 2019-01-26 04:51:15.310824!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1445
3. set (Dataset 17) being trained for epoch 4 by 2019-01-26 04:51:31.610824!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0977
4. set (Dataset 10) being trained for epoch 4 by 2019-01-26 04:51:49.216550!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1057
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 04:52:14.767318!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0974
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 04:52:40.808372!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1095
7. set (Dataset 2) being trained for epoch 4 by 2019-01-26 04:53:04.804114!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1155
8. set (Dataset 6) being trained for epoch 4 by 2019-01-26 04:53:22.774811!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1431
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 04:53:41.136011!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0822
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 04:53:59.151675!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1345
11. set (Dataset 18) being trained for epoch 4 by 2019-01-26 04:54:20.049230!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1298
12. set (Dataset 19) being trained for epoch 4 by 2019-01-26 04:54:40.200105!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1119
13. set (Dataset 11) being trained for epoch 4 by 2019-01-26 04:54:58.739402!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0816
14. set (Dataset 16) being trained for epoch 4 by 2019-01-26 04:55:21.805977!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0948
15. set (Dataset 21) being trained for epoch 4 by 2019-01-26 04:55:50.532712!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1401
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 04:56:11.941354!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0887
17. set (Dataset 24) being trained for epoch 4 by 2019-01-26 04:56:30.814452!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0753
18. set (Dataset 15) being trained for epoch 4 by 2019-01-26 04:56:49.660685!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0995
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 04:57:14.014141!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1109
20. set (Dataset 22) being trained for epoch 4 by 2019-01-26 04:57:39.561226!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0713
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 04:58:01.056796
1. set (Dataset 15) being trained for epoch 5 by 2019-01-26 04:58:07.388715!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0923
2. set (Dataset 22) being trained for epoch 5 by 2019-01-26 04:58:30.759055!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0691
3. set (Dataset 18) being trained for epoch 5 by 2019-01-26 04:58:53.033295!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.1152
4. set (Dataset 21) being trained for epoch 5 by 2019-01-26 04:59:14.942068!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1315
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 04:59:38.376003!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1039
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 05:00:04.655816!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1010
7. set (Dataset 24) being trained for epoch 5 by 2019-01-26 05:00:28.700652!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0723
8. set (Dataset 19) being trained for epoch 5 by 2019-01-26 05:00:46.114431!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0957
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 05:01:05.923313!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0936
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 05:01:29.344750!
Epoch 1/1
485/485 [==============================] - 13s 27ms/step - loss: 0.0817
11. set (Dataset 2) being trained for epoch 5 by 2019-01-26 05:01:47.372845!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.1111
12. set (Dataset 4) being trained for epoch 5 by 2019-01-26 05:02:08.040595!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.1129
13. set (Dataset 16) being trained for epoch 5 by 2019-01-26 05:02:36.247872!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0953
14. set (Dataset 1) being trained for epoch 5 by 2019-01-26 05:03:04.829364!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1278
15. set (Dataset 17) being trained for epoch 5 by 2019-01-26 05:03:20.865410!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0883
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 05:03:36.409832!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0873
17. set (Dataset 11) being trained for epoch 5 by 2019-01-26 05:03:55.920227!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0749
18. set (Dataset 10) being trained for epoch 5 by 2019-01-26 05:04:17.962527!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.1015
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 05:04:42.096181!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1320
20. set (Dataset 6) being trained for epoch 5 by 2019-01-26 05:05:01.590447!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1228
Epoch 5 completed!
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24,
'M14'), (19, 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'),
 (20, 'M12'), (11, 'M05'), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 05:05:16.893796
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 36.25 Degree
        The absolute mean error on Yaw angle estimation: 38.61 Degree
        The absolute mean error on Roll angle estimation: 9.42 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.63 Degree
        The absolute mean error on Yaw angle estimation: 30.99 Degree
        The absolute mean error on Roll angle estimation: 3.44 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 19.30 Degree
        The absolute mean error on Yaw angle estimation: 29.05 Degree
        The absolute mean error on Roll angle estimation: 7.98 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 12.10 Degree
        The absolute mean error on Yaw angle estimation: 33.15 Degree
        The absolute mean error on Roll angle estimation: 13.42 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 19.82 Degree
        The absolute mean error on Yaw angle estimations: 32.95 Degree
        The absolute mean error on Roll angle estimations: 8.56 Degree
Exp2019-01-26_04-28-49_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 05:06:46.281809
1. set (Dataset 10) being trained for epoch 1 by 2019-01-26 05:06:53.487837!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0907
2. set (Dataset 6) being trained for epoch 1 by 2019-01-26 05:07:16.836326!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1160
3. set (Dataset 2) being trained for epoch 1 by 2019-01-26 05:07:35.576914!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1048
4. set (Dataset 17) being trained for epoch 1 by 2019-01-26 05:07:52.301772!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0842
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 05:08:09.869102!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0984
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 05:08:34.489780!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1229
7. set (Dataset 11) being trained for epoch 1 by 2019-01-26 05:08:54.552133!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0702
8. set (Dataset 4) being trained for epoch 1 by 2019-01-26 05:09:16.792885!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1058
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 05:09:43.094568!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0948
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 05:10:09.666108!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0862
11. set (Dataset 24) being trained for epoch 1 by 2019-01-26 05:10:33.400735!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0744
12. set (Dataset 15) being trained for epoch 1 by 2019-01-26 05:10:51.983250!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0928
13. set (Dataset 1) being trained for epoch 1 by 2019-01-26 05:11:13.412373!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1173
14. set (Dataset 22) being trained for epoch 1 by 2019-01-26 05:11:32.154269!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0708
15. set (Dataset 18) being trained for epoch 1 by 2019-01-26 05:11:54.848362!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1170
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 05:12:15.664876!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0814
17. set (Dataset 16) being trained for epoch 1 by 2019-01-26 05:12:38.861152!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0840
18. set (Dataset 21) being trained for epoch 1 by 2019-01-26 05:13:07.396020!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1312
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 05:13:28.481224!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0840
20. set (Dataset 19) being trained for epoch 1 by 2019-01-26 05:13:45.686781!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0903
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:14:02.883146
1. set (Dataset 21) being trained for epoch 2 by 2019-01-26 05:14:08.885982!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1232
2. set (Dataset 19) being trained for epoch 2 by 2019-01-26 05:14:29.884001!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0862
3. set (Dataset 24) being trained for epoch 2 by 2019-01-26 05:14:46.810803!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0670
4. set (Dataset 18) being trained for epoch 2 by 2019-01-26 05:15:05.137386!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.1030
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 05:15:26.632837!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1153
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 05:15:46.201917!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0798
7. set (Dataset 16) being trained for epoch 2 by 2019-01-26 05:16:07.618261!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0812
8. set (Dataset 15) being trained for epoch 2 by 2019-01-26 05:16:36.542357!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0878
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 05:17:00.648825!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0965
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 05:17:28.041790!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0963
11. set (Dataset 11) being trained for epoch 2 by 2019-01-26 05:17:52.853509!
Epoch 1/1
572/572 [==============================] - 15s 27ms/step - loss: 0.0673
12. set (Dataset 10) being trained for epoch 2 by 2019-01-26 05:18:15.448685!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0876
13. set (Dataset 22) being trained for epoch 2 by 2019-01-26 05:18:40.700612!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0681
14. set (Dataset 6) being trained for epoch 2 by 2019-01-26 05:19:03.085189!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1190
15. set (Dataset 2) being trained for epoch 2 by 2019-01-26 05:19:22.274887!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1067
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 05:19:40.273199!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0803
17. set (Dataset 1) being trained for epoch 2 by 2019-01-26 05:19:59.083827!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1123
18. set (Dataset 17) being trained for epoch 2 by 2019-01-26 05:20:15.080463!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0803
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 05:20:32.538847!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0813
20. set (Dataset 4) being trained for epoch 2 by 2019-01-26 05:20:58.154980!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1083
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:21:21.422255
1. set (Dataset 17) being trained for epoch 3 by 2019-01-26 05:21:25.174028!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0803
2. set (Dataset 4) being trained for epoch 3 by 2019-01-26 05:21:42.522719!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0993
3. set (Dataset 11) being trained for epoch 3 by 2019-01-26 05:22:07.585812!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0697
4. set (Dataset 2) being trained for epoch 3 by 2019-01-26 05:22:27.138369!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1028
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 05:22:45.045150!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0759
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 05:23:04.475726!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0787
7. set (Dataset 1) being trained for epoch 3 by 2019-01-26 05:23:28.167714!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1104
8. set (Dataset 10) being trained for epoch 3 by 2019-01-26 05:23:48.192718!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0830
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 05:24:12.006273!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1211
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 05:24:33.827329!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0915
11. set (Dataset 16) being trained for epoch 3 by 2019-01-26 05:25:01.870736!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0853
12. set (Dataset 21) being trained for epoch 3 by 2019-01-26 05:25:31.176226!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1280
13. set (Dataset 6) being trained for epoch 3 by 2019-01-26 05:25:52.561853!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1125
14. set (Dataset 19) being trained for epoch 3 by 2019-01-26 05:26:10.902529!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0869
15. set (Dataset 24) being trained for epoch 3 by 2019-01-26 05:26:28.624758!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0641
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 05:26:46.896239!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0737
17. set (Dataset 22) being trained for epoch 3 by 2019-01-26 05:27:07.205811!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0646
18. set (Dataset 18) being trained for epoch 3 by 2019-01-26 05:27:29.612889!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.1046
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 05:27:52.836162!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0943
20. set (Dataset 15) being trained for epoch 3 by 2019-01-26 05:28:18.574130!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0844
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:28:39.201414
1. set (Dataset 18) being trained for epoch 4 by 2019-01-26 05:28:45.075009!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1008
2. set (Dataset 15) being trained for epoch 4 by 2019-01-26 05:29:06.895639!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0819
3. set (Dataset 16) being trained for epoch 4 by 2019-01-26 05:29:31.959484!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0797
4. set (Dataset 24) being trained for epoch 4 by 2019-01-26 05:29:59.834317!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0624
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 05:30:19.358131!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0809
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 05:30:45.142863!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0931
7. set (Dataset 22) being trained for epoch 4 by 2019-01-26 05:31:10.120943!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0610
8. set (Dataset 21) being trained for epoch 4 by 2019-01-26 05:31:33.125414!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1230
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 05:31:53.959870!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0791
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 05:32:11.372467!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1143
11. set (Dataset 1) being trained for epoch 4 by 2019-01-26 05:32:30.954705!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1090
12. set (Dataset 17) being trained for epoch 4 by 2019-01-26 05:32:47.624325!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0776
13. set (Dataset 19) being trained for epoch 4 by 2019-01-26 05:33:02.574953!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0810
14. set (Dataset 4) being trained for epoch 4 by 2019-01-26 05:33:23.114068!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1004
15. set (Dataset 11) being trained for epoch 4 by 2019-01-26 05:33:47.776908!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0676
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 05:34:07.405923!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0769
17. set (Dataset 6) being trained for epoch 4 by 2019-01-26 05:34:26.611031!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1106
18. set (Dataset 2) being trained for epoch 4 by 2019-01-26 05:34:45.181404!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1011
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 05:35:05.598144!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0899
20. set (Dataset 10) being trained for epoch 4 by 2019-01-26 05:35:32.158380!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0848
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:35:55.067866
1. set (Dataset 2) being trained for epoch 5 by 2019-01-26 05:36:00.146684!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0933
2. set (Dataset 10) being trained for epoch 5 by 2019-01-26 05:36:20.274787!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0827
3. set (Dataset 1) being trained for epoch 5 by 2019-01-26 05:36:44.417882!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0984
4. set (Dataset 11) being trained for epoch 5 by 2019-01-26 05:37:02.549156!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0696
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 05:37:24.203492!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0904
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 05:37:50.393732!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0840
7. set (Dataset 6) being trained for epoch 5 by 2019-01-26 05:38:15.485552!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1144
8. set (Dataset 17) being trained for epoch 5 by 2019-01-26 05:38:33.326186!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0759
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 05:38:50.940982!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0752
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 05:39:14.606836!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0728
11. set (Dataset 22) being trained for epoch 5 by 2019-01-26 05:39:33.270108!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0625
12. set (Dataset 18) being trained for epoch 5 by 2019-01-26 05:39:56.128320!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1029
13. set (Dataset 4) being trained for epoch 5 by 2019-01-26 05:40:18.692437!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0972
14. set (Dataset 15) being trained for epoch 5 by 2019-01-26 05:40:43.591286!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0859
15. set (Dataset 16) being trained for epoch 5 by 2019-01-26 05:41:08.869091!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0798
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 05:41:36.923325!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0750
17. set (Dataset 19) being trained for epoch 5 by 2019-01-26 05:41:55.575194!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0813
18. set (Dataset 24) being trained for epoch 5 by 2019-01-26 05:42:12.656292!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0597
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 05:42:30.447787!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1114
20. set (Dataset 21) being trained for epoch 5 by 2019-01-26 05:42:51.455067!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1217
Epoch 5 completed!
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F0
6'), (17, 'M10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'),
(20, 'M12'), (19, 'M11'), (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 05:43:09.288916
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.89 Degree
        The absolute mean error on Yaw angle estimation: 27.86 Degree
        The absolute mean error on Roll angle estimation: 24.26 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.54 Degree
        The absolute mean error on Yaw angle estimation: 28.96 Degree
        The absolute mean error on Roll angle estimation: 5.38 Degree
For the Subject 9 (M03):
882/882 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 25.41 Degree
        The absolute mean error on Yaw angle estimation: 24.68 Degree
        The absolute mean error on Roll angle estimation: 10.63 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.51 Degree
        The absolute mean error on Yaw angle estimation: 28.15 Degree
        The absolute mean error on Roll angle estimation: 14.35 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.09 Degree
        The absolute mean error on Yaw angle estimations: 27.41 Degree
        The absolute mean error on Roll angle estimations: 13.65 Degree
Exp2019-01-26_04-28-49_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 05:44:38.355710
1. set (Dataset 24) being trained for epoch 1 by 2019-01-26 05:44:43.027356!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0613
2. set (Dataset 21) being trained for epoch 1 by 2019-01-26 05:45:01.290534!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1124
3. set (Dataset 22) being trained for epoch 1 by 2019-01-26 05:45:23.491057!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0607
4. set (Dataset 16) being trained for epoch 1 by 2019-01-26 05:45:49.049931!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0774
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 05:46:20.013710!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0886
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 05:46:44.627027!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1069
7. set (Dataset 19) being trained for epoch 1 by 2019-01-26 05:47:03.763761!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0823
8. set (Dataset 18) being trained for epoch 1 by 2019-01-26 05:47:22.257770!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0986
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 05:47:45.462492!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0871
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 05:48:11.846161!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0799
11. set (Dataset 6) being trained for epoch 1 by 2019-01-26 05:48:35.172784!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1044
12. set (Dataset 2) being trained for epoch 1 by 2019-01-26 05:48:53.755356!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0928
13. set (Dataset 15) being trained for epoch 1 by 2019-01-26 05:49:13.371753!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0816
14. set (Dataset 10) being trained for epoch 1 by 2019-01-26 05:49:37.606429!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0832
15. set (Dataset 1) being trained for epoch 1 by 2019-01-26 05:50:01.667421!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0957
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 05:50:19.472535!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0730
17. set (Dataset 4) being trained for epoch 1 by 2019-01-26 05:50:40.668341!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0950
18. set (Dataset 11) being trained for epoch 1 by 2019-01-26 05:51:05.356600!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0679
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 05:51:24.961216!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0706
20. set (Dataset 17) being trained for epoch 1 by 2019-01-26 05:51:40.950399!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0731
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:51:55.270329
1. set (Dataset 11) being trained for epoch 2 by 2019-01-26 05:52:00.964363!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0679
2. set (Dataset 17) being trained for epoch 2 by 2019-01-26 05:52:19.577005!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0738
3. set (Dataset 6) being trained for epoch 2 by 2019-01-26 05:52:34.612518!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1066
4. set (Dataset 1) being trained for epoch 2 by 2019-01-26 05:52:53.501477!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0987
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 05:53:11.624896!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1073
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 05:53:31.478954!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0677
7. set (Dataset 4) being trained for epoch 2 by 2019-01-26 05:53:51.126394!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0920
8. set (Dataset 2) being trained for epoch 2 by 2019-01-26 05:54:14.841847!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0914
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 05:54:35.500334!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0862
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 05:55:02.991929!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0808
11. set (Dataset 19) being trained for epoch 2 by 2019-01-26 05:55:26.978931!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0841
12. set (Dataset 24) being trained for epoch 2 by 2019-01-26 05:55:44.149627!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0633
13. set (Dataset 10) being trained for epoch 2 by 2019-01-26 05:56:03.493398!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0799
14. set (Dataset 21) being trained for epoch 2 by 2019-01-26 05:56:27.968519!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1205
15. set (Dataset 22) being trained for epoch 2 by 2019-01-26 05:56:51.068281!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0598
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 05:57:12.749650!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0727
17. set (Dataset 15) being trained for epoch 2 by 2019-01-26 05:57:33.092315!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0796
18. set (Dataset 16) being trained for epoch 2 by 2019-01-26 05:57:58.513862!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0790
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 05:58:28.795697!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0750
20. set (Dataset 18) being trained for epoch 2 by 2019-01-26 05:58:53.436519!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0988
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 05:59:13.268392
1. set (Dataset 16) being trained for epoch 3 by 2019-01-26 05:59:21.989897!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0765
2. set (Dataset 18) being trained for epoch 3 by 2019-01-26 05:59:51.115318!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0931
3. set (Dataset 19) being trained for epoch 3 by 2019-01-26 06:00:11.396071!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0789
4. set (Dataset 22) being trained for epoch 3 by 2019-01-26 06:00:30.249512!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0594
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 06:00:52.433249!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0705
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 06:01:12.092859!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0724
7. set (Dataset 15) being trained for epoch 3 by 2019-01-26 06:01:37.516640!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0788
8. set (Dataset 24) being trained for epoch 3 by 2019-01-26 06:01:59.446643!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0591
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 06:02:17.336046!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1073
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 06:02:39.534748!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0852
11. set (Dataset 4) being trained for epoch 3 by 2019-01-26 06:03:06.604413!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.0903
12. set (Dataset 11) being trained for epoch 3 by 2019-01-26 06:03:30.568696!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0637
13. set (Dataset 21) being trained for epoch 3 by 2019-01-26 06:03:51.391728!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1173
14. set (Dataset 17) being trained for epoch 3 by 2019-01-26 06:04:11.049850!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0746
15. set (Dataset 6) being trained for epoch 3 by 2019-01-26 06:04:26.327237!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1021
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 06:04:45.320249!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0706
17. set (Dataset 10) being trained for epoch 3 by 2019-01-26 06:05:06.418178!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0793
18. set (Dataset 1) being trained for epoch 3 by 2019-01-26 06:05:30.634262!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0940
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 06:05:50.727014!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0824
20. set (Dataset 2) being trained for epoch 3 by 2019-01-26 06:06:14.500544!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0957
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:06:31.283130
1. set (Dataset 1) being trained for epoch 4 by 2019-01-26 06:06:36.314993!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0910
2. set (Dataset 2) being trained for epoch 4 by 2019-01-26 06:06:54.098015!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0827
3. set (Dataset 4) being trained for epoch 4 by 2019-01-26 06:07:14.794473!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0930
4. set (Dataset 6) being trained for epoch 4 by 2019-01-26 06:07:39.018507!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1079
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 06:07:59.878546!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0704
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 06:08:25.962126!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0815
7. set (Dataset 10) being trained for epoch 4 by 2019-01-26 06:08:52.029688!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0760
8. set (Dataset 11) being trained for epoch 4 by 2019-01-26 06:09:15.735446!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0651
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 06:09:35.034384!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0710
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 06:09:52.735269!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1054
11. set (Dataset 15) being trained for epoch 4 by 2019-01-26 06:10:13.221679!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0811
12. set (Dataset 16) being trained for epoch 4 by 2019-01-26 06:10:38.337325!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0746
13. set (Dataset 17) being trained for epoch 4 by 2019-01-26 06:11:05.635006!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0725
14. set (Dataset 18) being trained for epoch 4 by 2019-01-26 06:11:21.894900!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0971
15. set (Dataset 19) being trained for epoch 4 by 2019-01-26 06:11:42.608912!
Epoch 1/1
502/502 [==============================] - 11s 22ms/step - loss: 0.0777
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 06:11:59.194360!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0650
17. set (Dataset 21) being trained for epoch 4 by 2019-01-26 06:12:19.059406!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1165
18. set (Dataset 22) being trained for epoch 4 by 2019-01-26 06:12:41.797711!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0589
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 06:13:05.914645!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0843
20. set (Dataset 24) being trained for epoch 4 by 2019-01-26 06:13:30.460105!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0582
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:13:47.177261
1. set (Dataset 22) being trained for epoch 5 by 2019-01-26 06:13:53.572974!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0550
2. set (Dataset 24) being trained for epoch 5 by 2019-01-26 06:14:15.402954!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0565
3. set (Dataset 15) being trained for epoch 5 by 2019-01-26 06:14:34.558485!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0759
4. set (Dataset 19) being trained for epoch 5 by 2019-01-26 06:14:55.689539!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0734
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 06:15:15.817698!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0830
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 06:15:42.047288!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0817
7. set (Dataset 21) being trained for epoch 5 by 2019-01-26 06:16:07.299976!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1131
8. set (Dataset 16) being trained for epoch 5 by 2019-01-26 06:16:32.376623!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0726
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 06:17:02.938477!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0716
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 06:17:26.482968!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0684
11. set (Dataset 10) being trained for epoch 5 by 2019-01-26 06:17:46.133796!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0748
12. set (Dataset 1) being trained for epoch 5 by 2019-01-26 06:18:09.704676!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0884
13. set (Dataset 18) being trained for epoch 5 by 2019-01-26 06:18:28.488470!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.1006
14. set (Dataset 2) being trained for epoch 5 by 2019-01-26 06:18:49.848012!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0859
15. set (Dataset 4) being trained for epoch 5 by 2019-01-26 06:19:09.930339!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0874
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 06:19:34.222813!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0729
17. set (Dataset 17) being trained for epoch 5 by 2019-01-26 06:19:51.313291!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0715
18. set (Dataset 6) being trained for epoch 5 by 2019-01-26 06:20:06.918240!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1027
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 06:20:26.629349!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1072
20. set (Dataset 11) being trained for epoch 5 by 2019-01-26 06:20:46.774843!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0617
Epoch 5 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21,
'F02'), (16, 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 06:21:03.480772
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.73 Degree
        The absolute mean error on Yaw angle estimation: 28.78 Degree
        The absolute mean error on Roll angle estimation: 15.75 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.48 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 4.81 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 27.43 Degree
        The absolute mean error on Yaw angle estimation: 21.95 Degree
        The absolute mean error on Roll angle estimation: 8.51 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.05 Degree
        The absolute mean error on Yaw angle estimation: 28.14 Degree
        The absolute mean error on Roll angle estimation: 13.34 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.67 Degree
        The absolute mean error on Yaw angle estimations: 26.39 Degree
        The absolute mean error on Roll angle estimations: 10.60 Degree
Exp2019-01-26_04-28-49_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 06:22:32.365899
1. set (Dataset 6) being trained for epoch 1 by 2019-01-26 06:22:37.549368!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1010
2. set (Dataset 11) being trained for epoch 1 by 2019-01-26 06:22:56.963269!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0593
3. set (Dataset 10) being trained for epoch 1 by 2019-01-26 06:23:18.452463!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0750
4. set (Dataset 4) being trained for epoch 1 by 2019-01-26 06:23:44.316888!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0889
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 06:24:10.461272!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0798
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 06:24:36.360582!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1076
7. set (Dataset 17) being trained for epoch 1 by 2019-01-26 06:24:54.416247!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0735
8. set (Dataset 1) being trained for epoch 1 by 2019-01-26 06:25:09.296393!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0914
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 06:25:29.823345!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0793
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 06:25:55.999272!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0657
11. set (Dataset 21) being trained for epoch 1 by 2019-01-26 06:26:20.101883!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1142
12. set (Dataset 22) being trained for epoch 1 by 2019-01-26 06:26:42.279079!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0572
13. set (Dataset 2) being trained for epoch 1 by 2019-01-26 06:27:04.056751!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0861
14. set (Dataset 24) being trained for epoch 1 by 2019-01-26 06:27:21.750660!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0624
15. set (Dataset 15) being trained for epoch 1 by 2019-01-26 06:27:40.477855!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0793
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 06:28:02.135544!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0658
17. set (Dataset 18) being trained for epoch 1 by 2019-01-26 06:28:22.189104!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0928
18. set (Dataset 19) being trained for epoch 1 by 2019-01-26 06:28:42.701004!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0768
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 06:29:00.490016!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0665
20. set (Dataset 16) being trained for epoch 1 by 2019-01-26 06:29:21.899223!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0724
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:29:48.744448
1. set (Dataset 19) being trained for epoch 2 by 2019-01-26 06:29:53.654326!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0749
2. set (Dataset 16) being trained for epoch 2 by 2019-01-26 06:30:14.667899!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0684
3. set (Dataset 21) being trained for epoch 2 by 2019-01-26 06:30:43.448925!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1090
4. set (Dataset 15) being trained for epoch 2 by 2019-01-26 06:31:06.258474!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0747
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 06:31:28.983845!
Epoch 1/1
569/569 [==============================] - 15s 25ms/step - loss: 0.0996
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 06:31:48.399290!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0662
7. set (Dataset 18) being trained for epoch 2 by 2019-01-26 06:32:06.403444!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0904
8. set (Dataset 22) being trained for epoch 2 by 2019-01-26 06:32:28.469473!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0566
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 06:32:52.688130!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0808
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 06:33:19.922978!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0769
11. set (Dataset 17) being trained for epoch 2 by 2019-01-26 06:33:42.335483!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0722
12. set (Dataset 6) being trained for epoch 2 by 2019-01-26 06:33:57.233743!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0988
13. set (Dataset 24) being trained for epoch 2 by 2019-01-26 06:34:15.173460!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0584
14. set (Dataset 11) being trained for epoch 2 by 2019-01-26 06:34:33.131784!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0605
15. set (Dataset 10) being trained for epoch 2 by 2019-01-26 06:34:54.902859!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0728
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 06:35:18.908211!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0699
17. set (Dataset 2) being trained for epoch 2 by 2019-01-26 06:35:38.348906!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0899
18. set (Dataset 4) being trained for epoch 2 by 2019-01-26 06:35:58.265034!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0887
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 06:36:24.177876!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0680
20. set (Dataset 1) being trained for epoch 2 by 2019-01-26 06:36:47.589158!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0869
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:37:04.294037
1. set (Dataset 4) being trained for epoch 3 by 2019-01-26 06:37:11.657852!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0834
2. set (Dataset 1) being trained for epoch 3 by 2019-01-26 06:37:35.699017!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0836
3. set (Dataset 17) being trained for epoch 3 by 2019-01-26 06:37:51.458126!
Epoch 1/1
395/395 [==============================] - 8s 20ms/step - loss: 0.0738
4. set (Dataset 10) being trained for epoch 3 by 2019-01-26 06:38:06.526138!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0714
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 06:38:29.685901!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0657
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 06:38:48.919626!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0645
7. set (Dataset 2) being trained for epoch 3 by 2019-01-26 06:39:12.318343!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0800
8. set (Dataset 6) being trained for epoch 3 by 2019-01-26 06:39:30.598890!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1073
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 06:39:49.771989!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1025
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 06:40:11.655101!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0776
11. set (Dataset 18) being trained for epoch 3 by 2019-01-26 06:40:36.950867!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0948
12. set (Dataset 19) being trained for epoch 3 by 2019-01-26 06:40:57.642910!
Epoch 1/1
502/502 [==============================] - 13s 27ms/step - loss: 0.0761
13. set (Dataset 11) being trained for epoch 3 by 2019-01-26 06:41:16.899161!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0585
14. set (Dataset 16) being trained for epoch 3 by 2019-01-26 06:41:40.376685!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0711
15. set (Dataset 21) being trained for epoch 3 by 2019-01-26 06:42:09.952505!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1091
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 06:42:31.562126!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0641
17. set (Dataset 24) being trained for epoch 3 by 2019-01-26 06:42:50.197140!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0564
18. set (Dataset 15) being trained for epoch 3 by 2019-01-26 06:43:09.116688!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0729
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 06:43:33.512179!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0797
20. set (Dataset 22) being trained for epoch 3 by 2019-01-26 06:43:59.101112!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0531
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:44:20.072421
1. set (Dataset 15) being trained for epoch 4 by 2019-01-26 06:44:26.430470!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0717
2. set (Dataset 22) being trained for epoch 4 by 2019-01-26 06:44:48.896138!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0540
3. set (Dataset 18) being trained for epoch 4 by 2019-01-26 06:45:11.719960!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0918
4. set (Dataset 21) being trained for epoch 4 by 2019-01-26 06:45:33.763074!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1085
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 06:45:56.786181!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0693
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 06:46:22.638520!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0779
7. set (Dataset 24) being trained for epoch 4 by 2019-01-26 06:46:46.033958!
Epoch 1/1
492/492 [==============================] - 13s 25ms/step - loss: 0.0610
8. set (Dataset 19) being trained for epoch 4 by 2019-01-26 06:47:03.546703!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0732
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 06:47:20.951917!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0656
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 06:47:39.038326!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0999
11. set (Dataset 2) being trained for epoch 4 by 2019-01-26 06:47:58.236340!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0817
12. set (Dataset 4) being trained for epoch 4 by 2019-01-26 06:48:18.467803!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0868
13. set (Dataset 16) being trained for epoch 4 by 2019-01-26 06:48:45.533716!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0732
14. set (Dataset 1) being trained for epoch 4 by 2019-01-26 06:49:13.197691!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0894
15. set (Dataset 17) being trained for epoch 4 by 2019-01-26 06:49:29.304670!
Epoch 1/1
395/395 [==============================] - 9s 23ms/step - loss: 0.0698
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 06:49:44.041615!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0636
17. set (Dataset 11) being trained for epoch 4 by 2019-01-26 06:50:03.619269!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0585
18. set (Dataset 10) being trained for epoch 4 by 2019-01-26 06:50:25.343654!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0718
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 06:50:51.643237!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0776
20. set (Dataset 6) being trained for epoch 4 by 2019-01-26 06:51:16.848257!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1035
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 06:51:35.340047
1. set (Dataset 10) being trained for epoch 5 by 2019-01-26 06:51:42.554755!
Epoch 1/1
726/726 [==============================] - 19s 27ms/step - loss: 0.0709
2. set (Dataset 6) being trained for epoch 5 by 2019-01-26 06:52:07.067375!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0963
3. set (Dataset 2) being trained for epoch 5 by 2019-01-26 06:52:25.742531!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0825
4. set (Dataset 17) being trained for epoch 5 by 2019-01-26 06:52:42.232237!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0715
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 06:52:59.709024!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0732
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 06:53:25.998008!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0748
7. set (Dataset 11) being trained for epoch 5 by 2019-01-26 06:53:51.306157!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0563
8. set (Dataset 4) being trained for epoch 5 by 2019-01-26 06:54:13.107043!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0865
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 06:54:39.092066!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0657
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 06:55:02.043762!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0634
11. set (Dataset 24) being trained for epoch 5 by 2019-01-26 06:55:19.296313!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0609
12. set (Dataset 15) being trained for epoch 5 by 2019-01-26 06:55:38.686235!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0736
13. set (Dataset 1) being trained for epoch 5 by 2019-01-26 06:56:00.062741!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0869
14. set (Dataset 22) being trained for epoch 5 by 2019-01-26 06:56:19.089110!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0553
15. set (Dataset 18) being trained for epoch 5 by 2019-01-26 06:56:41.661206!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0942
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 06:57:02.345884!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0635
17. set (Dataset 16) being trained for epoch 5 by 2019-01-26 06:57:25.126876!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0706
18. set (Dataset 21) being trained for epoch 5 by 2019-01-26 06:57:54.141694!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1065
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 06:58:15.501673!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1000
20. set (Dataset 19) being trained for epoch 5 by 2019-01-26 06:58:34.975049!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0745
Epoch 5 completed!
The subjects are trained: [(10, 'M04'), (6, 'F06'), (2, 'F02'), (17, 'M10'), (7, 'M01'), (8, 'M02'), (11, 'M
05'), (4, 'F04'), (12, 'M06'), (13, 'M07'), (24, 'M14'), (15, 'F03'), (1, 'F01'), (22, 'M01'), (18, 'F05'),
(20, 'M12'), (16, 'M09'), (21, 'F02'), (23, 'M13'), (19, 'M11')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 06:58:49.971824
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.08 Degree
        The absolute mean error on Yaw angle estimation: 24.35 Degree
        The absolute mean error on Roll angle estimation: 15.10 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.11 Degree
        The absolute mean error on Yaw angle estimation: 28.24 Degree
        The absolute mean error on Roll angle estimation: 3.55 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 23.88 Degree
        The absolute mean error on Yaw angle estimation: 23.90 Degree
        The absolute mean error on Roll angle estimation: 7.48 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.77 Degree
        The absolute mean error on Yaw angle estimation: 29.53 Degree
        The absolute mean error on Roll angle estimation: 13.02 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.21 Degree
        The absolute mean error on Yaw angle estimations: 26.51 Degree
        The absolute mean error on Roll angle estimations: 9.79 Degree
Exp2019-01-26_04-28-49_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 07:00:18.968931
1. set (Dataset 21) being trained for epoch 1 by 2019-01-26 07:00:24.974574!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1082
2. set (Dataset 19) being trained for epoch 1 by 2019-01-26 07:00:46.229432!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0719
3. set (Dataset 24) being trained for epoch 1 by 2019-01-26 07:01:03.896012!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0556
4. set (Dataset 18) being trained for epoch 1 by 2019-01-26 07:01:22.617814!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0873
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 07:01:46.156159!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0835
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 07:02:11.205423!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0978
7. set (Dataset 16) being trained for epoch 1 by 2019-01-26 07:02:34.900086!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0700
8. set (Dataset 15) being trained for epoch 1 by 2019-01-26 07:03:03.902567!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0756
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 07:03:27.507377!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0789
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 07:03:53.028437!
Epoch 1/1
732/732 [==============================] - 18s 24ms/step - loss: 0.0649
11. set (Dataset 11) being trained for epoch 1 by 2019-01-26 07:04:16.677250!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0591
12. set (Dataset 10) being trained for epoch 1 by 2019-01-26 07:04:38.516222!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0718
13. set (Dataset 22) being trained for epoch 1 by 2019-01-26 07:05:02.752774!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0547
14. set (Dataset 6) being trained for epoch 1 by 2019-01-26 07:05:24.574888!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0999
15. set (Dataset 2) being trained for epoch 1 by 2019-01-26 07:05:43.835108!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0779
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 07:06:01.936891!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0661
17. set (Dataset 1) being trained for epoch 1 by 2019-01-26 07:06:21.110182!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0919
18. set (Dataset 17) being trained for epoch 1 by 2019-01-26 07:06:37.964055!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0714
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 07:06:53.286172!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0644
20. set (Dataset 4) being trained for epoch 1 by 2019-01-26 07:07:12.998053!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0859
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:07:36.282711
1. set (Dataset 17) being trained for epoch 2 by 2019-01-26 07:07:40.027415!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0696
2. set (Dataset 4) being trained for epoch 2 by 2019-01-26 07:07:57.717044!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0831
3. set (Dataset 11) being trained for epoch 2 by 2019-01-26 07:08:22.671893!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0591
4. set (Dataset 2) being trained for epoch 2 by 2019-01-26 07:08:42.743856!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0796
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 07:09:01.413696!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1011
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 07:09:20.549134!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0618
7. set (Dataset 1) being trained for epoch 2 by 2019-01-26 07:09:37.575631!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0829
8. set (Dataset 10) being trained for epoch 2 by 2019-01-26 07:09:57.288650!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0698
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 07:10:23.674479!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0749
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 07:10:50.289780!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0746
11. set (Dataset 16) being trained for epoch 2 by 2019-01-26 07:11:18.395470!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0721
12. set (Dataset 21) being trained for epoch 2 by 2019-01-26 07:11:47.806766!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1106
13. set (Dataset 6) being trained for epoch 2 by 2019-01-26 07:12:09.461733!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0963
14. set (Dataset 19) being trained for epoch 2 by 2019-01-26 07:12:27.809420!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0712
15. set (Dataset 24) being trained for epoch 2 by 2019-01-26 07:12:45.280018!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0567
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 07:13:02.983279!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0623
17. set (Dataset 22) being trained for epoch 2 by 2019-01-26 07:13:23.600502!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0539
18. set (Dataset 18) being trained for epoch 2 by 2019-01-26 07:13:46.626894!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0864
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 07:14:09.685006!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0715
20. set (Dataset 15) being trained for epoch 2 by 2019-01-26 07:14:34.227531!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0773
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:14:55.476377
1. set (Dataset 18) being trained for epoch 3 by 2019-01-26 07:15:01.371624!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0886
2. set (Dataset 15) being trained for epoch 3 by 2019-01-26 07:15:23.230052!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0706
3. set (Dataset 16) being trained for epoch 3 by 2019-01-26 07:15:48.551778!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0669
4. set (Dataset 24) being trained for epoch 3 by 2019-01-26 07:16:16.025434!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0543
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 07:16:33.498287!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0648
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 07:16:53.601546!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0663
7. set (Dataset 22) being trained for epoch 3 by 2019-01-26 07:17:18.920329!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0533
8. set (Dataset 21) being trained for epoch 3 by 2019-01-26 07:17:41.665276!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1080
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 07:18:03.540026!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0990
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 07:18:25.941365!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0785
11. set (Dataset 1) being trained for epoch 3 by 2019-01-26 07:18:50.556563!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0874
12. set (Dataset 17) being trained for epoch 3 by 2019-01-26 07:19:06.922002!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0679
13. set (Dataset 19) being trained for epoch 3 by 2019-01-26 07:19:22.126006!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0723
14. set (Dataset 4) being trained for epoch 3 by 2019-01-26 07:19:41.758889!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0858
15. set (Dataset 11) being trained for epoch 3 by 2019-01-26 07:20:05.989063!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0570
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 07:20:25.846769!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0663
17. set (Dataset 6) being trained for epoch 3 by 2019-01-26 07:20:44.899385!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0956
18. set (Dataset 2) being trained for epoch 3 by 2019-01-26 07:21:03.355762!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0780
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 07:21:23.836741!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0733
20. set (Dataset 10) being trained for epoch 3 by 2019-01-26 07:21:49.945170!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0710
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:22:12.219164
1. set (Dataset 2) being trained for epoch 4 by 2019-01-26 07:22:17.293133!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0762
2. set (Dataset 10) being trained for epoch 4 by 2019-01-26 07:22:37.266400!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0673
3. set (Dataset 1) being trained for epoch 4 by 2019-01-26 07:23:00.319661!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0842
4. set (Dataset 11) being trained for epoch 4 by 2019-01-26 07:23:18.680572!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0564
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 07:23:40.720390!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0616
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 07:24:06.668910!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0736
7. set (Dataset 6) being trained for epoch 4 by 2019-01-26 07:24:30.513360!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0989
8. set (Dataset 17) being trained for epoch 4 by 2019-01-26 07:24:47.466546!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0713
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 07:25:02.287289!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0603
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 07:25:19.716311!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1027
11. set (Dataset 22) being trained for epoch 4 by 2019-01-26 07:25:40.561218!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0522
12. set (Dataset 18) being trained for epoch 4 by 2019-01-26 07:26:02.998499!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0895
13. set (Dataset 4) being trained for epoch 4 by 2019-01-26 07:26:26.148324!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0899
14. set (Dataset 15) being trained for epoch 4 by 2019-01-26 07:26:51.458968!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0734
15. set (Dataset 16) being trained for epoch 4 by 2019-01-26 07:27:16.972203!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0682
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 07:27:45.717291!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0615
17. set (Dataset 19) being trained for epoch 4 by 2019-01-26 07:28:04.728431!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0704
18. set (Dataset 24) being trained for epoch 4 by 2019-01-26 07:28:22.194574!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0551
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 07:28:42.069694!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0771
20. set (Dataset 21) being trained for epoch 4 by 2019-01-26 07:29:07.733035!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1047
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:29:28.251958
1. set (Dataset 24) being trained for epoch 5 by 2019-01-26 07:29:32.928084!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0554
2. set (Dataset 21) being trained for epoch 5 by 2019-01-26 07:29:51.614716!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1062
3. set (Dataset 22) being trained for epoch 5 by 2019-01-26 07:30:14.432678!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0513
4. set (Dataset 16) being trained for epoch 5 by 2019-01-26 07:30:40.616721!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0674
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 07:31:11.588170!
Epoch 1/1
745/745 [==============================] - 18s 24ms/step - loss: 0.0747
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 07:31:37.580869!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0733
7. set (Dataset 19) being trained for epoch 5 by 2019-01-26 07:32:02.306500!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0728
8. set (Dataset 18) being trained for epoch 5 by 2019-01-26 07:32:21.431948!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0893
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 07:32:44.180337!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0655
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 07:33:07.663397!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0593
11. set (Dataset 6) being trained for epoch 5 by 2019-01-26 07:33:25.060136!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0975
12. set (Dataset 2) being trained for epoch 5 by 2019-01-26 07:33:43.535147!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0791
13. set (Dataset 15) being trained for epoch 5 by 2019-01-26 07:34:02.995378!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0715
14. set (Dataset 10) being trained for epoch 5 by 2019-01-26 07:34:26.540607!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0724
15. set (Dataset 1) being trained for epoch 5 by 2019-01-26 07:34:50.182225!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0843
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 07:35:08.172488!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0666
17. set (Dataset 4) being trained for epoch 5 by 2019-01-26 07:35:29.077850!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0831
18. set (Dataset 11) being trained for epoch 5 by 2019-01-26 07:35:53.621620!
Epoch 1/1
572/572 [==============================] - 15s 27ms/step - loss: 0.0553
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 07:36:14.354175!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0998
20. set (Dataset 17) being trained for epoch 5 by 2019-01-26 07:36:32.529632!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0692
Epoch 5 completed!
The subjects are trained: [(24, 'M14'), (21, 'F02'), (22, 'M01'), (16, 'M09'), (7, 'M01'), (8, 'M02'), (19,
'M11'), (18, 'F05'), (12, 'M06'), (13, 'M07'), (6, 'F06'), (2, 'F02'), (15, 'F03'), (10, 'M04'), (1, 'F01'),
 (20, 'M12'), (4, 'F04'), (11, 'M05'), (23, 'M13'), (17, 'M10')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 07:36:44.945946
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.64 Degree
        The absolute mean error on Yaw angle estimation: 29.55 Degree
        The absolute mean error on Roll angle estimation: 13.68 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.39 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 3.36 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 28.40 Degree
        The absolute mean error on Yaw angle estimation: 21.91 Degree
        The absolute mean error on Roll angle estimation: 7.64 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 17.03 Degree
        The absolute mean error on Yaw angle estimation: 28.84 Degree
        The absolute mean error on Roll angle estimation: 13.32 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.62 Degree
        The absolute mean error on Yaw angle estimations: 26.75 Degree
        The absolute mean error on Roll angle estimations: 9.50 Degree
Exp2019-01-26_04-28-49_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_04-28-49
All frames and annotations from 20 datasets have been read by 2019-01-26 07:38:13.760168
1. set (Dataset 11) being trained for epoch 1 by 2019-01-26 07:38:19.476468!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0561
2. set (Dataset 17) being trained for epoch 1 by 2019-01-26 07:38:37.523606!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0695
3. set (Dataset 6) being trained for epoch 1 by 2019-01-26 07:38:52.835590!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0959
4. set (Dataset 1) being trained for epoch 1 by 2019-01-26 07:39:11.158400!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0903
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 07:39:31.974097!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0744
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 07:39:56.390712!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0982
7. set (Dataset 4) being trained for epoch 1 by 2019-01-26 07:40:18.217087!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0811
8. set (Dataset 2) being trained for epoch 1 by 2019-01-26 07:40:42.113672!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0773
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 07:41:02.341650!
Epoch 1/1
745/745 [==============================] - 20s 26ms/step - loss: 0.0762
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 07:41:29.173613!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0589
11. set (Dataset 19) being trained for epoch 1 by 2019-01-26 07:41:53.285005!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0746
12. set (Dataset 24) being trained for epoch 1 by 2019-01-26 07:42:11.113035!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0577
13. set (Dataset 10) being trained for epoch 1 by 2019-01-26 07:42:30.466367!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0719
14. set (Dataset 21) being trained for epoch 1 by 2019-01-26 07:42:55.009466!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1089
15. set (Dataset 22) being trained for epoch 1 by 2019-01-26 07:43:17.347919!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0533
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 07:43:39.598660!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0624
17. set (Dataset 15) being trained for epoch 1 by 2019-01-26 07:43:59.641668!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0724
18. set (Dataset 16) being trained for epoch 1 by 2019-01-26 07:44:25.196358!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0674
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 07:44:52.843080!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0631
20. set (Dataset 18) being trained for epoch 1 by 2019-01-26 07:45:10.985443!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0898
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:45:30.832964
1. set (Dataset 16) being trained for epoch 2 by 2019-01-26 07:45:39.560001!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0646
2. set (Dataset 18) being trained for epoch 2 by 2019-01-26 07:46:08.604152!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0807
3. set (Dataset 19) being trained for epoch 2 by 2019-01-26 07:46:29.011900!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0727
4. set (Dataset 22) being trained for epoch 2 by 2019-01-26 07:46:47.904968!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0527
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 07:47:09.606779!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0971
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 07:47:29.341320!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0629
7. set (Dataset 15) being trained for epoch 2 by 2019-01-26 07:47:48.141713!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0702
8. set (Dataset 24) being trained for epoch 2 by 2019-01-26 07:48:09.096391!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0521
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 07:48:29.059040!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0758
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 07:48:56.520902!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0764
11. set (Dataset 4) being trained for epoch 2 by 2019-01-26 07:49:22.946224!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0811
12. set (Dataset 11) being trained for epoch 2 by 2019-01-26 07:49:47.460491!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0570
13. set (Dataset 21) being trained for epoch 2 by 2019-01-26 07:50:08.449487!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1066
14. set (Dataset 17) being trained for epoch 2 by 2019-01-26 07:50:28.385538!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0733
15. set (Dataset 6) being trained for epoch 2 by 2019-01-26 07:50:43.172187!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0943
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 07:51:02.167556!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0633
17. set (Dataset 10) being trained for epoch 2 by 2019-01-26 07:51:23.498855!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0732
18. set (Dataset 1) being trained for epoch 2 by 2019-01-26 07:51:47.190611!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0886
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 07:52:07.465784!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0598
20. set (Dataset 2) being trained for epoch 2 by 2019-01-26 07:52:30.802860!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0787
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 07:52:48.021722
1. set (Dataset 1) being trained for epoch 3 by 2019-01-26 07:52:53.059088!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0825
2. set (Dataset 2) being trained for epoch 3 by 2019-01-26 07:53:10.749161!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0724
3. set (Dataset 4) being trained for epoch 3 by 2019-01-26 07:53:30.833814!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0806
4. set (Dataset 6) being trained for epoch 3 by 2019-01-26 07:53:54.925029!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0998
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 07:54:13.271986!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0606
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 07:54:32.533976!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0608
7. set (Dataset 10) being trained for epoch 3 by 2019-01-26 07:54:57.974745!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0662
8. set (Dataset 11) being trained for epoch 3 by 2019-01-26 07:55:22.385488!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0553
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 07:55:42.096824!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0993
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 07:56:04.567371!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0742
11. set (Dataset 15) being trained for epoch 3 by 2019-01-26 07:56:30.913814!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0771
12. set (Dataset 16) being trained for epoch 3 by 2019-01-26 07:56:56.403145!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0664
13. set (Dataset 17) being trained for epoch 3 by 2019-01-26 07:57:23.254364!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0673
14. set (Dataset 18) being trained for epoch 3 by 2019-01-26 07:57:39.040620!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0873
15. set (Dataset 19) being trained for epoch 3 by 2019-01-26 07:57:59.178552!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0703
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 07:58:17.738052!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0592
17. set (Dataset 21) being trained for epoch 3 by 2019-01-26 07:58:37.579680!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1058
18. set (Dataset 22) being trained for epoch 3 by 2019-01-26 07:59:00.104717!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0504
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 07:59:24.393397!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0780
20. set (Dataset 24) being trained for epoch 3 by 2019-01-26 07:59:47.713737!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0556
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 08:00:04.571187
1. set (Dataset 22) being trained for epoch 4 by 2019-01-26 08:00:10.960619!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0514
2. set (Dataset 24) being trained for epoch 4 by 2019-01-26 08:00:32.805237!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0534
3. set (Dataset 15) being trained for epoch 4 by 2019-01-26 08:00:51.459594!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0702
4. set (Dataset 19) being trained for epoch 4 by 2019-01-26 08:01:12.680806!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0655
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 08:01:32.813718!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0674
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 08:01:58.771406!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0740
7. set (Dataset 21) being trained for epoch 4 by 2019-01-26 08:02:24.272413!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1082
8. set (Dataset 16) being trained for epoch 4 by 2019-01-26 08:02:48.851880!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0675
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 08:03:16.820251!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0616
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 08:03:34.864238!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0964
11. set (Dataset 10) being trained for epoch 4 by 2019-01-26 08:03:56.708783!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0724
12. set (Dataset 1) being trained for epoch 4 by 2019-01-26 08:04:20.506022!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0853
13. set (Dataset 18) being trained for epoch 4 by 2019-01-26 08:04:39.530026!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0905
14. set (Dataset 2) being trained for epoch 4 by 2019-01-26 08:05:00.047339!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0752
15. set (Dataset 4) being trained for epoch 4 by 2019-01-26 08:05:20.299933!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0832
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 08:05:44.483862!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0653
17. set (Dataset 17) being trained for epoch 4 by 2019-01-26 08:06:02.229564!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0684
18. set (Dataset 6) being trained for epoch 4 by 2019-01-26 08:06:17.654992!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0950
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 08:06:39.338423!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0739
20. set (Dataset 11) being trained for epoch 4 by 2019-01-26 08:07:04.665108!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0528
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 08:07:24.032018
1. set (Dataset 6) being trained for epoch 5 by 2019-01-26 08:07:29.216261!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0919
2. set (Dataset 11) being trained for epoch 5 by 2019-01-26 08:07:48.782211!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0525
3. set (Dataset 10) being trained for epoch 5 by 2019-01-26 08:08:10.759908!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0692
4. set (Dataset 4) being trained for epoch 5 by 2019-01-26 08:08:36.859104!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0805
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 08:09:03.540836!
Epoch 1/1
745/745 [==============================] - 20s 26ms/step - loss: 0.0748
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 08:09:30.970720!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0723
7. set (Dataset 17) being trained for epoch 5 by 2019-01-26 08:09:54.585840!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0682
8. set (Dataset 1) being trained for epoch 5 by 2019-01-26 08:10:09.464996!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0863
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 08:10:29.073630!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0595
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 08:10:52.447912!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0590
11. set (Dataset 21) being trained for epoch 5 by 2019-01-26 08:11:10.613418!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1097
12. set (Dataset 22) being trained for epoch 5 by 2019-01-26 08:11:32.742420!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0532
13. set (Dataset 2) being trained for epoch 5 by 2019-01-26 08:11:55.222057!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0768
14. set (Dataset 24) being trained for epoch 5 by 2019-01-26 08:12:12.532791!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0564
15. set (Dataset 15) being trained for epoch 5 by 2019-01-26 08:12:31.395378!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0740
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 08:12:52.955901!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0624
17. set (Dataset 18) being trained for epoch 5 by 2019-01-26 08:13:13.236528!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0840
18. set (Dataset 19) being trained for epoch 5 by 2019-01-26 08:13:33.948401!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0686
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 08:13:51.863731!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0950
20. set (Dataset 16) being trained for epoch 5 by 2019-01-26 08:14:15.452437!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0640
Epoch 5 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (7, 'M01'), (8, 'M02'), (17, 'M
10'), (1, 'F01'), (12, 'M06'), (13, 'M07'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'),
(20, 'M12'), (18, 'F05'), (19, 'M11'), (23, 'M13'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm10_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_04-28-49
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 08:14:40.592843
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.24 Degree
        The absolute mean error on Yaw angle estimation: 23.13 Degree
        The absolute mean error on Roll angle estimation: 15.32 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 14.24 Degree
        The absolute mean error on Yaw angle estimation: 27.71 Degree
        The absolute mean error on Roll angle estimation: 3.81 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 26.10 Degree
        The absolute mean error on Yaw angle estimation: 26.98 Degree
        The absolute mean error on Roll angle estimation: 7.45 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.52 Degree
        The absolute mean error on Yaw angle estimation: 30.62 Degree
        The absolute mean error on Roll angle estimation: 13.39 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 16.53 Degree
        The absolute mean error on Yaw angle estimations: 27.11 Degree
        The absolute mean error on Roll angle estimations: 9.99 Degree
Exp2019-01-26_04-28-49_part6 completed!
subject3_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.343922.
subject5_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.543420.
subject9_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.741643.
subject14_Exp2019-01-26_04-28-49.png has been saved by 2019-01-26 08:16:05.957915.
Model Exp2019-01-26_04-28-49 has been evaluated successfully.
Model Exp2019-01-26_04-28-49 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-26_04-28-49 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.p
y", line 52, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 260, in load_model
    f = h5py.File(filepath, mode='r')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py", line 269, in __init__
    fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/_hl/files.py", line 99, in make_fid
    fid = h5f.open(name, flags, fapl=fapl)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 78, in h5py.h5f.open
OSError: Unable to open file (unable to open file: name = '/home/mcicek/Datasets/Keras_Models/Exp2019-01-26_
04-28-49.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 13:37:42.044613: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 13:37:42.141623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 13:37:42.141885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 13:37:42.141897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 13:37:42.297498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 13:37:42.297525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 13:37:42.297529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 13:37:42.297666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_13-37-43 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 13:37:47.496164
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 13:37:53.897956!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.2134
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 13:38:17.428122!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.1537
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 13:38:35.826012!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.1627
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 13:38:57.157694!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1557
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 13:39:17.582164!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.1922
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 13:39:41.965695!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1938
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 13:40:02.478005!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1931
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 13:40:27.363862!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1485
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 13:40:58.079955!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.1751
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 13:41:24.479678!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.1414
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 13:41:50.880939!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.1474
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 13:42:13.757991!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1588
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 13:42:31.935777!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.1739
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 13:42:52.737112!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1646
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 13:43:12.947118!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1508
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 13:43:37.021597!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.1290
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 13:43:54.862511!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.1054
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 13:44:09.850323!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1716
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 13:44:28.180793!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.1019
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 13:44:45.863761!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.1089
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 13:45:04.521469
1. set (Dataset 6) being trained for epoch 2 by 2019-01-26 13:45:09.706344!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1545
2. set (Dataset 11) being trained for epoch 2 by 2019-01-26 13:45:28.570247!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0893
3. set (Dataset 10) being trained for epoch 2 by 2019-01-26 13:45:50.489836!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1188
4. set (Dataset 4) being trained for epoch 2 by 2019-01-26 13:46:16.252836!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.1267
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 13:46:40.262662!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1590
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 13:46:59.524336!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0916
7. set (Dataset 17) being trained for epoch 2 by 2019-01-26 13:47:15.329745!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.1010
8. set (Dataset 1) being trained for epoch 2 by 2019-01-26 13:47:30.105209!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1335
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 13:47:50.052174!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.1162
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 13:48:17.559026!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1081
11. set (Dataset 21) being trained for epoch 2 by 2019-01-26 13:48:42.280547!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1569
12. set (Dataset 22) being trained for epoch 2 by 2019-01-26 13:49:05.128362!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0871
13. set (Dataset 2) being trained for epoch 2 by 2019-01-26 13:49:27.366885!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1146
14. set (Dataset 24) being trained for epoch 2 by 2019-01-26 13:49:45.027558!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0905
15. set (Dataset 15) being trained for epoch 2 by 2019-01-26 13:50:03.433363!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.1117
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 13:50:25.737346!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0934
17. set (Dataset 18) being trained for epoch 2 by 2019-01-26 13:50:45.510993!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1411
18. set (Dataset 19) being trained for epoch 2 by 2019-01-26 13:51:05.647126!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1030
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 13:51:25.720885!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.1043
20. set (Dataset 16) being trained for epoch 2 by 2019-01-26 13:51:53.325940!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0994
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 13:52:20.929231
1. set (Dataset 19) being trained for epoch 3 by 2019-01-26 13:52:25.806612!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0918
2. set (Dataset 16) being trained for epoch 3 by 2019-01-26 13:52:47.585047!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0907
3. set (Dataset 21) being trained for epoch 3 by 2019-01-26 13:53:16.953899!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1418
4. set (Dataset 15) being trained for epoch 3 by 2019-01-26 13:53:39.021422!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.1024
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 13:54:01.136997!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0883
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 13:54:20.773588!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0873
7. set (Dataset 18) being trained for epoch 3 by 2019-01-26 13:54:44.983117!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1255
8. set (Dataset 22) being trained for epoch 3 by 2019-01-26 13:55:06.658491!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0741
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 13:55:29.571770!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1308
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 13:55:51.945928!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1072
11. set (Dataset 17) being trained for epoch 3 by 2019-01-26 13:56:14.805050!
Epoch 1/1
395/395 [==============================] - 11s 27ms/step - loss: 0.0876
12. set (Dataset 6) being trained for epoch 3 by 2019-01-26 13:56:30.578744!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1422
13. set (Dataset 24) being trained for epoch 3 by 2019-01-26 13:56:48.763587!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0733
14. set (Dataset 11) being trained for epoch 3 by 2019-01-26 13:57:06.880216!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0777
15. set (Dataset 10) being trained for epoch 3 by 2019-01-26 13:57:28.530031!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0960
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 13:57:51.998912!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0825
17. set (Dataset 2) being trained for epoch 3 by 2019-01-26 13:58:10.875157!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1068
18. set (Dataset 4) being trained for epoch 3 by 2019-01-26 13:58:30.760695!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.1154
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 13:58:57.795830!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0960
20. set (Dataset 1) being trained for epoch 3 by 2019-01-26 13:59:22.097381!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1149
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 13:59:38.836575
1. set (Dataset 4) being trained for epoch 4 by 2019-01-26 13:59:46.216868!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1029
2. set (Dataset 1) being trained for epoch 4 by 2019-01-26 14:00:10.228407!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1006
3. set (Dataset 17) being trained for epoch 4 by 2019-01-26 14:00:26.117745!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0861
4. set (Dataset 10) being trained for epoch 4 by 2019-01-26 14:00:43.553951!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0870
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 14:01:08.957133!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0800
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 14:01:35.209997!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0923
7. set (Dataset 2) being trained for epoch 4 by 2019-01-26 14:01:58.979624!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0928
8. set (Dataset 6) being trained for epoch 4 by 2019-01-26 14:02:17.108006!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1296
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 14:02:36.248809!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0765
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 14:02:53.633478!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1278
11. set (Dataset 18) being trained for epoch 4 by 2019-01-26 14:03:13.834329!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1228
12. set (Dataset 19) being trained for epoch 4 by 2019-01-26 14:03:34.280314!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0917
13. set (Dataset 11) being trained for epoch 4 by 2019-01-26 14:03:52.973101!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0704
14. set (Dataset 16) being trained for epoch 4 by 2019-01-26 14:04:16.136605!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0918
15. set (Dataset 21) being trained for epoch 4 by 2019-01-26 14:04:45.354851!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1347
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 14:05:06.858769!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0763
17. set (Dataset 24) being trained for epoch 4 by 2019-01-26 14:05:25.821704!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0739
18. set (Dataset 15) being trained for epoch 4 by 2019-01-26 14:05:44.538353!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0889
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 14:06:08.338253!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0963
20. set (Dataset 22) being trained for epoch 4 by 2019-01-26 14:06:34.493181!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0659
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:06:56.001676
1. set (Dataset 15) being trained for epoch 5 by 2019-01-26 14:07:02.350262!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0907
2. set (Dataset 22) being trained for epoch 5 by 2019-01-26 14:07:24.954878!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0629
3. set (Dataset 18) being trained for epoch 5 by 2019-01-26 14:07:47.222554!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1152
4. set (Dataset 21) being trained for epoch 5 by 2019-01-26 14:08:08.506640!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1261
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 14:08:32.683875!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0944
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 14:08:59.336003!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0897
7. set (Dataset 24) being trained for epoch 5 by 2019-01-26 14:09:23.357499!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0710
8. set (Dataset 19) being trained for epoch 5 by 2019-01-26 14:09:40.108793!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0900
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 14:09:59.885154!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0820
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 14:10:22.924358!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0757
11. set (Dataset 2) being trained for epoch 5 by 2019-01-26 14:10:39.682459!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0959
12. set (Dataset 4) being trained for epoch 5 by 2019-01-26 14:10:59.791402!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0988
13. set (Dataset 16) being trained for epoch 5 by 2019-01-26 14:11:27.180528!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0876
14. set (Dataset 1) being trained for epoch 5 by 2019-01-26 14:11:55.313939!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1049
15. set (Dataset 17) being trained for epoch 5 by 2019-01-26 14:12:11.944553!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0873
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 14:12:27.214537!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0772
17. set (Dataset 11) being trained for epoch 5 by 2019-01-26 14:12:46.572954!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0681
18. set (Dataset 10) being trained for epoch 5 by 2019-01-26 14:13:08.055256!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0824
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 14:13:32.359488!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1233
20. set (Dataset 6) being trained for epoch 5 by 2019-01-26 14:13:52.056260!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1204
Epoch 5 completed!
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24,
'M14'), (19, 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'),
 (20, 'M12'), (11, 'M05'), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 14:14:07.707520
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.43 Degree
        The absolute mean error on Yaw angle estimation: 32.58 Degree
        The absolute mean error on Roll angle estimation: 14.18 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.77 Degree
        The absolute mean error on Yaw angle estimation: 28.26 Degree
        The absolute mean error on Roll angle estimation: 3.76 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 25.82 Degree
        The absolute mean error on Yaw angle estimation: 27.21 Degree
        The absolute mean error on Roll angle estimation: 8.53 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 15.19 Degree
        The absolute mean error on Yaw angle estimation: 26.62 Degree
        The absolute mean error on Roll angle estimation: 13.37 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.80 Degree
        The absolute mean error on Yaw angle estimations: 28.67 Degree
        The absolute mean error on Roll angle estimations: 9.96 Degree
Exp2019-01-26_13-37-43_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 14:15:36.631298
1. set (Dataset 10) being trained for epoch 1 by 2019-01-26 14:15:43.894822!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0806
2. set (Dataset 6) being trained for epoch 1 by 2019-01-26 14:16:07.691099!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1075
3. set (Dataset 2) being trained for epoch 1 by 2019-01-26 14:16:26.981420!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0848
4. set (Dataset 17) being trained for epoch 1 by 2019-01-26 14:16:43.496499!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0839
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 14:17:01.099752!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0848
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 14:17:25.378604!
Epoch 1/1
569/569 [==============================] - 15s 25ms/step - loss: 0.1156
7. set (Dataset 11) being trained for epoch 1 by 2019-01-26 14:17:45.669515!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0668
8. set (Dataset 4) being trained for epoch 1 by 2019-01-26 14:18:07.459276!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0945
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 14:18:33.578293!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0850
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 14:18:59.921008!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0698
11. set (Dataset 24) being trained for epoch 1 by 2019-01-26 14:19:22.859512!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0709
12. set (Dataset 15) being trained for epoch 1 by 2019-01-26 14:19:41.322784!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0905
13. set (Dataset 1) being trained for epoch 1 by 2019-01-26 14:20:02.815468!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0977
14. set (Dataset 22) being trained for epoch 1 by 2019-01-26 14:20:21.977093!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0649
15. set (Dataset 18) being trained for epoch 1 by 2019-01-26 14:20:44.434148!
Epoch 1/1
614/614 [==============================] - 16s 27ms/step - loss: 0.1079
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 14:21:06.282758!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0746
17. set (Dataset 16) being trained for epoch 1 by 2019-01-26 14:21:29.064175!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0826
18. set (Dataset 21) being trained for epoch 1 by 2019-01-26 14:21:57.827123!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1227
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 14:22:18.219033!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0772
20. set (Dataset 19) being trained for epoch 1 by 2019-01-26 14:22:35.056914!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0848
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:22:52.561555
1. set (Dataset 21) being trained for epoch 2 by 2019-01-26 14:22:58.592694!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1177
2. set (Dataset 19) being trained for epoch 2 by 2019-01-26 14:23:19.214919!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0805
3. set (Dataset 24) being trained for epoch 2 by 2019-01-26 14:23:36.919947!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0658
4. set (Dataset 18) being trained for epoch 2 by 2019-01-26 14:23:55.483760!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0984
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 14:24:16.490951!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1094
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 14:24:35.525900!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0719
7. set (Dataset 16) being trained for epoch 2 by 2019-01-26 14:24:56.465532!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0784
8. set (Dataset 15) being trained for epoch 2 by 2019-01-26 14:25:26.515800!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0824
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 14:25:50.671417!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0854
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 14:26:17.385570!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0831
11. set (Dataset 11) being trained for epoch 2 by 2019-01-26 14:26:41.670344!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0637
12. set (Dataset 10) being trained for epoch 2 by 2019-01-26 14:27:03.043151!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0772
13. set (Dataset 22) being trained for epoch 2 by 2019-01-26 14:27:28.548327!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0610
14. set (Dataset 6) being trained for epoch 2 by 2019-01-26 14:27:50.083163!
Epoch 1/1
542/542 [==============================] - 19s 34ms/step - loss: 0.1109
15. set (Dataset 2) being trained for epoch 2 by 2019-01-26 14:28:13.914915!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0843
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 14:28:32.537184!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0704
17. set (Dataset 1) being trained for epoch 2 by 2019-01-26 14:28:51.658246!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0978
18. set (Dataset 17) being trained for epoch 2 by 2019-01-26 14:29:07.850546!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0800
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 14:29:25.241590!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0711
20. set (Dataset 4) being trained for epoch 2 by 2019-01-26 14:29:51.462921!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0922
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:30:14.554037
1. set (Dataset 17) being trained for epoch 3 by 2019-01-26 14:30:18.297763!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0801
2. set (Dataset 4) being trained for epoch 3 by 2019-01-26 14:30:35.806538!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0854
3. set (Dataset 11) being trained for epoch 3 by 2019-01-26 14:31:00.196995!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0640
4. set (Dataset 2) being trained for epoch 3 by 2019-01-26 14:31:19.830724!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0831
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 14:31:37.756635!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0701
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 14:31:57.522353!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0692
7. set (Dataset 1) being trained for epoch 3 by 2019-01-26 14:32:20.584402!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0901
8. set (Dataset 10) being trained for epoch 3 by 2019-01-26 14:32:40.230857!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0744
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 14:33:04.376025!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1157
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 14:33:27.193604!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0784
11. set (Dataset 16) being trained for epoch 3 by 2019-01-26 14:33:54.900027!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0800
12. set (Dataset 21) being trained for epoch 3 by 2019-01-26 14:34:23.670919!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1205
13. set (Dataset 6) being trained for epoch 3 by 2019-01-26 14:34:44.401241!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1039
14. set (Dataset 19) being trained for epoch 3 by 2019-01-26 14:35:02.652340!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0833
15. set (Dataset 24) being trained for epoch 3 by 2019-01-26 14:35:20.053081!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0613
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 14:35:37.213640!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0671
17. set (Dataset 22) being trained for epoch 3 by 2019-01-26 14:35:57.395653!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0584
18. set (Dataset 18) being trained for epoch 3 by 2019-01-26 14:36:19.582540!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0981
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 14:36:43.209261!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0826
20. set (Dataset 15) being trained for epoch 3 by 2019-01-26 14:37:08.046709!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0827
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:37:28.835226
1. set (Dataset 18) being trained for epoch 4 by 2019-01-26 14:37:34.741326!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0962
2. set (Dataset 15) being trained for epoch 4 by 2019-01-26 14:37:56.411012!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0786
3. set (Dataset 16) being trained for epoch 4 by 2019-01-26 14:38:21.895220!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0727
4. set (Dataset 24) being trained for epoch 4 by 2019-01-26 14:38:50.204041!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0595
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 14:39:10.261066!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0717
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 14:39:36.658326!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0778
7. set (Dataset 22) being trained for epoch 4 by 2019-01-26 14:40:01.592545!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0590
8. set (Dataset 21) being trained for epoch 4 by 2019-01-26 14:40:24.038604!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1149
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 14:40:44.774689!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0690
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 14:41:02.443332!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1090
11. set (Dataset 1) being trained for epoch 4 by 2019-01-26 14:41:22.117326!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0941
12. set (Dataset 17) being trained for epoch 4 by 2019-01-26 14:41:38.320863!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0777
13. set (Dataset 19) being trained for epoch 4 by 2019-01-26 14:41:53.014440!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0800
14. set (Dataset 4) being trained for epoch 4 by 2019-01-26 14:42:13.100160!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0921
15. set (Dataset 11) being trained for epoch 4 by 2019-01-26 14:42:37.908668!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0609
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 14:42:57.846931!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0687
17. set (Dataset 6) being trained for epoch 4 by 2019-01-26 14:43:17.306900!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0986
18. set (Dataset 2) being trained for epoch 4 by 2019-01-26 14:43:36.298587!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0835
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 14:43:56.903694!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0804
20. set (Dataset 10) being trained for epoch 4 by 2019-01-26 14:44:23.278208!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0745
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:44:45.259911
1. set (Dataset 2) being trained for epoch 5 by 2019-01-26 14:44:50.346808!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0803
2. set (Dataset 10) being trained for epoch 5 by 2019-01-26 14:45:10.257256!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0710
3. set (Dataset 1) being trained for epoch 5 by 2019-01-26 14:45:33.129980!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0863
4. set (Dataset 11) being trained for epoch 5 by 2019-01-26 14:45:51.535776!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0590
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 14:46:13.206475!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0731
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 14:46:39.971290!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0760
7. set (Dataset 6) being trained for epoch 5 by 2019-01-26 14:47:05.145133!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1072
8. set (Dataset 17) being trained for epoch 5 by 2019-01-26 14:47:22.073022!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0794
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 14:47:39.193066!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0648
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 14:48:02.522140!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0632
11. set (Dataset 22) being trained for epoch 5 by 2019-01-26 14:48:21.298966!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0586
12. set (Dataset 18) being trained for epoch 5 by 2019-01-26 14:48:44.434982!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0992
13. set (Dataset 4) being trained for epoch 5 by 2019-01-26 14:49:07.565375!
Epoch 1/1
744/744 [==============================] - 20s 27ms/step - loss: 0.0859
14. set (Dataset 15) being trained for epoch 5 by 2019-01-26 14:49:33.957366!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0804
15. set (Dataset 16) being trained for epoch 5 by 2019-01-26 14:49:59.240280!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0741
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 14:50:27.181489!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0629
17. set (Dataset 19) being trained for epoch 5 by 2019-01-26 14:50:45.838729!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0785
18. set (Dataset 24) being trained for epoch 5 by 2019-01-26 14:51:03.296781!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0595
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 14:51:20.446707!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1070
20. set (Dataset 21) being trained for epoch 5 by 2019-01-26 14:51:40.713904!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1156
Epoch 5 completed!
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F0
6'), (17, 'M10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'),
(20, 'M12'), (19, 'M11'), (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 14:51:59.193524
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.48 Degree
        The absolute mean error on Yaw angle estimation: 27.58 Degree
        The absolute mean error on Roll angle estimation: 21.84 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.26 Degree
        The absolute mean error on Yaw angle estimation: 28.83 Degree
        The absolute mean error on Roll angle estimation: 4.95 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.45 Degree
        The absolute mean error on Yaw angle estimation: 26.92 Degree
        The absolute mean error on Roll angle estimation: 9.55 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.09 Degree
        The absolute mean error on Yaw angle estimation: 30.08 Degree
        The absolute mean error on Roll angle estimation: 14.75 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.57 Degree
        The absolute mean error on Yaw angle estimations: 28.35 Degree
        The absolute mean error on Roll angle estimations: 12.77 Degree
Exp2019-01-26_13-37-43_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 14:53:28.013376
1. set (Dataset 24) being trained for epoch 1 by 2019-01-26 14:53:32.683865!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0556
2. set (Dataset 21) being trained for epoch 1 by 2019-01-26 14:53:51.583583!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1053
3. set (Dataset 22) being trained for epoch 1 by 2019-01-26 14:54:13.858749!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0570
4. set (Dataset 16) being trained for epoch 1 by 2019-01-26 14:54:40.095091!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0724
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 14:55:10.621983!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0798
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 14:55:35.651794!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1042
7. set (Dataset 19) being trained for epoch 1 by 2019-01-26 14:55:54.796342!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0755
8. set (Dataset 18) being trained for epoch 1 by 2019-01-26 14:56:12.892481!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0896
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 14:56:35.666188!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0765
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 14:57:01.360934!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0680
11. set (Dataset 6) being trained for epoch 1 by 2019-01-26 14:57:25.408468!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0960
12. set (Dataset 2) being trained for epoch 1 by 2019-01-26 14:57:43.921788!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0760
13. set (Dataset 15) being trained for epoch 1 by 2019-01-26 14:58:03.410819!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0798
14. set (Dataset 10) being trained for epoch 1 by 2019-01-26 14:58:26.882251!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0747
15. set (Dataset 1) being trained for epoch 1 by 2019-01-26 14:58:50.511601!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0844
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 14:59:08.683661!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0666
17. set (Dataset 4) being trained for epoch 1 by 2019-01-26 14:59:30.163833!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.0846
18. set (Dataset 11) being trained for epoch 1 by 2019-01-26 14:59:54.010793!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0596
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 15:00:13.509759!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0662
20. set (Dataset 17) being trained for epoch 1 by 2019-01-26 15:00:29.177627!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0744
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:00:44.021509
1. set (Dataset 11) being trained for epoch 2 by 2019-01-26 15:00:49.721220!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0565
2. set (Dataset 17) being trained for epoch 2 by 2019-01-26 15:01:08.042173!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0719
3. set (Dataset 6) being trained for epoch 2 by 2019-01-26 15:01:23.260939!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0997
4. set (Dataset 1) being trained for epoch 2 by 2019-01-26 15:01:41.969715!
Epoch 1/1
498/498 [==============================] - 13s 27ms/step - loss: 0.0848
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 15:02:00.744307!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1043
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 15:02:19.546123!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0668
7. set (Dataset 4) being trained for epoch 2 by 2019-01-26 15:02:39.382976!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0821
8. set (Dataset 2) being trained for epoch 2 by 2019-01-26 15:03:03.346989!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0785
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 15:03:24.184414!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0758
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 15:03:51.442214!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0734
11. set (Dataset 19) being trained for epoch 2 by 2019-01-26 15:04:15.111560!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0796
12. set (Dataset 24) being trained for epoch 2 by 2019-01-26 15:04:32.531618!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0584
13. set (Dataset 10) being trained for epoch 2 by 2019-01-26 15:04:50.832725!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0698
14. set (Dataset 21) being trained for epoch 2 by 2019-01-26 15:05:15.200154!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1133
15. set (Dataset 22) being trained for epoch 2 by 2019-01-26 15:05:37.696727!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0552
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 15:05:59.711142!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0644
17. set (Dataset 15) being trained for epoch 2 by 2019-01-26 15:06:19.705067!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0777
18. set (Dataset 16) being trained for epoch 2 by 2019-01-26 15:06:45.316879!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0719
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 15:07:15.426576!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0651
20. set (Dataset 18) being trained for epoch 2 by 2019-01-26 15:07:39.970224!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0942
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:07:59.804347
1. set (Dataset 16) being trained for epoch 3 by 2019-01-26 15:08:08.531468!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0687
2. set (Dataset 18) being trained for epoch 3 by 2019-01-26 15:08:37.692824!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0860
3. set (Dataset 19) being trained for epoch 3 by 2019-01-26 15:08:58.000695!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0728
4. set (Dataset 22) being trained for epoch 3 by 2019-01-26 15:09:16.887071!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0549
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 15:09:38.574291!
Epoch 1/1
485/485 [==============================] - 13s 27ms/step - loss: 0.0667
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 15:09:58.896934!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0617
7. set (Dataset 15) being trained for epoch 3 by 2019-01-26 15:10:24.259197!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0755
8. set (Dataset 24) being trained for epoch 3 by 2019-01-26 15:10:45.263651!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0557
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 15:11:03.498922!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1008
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 15:11:25.928887!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0757
11. set (Dataset 4) being trained for epoch 3 by 2019-01-26 15:11:53.526348!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0847
12. set (Dataset 11) being trained for epoch 3 by 2019-01-26 15:12:17.991874!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0577
13. set (Dataset 21) being trained for epoch 3 by 2019-01-26 15:12:38.579035!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1129
14. set (Dataset 17) being trained for epoch 3 by 2019-01-26 15:12:58.912190!
Epoch 1/1
395/395 [==============================] - 11s 27ms/step - loss: 0.0767
15. set (Dataset 6) being trained for epoch 3 by 2019-01-26 15:13:14.906001!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0906
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 15:13:33.759581!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0600
17. set (Dataset 10) being trained for epoch 3 by 2019-01-26 15:13:55.440825!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0734
18. set (Dataset 1) being trained for epoch 3 by 2019-01-26 15:14:18.694931!
Epoch 1/1
498/498 [==============================] - 13s 27ms/step - loss: 0.0831
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 15:14:39.758589!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0715
20. set (Dataset 2) being trained for epoch 3 by 2019-01-26 15:15:03.620327!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0723
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:15:21.093228
1. set (Dataset 1) being trained for epoch 4 by 2019-01-26 15:15:26.141843!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0784
2. set (Dataset 2) being trained for epoch 4 by 2019-01-26 15:15:44.034335!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0656
3. set (Dataset 4) being trained for epoch 4 by 2019-01-26 15:16:04.373438!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0778
4. set (Dataset 6) being trained for epoch 4 by 2019-01-26 15:16:28.173172!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0969
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 15:16:49.626348!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0581
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 15:17:15.916878!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0699
7. set (Dataset 10) being trained for epoch 4 by 2019-01-26 15:17:42.215000!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0656
8. set (Dataset 11) being trained for epoch 4 by 2019-01-26 15:18:06.302530!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0526
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 15:18:25.986049!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0638
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 15:18:43.921722!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1043
11. set (Dataset 15) being trained for epoch 4 by 2019-01-26 15:19:04.582571!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0772
12. set (Dataset 16) being trained for epoch 4 by 2019-01-26 15:19:29.985996!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0682
13. set (Dataset 17) being trained for epoch 4 by 2019-01-26 15:19:57.252664!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0717
14. set (Dataset 18) being trained for epoch 4 by 2019-01-26 15:20:13.196367!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0890
15. set (Dataset 19) being trained for epoch 4 by 2019-01-26 15:20:33.763898!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0732
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 15:20:52.082695!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0595
17. set (Dataset 21) being trained for epoch 4 by 2019-01-26 15:21:11.724748!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1048
18. set (Dataset 22) being trained for epoch 4 by 2019-01-26 15:21:33.904824!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0535
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 15:21:58.527580!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0730
20. set (Dataset 24) being trained for epoch 4 by 2019-01-26 15:22:22.924578!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0574
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:22:39.761675
1. set (Dataset 22) being trained for epoch 5 by 2019-01-26 15:22:46.171505!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0514
2. set (Dataset 24) being trained for epoch 5 by 2019-01-26 15:23:08.056579!
Epoch 1/1
492/492 [==============================] - 13s 25ms/step - loss: 0.0513
3. set (Dataset 15) being trained for epoch 5 by 2019-01-26 15:23:26.987199!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0744
4. set (Dataset 19) being trained for epoch 5 by 2019-01-26 15:23:48.803197!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0723
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 15:24:09.478706!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0744
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 15:24:36.570734!
Epoch 1/1
772/772 [==============================] - 21s 27ms/step - loss: 0.0702
7. set (Dataset 21) being trained for epoch 5 by 2019-01-26 15:25:03.568212!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1103
8. set (Dataset 16) being trained for epoch 5 by 2019-01-26 15:25:27.941645!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0702
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 15:25:58.472955!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0618
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 15:26:21.856302!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0623
11. set (Dataset 10) being trained for epoch 5 by 2019-01-26 15:26:41.507061!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0709
12. set (Dataset 1) being trained for epoch 5 by 2019-01-26 15:27:04.858152!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0811
13. set (Dataset 18) being trained for epoch 5 by 2019-01-26 15:27:22.955725!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0924
14. set (Dataset 2) being trained for epoch 5 by 2019-01-26 15:27:43.988864!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0718
15. set (Dataset 4) being trained for epoch 5 by 2019-01-26 15:28:04.251771!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0815
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 15:28:28.784851!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0659
17. set (Dataset 17) being trained for epoch 5 by 2019-01-26 15:28:46.651534!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0720
18. set (Dataset 6) being trained for epoch 5 by 2019-01-26 15:29:01.677170!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0953
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 15:29:21.050426!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0983
20. set (Dataset 11) being trained for epoch 5 by 2019-01-26 15:29:41.296736!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0551
Epoch 5 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21,
'F02'), (16, 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 15:29:57.340457
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.03 Degree
        The absolute mean error on Yaw angle estimation: 32.77 Degree
        The absolute mean error on Roll angle estimation: 14.60 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.85 Degree
        The absolute mean error on Yaw angle estimation: 25.93 Degree
        The absolute mean error on Roll angle estimation: 4.87 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 26.39 Degree
        The absolute mean error on Yaw angle estimation: 26.14 Degree
        The absolute mean error on Roll angle estimation: 9.19 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.17 Degree
        The absolute mean error on Yaw angle estimation: 30.83 Degree
        The absolute mean error on Roll angle estimation: 14.46 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.86 Degree
        The absolute mean error on Yaw angle estimations: 28.91 Degree
        The absolute mean error on Roll angle estimations: 10.78 Degree
Exp2019-01-26_13-37-43_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 15:31:26.022829
1. set (Dataset 6) being trained for epoch 1 by 2019-01-26 15:31:31.209621!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0885
2. set (Dataset 11) being trained for epoch 1 by 2019-01-26 15:31:50.559580!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0526
3. set (Dataset 10) being trained for epoch 1 by 2019-01-26 15:32:12.645515!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0641
4. set (Dataset 4) being trained for epoch 1 by 2019-01-26 15:32:39.257999!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0763
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 15:33:06.133257!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0729
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 15:33:31.019260!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1046
7. set (Dataset 17) being trained for epoch 1 by 2019-01-26 15:33:48.722368!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0718
8. set (Dataset 1) being trained for epoch 1 by 2019-01-26 15:34:04.095278!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0844
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 15:34:24.430393!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0700
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 15:34:50.428321!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0580
11. set (Dataset 21) being trained for epoch 1 by 2019-01-26 15:35:15.230250!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1072
12. set (Dataset 22) being trained for epoch 1 by 2019-01-26 15:35:37.557405!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0546
13. set (Dataset 2) being trained for epoch 1 by 2019-01-26 15:35:59.544554!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0717
14. set (Dataset 24) being trained for epoch 1 by 2019-01-26 15:36:16.864275!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0606
15. set (Dataset 15) being trained for epoch 1 by 2019-01-26 15:36:35.410444!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0729
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 15:36:56.920170!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0595
17. set (Dataset 18) being trained for epoch 1 by 2019-01-26 15:37:16.517392!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0869
18. set (Dataset 19) being trained for epoch 1 by 2019-01-26 15:37:37.477243!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0717
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 15:37:55.651253!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0637
20. set (Dataset 16) being trained for epoch 1 by 2019-01-26 15:38:16.772632!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0693
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:38:44.121402
1. set (Dataset 19) being trained for epoch 2 by 2019-01-26 15:38:49.028269!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0678
2. set (Dataset 16) being trained for epoch 2 by 2019-01-26 15:39:10.359793!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0648
3. set (Dataset 21) being trained for epoch 2 by 2019-01-26 15:39:39.705659!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1059
4. set (Dataset 15) being trained for epoch 2 by 2019-01-26 15:40:02.349958!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0736
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 15:40:24.647596!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0958
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 15:40:44.093892!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0611
7. set (Dataset 18) being trained for epoch 2 by 2019-01-26 15:41:02.327292!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0835
8. set (Dataset 22) being trained for epoch 2 by 2019-01-26 15:41:24.502344!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0504
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 15:41:49.415675!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0751
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 15:42:16.261823!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0693
11. set (Dataset 17) being trained for epoch 2 by 2019-01-26 15:42:39.304281!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0724
12. set (Dataset 6) being trained for epoch 2 by 2019-01-26 15:42:54.182426!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0920
13. set (Dataset 24) being trained for epoch 2 by 2019-01-26 15:43:12.227640!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0529
14. set (Dataset 11) being trained for epoch 2 by 2019-01-26 15:43:29.707274!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0538
15. set (Dataset 10) being trained for epoch 2 by 2019-01-26 15:43:51.142722!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0646
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 15:44:15.210325!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0629
17. set (Dataset 2) being trained for epoch 2 by 2019-01-26 15:44:34.544753!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0742
18. set (Dataset 4) being trained for epoch 2 by 2019-01-26 15:44:54.992131!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0771
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 15:45:21.132024!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0604
20. set (Dataset 1) being trained for epoch 2 by 2019-01-26 15:45:44.421336!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0774
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:46:01.458284
1. set (Dataset 4) being trained for epoch 3 by 2019-01-26 15:46:08.847242!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0745
2. set (Dataset 1) being trained for epoch 3 by 2019-01-26 15:46:32.261423!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0753
3. set (Dataset 17) being trained for epoch 3 by 2019-01-26 15:46:48.748779!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0736
4. set (Dataset 10) being trained for epoch 3 by 2019-01-26 15:47:05.944224!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0633
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 15:47:28.861424!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0586
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 15:47:48.331539!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0575
7. set (Dataset 2) being trained for epoch 3 by 2019-01-26 15:48:12.147193!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0680
8. set (Dataset 6) being trained for epoch 3 by 2019-01-26 15:48:30.303864!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0920
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 15:48:49.181829!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0941
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 15:49:11.303334!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0702
11. set (Dataset 18) being trained for epoch 3 by 2019-01-26 15:49:36.432195!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0902
12. set (Dataset 19) being trained for epoch 3 by 2019-01-26 15:49:56.472461!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0699
13. set (Dataset 11) being trained for epoch 3 by 2019-01-26 15:50:14.624550!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0541
14. set (Dataset 16) being trained for epoch 3 by 2019-01-26 15:50:37.927450!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0644
15. set (Dataset 21) being trained for epoch 3 by 2019-01-26 15:51:07.133659!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1037
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 15:51:28.673899!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0594
17. set (Dataset 24) being trained for epoch 3 by 2019-01-26 15:51:47.409754!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0519
18. set (Dataset 15) being trained for epoch 3 by 2019-01-26 15:52:06.064945!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0699
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 15:52:29.918049!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0697
20. set (Dataset 22) being trained for epoch 3 by 2019-01-26 15:52:55.414448!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0506
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:53:16.990405
1. set (Dataset 15) being trained for epoch 4 by 2019-01-26 15:53:23.379986!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0697
2. set (Dataset 22) being trained for epoch 4 by 2019-01-26 15:53:46.599274!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0499
3. set (Dataset 18) being trained for epoch 4 by 2019-01-26 15:54:09.676822!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0848
4. set (Dataset 21) being trained for epoch 4 by 2019-01-26 15:54:31.845267!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1044
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 15:54:55.046066!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0597
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 15:55:21.534973!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0685
7. set (Dataset 24) being trained for epoch 4 by 2019-01-26 15:55:45.040976!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0544
8. set (Dataset 19) being trained for epoch 4 by 2019-01-26 15:56:02.348301!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0647
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 15:56:19.567199!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0607
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 15:56:36.983751!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0968
11. set (Dataset 2) being trained for epoch 4 by 2019-01-26 15:56:56.414260!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0721
12. set (Dataset 4) being trained for epoch 4 by 2019-01-26 15:57:16.764899!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0779
13. set (Dataset 16) being trained for epoch 4 by 2019-01-26 15:57:43.989061!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0670
14. set (Dataset 1) being trained for epoch 4 by 2019-01-26 15:58:11.981977!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0806
15. set (Dataset 17) being trained for epoch 4 by 2019-01-26 15:58:28.465115!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0678
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 15:58:44.332699!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0611
17. set (Dataset 11) being trained for epoch 4 by 2019-01-26 15:59:03.861195!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0522
18. set (Dataset 10) being trained for epoch 4 by 2019-01-26 15:59:25.516166!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0632
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 15:59:51.240423!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0710
20. set (Dataset 6) being trained for epoch 4 by 2019-01-26 16:00:15.725953!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0948
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:00:34.098364
1. set (Dataset 10) being trained for epoch 5 by 2019-01-26 16:00:41.317723!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0646
2. set (Dataset 6) being trained for epoch 5 by 2019-01-26 16:01:04.767443!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0869
3. set (Dataset 2) being trained for epoch 5 by 2019-01-26 16:01:23.669140!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0714
4. set (Dataset 17) being trained for epoch 5 by 2019-01-26 16:01:40.262674!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0696
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 16:01:57.587131!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0649
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 16:02:24.143803!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0657
7. set (Dataset 11) being trained for epoch 5 by 2019-01-26 16:02:49.187910!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0514
8. set (Dataset 4) being trained for epoch 5 by 2019-01-26 16:03:11.379922!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0731
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 16:03:37.521355!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0591
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 16:04:01.281888!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0598
11. set (Dataset 24) being trained for epoch 5 by 2019-01-26 16:04:18.189454!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0578
12. set (Dataset 15) being trained for epoch 5 by 2019-01-26 16:04:36.755933!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0700
13. set (Dataset 1) being trained for epoch 5 by 2019-01-26 16:04:58.299822!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0809
14. set (Dataset 22) being trained for epoch 5 by 2019-01-26 16:05:17.054063!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0514
15. set (Dataset 18) being trained for epoch 5 by 2019-01-26 16:05:39.426668!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0835
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 16:06:00.170539!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0591
17. set (Dataset 16) being trained for epoch 5 by 2019-01-26 16:06:22.762796!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0643
18. set (Dataset 21) being trained for epoch 5 by 2019-01-26 16:06:51.575732!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1013
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 16:07:12.770027!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0944
20. set (Dataset 19) being trained for epoch 5 by 2019-01-26 16:07:32.081873!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0670
Epoch 5 completed!
The subjects are trained: [(10, 'M04'), (6, 'F06'), (2, 'F02'), (17, 'M10'), (7, 'M01'), (8, 'M02'), (11, 'M
05'), (4, 'F04'), (12, 'M06'), (13, 'M07'), (24, 'M14'), (15, 'F03'), (1, 'F01'), (22, 'M01'), (18, 'F05'),
(20, 'M12'), (16, 'M09'), (21, 'F02'), (23, 'M13'), (19, 'M11')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 16:07:46.712039
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 7.65 Degree
        The absolute mean error on Yaw angle estimation: 27.07 Degree
        The absolute mean error on Roll angle estimation: 15.22 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.97 Degree
        The absolute mean error on Yaw angle estimation: 26.83 Degree
        The absolute mean error on Roll angle estimation: 3.93 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 25.70 Degree
        The absolute mean error on Yaw angle estimation: 25.65 Degree
        The absolute mean error on Roll angle estimation: 6.89 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 16.03 Degree
        The absolute mean error on Yaw angle estimation: 31.47 Degree
        The absolute mean error on Roll angle estimation: 13.76 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.84 Degree
        The absolute mean error on Yaw angle estimations: 27.76 Degree
        The absolute mean error on Roll angle estimations: 9.95 Degree
Exp2019-01-26_13-37-43_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 16:09:15.448359
1. set (Dataset 21) being trained for epoch 1 by 2019-01-26 16:09:21.455918!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0995
2. set (Dataset 19) being trained for epoch 1 by 2019-01-26 16:09:42.330645!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0658
3. set (Dataset 24) being trained for epoch 1 by 2019-01-26 16:10:00.033674!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0510
4. set (Dataset 18) being trained for epoch 1 by 2019-01-26 16:10:19.012394!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0801
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 16:10:41.986488!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0744
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 16:11:06.864552!
Epoch 1/1
569/569 [==============================] - 15s 25ms/step - loss: 0.0925
7. set (Dataset 16) being trained for epoch 1 by 2019-01-26 16:11:30.336567!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0626
8. set (Dataset 15) being trained for epoch 1 by 2019-01-26 16:11:59.535800!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0704
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 16:12:23.652063!
Epoch 1/1
745/745 [==============================] - 20s 26ms/step - loss: 0.0663
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 16:12:50.691746!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0575
11. set (Dataset 11) being trained for epoch 1 by 2019-01-26 16:13:15.058794!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0509
12. set (Dataset 10) being trained for epoch 1 by 2019-01-26 16:13:36.809569!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0613
13. set (Dataset 22) being trained for epoch 1 by 2019-01-26 16:14:01.538245!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0516
14. set (Dataset 6) being trained for epoch 1 by 2019-01-26 16:14:23.051677!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0948
15. set (Dataset 2) being trained for epoch 1 by 2019-01-26 16:14:41.637042!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0706
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 16:14:59.534141!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0601
17. set (Dataset 1) being trained for epoch 1 by 2019-01-26 16:15:18.377651!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0809
18. set (Dataset 17) being trained for epoch 1 by 2019-01-26 16:15:35.102194!
Epoch 1/1
395/395 [==============================] - 10s 27ms/step - loss: 0.0687
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 16:15:50.440488!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0600
20. set (Dataset 4) being trained for epoch 1 by 2019-01-26 16:16:09.880021!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0722
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:16:33.356972
1. set (Dataset 17) being trained for epoch 2 by 2019-01-26 16:16:37.102283!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0669
2. set (Dataset 4) being trained for epoch 2 by 2019-01-26 16:16:54.547558!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0693
3. set (Dataset 11) being trained for epoch 2 by 2019-01-26 16:17:18.965668!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0507
4. set (Dataset 2) being trained for epoch 2 by 2019-01-26 16:17:38.515934!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0666
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 16:17:56.689288!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0959
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 16:18:15.945438!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0573
7. set (Dataset 1) being trained for epoch 2 by 2019-01-26 16:18:33.083658!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0760
8. set (Dataset 10) being trained for epoch 2 by 2019-01-26 16:18:52.833091!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0633
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 16:19:19.455341!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0676
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 16:19:45.833543!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0651
11. set (Dataset 16) being trained for epoch 2 by 2019-01-26 16:20:13.369937!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0680
12. set (Dataset 21) being trained for epoch 2 by 2019-01-26 16:20:42.714073!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1023
13. set (Dataset 6) being trained for epoch 2 by 2019-01-26 16:21:04.639960!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0882
14. set (Dataset 19) being trained for epoch 2 by 2019-01-26 16:21:23.361145!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0640
15. set (Dataset 24) being trained for epoch 2 by 2019-01-26 16:21:40.827094!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0530
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 16:21:58.654456!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0543
17. set (Dataset 22) being trained for epoch 2 by 2019-01-26 16:22:18.416295!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0489
18. set (Dataset 18) being trained for epoch 2 by 2019-01-26 16:22:41.282349!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0825
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 16:23:03.894807!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0596
20. set (Dataset 15) being trained for epoch 2 by 2019-01-26 16:23:28.388406!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0676
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:23:49.040657
1. set (Dataset 18) being trained for epoch 3 by 2019-01-26 16:23:55.054799!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0805
2. set (Dataset 15) being trained for epoch 3 by 2019-01-26 16:24:16.651404!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0651
3. set (Dataset 16) being trained for epoch 3 by 2019-01-26 16:24:41.811652!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0629
4. set (Dataset 24) being trained for epoch 3 by 2019-01-26 16:25:09.454454!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0530
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 16:25:26.721698!
Epoch 1/1
485/485 [==============================] - 11s 24ms/step - loss: 0.0607
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 16:25:45.517937!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0551
7. set (Dataset 22) being trained for epoch 3 by 2019-01-26 16:26:10.322323!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0468
8. set (Dataset 21) being trained for epoch 3 by 2019-01-26 16:26:33.596709!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1036
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 16:26:54.912398!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0915
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 16:27:17.293376!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0705
11. set (Dataset 1) being trained for epoch 3 by 2019-01-26 16:27:42.286234!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0784
12. set (Dataset 17) being trained for epoch 3 by 2019-01-26 16:27:58.547606!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0679
13. set (Dataset 19) being trained for epoch 3 by 2019-01-26 16:28:13.210919!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0660
14. set (Dataset 4) being trained for epoch 3 by 2019-01-26 16:28:33.076904!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0726
15. set (Dataset 11) being trained for epoch 3 by 2019-01-26 16:28:57.614080!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0535
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 16:29:17.249998!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0623
17. set (Dataset 6) being trained for epoch 3 by 2019-01-26 16:29:36.449165!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0855
18. set (Dataset 2) being trained for epoch 3 by 2019-01-26 16:29:55.119490!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0668
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 16:30:15.373297!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0669
20. set (Dataset 10) being trained for epoch 3 by 2019-01-26 16:30:41.602066!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0631
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:31:04.298281
1. set (Dataset 2) being trained for epoch 4 by 2019-01-26 16:31:09.387196!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0695
2. set (Dataset 10) being trained for epoch 4 by 2019-01-26 16:31:29.525710!
Epoch 1/1
726/726 [==============================] - 19s 27ms/step - loss: 0.0609
3. set (Dataset 1) being trained for epoch 4 by 2019-01-26 16:31:53.873614!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0734
4. set (Dataset 11) being trained for epoch 4 by 2019-01-26 16:32:11.992026!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0506
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 16:32:33.884836!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0562
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 16:33:00.418875!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0647
7. set (Dataset 6) being trained for epoch 4 by 2019-01-26 16:33:24.563486!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0933
8. set (Dataset 17) being trained for epoch 4 by 2019-01-26 16:33:42.317598!
Epoch 1/1
395/395 [==============================] - 11s 27ms/step - loss: 0.0680
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 16:33:57.675023!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0580
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 16:34:15.327010!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0933
11. set (Dataset 22) being trained for epoch 4 by 2019-01-26 16:34:36.357546!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0486
12. set (Dataset 18) being trained for epoch 4 by 2019-01-26 16:34:59.111737!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0784
13. set (Dataset 4) being trained for epoch 4 by 2019-01-26 16:35:22.287548!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0767
14. set (Dataset 15) being trained for epoch 4 by 2019-01-26 16:35:47.059930!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0685
15. set (Dataset 16) being trained for epoch 4 by 2019-01-26 16:36:11.705119!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0630
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 16:36:40.170278!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0545
17. set (Dataset 19) being trained for epoch 4 by 2019-01-26 16:36:59.155749!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0642
18. set (Dataset 24) being trained for epoch 4 by 2019-01-26 16:37:17.040741!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0513
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 16:37:37.244257!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0702
20. set (Dataset 21) being trained for epoch 4 by 2019-01-26 16:38:02.889262!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1000
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:38:23.728616
1. set (Dataset 24) being trained for epoch 5 by 2019-01-26 16:38:28.404836!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0504
2. set (Dataset 21) being trained for epoch 5 by 2019-01-26 16:38:47.204659!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.0993
3. set (Dataset 22) being trained for epoch 5 by 2019-01-26 16:39:09.963603!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0481
4. set (Dataset 16) being trained for epoch 5 by 2019-01-26 16:39:35.776611!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0608
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 16:40:07.038945!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0644
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 16:40:33.245417!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0684
7. set (Dataset 19) being trained for epoch 5 by 2019-01-26 16:40:57.753317!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0652
8. set (Dataset 18) being trained for epoch 5 by 2019-01-26 16:41:16.413715!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0832
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 16:41:38.847877!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0565
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 16:42:02.026829!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0573
11. set (Dataset 6) being trained for epoch 5 by 2019-01-26 16:42:19.738542!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0863
12. set (Dataset 2) being trained for epoch 5 by 2019-01-26 16:42:38.074033!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0659
13. set (Dataset 15) being trained for epoch 5 by 2019-01-26 16:42:57.453970!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0700
14. set (Dataset 10) being trained for epoch 5 by 2019-01-26 16:43:21.859920!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0617
15. set (Dataset 1) being trained for epoch 5 by 2019-01-26 16:43:45.313149!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0741
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 16:44:03.566733!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0595
17. set (Dataset 4) being trained for epoch 5 by 2019-01-26 16:44:25.092482!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0699
18. set (Dataset 11) being trained for epoch 5 by 2019-01-26 16:44:49.256455!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0489
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 16:45:09.311314!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0905
20. set (Dataset 17) being trained for epoch 5 by 2019-01-26 16:45:27.482368!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0677
Epoch 5 completed!
The subjects are trained: [(24, 'M14'), (21, 'F02'), (22, 'M01'), (16, 'M09'), (7, 'M01'), (8, 'M02'), (19,
'M11'), (18, 'F05'), (12, 'M06'), (13, 'M07'), (6, 'F06'), (2, 'F02'), (15, 'F03'), (10, 'M04'), (1, 'F01'),
 (20, 'M12'), (4, 'F04'), (11, 'M05'), (23, 'M13'), (17, 'M10')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 16:45:39.737107
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 7.24 Degree
        The absolute mean error on Yaw angle estimation: 30.34 Degree
        The absolute mean error on Roll angle estimation: 12.49 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 7.41 Degree
        The absolute mean error on Yaw angle estimation: 25.61 Degree
        The absolute mean error on Roll angle estimation: 4.46 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 31.00 Degree
        The absolute mean error on Yaw angle estimation: 25.10 Degree
        The absolute mean error on Roll angle estimation: 5.99 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.66 Degree
        The absolute mean error on Yaw angle estimation: 29.90 Degree
        The absolute mean error on Roll angle estimation: 14.03 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.83 Degree
        The absolute mean error on Yaw angle estimations: 27.74 Degree
        The absolute mean error on Roll angle estimations: 9.24 Degree
Exp2019-01-26_13-37-43_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 16:47:08.552155
1. set (Dataset 11) being trained for epoch 1 by 2019-01-26 16:47:14.254657!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0502
2. set (Dataset 17) being trained for epoch 1 by 2019-01-26 16:47:31.832847!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0667
3. set (Dataset 6) being trained for epoch 1 by 2019-01-26 16:47:47.276031!
Epoch 1/1
542/542 [==============================] - 15s 27ms/step - loss: 0.0834
4. set (Dataset 1) being trained for epoch 1 by 2019-01-26 16:48:06.961337!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0765
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 16:48:27.476170!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0676
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 16:48:52.670483!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0925
7. set (Dataset 4) being trained for epoch 1 by 2019-01-26 16:49:14.450330!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0708
8. set (Dataset 2) being trained for epoch 1 by 2019-01-26 16:49:38.499024!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0620
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 16:49:59.066549!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0622
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 16:50:24.841588!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0530
11. set (Dataset 19) being trained for epoch 1 by 2019-01-26 16:50:48.635618!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0640
12. set (Dataset 24) being trained for epoch 1 by 2019-01-26 16:51:06.196973!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0513
13. set (Dataset 10) being trained for epoch 1 by 2019-01-26 16:51:26.567616!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0610
14. set (Dataset 21) being trained for epoch 1 by 2019-01-26 16:51:51.187248!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0976
15. set (Dataset 22) being trained for epoch 1 by 2019-01-26 16:52:13.382875!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0476
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 16:52:35.807660!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0576
17. set (Dataset 15) being trained for epoch 1 by 2019-01-26 16:52:56.353703!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0651
18. set (Dataset 16) being trained for epoch 1 by 2019-01-26 16:53:21.266610!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0595
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 16:53:49.635463!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0556
20. set (Dataset 18) being trained for epoch 1 by 2019-01-26 16:54:07.900380!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0792
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:54:27.717076
1. set (Dataset 16) being trained for epoch 2 by 2019-01-26 16:54:36.466012!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0602
2. set (Dataset 18) being trained for epoch 2 by 2019-01-26 16:55:05.585875!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0758
3. set (Dataset 19) being trained for epoch 2 by 2019-01-26 16:55:26.459891!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0646
4. set (Dataset 22) being trained for epoch 2 by 2019-01-26 16:55:45.617379!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0484
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 16:56:07.786053!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0891
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 16:56:26.803923!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0571
7. set (Dataset 15) being trained for epoch 2 by 2019-01-26 16:56:45.429181!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0666
8. set (Dataset 24) being trained for epoch 2 by 2019-01-26 16:57:06.433972!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0505
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 16:57:26.981123!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0671
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 16:57:54.708229!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0654
11. set (Dataset 4) being trained for epoch 2 by 2019-01-26 16:58:20.979987!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0698
12. set (Dataset 11) being trained for epoch 2 by 2019-01-26 16:58:45.827603!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0495
13. set (Dataset 21) being trained for epoch 2 by 2019-01-26 16:59:06.041720!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1027
14. set (Dataset 17) being trained for epoch 2 by 2019-01-26 16:59:25.807133!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0691
15. set (Dataset 6) being trained for epoch 2 by 2019-01-26 16:59:40.558558!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0864
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 16:59:59.471893!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0555
17. set (Dataset 10) being trained for epoch 2 by 2019-01-26 17:00:21.174942!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0642
18. set (Dataset 1) being trained for epoch 2 by 2019-01-26 17:00:44.374979!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0748
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 17:01:04.393295!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0525
20. set (Dataset 2) being trained for epoch 2 by 2019-01-26 17:01:28.438906!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0662
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 17:01:45.893065
1. set (Dataset 1) being trained for epoch 3 by 2019-01-26 17:01:50.934464!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0790
2. set (Dataset 2) being trained for epoch 3 by 2019-01-26 17:02:08.432710!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0616
3. set (Dataset 4) being trained for epoch 3 by 2019-01-26 17:02:28.937703!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0708
4. set (Dataset 6) being trained for epoch 3 by 2019-01-26 17:02:53.247164!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0857
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 17:03:11.613937!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0556
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 17:03:30.970740!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0508
7. set (Dataset 10) being trained for epoch 3 by 2019-01-26 17:03:57.123069!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0589
8. set (Dataset 11) being trained for epoch 3 by 2019-01-26 17:04:20.910822!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0454
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 17:04:40.792617!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0939
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 17:05:03.286982!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0665
11. set (Dataset 15) being trained for epoch 3 by 2019-01-26 17:05:28.773460!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0680
12. set (Dataset 16) being trained for epoch 3 by 2019-01-26 17:05:54.056530!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0593
13. set (Dataset 17) being trained for epoch 3 by 2019-01-26 17:06:20.886557!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0661
14. set (Dataset 18) being trained for epoch 3 by 2019-01-26 17:06:37.021396!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.0780
15. set (Dataset 19) being trained for epoch 3 by 2019-01-26 17:06:56.984316!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0607
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 17:07:15.626451!
Epoch 1/1
556/556 [==============================] - 15s 26ms/step - loss: 0.0531
17. set (Dataset 21) being trained for epoch 3 by 2019-01-26 17:07:36.308932!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0962
18. set (Dataset 22) being trained for epoch 3 by 2019-01-26 17:07:58.608925!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0471
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 17:08:22.820943!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0622
20. set (Dataset 24) being trained for epoch 3 by 2019-01-26 17:08:46.566974!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0531
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 17:09:03.625996
1. set (Dataset 22) being trained for epoch 4 by 2019-01-26 17:09:10.012108!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0468
2. set (Dataset 24) being trained for epoch 4 by 2019-01-26 17:09:31.345373!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0477
3. set (Dataset 15) being trained for epoch 4 by 2019-01-26 17:09:50.125437!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0636
4. set (Dataset 19) being trained for epoch 4 by 2019-01-26 17:10:11.264555!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0626
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 17:10:31.695056!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0550
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 17:10:57.889170!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0639
7. set (Dataset 21) being trained for epoch 4 by 2019-01-26 17:11:23.283327!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1014
8. set (Dataset 16) being trained for epoch 4 by 2019-01-26 17:11:47.768877!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0635
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 17:12:15.977478!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0556
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 17:12:34.164758!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0883
11. set (Dataset 10) being trained for epoch 4 by 2019-01-26 17:12:55.913165!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0617
12. set (Dataset 1) being trained for epoch 4 by 2019-01-26 17:13:19.661703!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0721
13. set (Dataset 18) being trained for epoch 4 by 2019-01-26 17:13:38.715243!
Epoch 1/1
614/614 [==============================] - 17s 28ms/step - loss: 0.0804
14. set (Dataset 2) being trained for epoch 4 by 2019-01-26 17:14:00.879291!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0667
15. set (Dataset 4) being trained for epoch 4 by 2019-01-26 17:14:21.437279!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0708
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 17:14:45.611616!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0558
17. set (Dataset 17) being trained for epoch 4 by 2019-01-26 17:15:03.497338!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0642
18. set (Dataset 6) being trained for epoch 4 by 2019-01-26 17:15:18.858449!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0851
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 17:15:40.787378!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0696
20. set (Dataset 11) being trained for epoch 4 by 2019-01-26 17:16:06.303843!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0479
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 17:16:25.259243
1. set (Dataset 6) being trained for epoch 5 by 2019-01-26 17:16:30.453304!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0802
2. set (Dataset 11) being trained for epoch 5 by 2019-01-26 17:16:49.830887!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0479
3. set (Dataset 10) being trained for epoch 5 by 2019-01-26 17:17:11.553813!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0579
4. set (Dataset 4) being trained for epoch 5 by 2019-01-26 17:17:37.315399!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0682
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 17:18:03.395014!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0616
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 17:18:30.121674!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0645
7. set (Dataset 17) being trained for epoch 5 by 2019-01-26 17:18:53.627234!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0692
8. set (Dataset 1) being trained for epoch 5 by 2019-01-26 17:19:08.876277!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0729
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 17:19:28.352156!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0513
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 17:19:51.285722!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0519
11. set (Dataset 21) being trained for epoch 5 by 2019-01-26 17:20:10.199372!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.0980
12. set (Dataset 22) being trained for epoch 5 by 2019-01-26 17:20:33.342570!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0472
13. set (Dataset 2) being trained for epoch 5 by 2019-01-26 17:20:55.709579!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0634
14. set (Dataset 24) being trained for epoch 5 by 2019-01-26 17:21:13.082804!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0522
15. set (Dataset 15) being trained for epoch 5 by 2019-01-26 17:21:31.715084!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0655
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 17:21:53.275933!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0549
17. set (Dataset 18) being trained for epoch 5 by 2019-01-26 17:22:13.521691!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0795
18. set (Dataset 19) being trained for epoch 5 by 2019-01-26 17:22:34.102147!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0599
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 17:22:51.896655!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0876
20. set (Dataset 16) being trained for epoch 5 by 2019-01-26 17:23:15.005906!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0607
Epoch 5 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (7, 'M01'), (8, 'M02'), (17, 'M
10'), (1, 'F01'), (12, 'M06'), (13, 'M07'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'),
(20, 'M12'), (18, 'F05'), (19, 'M11'), (23, 'M13'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 17:23:40.588003
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.11 Degree
        The absolute mean error on Yaw angle estimation: 28.69 Degree
        The absolute mean error on Roll angle estimation: 16.96 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.79 Degree
        The absolute mean error on Yaw angle estimation: 25.86 Degree
        The absolute mean error on Roll angle estimation: 3.97 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 27.42 Degree
        The absolute mean error on Yaw angle estimation: 26.50 Degree
        The absolute mean error on Roll angle estimation: 6.38 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.56 Degree
        The absolute mean error on Yaw angle estimation: 31.54 Degree
        The absolute mean error on Roll angle estimation: 13.50 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.97 Degree
        The absolute mean error on Yaw angle estimations: 28.15 Degree
        The absolute mean error on Roll angle estimations: 10.20 Degree
Exp2019-01-26_13-37-43_part6 completed!
Exp2019-01-26_13-37-43.h5 has been saved.
subject3_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:05.490411.
subject5_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:05.689277.
subject9_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:05.885095.
subject14_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:06.100548.
Model Exp2019-01-26_13-37-43 has been evaluated successfully.
Model Exp2019-01-26_13-37-43 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-26_13-37-43 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 11:51:35.266816: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 11:51:35.364688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 11:51:35.364997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 11:51:35.365012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 11:51:35.520660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 11:51:35.520686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 11:51:35.520691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 11:51:35.520869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-26_13-37-43.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model Exp2019-01-26_13-37-43_and_2019-01-27_11-51-36
All frames and annotations from 20 datasets have been read by 2019-01-27 11:51:41.303554
1. set (Dataset 22) being trained for epoch 1 by 2019-01-27 11:51:47.702222!
Epoch 1/1
2019-01-27 11:51:48.937122: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_arra
y_ops.cc:121 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does no
t exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327,
in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312,
in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420,
in _call_tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line
 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_619)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 48, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 39, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 29, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on
_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482,
in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, i
n run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140,
in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321,
in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340,
in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_619)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGra
dV3', defined at:
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.p
y", line 52, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_tra
in_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519,
in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 48
8, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 37
9, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line
 104, in _TensorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line
 6221, in tensor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", l
ine 787, in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, i
n create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, i
n __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.p
y", line 52, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserializ
e
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deser
ialize_keras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771,
in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754,
in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Tenso
rArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_619)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 11:56:53.344926: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 11:56:53.442347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 11:56:53.442603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 11:56:53.442616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 11:56:53.597546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 11:56:53.597572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 11:56:53.597577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 11:56:53.597719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_11-56-54 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_11-56-54
All frames and annotations from 1 datasets have been read by 2019-01-27 11:56:55.212496
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 11:57:04.107940!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 561.1528 - mean_absolute_error: 18.3496
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 11:57:29.215899
1. set (Dataset 9) being trained for epoch 2 by 2019-01-27 11:57:38.100679!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 552.5623 - mean_absolute_error: 18.1862
Epoch 2 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 11:58:01.075357
1. set (Dataset 9) being trained for epoch 3 by 2019-01-27 11:58:09.943610!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 546.9199 - mean_absolute_error: 18.0749
Epoch 3 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 11:58:33.073716
1. set (Dataset 9) being trained for epoch 4 by 2019-01-27 11:58:41.959666!
Epoch 1/1
303/882 [=========>....................] - ETA: 14s - loss: 534.0697 - mean_absolute_error: 17.7895^C
Model Exp2019-01-27_11-56-54_part1 has been interrupted.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 11:59:17.787248: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 11:59:17.885464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 11:59:17.885726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 11:59:17.885738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 11:59:18.041228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 11:59:18.041254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 11:59:18.041259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 11:59:18.041394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_11-59-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_11-59-18
All frames and annotations from 1 datasets have been read by 2019-01-27 11:59:19.688533
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 11:59:28.548652!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 561.1989 - mean_absolute_error: 18.3361
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_11-59-18
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 11:59:53.249342
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 1848.72 Degree
        The absolute mean error on Yaw angle estimation: 2441.66 Degree
        The absolute mean error on Roll angle estimation: 637.17 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 1848.72 Degree
        The absolute mean error on Yaw angle estimations: 2441.66 Degree
        The absolute mean error on Roll angle estimations: 637.17 Degree
Exp2019-01-27_11-59-18_part1 completed!
Exp2019-01-27_11-59-18.h5 has been saved.
subject9_Exp2019-01-27_11-59-18.png has been saved by 2019-01-27 12:00:15.977069.
Model Exp2019-01-27_11-59-18 has been evaluated successfully.
Model Exp2019-01-27_11-59-18 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:14:24.593442: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:14:24.689153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:14:24.689455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:14:24.689470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:14:24.844834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:14:24.844858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:14:24.844863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:14:24.845042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-14-25 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-14-25
All frames and annotations from 1 datasets have been read by 2019-01-27 13:14:26.511408
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:14:35.402054!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 571.6917 - mean_absolute_error: 18.6066
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-14-25
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:15:01.310412
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 20.74 Degree
        The absolute mean error on Yaw angle estimation: 27.23 Degree
        The absolute mean error on Roll angle estimation: 7.47 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.74 Degree
        The absolute mean error on Yaw angle estimations: 27.23 Degree
        The absolute mean error on Roll angle estimations: 7.47 Degree
Exp2019-01-27_13-14-25_part1 completed!
Exp2019-01-27_13-14-25.h5 has been saved.
subject9_Exp2019-01-27_13-14-25.png has been saved by 2019-01-27 13:15:24.025377.
Model Exp2019-01-27_13-14-25 has been evaluated successfully.
Model Exp2019-01-27_13-14-25 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:25:49.065244: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:25:49.161134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:25:49.161393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:25:49.161412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:25:49.317764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:25:49.317791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:25:49.317796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:25:49.317934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-25-50 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-25-50
All frames and annotations from 1 datasets have been read by 2019-01-27 13:25:50.872564
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:25:59.772653!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 568.6984 - mean_absolute_error: 18.4823
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-25-50
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:26:18.258004
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 20.06 Degree
        The absolute mean error on Yaw angle estimation: 27.29 Degree
        The absolute mean error on Roll angle estimation: 7.17 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.06 Degree
        The absolute mean error on Yaw angle estimations: 27.29 Degree
        The absolute mean error on Roll angle estimations: 7.17 Degree
Exp2019-01-27_13-25-50_part1 completed!
Exp2019-01-27_13-25-50.h5 has been saved.
subject9_Exp2019-01-27_13-25-50.png has been saved by 2019-01-27 13:26:35.981633.
Model Exp2019-01-27_13-25-50 has been evaluated successfully.
Model Exp2019-01-27_13-25-50 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:35:43.280212: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:35:43.378279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:35:43.378539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:35:43.378554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:35:43.533794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:35:43.533820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:35:43.533828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:35:43.533967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-35-44 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
1000_2019-01-27_13-35-44
All frames and annotations from 1 datasets have been read by 2019-01-27 13:35:45.097587
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:35:53.993757!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2081
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
001000_2019-01-27_13-35-44
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:36:12.285508
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.20 Degree
        The absolute mean error on Yaw angle estimation: 34.32 Degree
        The absolute mean error on Roll angle estimation: 10.73 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.20 Degree
        The absolute mean error on Yaw angle estimations: 34.32 Degree
        The absolute mean error on Roll angle estimations: 10.73 Degree
Exp2019-01-27_13-35-44_part1 completed!
Exp2019-01-27_13-35-44.h5 has been saved.
subject9_Exp2019-01-27_13-35-44.png has been saved by 2019-01-27 13:36:30.003511.
Model Exp2019-01-27_13-35-44 has been evaluated successfully.
Model Exp2019-01-27_13-35-44 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:36:51.400187: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:36:51.480776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:36:51.481041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:36:51.481056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:36:51.636542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:36:51.636570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:36:51.636579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:36:51.636720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-36-52 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.01
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.01
0000_2019-01-27_13-36-52
All frames and annotations from 1 datasets have been read by 2019-01-27 13:36:53.171766
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:37:02.060342!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2109
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
010000_2019-01-27_13-36-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:37:20.526908
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.27 Degree
        The absolute mean error on Yaw angle estimation: 26.28 Degree
        The absolute mean error on Roll angle estimation: 7.14 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 18.27 Degree
        The absolute mean error on Yaw angle estimations: 26.28 Degree
        The absolute mean error on Roll angle estimations: 7.14 Degree
Exp2019-01-27_13-36-52_part1 completed!
Exp2019-01-27_13-36-52.h5 has been saved.
subject9_Exp2019-01-27_13-36-52.png has been saved by 2019-01-27 13:37:38.307345.
Model Exp2019-01-27_13-36-52 has been evaluated successfully.
Model Exp2019-01-27_13-36-52 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:39:31.969676: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:39:32.067036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:39:32.067313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:39:32.067325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:39:32.222315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:39:32.222340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:39:32.222345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:39:32.222488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-39-33 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-39-33
All frames and annotations from 1 datasets have been read by 2019-01-27 13:39:33.866209
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:39:42.750884!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.3898
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-39-33
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:40:07.461262
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.67 Degree
        The absolute mean error on Yaw angle estimation: 33.01 Degree
        The absolute mean error on Roll angle estimation: 11.71 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.67 Degree
        The absolute mean error on Yaw angle estimations: 33.01 Degree
        The absolute mean error on Roll angle estimations: 11.71 Degree
Exp2019-01-27_13-39-33_part1 completed!
Exp2019-01-27_13-39-33.h5 has been saved.
subject9_Exp2019-01-27_13-39-33.png has been saved by 2019-01-27 13:40:30.273942.
Model Exp2019-01-27_13-39-33 has been evaluated successfully.
Model Exp2019-01-27_13-39-33 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:41:29.649761: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:41:29.748526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:41:29.748787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:41:29.748804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:41:29.904282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:41:29.904306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:41:29.904311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:41:29.904456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-41-30 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0010_2019-01-27_13-41-30
All frames and annotations from 1 datasets have been read by 2019-01-27 13:41:31.534768
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:41:40.407342!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2185
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000010_2019-01-27_13-41-30
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:42:05.188256
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.48 Degree
        The absolute mean error on Yaw angle estimation: 19.37 Degree
        The absolute mean error on Roll angle estimation: 11.52 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.48 Degree
        The absolute mean error on Yaw angle estimations: 19.37 Degree
        The absolute mean error on Roll angle estimations: 11.52 Degree
Exp2019-01-27_13-41-30_part1 completed!
Exp2019-01-27_13-41-30.h5 has been saved.
subject9_Exp2019-01-27_13-41-30.png has been saved by 2019-01-27 13:42:27.969351.
Model Exp2019-01-27_13-41-30 has been evaluated successfully.
Model Exp2019-01-27_13-41-30 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:43:19.044773: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:43:19.143056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:43:19.143316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:43:19.143334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:43:19.298473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:43:19.298499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:43:19.298504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:43:19.298643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-43-20 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.000001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0001_2019-01-27_13-43-20
All frames and annotations from 1 datasets have been read by 2019-01-27 13:43:20.995841
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:43:29.888634!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.3066
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000001_2019-01-27_13-43-20
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:43:55.435424
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.65 Degree
        The absolute mean error on Yaw angle estimation: 58.86 Degree
        The absolute mean error on Roll angle estimation: 13.83 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 16.65 Degree
        The absolute mean error on Yaw angle estimations: 58.86 Degree
        The absolute mean error on Roll angle estimations: 13.83 Degree
Exp2019-01-27_13-43-20_part1 completed!
Exp2019-01-27_13-43-20.h5 has been saved.
subject9_Exp2019-01-27_13-43-20.png has been saved by 2019-01-27 13:44:18.221699.
Model Exp2019-01-27_13-43-20 has been evaluated successfully.
Model Exp2019-01-27_13-43-20 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:44:41.602901: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:44:41.682597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:44:41.682852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:44:41.682865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:44:41.837607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:44:41.837631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:44:41.837636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:44:41.837777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-44-42 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-44-42
All frames and annotations from 1 datasets have been read by 2019-01-27 13:44:43.446538
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:44:52.329254!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2836
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-44-42
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:45:16.772193
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 13.25 Degree
        The absolute mean error on Yaw angle estimation: 22.60 Degree
        The absolute mean error on Roll angle estimation: 8.22 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 13.25 Degree
        The absolute mean error on Yaw angle estimations: 22.60 Degree
        The absolute mean error on Roll angle estimations: 8.22 Degree
Exp2019-01-27_13-44-42_part1 completed!
Exp2019-01-27_13-44-42.h5 has been saved.
subject9_Exp2019-01-27_13-44-42.png has been saved by 2019-01-27 13:45:39.565177.
Model Exp2019-01-27_13-44-42 has been evaluated successfully.
Model Exp2019-01-27_13-44-42 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:46:18.055774: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:46:18.153497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:46:18.153762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:46:18.153775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:46:18.309810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:46:18.309837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:46:18.309842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:46:18.309980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-46-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-46-18
All frames and annotations from 1 datasets have been read by 2019-01-27 13:46:19.828002
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:46:28.727202!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1927
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-46-18
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:46:47.092500
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.64 Degree
        The absolute mean error on Yaw angle estimation: 19.06 Degree
        The absolute mean error on Roll angle estimation: 10.65 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.64 Degree
        The absolute mean error on Yaw angle estimations: 19.06 Degree
        The absolute mean error on Roll angle estimations: 10.65 Degree
Exp2019-01-27_13-46-18_part1 completed!
Exp2019-01-27_13-46-18.h5 has been saved.
subject9_Exp2019-01-27_13-46-18.png has been saved by 2019-01-27 13:47:04.915175.
Model Exp2019-01-27_13-46-18 has been evaluated successfully.
Model Exp2019-01-27_13-46-18 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:54:21.987780: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:54:22.084073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:54:22.084330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:54:22.084342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:54:22.239653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:54:22.239680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:54:22.239685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:54:22.239831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-54-23 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-54-23
All frames and annotations from 1 datasets have been read by 2019-01-27 13:54:23.874366
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:54:32.746147!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 0.2940
Epoch 1 completed!
Exp2019-01-27_13-54-23_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-54-23
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:54:58.244899
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 32.75 Degree
        The absolute mean error on Yaw angle estimation: 47.04 Degree
        The absolute mean error on Roll angle estimation: 7.60 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 32.75 Degree
        The absolute mean error on Yaw angle estimations: 47.04 Degree
        The absolute mean error on Roll angle estimations: 7.60 Degree
Exp2019-01-27_13-54-23_part1 completed!
Exp2019-01-27_13-54-23.h5 has been saved.
subject9_Exp2019-01-27_13-54-23.png has been saved by 2019-01-27 13:55:20.896750.
Model Exp2019-01-27_13-54-23 has been evaluated successfully.
Model Exp2019-01-27_13-54-23 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:55:58.877868: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:55:58.975696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:55:58.975951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:55:58.975964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:55:59.131706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:55:59.131732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:55:59.131737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:55:59.131875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-55-59 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0010_2019-01-27_13-55-59
All frames and annotations from 1 datasets have been read by 2019-01-27 13:56:00.769742
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:56:09.636566!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2676
Epoch 1 completed!
Exp2019-01-27_13-55-59_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000010_2019-01-27_13-55-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:56:35.010867
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 32.57 Degree
        The absolute mean error on Yaw angle estimation: 22.06 Degree
        The absolute mean error on Roll angle estimation: 20.35 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 32.57 Degree
        The absolute mean error on Yaw angle estimations: 22.06 Degree
        The absolute mean error on Roll angle estimations: 20.35 Degree
Exp2019-01-27_13-55-59_part1 completed!
Exp2019-01-27_13-55-59.h5 has been saved.
subject9_Exp2019-01-27_13-55-59.png has been saved by 2019-01-27 13:56:57.626130.
Model Exp2019-01-27_13-55-59 has been evaluated successfully.
Model Exp2019-01-27_13-55-59 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:57:55.885697: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:57:55.983644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:57:55.983905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:57:55.983917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:57:56.138800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:57:56.138826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:57:56.138831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:57:56.138966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-57-56 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 10
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 13:57:57.775817
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:58:06.669573!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2257
Epoch 1 completed!
Exp2019-01-27_13-57-56_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:58:32.017535
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.54 Degree
        The absolute mean error on Yaw angle estimation: 23.62 Degree
        The absolute mean error on Roll angle estimation: 14.57 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 12.54 Degree
        The absolute mean error on Yaw angle estimations: 23.62 Degree
        The absolute mean error on Roll angle estimations: 14.57 Degree
Exp2019-01-27_13-57-56_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 13:58:54.976084
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:59:03.779226!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.2100
Epoch 1 completed!
Exp2019-01-27_13-57-56_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:59:26.953275
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.54 Degree
        The absolute mean error on Yaw angle estimation: 21.65 Degree
        The absolute mean error on Roll angle estimation: 15.12 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.54 Degree
        The absolute mean error on Yaw angle estimations: 21.65 Degree
        The absolute mean error on Roll angle estimations: 15.12 Degree
Exp2019-01-27_13-57-56_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 13:59:49.888616
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:59:58.688990!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.2033
Epoch 1 completed!
Exp2019-01-27_13-57-56_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:00:22.263114
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.26 Degree
        The absolute mean error on Yaw angle estimation: 20.37 Degree
        The absolute mean error on Roll angle estimation: 13.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.26 Degree
        The absolute mean error on Yaw angle estimations: 20.37 Degree
        The absolute mean error on Roll angle estimations: 13.37 Degree
Exp2019-01-27_13-57-56_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:00:45.154755
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:00:54.002816!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1953
Epoch 1 completed!
Exp2019-01-27_13-57-56_part4.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:01:16.972010
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.83 Degree
        The absolute mean error on Yaw angle estimation: 19.05 Degree
        The absolute mean error on Roll angle estimation: 9.71 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.83 Degree
        The absolute mean error on Yaw angle estimations: 19.05 Degree
        The absolute mean error on Roll angle estimations: 9.71 Degree
Exp2019-01-27_13-57-56_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:01:39.820111
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:01:48.623665!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1938
Epoch 1 completed!
Exp2019-01-27_13-57-56_part5.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:02:11.496574
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.23 Degree
        The absolute mean error on Yaw angle estimation: 20.05 Degree
        The absolute mean error on Roll angle estimation: 7.67 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 12.23 Degree
        The absolute mean error on Yaw angle estimations: 20.05 Degree
        The absolute mean error on Roll angle estimations: 7.67 Degree
Exp2019-01-27_13-57-56_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:02:34.365482
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:02:43.210671!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1874
Epoch 1 completed!
Exp2019-01-27_13-57-56_part6.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:03:06.544020
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.95 Degree
        The absolute mean error on Yaw angle estimation: 17.27 Degree
        The absolute mean error on Roll angle estimation: 7.98 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.95 Degree
        The absolute mean error on Yaw angle estimations: 17.27 Degree
        The absolute mean error on Roll angle estimations: 7.98 Degree
Exp2019-01-27_13-57-56_part6 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:03:29.498885
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:03:38.322744!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1814
Epoch 1 completed!
Exp2019-01-27_13-57-56_part7.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:04:01.518257
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.15 Degree
        The absolute mean error on Yaw angle estimation: 17.84 Degree
        The absolute mean error on Roll angle estimation: 14.80 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 12.15 Degree
        The absolute mean error on Yaw angle estimations: 17.84 Degree
        The absolute mean error on Roll angle estimations: 14.80 Degree
Exp2019-01-27_13-57-56_part7 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:04:24.432853
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:04:33.233569!
Epoch 1/1
882/882 [==============================] - 21s 24ms/step - loss: 0.1780
Epoch 1 completed!
Exp2019-01-27_13-57-56_part8.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:04:55.710285
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.23 Degree
        The absolute mean error on Yaw angle estimation: 17.05 Degree
        The absolute mean error on Roll angle estimation: 5.60 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.23 Degree
        The absolute mean error on Yaw angle estimations: 17.05 Degree
        The absolute mean error on Roll angle estimations: 5.60 Degree
Exp2019-01-27_13-57-56_part8 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:05:18.610510
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:05:27.420508!
Epoch 1/1
882/882 [==============================] - 21s 24ms/step - loss: 0.1765
Epoch 1 completed!
Exp2019-01-27_13-57-56_part9.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:05:50.080239
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.06 Degree
        The absolute mean error on Yaw angle estimation: 16.22 Degree
        The absolute mean error on Roll angle estimation: 10.89 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.06 Degree
        The absolute mean error on Yaw angle estimations: 16.22 Degree
        The absolute mean error on Roll angle estimations: 10.89 Degree
Exp2019-01-27_13-57-56_part9 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:06:12.968372
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:06:21.765093!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1763
Epoch 1 completed!
Exp2019-01-27_13-57-56_part10.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:06:44.596901
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.00 Degree
        The absolute mean error on Yaw angle estimation: 15.07 Degree
        The absolute mean error on Roll angle estimation: 8.79 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.00 Degree
        The absolute mean error on Yaw angle estimations: 15.07 Degree
        The absolute mean error on Roll angle estimations: 8.79 Degree
Exp2019-01-27_13-57-56_part10 completed!
Exp2019-01-27_13-57-56.h5 has been saved.
subject9_Exp2019-01-27_13-57-56.png has been saved by 2019-01-27 14:07:07.085305.
Model Exp2019-01-27_13-57-56 has been evaluated successfully.
Model Exp2019-01-27_13-57-56 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:12:08.064165: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:12:08.161692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 14:12:08.161949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:12:08.161962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 14:12:08.317579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 14:12:08.317603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:12:08.317607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:12:08.317750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-12-09 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 10
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:12:09.868808
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:12:18.758786!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1528
Epoch 1 completed!
Exp2019-01-27_14-12-09_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:12:37.055194
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.56 Degree
        The absolute mean error on Yaw angle estimation: 11.10 Degree
        The absolute mean error on Roll angle estimation: 3.78 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.56 Degree
        The absolute mean error on Yaw angle estimations: 11.10 Degree
        The absolute mean error on Roll angle estimations: 3.78 Degree
Exp2019-01-27_14-12-09_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:12:55.111877
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:13:03.981355!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0974
Epoch 1 completed!
Exp2019-01-27_14-12-09_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:13:20.712950
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.91 Degree
        The absolute mean error on Yaw angle estimation: 29.45 Degree
        The absolute mean error on Roll angle estimation: 4.58 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.91 Degree
        The absolute mean error on Yaw angle estimations: 29.45 Degree
        The absolute mean error on Roll angle estimations: 4.58 Degree
Exp2019-01-27_14-12-09_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:13:38.719507
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:13:47.612435!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0982
Epoch 1 completed!
Exp2019-01-27_14-12-09_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:14:04.328626
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.08 Degree
        The absolute mean error on Yaw angle estimation: 6.44 Degree
        The absolute mean error on Roll angle estimation: 3.11 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 4.08 Degree
        The absolute mean error on Yaw angle estimations: 6.44 Degree
        The absolute mean error on Roll angle estimations: 3.11 Degree
Exp2019-01-27_14-12-09_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:14:22.341498
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:14:31.227592!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 0.0778
Epoch 1 completed!
Exp2019-01-27_14-12-09_part4.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:14:47.746107
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.09 Degree
        The absolute mean error on Yaw angle estimation: 4.83 Degree
        The absolute mean error on Roll angle estimation: 2.56 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.09 Degree
        The absolute mean error on Yaw angle estimations: 4.83 Degree
        The absolute mean error on Roll angle estimations: 2.56 Degree
Exp2019-01-27_14-12-09_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:15:05.793326
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:15:14.667683!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 0.0697
Epoch 1 completed!
Exp2019-01-27_14-12-09_part5.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:15:31.070599
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.07 Degree
        The absolute mean error on Yaw angle estimation: 4.55 Degree
        The absolute mean error on Roll angle estimation: 2.26 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 4.07 Degree
        The absolute mean error on Yaw angle estimations: 4.55 Degree
        The absolute mean error on Roll angle estimations: 2.26 Degree
Exp2019-01-27_14-12-09_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:15:49.091542
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:15:57.967979!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0669
Epoch 1 completed!
Exp2019-01-27_14-12-09_part6.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:16:14.901958
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.64 Degree
        The absolute mean error on Yaw angle estimation: 4.86 Degree
        The absolute mean error on Roll angle estimation: 1.93 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.64 Degree
        The absolute mean error on Yaw angle estimations: 4.86 Degree
        The absolute mean error on Roll angle estimations: 1.93 Degree
Exp2019-01-27_14-12-09_part6 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:16:32.937169
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:16:41.816754!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0619
Epoch 1 completed!
Exp2019-01-27_14-12-09_part7.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:16:58.759968
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.21 Degree
        The absolute mean error on Yaw angle estimation: 6.93 Degree
        The absolute mean error on Roll angle estimation: 2.26 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.21 Degree
        The absolute mean error on Yaw angle estimations: 6.93 Degree
        The absolute mean error on Roll angle estimations: 2.26 Degree
Exp2019-01-27_14-12-09_part7 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:17:16.815489
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:17:25.690155!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0575
Epoch 1 completed!
Exp2019-01-27_14-12-09_part8.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:17:42.661223
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.90 Degree
        The absolute mean error on Yaw angle estimation: 3.49 Degree
        The absolute mean error on Roll angle estimation: 2.17 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.90 Degree
        The absolute mean error on Yaw angle estimations: 3.49 Degree
        The absolute mean error on Roll angle estimations: 2.17 Degree
Exp2019-01-27_14-12-09_part8 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:18:00.695985
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:18:09.561088!
Epoch 1/1
882/882 [==============================] - 15s 18ms/step - loss: 0.0564
Epoch 1 completed!
Exp2019-01-27_14-12-09_part9.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:18:26.137622
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.13 Degree
        The absolute mean error on Yaw angle estimation: 3.93 Degree
        The absolute mean error on Roll angle estimation: 2.71 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.13 Degree
        The absolute mean error on Yaw angle estimations: 3.93 Degree
        The absolute mean error on Roll angle estimations: 2.71 Degree
Exp2019-01-27_14-12-09_part9 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:18:44.216116
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:18:53.085128!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0537
Epoch 1 completed!
Exp2019-01-27_14-12-09_part10.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:19:10.244000
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.78 Degree
        The absolute mean error on Yaw angle estimation: 6.33 Degree
        The absolute mean error on Roll angle estimation: 1.94 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.78 Degree
        The absolute mean error on Yaw angle estimations: 6.33 Degree
        The absolute mean error on Roll angle estimations: 1.94 Degree
Exp2019-01-27_14-12-09_part10 completed!
Exp2019-01-27_14-12-09.h5 has been saved.
subject9_Exp2019-01-27_14-12-09.png has been saved by 2019-01-27 14:19:27.955675.
Model Exp2019-01-27_14-12-09 has been evaluated successfully.
Model Exp2019-01-27_14-12-09 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:22:50.513479: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:22:50.650116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 14:22:50.650383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:22:50.650398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 14:22:50.805215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 14:22:50.805239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:22:50.805244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:22:50.805379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-22-51 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 10
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:22:52.336614
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:23:01.233406!
Epoch 1/1
882/882 [==============================] - 17s 20ms/step - loss: 0.1814
Epoch 1 completed!
Exp2019-01-27_14-22-51_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:23:20.524612
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.54 Degree
        The absolute mean error on Yaw angle estimation: 24.20 Degree
        The absolute mean error on Roll angle estimation: 7.94 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.54 Degree
        The absolute mean error on Yaw angle estimations: 24.20 Degree
        The absolute mean error on Roll angle estimations: 7.94 Degree
Exp2019-01-27_14-22-51_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:23:38.644955
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:23:47.535765!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1530
Epoch 1 completed!
Exp2019-01-27_14-22-51_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:24:04.341496
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.36 Degree
        The absolute mean error on Yaw angle estimation: 11.74 Degree
        The absolute mean error on Roll angle estimation: 8.61 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.36 Degree
        The absolute mean error on Yaw angle estimations: 11.74 Degree
        The absolute mean error on Roll angle estimations: 8.61 Degree
Exp2019-01-27_14-22-51_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:24:22.336096
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:24:31.224476!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1379
Epoch 1 completed!
Exp2019-01-27_14-22-51_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:24:48.042813
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.13 Degree
        The absolute mean error on Yaw angle estimation: 14.27 Degree
        The absolute mean error on Roll angle estimation: 5.87 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.13 Degree
        The absolute mean error on Yaw angle estimations: 14.27 Degree
        The absolute mean error on Roll angle estimations: 5.87 Degree
Exp2019-01-27_14-22-51_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:25:06.099666
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:25:14.967599!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1240
Epoch 1 completed!
Exp2019-01-27_14-22-51_part4.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:25:31.871593
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.11 Degree
        The absolute mean error on Yaw angle estimation: 14.73 Degree
        The absolute mean error on Roll angle estimation: 5.53 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.11 Degree
        The absolute mean error on Yaw angle estimations: 14.73 Degree
        The absolute mean error on Roll angle estimations: 5.53 Degree
Exp2019-01-27_14-22-51_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:25:49.948082
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:25:58.877512!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1127
Epoch 1 completed!
Exp2019-01-27_14-22-51_part5.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:26:15.707754
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 13.96 Degree
        The absolute mean error on Yaw angle estimation: 13.96 Degree
        The absolute mean error on Roll angle estimation: 9.96 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 13.96 Degree
        The absolute mean error on Yaw angle estimations: 13.96 Degree
        The absolute mean error on Roll angle estimations: 9.96 Degree
Exp2019-01-27_14-22-51_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:26:33.713023
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:26:42.584351!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1134
Epoch 1 completed!
Exp2019-01-27_14-22-51_part6.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:26:59.591930
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.31 Degree
        The absolute mean error on Yaw angle estimation: 13.77 Degree
        The absolute mean error on Roll angle estimation: 5.05 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 8.31 Degree
        The absolute mean error on Yaw angle estimations: 13.77 Degree
        The absolute mean error on Roll angle estimations: 5.05 Degree
Exp2019-01-27_14-22-51_part6 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:27:17.610320
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:27:26.491399!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1022
Epoch 1 completed!
Exp2019-01-27_14-22-51_part7.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:27:43.732378
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.73 Degree
        The absolute mean error on Yaw angle estimation: 12.42 Degree
        The absolute mean error on Roll angle estimation: 4.29 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.73 Degree
        The absolute mean error on Yaw angle estimations: 12.42 Degree
        The absolute mean error on Roll angle estimations: 4.29 Degree
Exp2019-01-27_14-22-51_part7 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:28:01.799529
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:28:10.669395!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1000
Epoch 1 completed!
Exp2019-01-27_14-22-51_part8.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:28:28.011063
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 11.97 Degree
        The absolute mean error on Yaw angle estimation: 10.27 Degree
        The absolute mean error on Roll angle estimation: 6.08 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.97 Degree
        The absolute mean error on Yaw angle estimations: 10.27 Degree
        The absolute mean error on Roll angle estimations: 6.08 Degree
Exp2019-01-27_14-22-51_part8 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:28:46.030915
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:28:54.916290!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0954
Epoch 1 completed!
Exp2019-01-27_14-22-51_part9.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:29:12.107028
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.79 Degree
        The absolute mean error on Yaw angle estimation: 12.49 Degree
        The absolute mean error on Roll angle estimation: 5.46 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.79 Degree
        The absolute mean error on Yaw angle estimations: 12.49 Degree
        The absolute mean error on Roll angle estimations: 5.46 Degree
Exp2019-01-27_14-22-51_part9 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:29:30.425215
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:29:39.305690!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0940
Epoch 1 completed!
Exp2019-01-27_14-22-51_part10.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:29:56.096427
^[[B
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 11.99 Degree
        The absolute mean error on Yaw angle estimation: 11.38 Degree
        The absolute mean error on Roll angle estimation: 6.25 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.99 Degree
        The absolute mean error on Yaw angle estimations: 11.38 Degree
        The absolute mean error on Roll angle estimations: 6.25 Degree
Exp2019-01-27_14-22-51_part10 completed!
Exp2019-01-27_14-22-51.h5 has been saved.
subject9_Exp2019-01-27_14-22-51.png has been saved by 2019-01-27 14:30:13.810628.
Model Exp2019-01-27_14-22-51 has been evaluated successfully.
Model Exp2019-01-27_14-22-51 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:36:59.292172: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:36:59.389454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 14:36:59.389717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:36:59.389730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 14:36:59.545699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 14:36:59.545725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:36:59.545730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:36:59.545869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-37-00 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-37-00
All frames and annotations from 1 datasets have been read by 2019-01-27 14:37:01.097217
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:37:10.135526!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1785
Epoch 1 completed!
Exp2019-01-27_14-37-00_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-37-00
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:37:28.701735
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 13.42 Degree
        The absolute mean error on Yaw angle estimation: 25.46 Degree
        The absolute mean error on Roll angle estimation: 6.02 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 13.42 Degree
        The absolute mean error on Yaw angle estimations: 25.46 Degree
        The absolute mean error on Roll angle estimations: 6.02 Degree
Exp2019-01-27_14-37-00_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-37-00
All frames and annotations from 1 datasets have been read by 2019-01-27 14:37:46.723326
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:37:55.599698!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1463
Epoch 1 completed!
Exp2019-01-27_14-37-00_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-37-00
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:38:12.734572
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.80 Degree
        The absolute mean error on Yaw angle estimation: 20.47 Degree
        The absolute mean error on Roll angle estimation: 4.20 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 8.80 Degree
        The absolute mean error on Yaw angle estimations: 20.47 Degree
        The absolute mean error on Roll angle estimations: 4.20 Degree
Exp2019-01-27_14-37-00_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-37-00
All frames and annotations from 1 datasets have been read by 2019-01-27 14:38:30.715357
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:38:39.589163!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1358
Epoch 1 completed!
Exp2019-01-27_14-37-00_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-37-00
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:38:56.811656
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 9.34 Degree
        The absolute mean error on Yaw angle estimation: 15.15 Degree
        The absolute mean error on Roll angle estimation: 3.78 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 9.34 Degree
        The absolute mean error on Yaw angle estimations: 15.15 Degree
        The absolute mean error on Roll angle estimations: 3.78 Degree
Exp2019-01-27_14-37-00_part3 completed!
Exp2019-01-27_14-37-00.h5 has been saved.
subject9_Exp2019-01-27_14-37-00.png has been saved by 2019-01-27 14:39:14.497177.
Model Exp2019-01-27_14-37-00 has been evaluated successfully.
Model Exp2019-01-27_14-37-00 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:44:54.499002: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:44:54.595898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 14:44:54.596158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:44:54.596174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 14:44:54.750996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 14:44:54.751022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:44:54.751027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:44:54.751169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-44-55 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-44-55
All frames and annotations from 1 datasets have been read by 2019-01-27 14:44:56.326835
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:45:05.231495!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 18.0210
Epoch 1 completed!
Exp2019-01-27_14-44-55_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-44-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:45:23.834406
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.95 Degree
        The absolute mean error on Yaw angle estimation: 27.62 Degree
        The absolute mean error on Roll angle estimation: 6.91 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.95 Degree
        The absolute mean error on Yaw angle estimations: 27.62 Degree
        The absolute mean error on Roll angle estimations: 6.91 Degree
Exp2019-01-27_14-44-55_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-44-55
All frames and annotations from 1 datasets have been read by 2019-01-27 14:45:41.911863
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:45:50.815900!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 15.1474
Epoch 1 completed!
Exp2019-01-27_14-44-55_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-44-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:46:07.944194
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.33 Degree
        The absolute mean error on Yaw angle estimation: 51.37 Degree
        The absolute mean error on Roll angle estimation: 11.31 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.33 Degree
        The absolute mean error on Yaw angle estimations: 51.37 Degree
        The absolute mean error on Roll angle estimations: 11.31 Degree
Exp2019-01-27_14-44-55_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-44-55
All frames and annotations from 1 datasets have been read by 2019-01-27 14:46:25.939242
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:46:34.826481!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 14.7312
Epoch 1 completed!
Exp2019-01-27_14-44-55_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-44-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:46:51.759112
7For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.61 Degree
        The absolute mean error on Yaw angle estimation: 24.38 Degree
        The absolute mean error on Roll angle estimation: 9.90 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.61 Degree
        The absolute mean error on Yaw angle estimations: 24.38 Degree
        The absolute mean error on Roll angle estimations: 9.90 Degree
Exp2019-01-27_14-44-55_part3 completed!
Exp2019-01-27_14-44-55.h5 has been saved.
subject9_Exp2019-01-27_14-44-55.png has been saved by 2019-01-27 14:47:09.458016.
Model Exp2019-01-27_14-44-55 has been evaluated successfully.
Model Exp2019-01-27_14-44-55 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:06:26.706710: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:06:26.804736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:06:26.805046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:06:26.805060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:06:26.960746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:06:26.960773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:06:26.960778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:06:26.960958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-06-27 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-06-27
All frames and annotations from 1 datasets have been read by 2019-01-27 17:06:28.499394
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:06:37.386609!
Epoch 1/1
882/882 [==============================] - 16s 19ms/step - loss: 0.2803
Epoch 1 completed!
Exp2019-01-27_17-06-27_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-06-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:06:55.392619
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.37 Degree
        The absolute mean error on Yaw angle estimation: 27.08 Degree
        The absolute mean error on Roll angle estimation: 7.21 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 18.37 Degree
        The absolute mean error on Yaw angle estimations: 27.08 Degree
        The absolute mean error on Roll angle estimations: 7.21 Degree
Exp2019-01-27_17-06-27_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-06-27
All frames and annotations from 1 datasets have been read by 2019-01-27 17:07:13.549917
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:07:22.460788!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1686
Epoch 1 completed!
Exp2019-01-27_17-06-27_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-06-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:07:39.313559
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.90 Degree
        The absolute mean error on Yaw angle estimation: 26.35 Degree
        The absolute mean error on Roll angle estimation: 6.72 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.90 Degree
        The absolute mean error on Yaw angle estimations: 26.35 Degree
        The absolute mean error on Roll angle estimations: 6.72 Degree
Exp2019-01-27_17-06-27_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-06-27
All frames and annotations from 1 datasets have been read by 2019-01-27 17:07:57.303622
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:08:06.208615!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 0.3985
Epoch 1 completed!
Exp2019-01-27_17-06-27_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-06-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:08:22.727079
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 96.42 Degree
        The absolute mean error on Yaw angle estimation: 24.90 Degree
        The absolute mean error on Roll angle estimation: 20.49 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 96.42 Degree
        The absolute mean error on Yaw angle estimations: 24.90 Degree
        The absolute mean error on Roll angle estimations: 20.49 Degree
Exp2019-01-27_17-06-27_part3 completed!
Exp2019-01-27_17-06-27.h5 has been saved.
subject9_Exp2019-01-27_17-06-27.png has been saved by 2019-01-27 17:08:40.336369.
Model Exp2019-01-27_17-06-27 has been evaluated successfully.
Model Exp2019-01-27_17-06-27 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:11:07.981136: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:11:08.079072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:11:08.079329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:11:08.079342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:11:08.234938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:11:08.234964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:11:08.234972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:11:08.235112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-11-08 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-11-08
All frames and annotations from 1 datasets have been read by 2019-01-27 17:11:09.772831
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:11:18.668204!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 423.8770
Epoch 1 completed!
Exp2019-01-27_17-11-08_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-11-08
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:11:37.070157
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 82.24 Degree
        The absolute mean error on Yaw angle estimation: 161.83 Degree
        The absolute mean error on Roll angle estimation: 43.80 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 82.24 Degree
        The absolute mean error on Yaw angle estimations: 161.83 Degree
        The absolute mean error on Roll angle estimations: 43.80 Degree
Exp2019-01-27_17-11-08_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-11-08
All frames and annotations from 1 datasets have been read by 2019-01-27 17:11:55.115036
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:12:04.009826!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 268.4798
Epoch 1 completed!
Exp2019-01-27_17-11-08_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-11-08
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:12:20.789575
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5466.06 Degree
        The absolute mean error on Yaw angle estimation: 2227.08 Degree
        The absolute mean error on Roll angle estimation: 8169.04 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5466.06 Degree
        The absolute mean error on Yaw angle estimations: 2227.08 Degree
        The absolute mean error on Roll angle estimations: 8169.04 Degree
Exp2019-01-27_17-11-08_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-11-08
All frames and annotations from 1 datasets have been read by 2019-01-27 17:12:38.850034
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:12:47.740026!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 182.1086
Epoch 1 completed!
Exp2019-01-27_17-11-08_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-11-08
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:13:04.144654
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 21.67 Degree
        The absolute mean error on Yaw angle estimation: 53.25 Degree
        The absolute mean error on Roll angle estimation: 13.67 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 21.67 Degree
        The absolute mean error on Yaw angle estimations: 53.25 Degree
        The absolute mean error on Roll angle estimations: 13.67 Degree
Exp2019-01-27_17-11-08_part3 completed!
Exp2019-01-27_17-11-08.h5 has been saved.
subject9_Exp2019-01-27_17-11-08.png has been saved by 2019-01-27 17:13:21.784073.
Model Exp2019-01-27_17-11-08 has been evaluated successfully.
Model Exp2019-01-27_17-11-08 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:13:58.513362: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:13:58.596645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:13:58.596958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:13:58.596973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:13:58.752296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:13:58.752321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:13:58.752326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:13:58.752507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-13-59 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-13-59
All frames and annotations from 1 datasets have been read by 2019-01-27 17:14:00.270949
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:14:09.169790!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 5111.3121 - mean_absolute_error: 58.9947
Epoch 1 completed!
Exp2019-01-27_17-13-59_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-13-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:14:27.554966
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 258.08 Degree
        The absolute mean error on Yaw angle estimation: 154.25 Degree
        The absolute mean error on Roll angle estimation: 200.68 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 258.08 Degree
        The absolute mean error on Yaw angle estimations: 154.25 Degree
        The absolute mean error on Roll angle estimations: 200.68 Degree
Exp2019-01-27_17-13-59_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-13-59
All frames and annotations from 1 datasets have been read by 2019-01-27 17:14:45.661435
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:14:54.559045!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 3502.2638 - mean_absolute_error: 45.6296
Epoch 1 completed!
Exp2019-01-27_17-13-59_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-13-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:15:11.027305
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 222.44 Degree
        The absolute mean error on Yaw angle estimation: 96.18 Degree
        The absolute mean error on Roll angle estimation: 161.77 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 222.44 Degree
        The absolute mean error on Yaw angle estimations: 96.18 Degree
        The absolute mean error on Roll angle estimations: 161.77 Degree
Exp2019-01-27_17-13-59_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-13-59
All frames and annotations from 1 datasets have been read by 2019-01-27 17:15:29.051575
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:15:37.925067!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 2396.5377 - mean_absolute_error: 37.6626
Epoch 1 completed!
Exp2019-01-27_17-13-59_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-13-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:15:54.623638
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 127.72 Degree
        The absolute mean error on Yaw angle estimation: 36.24 Degree
        The absolute mean error on Roll angle estimation: 46.21 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 127.72 Degree
        The absolute mean error on Yaw angle estimations: 36.24 Degree
        The absolute mean error on Roll angle estimations: 46.21 Degree
Exp2019-01-27_17-13-59_part3 completed!
Exp2019-01-27_17-13-59.h5 has been saved.
subject9_Exp2019-01-27_17-13-59.png has been saved by 2019-01-27 17:16:12.281103.
Model Exp2019-01-27_17-13-59 has been evaluated successfully.
Model Exp2019-01-27_17-13-59 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:37:51.423768: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:37:51.519611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:37:51.519866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:37:51.519880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:37:51.675047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:37:51.675074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:37:51.675082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:37:51.675220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-37-52 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-37-52
All frames and annotations from 1 datasets have been read by 2019-01-27 17:37:53.223646
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:38:02.111424!
Epoch 1/1
882/882 [==============================] - 16s 19ms/step - loss: 20.5013
Epoch 1 completed!
Exp2019-01-27_17-37-52_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-37-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:38:20.198796
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 20.21 Degree
        The absolute mean error on Yaw angle estimation: 22.74 Degree
        The absolute mean error on Roll angle estimation: 39.90 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.21 Degree
        The absolute mean error on Yaw angle estimations: 22.74 Degree
        The absolute mean error on Roll angle estimations: 39.90 Degree
Exp2019-01-27_17-37-52_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-37-52
All frames and annotations from 1 datasets have been read by 2019-01-27 17:38:38.216487
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:38:47.094084!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 15.5593
Epoch 1 completed!
Exp2019-01-27_17-37-52_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-37-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:39:03.953601
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: nan Degree
        The absolute mean error on Yaw angle estimation: nan Degree
        The absolute mean error on Roll angle estimation: nan Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: nan Degree
        The absolute mean error on Yaw angle estimations: nan Degree
        The absolute mean error on Roll angle estimations: nan Degree
Exp2019-01-27_17-37-52_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-37-52
All frames and annotations from 1 datasets have been read by 2019-01-27 17:39:21.904712
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:39:30.782926!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 14.6536
Epoch 1 completed!
Exp2019-01-27_17-37-52_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-37-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:39:47.288458
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 53.75 Degree
        The absolute mean error on Yaw angle estimation: 53.80 Degree
        The absolute mean error on Roll angle estimation: 45.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 53.75 Degree
        The absolute mean error on Yaw angle estimations: 53.80 Degree
        The absolute mean error on Roll angle estimations: 45.37 Degree
Exp2019-01-27_17-37-52_part3 completed!
Exp2019-01-27_17-37-52.h5 has been saved.
subject9_Exp2019-01-27_17-37-52.png has been saved by 2019-01-27 17:40:04.934461.
Model Exp2019-01-27_17-37-52 has been evaluated successfully.
Model Exp2019-01-27_17-37-52 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:41:44.141756: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:41:44.239114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:41:44.239370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:41:44.239382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:41:44.394416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:41:44.394442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:41:44.394446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:41:44.394584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-41-45 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-41-45
All frames and annotations from 1 datasets have been read by 2019-01-27 17:41:45.911711
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:41:54.805160!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 17.7891
Epoch 1 completed!
Exp2019-01-27_17-41-45_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-41-45
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:42:13.349871
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 19.15 Degree
        The absolute mean error on Yaw angle estimation: 26.85 Degree
        The absolute mean error on Roll angle estimation: 6.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 19.15 Degree
        The absolute mean error on Yaw angle estimations: 26.85 Degree
        The absolute mean error on Roll angle estimations: 6.37 Degree
Exp2019-01-27_17-41-45_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-41-45
All frames and annotations from 1 datasets have been read by 2019-01-27 17:42:31.474049
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:42:40.382404!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 17.3497
Epoch 1 completed!
Exp2019-01-27_17-41-45_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-41-45
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:42:57.337922
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.55 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 6.13 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 18.55 Degree
        The absolute mean error on Yaw angle estimations: 26.69 Degree
        The absolute mean error on Roll angle estimations: 6.13 Degree
Exp2019-01-27_17-41-45_part2 completed!
Exp2019-01-27_17-41-45.h5 has been saved.
subject9_Exp2019-01-27_17-41-45.png has been saved by 2019-01-27 17:43:14.978960.
Model Exp2019-01-27_17-41-45 has been evaluated successfully.
Model Exp2019-01-27_17-41-45 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:44:39.001451: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:44:39.099178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:44:39.099434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:44:39.099447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:44:39.254759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:44:39.254784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:44:39.254789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:44:39.254931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-44-39 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-44-39
All frames and annotations from 1 datasets have been read by 2019-01-27 17:44:40.787081
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:44:49.683324!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1456
Epoch 1 completed!
Exp2019-01-27_17-44-39_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-44-39
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:45:07.929362
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 0.06 Degree
        The absolute mean error on Yaw angle estimation: 0.07 Degree
        The absolute mean error on Roll angle estimation: 0.04 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 0.06 Degree
        The absolute mean error on Yaw angle estimations: 0.07 Degree
        The absolute mean error on Roll angle estimations: 0.04 Degree
Exp2019-01-27_17-44-39_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-44-39
All frames and annotations from 1 datasets have been read by 2019-01-27 17:45:26.023434
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:45:34.894620!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0834
Epoch 1 completed!
Exp2019-01-27_17-44-39_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-44-39
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:45:51.901139
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 0.04 Degree
        The absolute mean error on Yaw angle estimation: 0.07 Degree
        The absolute mean error on Roll angle estimation: 0.03 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 0.04 Degree
        The absolute mean error on Yaw angle estimations: 0.07 Degree
        The absolute mean error on Roll angle estimations: 0.03 Degree
Exp2019-01-27_17-44-39_part2 completed!
Exp2019-01-27_17-44-39.h5 has been saved.
subject9_Exp2019-01-27_17-44-39.png has been saved by 2019-01-27 17:46:09.511789.
Model Exp2019-01-27_17-44-39 has been evaluated successfully.
Model Exp2019-01-27_17-44-39 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:47:11.540781: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:47:11.640112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:47:11.640371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:47:11.640383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:47:11.796872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:47:11.796897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:47:11.796901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:47:11.797041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-47-12 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-47-12
All frames and annotations from 1 datasets have been read by 2019-01-27 17:47:13.339605
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:47:22.252820!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1451
Epoch 1 completed!
Exp2019-01-27_17-47-12_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-47-12
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:47:40.685115
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.96 Degree
        The absolute mean error on Yaw angle estimation: 16.55 Degree
        The absolute mean error on Roll angle estimation: 7.15 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.96 Degree
        The absolute mean error on Yaw angle estimations: 16.55 Degree
        The absolute mean error on Roll angle estimations: 7.15 Degree
Exp2019-01-27_17-47-12_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-47-12
All frames and annotations from 1 datasets have been read by 2019-01-27 17:47:58.757568
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:48:07.651269!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0906
Epoch 1 completed!
Exp2019-01-27_17-47-12_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-47-12
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:48:24.654124
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.32 Degree
        The absolute mean error on Yaw angle estimation: 5.24 Degree
        The absolute mean error on Roll angle estimation: 4.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.32 Degree
        The absolute mean error on Yaw angle estimations: 5.24 Degree
        The absolute mean error on Roll angle estimations: 4.30 Degree
Exp2019-01-27_17-47-12_part2 completed!
Exp2019-01-27_17-47-12.h5 has been saved.
subject9_Exp2019-01-27_17-47-12.png has been saved by 2019-01-27 17:48:42.299472.
Model Exp2019-01-27_17-47-12 has been evaluated successfully.
Model Exp2019-01-27_17-47-12 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_17-47-12
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:56:06.162525: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:56:06.259846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:56:06.260107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:56:06.260119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:56:06.425913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:56:06.425939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:56:06.425943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:56:06.426081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-56-07
The subjects will be tested: [(9, 'M03')]
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 64, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, record = record)
TypeError: evaluateCNN_LSTM() missing 1 required positional argument: 'angles'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_17-47-12
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:57:18.819874: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:57:18.917262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:57:18.917521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:57:18.917535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:57:19.072422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:57:19.072448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:57:19.072453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:57:19.072589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-57-19
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:57:20.878985
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 5.32 Degree
        The absolute mean error on Yaw angle estimation: 5.24 Degree
        The absolute mean error on Roll angle estimation: 4.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.32 Degree
        The absolute mean error on Yaw angle estimations: 5.24 Degree
        The absolute mean error on Roll angle estimations: 4.30 Degree
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 64, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record =
record)
ValueError: too many values to unpack (expected 2)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_17-47-12
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:58:50.386018: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:58:50.482385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:58:50.482642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:58:50.482654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:58:50.637249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:58:50.637272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:58:50.637277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:58:50.637417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:58:52.478750
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 5.32 Degree
        The absolute mean error on Yaw angle estimation: 5.24 Degree
        The absolute mean error on Roll angle estimation: 4.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.32 Degree
        The absolute mean error on Yaw angle estimations: 5.24 Degree
        The absolute mean error on Roll angle estimations: 4.30 Degree
subject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51.png has been saved by 2019-01-27 17:59:10.617293.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51 has been evaluated successfully.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_17-47-12 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:59:25.695772: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:59:25.776512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:59:25.776819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:59:25.776833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:59:25.931452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:59:25.931479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:59:25.931484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:59:25.931665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26
All frames and annotations from 1 datasets have been read by 2019-01-27 17:59:27.765131
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:59:36.674876!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.0744
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 17:59:54.241822
1. set (Dataset 9) being trained for epoch 2 by 2019-01-27 18:00:03.102253!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0643
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 18:00:20.122391
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.99 Degree
        The absolute mean error on Yaw angle estimation: 5.12 Degree
        The absolute mean error on Roll angle estimation: 2.68 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.99 Degree
        The absolute mean error on Yaw angle estimations: 5.12 Degree
        The absolute mean error on Roll angle estimations: 2.68 Degree
subject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26.png has been saved by 2019-01-27 18:00:37.610534.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26 has been evaluated successfully.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "keep t
anh"
[master 68e0104] keep tanh
 70 files changed, 3673 insertions(+), 2172 deletions(-)
 rename DeepRL_For_HPE/FC_RNN_Evaluater/results/{Last_Model/output_Last_Model.txt => Exp2019-01-26_04-28-49/
output_Exp2019-01-26_04-28-49.txt} (89%)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject14_Exp2019-01-26_0
4-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject3_Exp2019-01-26_04
-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject5_Exp2019-01-26_04
-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject9_Exp2019-01-26_04
-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/output_Exp2019-01-26_13-3
7-43.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject14_Exp2019-01-26_1
3-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject3_Exp2019-01-26_13
-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject5_Exp2019-01-26_13
-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject9_Exp2019-01-26_13
-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_11-59-18/output_Exp2019-01-27_11-5
9-18.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_11-59-18/subject9_Exp2019-01-27_11
-59-18.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-14-25/output_Exp2019-01-27_13-1
4-25.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-14-25/subject9_Exp2019-01-27_13
-14-25.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-25-50/output_Exp2019-01-27_13-2
5-50.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-25-50/subject9_Exp2019-01-27_13
-25-50.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-35-44/output_Exp2019-01-27_13-3
5-44.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-35-44/subject9_Exp2019-01-27_13
-35-44.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-36-52/output_Exp2019-01-27_13-3
6-52.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-36-52/subject9_Exp2019-01-27_13
-36-52.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-39-33/output_Exp2019-01-27_13-3
9-33.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-39-33/subject9_Exp2019-01-27_13
-39-33.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-41-30/output_Exp2019-01-27_13-4
1-30.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-41-30/subject9_Exp2019-01-27_13
-41-30.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-43-20/output_Exp2019-01-27_13-4
3-20.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-43-20/subject9_Exp2019-01-27_13
-43-20.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-44-42/output_Exp2019-01-27_13-4
4-42.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-44-42/subject9_Exp2019-01-27_13
-44-42.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-46-18/output_Exp2019-01-27_13-4
6-18.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-46-18/subject9_Exp2019-01-27_13
-46-18.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-54-23/output_Exp2019-01-27_13-5
4-23.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-54-23/subject9_Exp2019-01-27_13
-54-23.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-55-59/output_Exp2019-01-27_13-5
5-59.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-55-59/subject9_Exp2019-01-27_13
-55-59.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-57-56/output_Exp2019-01-27_13-5
7-56.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-57-56/subject9_Exp2019-01-27_13
-57-56.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-12-09/output_Exp2019-01-27_14-1
2-09.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-12-09/subject9_Exp2019-01-27_14
-12-09.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-22-51/output_Exp2019-01-27_14-2
2-51.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-22-51/subject9_Exp2019-01-27_14
-22-51.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-37-00/output_Exp2019-01-27_14-3
7-00.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-37-00/subject9_Exp2019-01-27_14
-37-00.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-44-55/output_Exp2019-01-27_14-4
4-55.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-44-55/subject9_Exp2019-01-27_14
-44-55.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-06-27/output_Exp2019-01-27_17-0
6-27.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-06-27/subject9_Exp2019-01-27_17
-06-27.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-11-08/output_Exp2019-01-27_17-1
1-08.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-11-08/subject9_Exp2019-01-27_17
-11-08.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-13-59/output_Exp2019-01-27_17-1
3-59.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-13-59/subject9_Exp2019-01-27_17
-13-59.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-37-52/output_Exp2019-01-27_17-3
7-52.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-37-52/subject9_Exp2019-01-27_17
-37-52.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-41-45/output_Exp2019-01-27_17-4
1-45.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-41-45/subject9_Exp2019-01-27_17
-41-45.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-44-39/output_Exp2019-01-27_17-4
4-39.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-44-39/subject9_Exp2019-01-27_17
-44-39.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12/output_Exp2019-01-27_17-4
7-12.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12/subject9_Exp2019-01-27_17
-47-12.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51/o
utput_Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51/s
ubject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26/o
utput_Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26/s
ubject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model__/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model___/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 101, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (101/101), done.
Writing objects: 100% (101/101), 4.99 MiB | 697.00 KiB/s, done.
Total 101 (delta 34), reused 0 (delta 0)
remote: Resolving deltas: 100% (34/34), completed with 8 local objects.
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   fea1f6f..68e0104  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:15:52.548327: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:15:52.646140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:15:52.646395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:15:52.646407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:15:52.801675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:15:52.801708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:15:52.801713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:15:52.801850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-15-53 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-15-53
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = record)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, p
reprocess_input = preprocess_input)
TypeError: trainCNN_LSTM() got an unexpected keyword argument 'exp'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:18:36.279976: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:18:36.377512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:18:36.377780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:18:36.377792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:18:36.533234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:18:36.533260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:18:36.533265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:18:36.533404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-18-37 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-18-37
All frames and annotations from 24 datasets have been read by 2019-01-27 18:18:43.082578
1. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:18:47.765704!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.1347
^C
Model Exp2019-01-27_18-18-37_part1 has been interrupted.
Exp2019-01-27_18-18-37_part1.h5 has been saved.
Model Exp2019-01-27_18-18-37_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "adding
 experiment count to output"
[master 7b03ced] adding experiment count to output
 6 files changed, 214 insertions(+), 16 deletions(-)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_18-18-37_part1/output_Exp2019-01-2
7_18-18-37_part1.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 14, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (11/11), done.
Writing objects: 100% (14/14), 2.33 KiB | 0 bytes/s, done.
Total 14 (delta 8), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   68e0104..7b03ced  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:21:07.912509: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:21:08.007845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:21:08.008151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:21:08.008166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:21:08.163946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:21:08.163971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:21:08.163976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:21:08.164154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-21-08 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-21-08
All frames and annotations from 20 datasets have been read by 2019-01-27 18:21:13.332003
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:21:19.740762!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.0988
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, e
xp = exp, record = record)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, p
reprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 50, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, exp = exp, record = record, preprocess_input = preprocess_in
put)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 40, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, exp = exp, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 22, in trainImageModelOnSets
    exp = ' in Experiment %d' % (exp) if exp != -1 else ''
TypeError: %d format: a number is required, not str
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:36:49.313944: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:36:49.410235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:36:49.410499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:36:49.410513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:36:49.566088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:36:49.566116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:36:49.566124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:36:49.566264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-36-50 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-36-50
All frames and annotations from 20 datasets have been read by 2019-01-27 18:36:54.600296
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:37:00.996602!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.1272
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, e
xp = exp, record = record)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, p
reprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 50, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, exp = exp, record = record, preprocess_input = preprocess_in
put)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 40, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, exp = exp, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 22, in trainImageModelOnSets
    exp = ' in Experiment %d' % (exp) if exp != -1 else ''
TypeError: %d format: a number is required, not str
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:38:17.980728: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:38:18.079080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:38:18.079343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:38:18.079357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:38:18.234477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:38:18.234503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:38:18.234511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:38:18.234659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-38-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-38-18
All frames and annotations from 20 datasets have been read by 2019-01-27 18:38:23.546454
^C
Model Exp2019-01-27_18-38-18_part1 has been interrupted.
Exp2019-01-27_18-38-18_part1.h5 has been saved.
Model Exp2019-01-27_18-38-18_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:39:24.754830: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:39:24.851524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:39:24.851826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:39:24.851841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:39:25.007701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:39:25.007727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:39:25.007732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:39:25.007914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-39-25 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-39-25
All frames and annotations from 20 datasets have been read by 2019-01-27 18:39:30.089324
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:39:36.501210!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.1046
2. set (Dataset 24) being trained for epoch 1 in Experiment  in Experiment 1 by 2019-01-27 18:39:54.679586!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0894
3. set (Dataset 15) being trained for epoch 1 in Experiment  in Experiment  in Experiment 1 by 2019-01-27 18
:40:09.892744!
Epoch 1/1
585/654 [=========================>....] - ETA: 1s - loss: 0.1114^C
Model Exp2019-01-27_18-39-25_part1 has been interrupted.
Exp2019-01-27_18-39-25_part1.h5 has been saved.
Model Exp2019-01-27_18-39-25_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:42:46.900277: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:42:46.998566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:42:46.998830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:42:46.998848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:42:47.153779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:42:47.153801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:42:47.153805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:42:47.153942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-42-47 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 18:42:52.167787
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:42:58.567546!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.1332
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:43:16.446032!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.1030
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:43:31.678422!
Epoch 1/1
654/654 [==============================] - 11s 17ms/step - loss: 0.1170
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:43:47.988977!
Epoch 1/1
502/502 [==============================] - 9s 17ms/step - loss: 0.0964
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:44:09.966421!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.1351
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:44:28.904202!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1431
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:44:45.044640!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1318
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:45:05.035664!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0905
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:45:34.811942!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.1055
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:00.325613!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0923
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:20.764274!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.1623
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:41.652583!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.1402
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:56.999856!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1333
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:47:16.391078!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.1216
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:47:38.782751!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.1247
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:47:57.759704!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.1025
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:11.897512!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0824
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:24.302934!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1393
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:39.105880!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0769
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:57.259225!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0916
Epoch 1 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 18:49:11.641579
1. set (Dataset 6) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:49:16.843642!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1243
2. set (Dataset 11) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:49:32.332765!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0815
3. set (Dataset 10) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:49:49.609311!
Epoch 1/1
726/726 [==============================] - 12s 17ms/step - loss: 0.0987
4. set (Dataset 4) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:09.565960!
Epoch 1/1
744/744 [==============================] - 13s 17ms/step - loss: 0.0966
5. set (Dataset 23) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:28.081221!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1182
6. set (Dataset 13) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:43.183428!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0649
7. set (Dataset 17) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:55.520042!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0757
8. set (Dataset 1) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:51:07.798232!
Epoch 1/1
498/498 [==============================] - 9s 17ms/step - loss: 0.1162
9. set (Dataset 8) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:51:24.291008!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0975
10. set (Dataset 7) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:51:45.440368!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0630
11. set (Dataset 21) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:04.910538!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1129
12. set (Dataset 22) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:22.621362!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0575
13. set (Dataset 2) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:39.854964!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.1026
14. set (Dataset 24) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:53.871213!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0714
15. set (Dataset 15) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:08.998806!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0879
16. set (Dataset 20) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:26.052559!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0688
17. set (Dataset 18) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:42.068783!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1124
18. set (Dataset 19) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:57.886035!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0907
19. set (Dataset 12) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:54:14.117165!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0760
20. set (Dataset 16) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:54:36.113482!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0702
Epoch 2 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 18:54:57.024012
1. set (Dataset 19) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:01.904439!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0744
2. set (Dataset 16) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:19.604927!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0600
3. set (Dataset 21) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:42.182104!
Epoch 1/1
634/634 [==============================] - 11s 17ms/step - loss: 0.1023
4. set (Dataset 15) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:59.596651!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0770
5. set (Dataset 13) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:56:16.487845!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0635
6. set (Dataset 12) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:56:32.477186!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0669
7. set (Dataset 18) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:56:51.712415!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0923
8. set (Dataset 22) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:57:09.303466!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0501
9. set (Dataset 23) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:57:26.623977!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0977
10. set (Dataset 8) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:57:44.512881!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0903
11. set (Dataset 17) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:02.272823!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0711
12. set (Dataset 6) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:14.621591!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1086
13. set (Dataset 24) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:29.121195!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0588
14. set (Dataset 11) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:43.409428!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0676
15. set (Dataset 10) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:00.701733!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0780
16. set (Dataset 20) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:18.981708!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0635
17. set (Dataset 2) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:34.087527!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0843
18. set (Dataset 4) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:50.615252!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0904
19. set (Dataset 7) being trained for epoch 3 in Experiment 1 by 2019-01-27 19:00:11.650108!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0574
20. set (Dataset 1) being trained for epoch 3 in Experiment 1 by 2019-01-27 19:00:30.196131!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0908
Epoch 3 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:00:43.906526
1. set (Dataset 4) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:00:51.292632!
Epoch 1/1
744/744 [==============================] - 13s 17ms/step - loss: 0.0823
2. set (Dataset 1) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:09.220723!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0817
3. set (Dataset 17) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:21.795295!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.0696
4. set (Dataset 10) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:36.474761!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0665
5. set (Dataset 12) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:56.701255!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0626
6. set (Dataset 7) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:02:17.237521!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0487
7. set (Dataset 2) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:02:36.093606!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0725
8. set (Dataset 6) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:02:50.623866!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0963
9. set (Dataset 13) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:05.486486!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0602
10. set (Dataset 23) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:19.718768!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0938
11. set (Dataset 18) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:35.988681!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0930
12. set (Dataset 19) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:51.910868!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0711
13. set (Dataset 11) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:04:06.815106!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0594
14. set (Dataset 16) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:04:25.667899!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0643
15. set (Dataset 21) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:04:47.867466!
Epoch 1/1
634/634 [==============================] - 11s 17ms/step - loss: 0.1013
16. set (Dataset 20) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:04.400685!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0604
17. set (Dataset 24) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:19.161419!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0536
18. set (Dataset 15) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:34.469924!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0665
19. set (Dataset 8) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:53.785055!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0807
20. set (Dataset 22) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:06:13.877914!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0522
Epoch 4 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:06:30.065417
1. set (Dataset 15) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:06:36.424523!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0591
2. set (Dataset 22) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:06:54.411195!
Epoch 1/1
665/665 [==============================] - 12s 17ms/step - loss: 0.0525
3. set (Dataset 18) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:07:11.864472!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0834
4. set (Dataset 21) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:07:28.807315!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0908
5. set (Dataset 7) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:07:47.877576!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0462
6. set (Dataset 8) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:09.394655!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0667
7. set (Dataset 24) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:28.237416!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0523
8. set (Dataset 19) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:41.859859!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0664
9. set (Dataset 12) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:58.221870!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0637
10. set (Dataset 13) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:09:16.402229!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0589
11. set (Dataset 2) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:09:30.330811!
Epoch 1/1
511/511 [==============================] - 9s 17ms/step - loss: 0.0660
12. set (Dataset 4) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:09:46.495917!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0799
13. set (Dataset 16) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:08.533742!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0567
14. set (Dataset 1) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:29.877989!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0775
15. set (Dataset 17) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:42.680985!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0704
16. set (Dataset 20) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:54.988202!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0580
17. set (Dataset 11) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:11:10.829872!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0492
18. set (Dataset 10) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:11:28.531363!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0584
19. set (Dataset 23) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:11:47.079723!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0916
20. set (Dataset 6) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:12:02.366878!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0908
Epoch 5 for Experiment 1 completed!
Exp2019-01-27_18-42-47_part1.h5 has been saved.
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24,
'M14'), (19, 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'),
 (20, 'M12'), (11, 'M05'), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-27_18-42-47
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 19:12:14.444907
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.69 Degree
        The absolute mean error on Yaw angle estimation: 29.11 Degree
        The absolute mean error on Roll angle estimation: 10.60 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 16.35 Degree
        The absolute mean error on Yaw angle estimation: 28.69 Degree
        The absolute mean error on Roll angle estimation: 4.37 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.51 Degree
        The absolute mean error on Yaw angle estimation: 27.13 Degree
        The absolute mean error on Roll angle estimation: 6.82 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 14.23 Degree
        The absolute mean error on Yaw angle estimation: 26.67 Degree
        The absolute mean error on Roll angle estimation: 13.22 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.95 Degree
        The absolute mean error on Yaw angle estimations: 27.90 Degree
        The absolute mean error on Roll angle estimations: 8.75 Degree
Exp2019-01-27_18-42-47_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 19:13:45.881361
1. set (Dataset 10) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:13:53.105883!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0515
2. set (Dataset 6) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:11.232820!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0794
3. set (Dataset 2) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:26.018407!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0633
4. set (Dataset 17) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:38.917258!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0665
5. set (Dataset 8) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:53.816884!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0662
6. set (Dataset 23) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:15:13.404074!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0859
7. set (Dataset 11) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:15:29.481044!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0440
8. set (Dataset 4) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:15:46.996451!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0761
9. set (Dataset 7) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:16:08.061893!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0497
10. set (Dataset 12) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:16:28.893406!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0558
11. set (Dataset 24) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:16:47.101295!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0529
12. set (Dataset 15) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:02.157010!
Epoch 1/1
654/654 [==============================] - 11s 17ms/step - loss: 0.0630
13. set (Dataset 1) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:18.628263!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0766
14. set (Dataset 22) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:34.136817!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0493
15. set (Dataset 18) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:51.912838!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0826
16. set (Dataset 20) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:18:08.350942!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0552
17. set (Dataset 16) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:18:27.260683!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0558
18. set (Dataset 21) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:18:49.696981!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0912
19. set (Dataset 13) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:19:05.785579!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0575
20. set (Dataset 19) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:19:19.311789!
Epoch 1/1
502/502 [==============================] - 9s 17ms/step - loss: 0.0695
Epoch 1 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:19:32.436378
1. set (Dataset 21) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:19:38.461040!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0894
2. set (Dataset 19) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:19:54.845932!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0644
3. set (Dataset 24) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:08.663520!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0496
4. set (Dataset 18) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:23.375669!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0764
5. set (Dataset 23) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:39.674408!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0852
6. set (Dataset 13) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:54.754717!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0561
7. set (Dataset 16) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:21:12.347048!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0519
8. set (Dataset 15) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:21:35.495497!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0623
9. set (Dataset 8) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:21:55.127988!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0654
10. set (Dataset 7) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:22:16.433160!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0499
11. set (Dataset 11) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:22:35.514674!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0438
12. set (Dataset 10) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:22:53.199325!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0512
13. set (Dataset 22) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:12.349267!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0480
14. set (Dataset 6) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:29.528190!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0818
15. set (Dataset 2) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:44.260465!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0630
16. set (Dataset 20) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:58.968805!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0546
17. set (Dataset 1) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:24:13.936020!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0700
18. set (Dataset 17) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:24:26.725974!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0668
19. set (Dataset 12) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:24:41.178591!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0543
20. set (Dataset 4) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:25:01.567008!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0776
Epoch 2 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:25:19.202612
1. set (Dataset 17) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:25:22.957722!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0647
2. set (Dataset 4) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:25:37.684372!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0706
3. set (Dataset 11) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:25:57.095653!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0420
4. set (Dataset 2) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:26:12.445397!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0591
5. set (Dataset 13) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:26:26.658676!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0554
6. set (Dataset 12) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:26:42.694142!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0522
7. set (Dataset 1) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:01.165068!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0634
8. set (Dataset 10) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:17.541401!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0515
9. set (Dataset 23) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:36.211270!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0862
10. set (Dataset 8) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:54.364591!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0637
11. set (Dataset 16) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:28:17.221706!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0554
12. set (Dataset 21) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:28:39.842309!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0845
13. set (Dataset 6) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:28:56.693202!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0806
14. set (Dataset 19) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:11.488908!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0634
15. set (Dataset 24) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:25.224984!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0515
16. set (Dataset 20) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:39.422913!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0500
17. set (Dataset 22) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:55.895098!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0489
18. set (Dataset 18) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:30:13.784069!
Epoch 1/1
614/614 [==============================] - 11s 17ms/step - loss: 0.0733
19. set (Dataset 7) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:30:32.050765!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0507
20. set (Dataset 15) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:30:52.081075!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0611
Epoch 3 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:31:08.185533
1. set (Dataset 18) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:31:14.067362!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0689
2. set (Dataset 15) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:31:31.660767!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0579
3. set (Dataset 16) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:31:52.515029!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0528
4. set (Dataset 24) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:32:13.452763!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0482
5. set (Dataset 12) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:32:29.640838!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0500
6. set (Dataset 7) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:32:50.356766!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0447
7. set (Dataset 22) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:10.151733!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0452
8. set (Dataset 21) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:27.908699!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0838
9. set (Dataset 13) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:44.262728!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0562
10. set (Dataset 23) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:58.745057!
Epoch 1/1
569/569 [==============================] - 10s 17ms/step - loss: 0.0823
11. set (Dataset 1) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:13.668643!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0615
12. set (Dataset 17) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:26.262834!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0596
13. set (Dataset 19) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:38.325985!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0669
14. set (Dataset 4) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:54.786776!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0718
15. set (Dataset 11) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:14.019041!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0448
16. set (Dataset 20) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:29.774079!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0518
17. set (Dataset 6) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:45.025572!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0772
18. set (Dataset 2) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:59.940590!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0578
19. set (Dataset 8) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:36:16.975018!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0641
20. set (Dataset 10) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:36:37.983661!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0497
Epoch 4 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:36:55.087002
1. set (Dataset 2) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:00.175476!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0534
2. set (Dataset 10) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:16.571052!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0472
3. set (Dataset 1) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:34.387931!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0606
4. set (Dataset 11) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:49.327006!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0385
5. set (Dataset 7) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:38:07.289313!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0463
6. set (Dataset 8) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:38:28.359984!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0577
7. set (Dataset 6) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:38:47.577598!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0746
8. set (Dataset 17) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:01.233167!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0620
9. set (Dataset 12) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:15.800441!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0492
10. set (Dataset 13) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:34.032954!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0549
11. set (Dataset 22) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:49.140482!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0453
12. set (Dataset 18) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:40:06.950934!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0731
13. set (Dataset 4) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:40:25.611824!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0684
14. set (Dataset 15) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:40:45.387282!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0600
15. set (Dataset 16) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:05.911249!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0545
16. set (Dataset 20) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:27.543195!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0522
17. set (Dataset 19) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:42.451760!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0629
18. set (Dataset 24) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:56.228560!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0485
19. set (Dataset 23) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:42:10.292714!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0821
20. set (Dataset 21) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:42:26.813089!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0852
Epoch 5 for Experiment 2 completed!
Exp2019-01-27_18-42-47_part2.h5 has been saved.
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F0
6'), (17, 'M10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'),
(20, 'M12'), (19, 'M11'), (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-27_18-42-47
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 19:42:40.488846
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 14.47 Degree
        The absolute mean error on Yaw angle estimation: 26.24 Degree
        The absolute mean error on Roll angle estimation: 15.04 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.47 Degree
        The absolute mean error on Yaw angle estimation: 26.70 Degree
        The absolute mean error on Roll angle estimation: 4.47 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 30.05 Degree
        The absolute mean error on Yaw angle estimation: 28.99 Degree
        The absolute mean error on Roll angle estimation: 9.91 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 13.73 Degree
        The absolute mean error on Yaw angle estimation: 29.61 Degree
        The absolute mean error on Roll angle estimation: 13.48 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.18 Degree
        The absolute mean error on Yaw angle estimations: 27.89 Degree
        The absolute mean error on Roll angle estimations: 10.72 Degree
Exp2019-01-27_18-42-47_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 19:43:50.425386
1. set (Dataset 24) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:43:55.096427!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0478
2. set (Dataset 21) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:44:10.022010!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0777
3. set (Dataset 22) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:44:27.924823!
Epoch 1/1
665/665 [==============================] - 12s 17ms/step - loss: 0.0424
4. set (Dataset 16) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:44:48.306731!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0502
5. set (Dataset 8) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:45:12.700088!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.0618
6. set (Dataset 23) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:45:31.680416!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0782
7. set (Dataset 19) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:45:46.840017!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0628
8. set (Dataset 18) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:01.666710!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0685
9. set (Dataset 7) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:20.535815!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0447
10. set (Dataset 12) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:40.802422!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0478
11. set (Dataset 6) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:59.304550!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0766
12. set (Dataset 2) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:47:14.135651!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0566
13. set (Dataset 15) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:47:29.506956!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0570
14. set (Dataset 10) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:47:48.436347!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0481
15. set (Dataset 1) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:06.394290!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0608
16. set (Dataset 20) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:21.139462!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0514
17. set (Dataset 4) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:38.545496!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0633
18. set (Dataset 11) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:57.493891!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0441
19. set (Dataset 13) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:49:12.346897!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0563
20. set (Dataset 17) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:49:25.081186!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0589
Epoch 1 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:49:36.584927
1. set (Dataset 11) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:49:42.289848!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0387
2. set (Dataset 17) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:49:56.407994!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0579
3. set (Dataset 6) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:08.529818!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0742
4. set (Dataset 1) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:23.311135!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0644
5. set (Dataset 23) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:37.607357!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0774
6. set (Dataset 13) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:52.611932!
Epoch 1/1
485/485 [==============================] - 8s 17ms/step - loss: 0.0546
7. set (Dataset 4) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:51:08.469662!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0563
8. set (Dataset 2) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:51:27.223317!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0585
9. set (Dataset 8) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:51:44.445292!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0624
10. set (Dataset 7) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:06.177440!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0458
11. set (Dataset 19) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:24.295945!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0649
12. set (Dataset 24) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:38.025858!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0520
13. set (Dataset 10) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:54.182522!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0460
14. set (Dataset 21) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:53:13.065940!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0802
15. set (Dataset 22) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:53:30.978319!
Epoch 1/1
665/665 [==============================] - 12s 17ms/step - loss: 0.0418
16. set (Dataset 20) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:53:48.129821!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0492
17. set (Dataset 15) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:54:04.374761!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0593
18. set (Dataset 16) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:54:24.798719!
Epoch 1/1
914/914 [==============================] - 16s 17ms/step - loss: 0.0506
19. set (Dataset 12) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:54:47.871470!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0518
20. set (Dataset 18) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:55:06.784013!
Epoch 1/1
614/614 [==============================] - 10s 17ms/step - loss: 0.0681
Epoch 2 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:55:21.594316
1. set (Dataset 16) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:55:30.326945!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0472
2. set (Dataset 18) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:55:52.580136!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0645
3. set (Dataset 19) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:08.651444!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0605
4. set (Dataset 22) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:24.298028!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0435
5. set (Dataset 13) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:41.321855!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0548
6. set (Dataset 12) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:57.329044!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0460
7. set (Dataset 15) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:57:17.146784!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0573
8. set (Dataset 24) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:57:33.759377!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0486
9. set (Dataset 23) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:57:48.195378!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0792
10. set (Dataset 8) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:58:06.187953!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0603
11. set (Dataset 4) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:58:27.664460!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0601
12. set (Dataset 11) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:58:46.772628!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0427
13. set (Dataset 21) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:02.915899!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0811
14. set (Dataset 17) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:17.838180!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0608
15. set (Dataset 6) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:30.103961!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0735
16. set (Dataset 20) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:45.124216!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0504
17. set (Dataset 10) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:02.223293!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0485
18. set (Dataset 1) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:20.004548!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0625
19. set (Dataset 7) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:36.371809!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0469
20. set (Dataset 2) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:54.505796!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0580
Epoch 3 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:01:08.009750
1. set (Dataset 1) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:01:13.060101!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0585
2. set (Dataset 2) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:01:27.218336!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0492
3. set (Dataset 4) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:01:43.877738!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0583
4. set (Dataset 6) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:02.395349!
Epoch 1/1
542/542 [==============================] - 9s 17ms/step - loss: 0.0707
5. set (Dataset 12) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:19.140511!
Epoch 1/1
732/732 [==============================] - 13s 17ms/step - loss: 0.0489
6. set (Dataset 7) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:39.518390!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0454
7. set (Dataset 10) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:59.870638!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0471
8. set (Dataset 11) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:03:18.449754!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0397
9. set (Dataset 13) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:03:33.736004!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0529
10. set (Dataset 23) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:03:48.067782!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0742
11. set (Dataset 15) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:04.755729!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0604
12. set (Dataset 16) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:25.225887!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0495
13. set (Dataset 17) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:45.315949!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0586
14. set (Dataset 18) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:58.186891!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0647
15. set (Dataset 19) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:05:14.124219!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0625
16. set (Dataset 20) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:05:28.723316!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0504
17. set (Dataset 21) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:05:45.174951!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0803
18. set (Dataset 22) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:06:03.049500!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0423
19. set (Dataset 8) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:06:22.553963!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0605
20. set (Dataset 24) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:06:41.050421!
Epoch 1/1
492/492 [==============================] - 9s 19ms/step - loss: 0.0502
Epoch 4 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:06:54.715849
1. set (Dataset 22) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:01.109039!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0396
2. set (Dataset 24) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:17.505229!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0461
3. set (Dataset 15) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:32.809575!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0564
4. set (Dataset 19) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:49.916013!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0617
5. set (Dataset 7) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:08:06.670525!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0447
6. set (Dataset 8) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:08:27.687181!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0591
7. set (Dataset 21) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:08:47.637308!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0774
8. set (Dataset 16) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:09:07.855799!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0489
9. set (Dataset 12) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:09:31.685479!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0487
10. set (Dataset 13) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:09:49.699038!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0522
11. set (Dataset 10) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:05.729591!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0475
12. set (Dataset 1) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:24.075266!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0629
13. set (Dataset 18) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:38.977272!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0652
14. set (Dataset 2) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:54.967810!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0581
15. set (Dataset 4) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:11.612821!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0569
16. set (Dataset 20) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:30.109813!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0537
17. set (Dataset 17) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:43.802444!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0560
18. set (Dataset 6) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:56.252467!
Epoch 1/1
542/542 [==============================] - 9s 17ms/step - loss: 0.0721
19. set (Dataset 23) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:12:11.214010!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0778
20. set (Dataset 11) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:12:27.428342!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0424
Epoch 5 for Experiment 3 completed!
Exp2019-01-27_18-42-47_part3.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21,
'F02'), (16, 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-27_18-42-47
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 20:12:40.011331
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 12.71 Degree
        The absolute mean error on Yaw angle estimation: 39.63 Degree
        The absolute mean error on Roll angle estimation: 15.27 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.30 Degree
        The absolute mean error on Yaw angle estimation: 23.38 Degree
        The absolute mean error on Roll angle estimation: 3.83 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 29.03 Degree
        The absolute mean error on Yaw angle estimation: 24.58 Degree
        The absolute mean error on Roll angle estimation: 12.18 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.08 Degree
        The absolute mean error on Yaw angle estimation: 27.01 Degree
        The absolute mean error on Roll angle estimation: 12.92 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.53 Degree
        The absolute mean error on Yaw angle estimations: 28.65 Degree
        The absolute mean error on Roll angle estimations: 11.05 Degree
Exp2019-01-27_18-42-47_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 20:13:49.742692
1. set (Dataset 6) being trained for epoch 1 in Experiment 4 by 2019-01-27 20:13:54.931341!
Epoch 1/1
533/542 [============================>.] - ETA: 0s - loss: 0.0684^C
Model Exp2019-01-27_18-42-47_part4 has been interrupted.
Exp2019-01-27_18-42-47_part4.h5 has been saved.
Model Exp2019-01-27_18-42-47_part4 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_18-42-47_part4
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 20:15:02.889499: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 20:15:02.987129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 20:15:02.987433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 20:15:02.987446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 20:15:03.143127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 20:15:03.143154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 20:15:03.143158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 20:15:03.143337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_18-42-47_part4.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04
'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'),
(20, 'M12'), (21, 'F02'), (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-27_18-42-47_part4_and_2019-01-27_20-15-04
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 20:15:06.137359
For the Subject 3 (F03):
730/730 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 14.14 Degree
        The absolute mean error on Yaw angle estimation: 42.17 Degree
        The absolute mean error on Roll angle estimation: 16.91 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.27 Degree
        The absolute mean error on Yaw angle estimation: 25.17 Degree
        The absolute mean error on Roll angle estimation: 4.36 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 27.32 Degree
        The absolute mean error on Yaw angle estimation: 25.95 Degree
        The absolute mean error on Roll angle estimation: 12.79 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 19.02 Degree
        The absolute mean error on Yaw angle estimation: 27.32 Degree
        The absolute mean error on Roll angle estimation: 12.62 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.69 Degree
        The absolute mean error on Yaw angle estimations: 30.15 Degree
        The absolute mean error on Roll angle estimations: 11.67 Degree
subject3_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.239660.
subject5_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.438394.
subject9_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.637757.
subject14_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.855475.
Model Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04 has been evaluated successfully.
Model Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_18-42-47_part4 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 20:16:37.328762: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 20:16:37.426327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 20:16:37.426634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 20:16:37.426648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 20:16:37.582479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 20:16:37.582506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 20:16:37.582510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 20:16:37.582691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_18-42-47_part4.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model Exp2019-01-27_18-42-47_part4_and_2019-01-27_20-16-38
All frames and annotations from 20 datasets have been read by 2019-01-27 20:16:42.978898
num_ou1. set (Dataset 22) being trained for epoch 1 by 2019-01-27 20:16:49.379813!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.0427
2. set (Dataset 24) being trained for epoch 1 by 2019-01-27 20:17:07.191014!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0462
3. set (Dataset 15) being trained for epoch 1 by 2019-01-27 20:17:22.511085!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0568
4. set (Dataset 19) being trained for epoch 1 by 2019-01-27 20:17:39.144240!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0626
5. set (Dataset 8) being trained for epoch 1 by 2019-01-27 20:17:55.810600!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.0616
6. set (Dataset 23) being trained for epoch 1 by 2019-01-27 20:18:14.734283!
Epoch 1/1
569/569 [==============================] - 465s 817ms/step - loss: 0.0763
7. set (Dataset 21) being trained for epoch 1 by 2019-01-27 20:26:05.906729!
Epoch 1/1
421/634 [==================>...........] - ETA: 3s - loss: 0.0760
