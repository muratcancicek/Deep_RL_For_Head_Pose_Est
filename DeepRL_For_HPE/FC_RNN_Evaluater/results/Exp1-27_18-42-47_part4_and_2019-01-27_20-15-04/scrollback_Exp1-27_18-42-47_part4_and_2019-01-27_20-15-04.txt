mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-26 13:37:42.044613: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-26 13:37:42.141623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-26 13:37:42.141885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-26 13:37:42.141897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-26 13:37:42.297498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-26 13:37:42.297525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-26 13:37:42.297529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-26 13:37:42.297666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-26_13-37-43 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 13:37:47.496164
1. set (Dataset 22) being trained for epoch 1 by 2019-01-26 13:37:53.897956!
Epoch 1/1
665/665 [==============================] - 18s 27ms/step - loss: 0.2134
2. set (Dataset 24) being trained for epoch 1 by 2019-01-26 13:38:17.428122!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.1537
3. set (Dataset 15) being trained for epoch 1 by 2019-01-26 13:38:35.826012!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.1627
4. set (Dataset 19) being trained for epoch 1 by 2019-01-26 13:38:57.157694!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1557
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 13:39:17.582164!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.1922
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 13:39:41.965695!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1938
7. set (Dataset 21) being trained for epoch 1 by 2019-01-26 13:40:02.478005!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1931
8. set (Dataset 16) being trained for epoch 1 by 2019-01-26 13:40:27.363862!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.1485
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 13:40:58.079955!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.1751
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 13:41:24.479678!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.1414
11. set (Dataset 10) being trained for epoch 1 by 2019-01-26 13:41:50.880939!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.1474
12. set (Dataset 1) being trained for epoch 1 by 2019-01-26 13:42:13.757991!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1588
13. set (Dataset 18) being trained for epoch 1 by 2019-01-26 13:42:31.935777!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.1739
14. set (Dataset 2) being trained for epoch 1 by 2019-01-26 13:42:52.737112!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1646
15. set (Dataset 4) being trained for epoch 1 by 2019-01-26 13:43:12.947118!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1508
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 13:43:37.021597!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.1290
17. set (Dataset 17) being trained for epoch 1 by 2019-01-26 13:43:54.862511!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.1054
18. set (Dataset 6) being trained for epoch 1 by 2019-01-26 13:44:09.850323!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1716
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 13:44:28.180793!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.1019
20. set (Dataset 11) being trained for epoch 1 by 2019-01-26 13:44:45.863761!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.1089
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 13:45:04.521469
1. set (Dataset 6) being trained for epoch 2 by 2019-01-26 13:45:09.706344!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1545
2. set (Dataset 11) being trained for epoch 2 by 2019-01-26 13:45:28.570247!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0893
3. set (Dataset 10) being trained for epoch 2 by 2019-01-26 13:45:50.489836!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.1188
4. set (Dataset 4) being trained for epoch 2 by 2019-01-26 13:46:16.252836!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.1267
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 13:46:40.262662!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1590
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 13:46:59.524336!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0916
7. set (Dataset 17) being trained for epoch 2 by 2019-01-26 13:47:15.329745!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.1010
8. set (Dataset 1) being trained for epoch 2 by 2019-01-26 13:47:30.105209!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1335
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 13:47:50.052174!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.1162
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 13:48:17.559026!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.1081
11. set (Dataset 21) being trained for epoch 2 by 2019-01-26 13:48:42.280547!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1569
12. set (Dataset 22) being trained for epoch 2 by 2019-01-26 13:49:05.128362!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0871
13. set (Dataset 2) being trained for epoch 2 by 2019-01-26 13:49:27.366885!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.1146
14. set (Dataset 24) being trained for epoch 2 by 2019-01-26 13:49:45.027558!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0905
15. set (Dataset 15) being trained for epoch 2 by 2019-01-26 13:50:03.433363!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.1117
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 13:50:25.737346!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0934
17. set (Dataset 18) being trained for epoch 2 by 2019-01-26 13:50:45.510993!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1411
18. set (Dataset 19) being trained for epoch 2 by 2019-01-26 13:51:05.647126!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.1030
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 13:51:25.720885!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.1043
20. set (Dataset 16) being trained for epoch 2 by 2019-01-26 13:51:53.325940!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0994
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 13:52:20.929231
1. set (Dataset 19) being trained for epoch 3 by 2019-01-26 13:52:25.806612!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0918
2. set (Dataset 16) being trained for epoch 3 by 2019-01-26 13:52:47.585047!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0907
3. set (Dataset 21) being trained for epoch 3 by 2019-01-26 13:53:16.953899!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1418
4. set (Dataset 15) being trained for epoch 3 by 2019-01-26 13:53:39.021422!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.1024
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 13:54:01.136997!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0883
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 13:54:20.773588!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0873
7. set (Dataset 18) being trained for epoch 3 by 2019-01-26 13:54:44.983117!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1255
8. set (Dataset 22) being trained for epoch 3 by 2019-01-26 13:55:06.658491!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0741
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 13:55:29.571770!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1308
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 13:55:51.945928!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.1072
11. set (Dataset 17) being trained for epoch 3 by 2019-01-26 13:56:14.805050!
Epoch 1/1
395/395 [==============================] - 11s 27ms/step - loss: 0.0876
12. set (Dataset 6) being trained for epoch 3 by 2019-01-26 13:56:30.578744!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.1422
13. set (Dataset 24) being trained for epoch 3 by 2019-01-26 13:56:48.763587!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0733
14. set (Dataset 11) being trained for epoch 3 by 2019-01-26 13:57:06.880216!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0777
15. set (Dataset 10) being trained for epoch 3 by 2019-01-26 13:57:28.530031!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0960
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 13:57:51.998912!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0825
17. set (Dataset 2) being trained for epoch 3 by 2019-01-26 13:58:10.875157!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.1068
18. set (Dataset 4) being trained for epoch 3 by 2019-01-26 13:58:30.760695!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.1154
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 13:58:57.795830!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0960
20. set (Dataset 1) being trained for epoch 3 by 2019-01-26 13:59:22.097381!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.1149
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 13:59:38.836575
1. set (Dataset 4) being trained for epoch 4 by 2019-01-26 13:59:46.216868!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.1029
2. set (Dataset 1) being trained for epoch 4 by 2019-01-26 14:00:10.228407!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.1006
3. set (Dataset 17) being trained for epoch 4 by 2019-01-26 14:00:26.117745!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0861
4. set (Dataset 10) being trained for epoch 4 by 2019-01-26 14:00:43.553951!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0870
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 14:01:08.957133!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0800
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 14:01:35.209997!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0923
7. set (Dataset 2) being trained for epoch 4 by 2019-01-26 14:01:58.979624!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0928
8. set (Dataset 6) being trained for epoch 4 by 2019-01-26 14:02:17.108006!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1296
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 14:02:36.248809!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0765
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 14:02:53.633478!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1278
11. set (Dataset 18) being trained for epoch 4 by 2019-01-26 14:03:13.834329!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1228
12. set (Dataset 19) being trained for epoch 4 by 2019-01-26 14:03:34.280314!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0917
13. set (Dataset 11) being trained for epoch 4 by 2019-01-26 14:03:52.973101!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0704
14. set (Dataset 16) being trained for epoch 4 by 2019-01-26 14:04:16.136605!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0918
15. set (Dataset 21) being trained for epoch 4 by 2019-01-26 14:04:45.354851!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1347
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 14:05:06.858769!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0763
17. set (Dataset 24) being trained for epoch 4 by 2019-01-26 14:05:25.821704!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0739
18. set (Dataset 15) being trained for epoch 4 by 2019-01-26 14:05:44.538353!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0889
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 14:06:08.338253!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0963
20. set (Dataset 22) being trained for epoch 4 by 2019-01-26 14:06:34.493181!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0659
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:06:56.001676
1. set (Dataset 15) being trained for epoch 5 by 2019-01-26 14:07:02.350262!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0907
2. set (Dataset 22) being trained for epoch 5 by 2019-01-26 14:07:24.954878!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0629
3. set (Dataset 18) being trained for epoch 5 by 2019-01-26 14:07:47.222554!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.1152
4. set (Dataset 21) being trained for epoch 5 by 2019-01-26 14:08:08.506640!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1261
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 14:08:32.683875!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0944
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 14:08:59.336003!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0897
7. set (Dataset 24) being trained for epoch 5 by 2019-01-26 14:09:23.357499!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0710
8. set (Dataset 19) being trained for epoch 5 by 2019-01-26 14:09:40.108793!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0900
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 14:09:59.885154!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0820
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 14:10:22.924358!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0757
11. set (Dataset 2) being trained for epoch 5 by 2019-01-26 14:10:39.682459!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0959
12. set (Dataset 4) being trained for epoch 5 by 2019-01-26 14:10:59.791402!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0988
13. set (Dataset 16) being trained for epoch 5 by 2019-01-26 14:11:27.180528!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0876
14. set (Dataset 1) being trained for epoch 5 by 2019-01-26 14:11:55.313939!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.1049
15. set (Dataset 17) being trained for epoch 5 by 2019-01-26 14:12:11.944553!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0873
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 14:12:27.214537!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0772
17. set (Dataset 11) being trained for epoch 5 by 2019-01-26 14:12:46.572954!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0681
18. set (Dataset 10) being trained for epoch 5 by 2019-01-26 14:13:08.055256!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0824
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 14:13:32.359488!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1233
20. set (Dataset 6) being trained for epoch 5 by 2019-01-26 14:13:52.056260!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.1204
Epoch 5 completed!
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24,
'M14'), (19, 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'),
 (20, 'M12'), (11, 'M05'), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 14:14:07.707520
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.43 Degree
        The absolute mean error on Yaw angle estimation: 32.58 Degree
        The absolute mean error on Roll angle estimation: 14.18 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.77 Degree
        The absolute mean error on Yaw angle estimation: 28.26 Degree
        The absolute mean error on Roll angle estimation: 3.76 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 25.82 Degree
        The absolute mean error on Yaw angle estimation: 27.21 Degree
        The absolute mean error on Roll angle estimation: 8.53 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 15.19 Degree
        The absolute mean error on Yaw angle estimation: 26.62 Degree
        The absolute mean error on Roll angle estimation: 13.37 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.80 Degree
        The absolute mean error on Yaw angle estimations: 28.67 Degree
        The absolute mean error on Roll angle estimations: 9.96 Degree
Exp2019-01-26_13-37-43_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 14:15:36.631298
1. set (Dataset 10) being trained for epoch 1 by 2019-01-26 14:15:43.894822!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0806
2. set (Dataset 6) being trained for epoch 1 by 2019-01-26 14:16:07.691099!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.1075
3. set (Dataset 2) being trained for epoch 1 by 2019-01-26 14:16:26.981420!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0848
4. set (Dataset 17) being trained for epoch 1 by 2019-01-26 14:16:43.496499!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0839
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 14:17:01.099752!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0848
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 14:17:25.378604!
Epoch 1/1
569/569 [==============================] - 15s 25ms/step - loss: 0.1156
7. set (Dataset 11) being trained for epoch 1 by 2019-01-26 14:17:45.669515!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0668
8. set (Dataset 4) being trained for epoch 1 by 2019-01-26 14:18:07.459276!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0945
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 14:18:33.578293!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0850
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 14:18:59.921008!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0698
11. set (Dataset 24) being trained for epoch 1 by 2019-01-26 14:19:22.859512!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0709
12. set (Dataset 15) being trained for epoch 1 by 2019-01-26 14:19:41.322784!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0905
13. set (Dataset 1) being trained for epoch 1 by 2019-01-26 14:20:02.815468!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0977
14. set (Dataset 22) being trained for epoch 1 by 2019-01-26 14:20:21.977093!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0649
15. set (Dataset 18) being trained for epoch 1 by 2019-01-26 14:20:44.434148!
Epoch 1/1
614/614 [==============================] - 16s 27ms/step - loss: 0.1079
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 14:21:06.282758!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0746
17. set (Dataset 16) being trained for epoch 1 by 2019-01-26 14:21:29.064175!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0826
18. set (Dataset 21) being trained for epoch 1 by 2019-01-26 14:21:57.827123!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1227
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 14:22:18.219033!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0772
20. set (Dataset 19) being trained for epoch 1 by 2019-01-26 14:22:35.056914!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0848
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:22:52.561555
1. set (Dataset 21) being trained for epoch 2 by 2019-01-26 14:22:58.592694!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1177
2. set (Dataset 19) being trained for epoch 2 by 2019-01-26 14:23:19.214919!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0805
3. set (Dataset 24) being trained for epoch 2 by 2019-01-26 14:23:36.919947!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0658
4. set (Dataset 18) being trained for epoch 2 by 2019-01-26 14:23:55.483760!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0984
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 14:24:16.490951!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1094
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 14:24:35.525900!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0719
7. set (Dataset 16) being trained for epoch 2 by 2019-01-26 14:24:56.465532!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0784
8. set (Dataset 15) being trained for epoch 2 by 2019-01-26 14:25:26.515800!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0824
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 14:25:50.671417!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0854
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 14:26:17.385570!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0831
11. set (Dataset 11) being trained for epoch 2 by 2019-01-26 14:26:41.670344!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0637
12. set (Dataset 10) being trained for epoch 2 by 2019-01-26 14:27:03.043151!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0772
13. set (Dataset 22) being trained for epoch 2 by 2019-01-26 14:27:28.548327!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0610
14. set (Dataset 6) being trained for epoch 2 by 2019-01-26 14:27:50.083163!
Epoch 1/1
542/542 [==============================] - 19s 34ms/step - loss: 0.1109
15. set (Dataset 2) being trained for epoch 2 by 2019-01-26 14:28:13.914915!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0843
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 14:28:32.537184!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0704
17. set (Dataset 1) being trained for epoch 2 by 2019-01-26 14:28:51.658246!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0978
18. set (Dataset 17) being trained for epoch 2 by 2019-01-26 14:29:07.850546!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0800
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 14:29:25.241590!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0711
20. set (Dataset 4) being trained for epoch 2 by 2019-01-26 14:29:51.462921!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0922
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:30:14.554037
1. set (Dataset 17) being trained for epoch 3 by 2019-01-26 14:30:18.297763!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0801
2. set (Dataset 4) being trained for epoch 3 by 2019-01-26 14:30:35.806538!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0854
3. set (Dataset 11) being trained for epoch 3 by 2019-01-26 14:31:00.196995!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0640
4. set (Dataset 2) being trained for epoch 3 by 2019-01-26 14:31:19.830724!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0831
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 14:31:37.756635!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0701
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 14:31:57.522353!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0692
7. set (Dataset 1) being trained for epoch 3 by 2019-01-26 14:32:20.584402!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0901
8. set (Dataset 10) being trained for epoch 3 by 2019-01-26 14:32:40.230857!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0744
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 14:33:04.376025!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1157
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 14:33:27.193604!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0784
11. set (Dataset 16) being trained for epoch 3 by 2019-01-26 14:33:54.900027!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0800
12. set (Dataset 21) being trained for epoch 3 by 2019-01-26 14:34:23.670919!
Epoch 1/1
634/634 [==============================] - 15s 24ms/step - loss: 0.1205
13. set (Dataset 6) being trained for epoch 3 by 2019-01-26 14:34:44.401241!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1039
14. set (Dataset 19) being trained for epoch 3 by 2019-01-26 14:35:02.652340!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0833
15. set (Dataset 24) being trained for epoch 3 by 2019-01-26 14:35:20.053081!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0613
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 14:35:37.213640!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0671
17. set (Dataset 22) being trained for epoch 3 by 2019-01-26 14:35:57.395653!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0584
18. set (Dataset 18) being trained for epoch 3 by 2019-01-26 14:36:19.582540!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0981
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 14:36:43.209261!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0826
20. set (Dataset 15) being trained for epoch 3 by 2019-01-26 14:37:08.046709!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0827
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:37:28.835226
1. set (Dataset 18) being trained for epoch 4 by 2019-01-26 14:37:34.741326!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0962
2. set (Dataset 15) being trained for epoch 4 by 2019-01-26 14:37:56.411012!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0786
3. set (Dataset 16) being trained for epoch 4 by 2019-01-26 14:38:21.895220!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0727
4. set (Dataset 24) being trained for epoch 4 by 2019-01-26 14:38:50.204041!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0595
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 14:39:10.261066!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0717
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 14:39:36.658326!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0778
7. set (Dataset 22) being trained for epoch 4 by 2019-01-26 14:40:01.592545!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0590
8. set (Dataset 21) being trained for epoch 4 by 2019-01-26 14:40:24.038604!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1149
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 14:40:44.774689!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0690
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 14:41:02.443332!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1090
11. set (Dataset 1) being trained for epoch 4 by 2019-01-26 14:41:22.117326!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0941
12. set (Dataset 17) being trained for epoch 4 by 2019-01-26 14:41:38.320863!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0777
13. set (Dataset 19) being trained for epoch 4 by 2019-01-26 14:41:53.014440!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0800
14. set (Dataset 4) being trained for epoch 4 by 2019-01-26 14:42:13.100160!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0921
15. set (Dataset 11) being trained for epoch 4 by 2019-01-26 14:42:37.908668!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0609
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 14:42:57.846931!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0687
17. set (Dataset 6) being trained for epoch 4 by 2019-01-26 14:43:17.306900!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0986
18. set (Dataset 2) being trained for epoch 4 by 2019-01-26 14:43:36.298587!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0835
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 14:43:56.903694!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0804
20. set (Dataset 10) being trained for epoch 4 by 2019-01-26 14:44:23.278208!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0745
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 14:44:45.259911
1. set (Dataset 2) being trained for epoch 5 by 2019-01-26 14:44:50.346808!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0803
2. set (Dataset 10) being trained for epoch 5 by 2019-01-26 14:45:10.257256!
Epoch 1/1
726/726 [==============================] - 18s 24ms/step - loss: 0.0710
3. set (Dataset 1) being trained for epoch 5 by 2019-01-26 14:45:33.129980!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0863
4. set (Dataset 11) being trained for epoch 5 by 2019-01-26 14:45:51.535776!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0590
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 14:46:13.206475!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0731
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 14:46:39.971290!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0760
7. set (Dataset 6) being trained for epoch 5 by 2019-01-26 14:47:05.145133!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.1072
8. set (Dataset 17) being trained for epoch 5 by 2019-01-26 14:47:22.073022!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0794
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 14:47:39.193066!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0648
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 14:48:02.522140!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0632
11. set (Dataset 22) being trained for epoch 5 by 2019-01-26 14:48:21.298966!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0586
12. set (Dataset 18) being trained for epoch 5 by 2019-01-26 14:48:44.434982!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0992
13. set (Dataset 4) being trained for epoch 5 by 2019-01-26 14:49:07.565375!
Epoch 1/1
744/744 [==============================] - 20s 27ms/step - loss: 0.0859
14. set (Dataset 15) being trained for epoch 5 by 2019-01-26 14:49:33.957366!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0804
15. set (Dataset 16) being trained for epoch 5 by 2019-01-26 14:49:59.240280!
Epoch 1/1
914/914 [==============================] - 22s 25ms/step - loss: 0.0741
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 14:50:27.181489!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0629
17. set (Dataset 19) being trained for epoch 5 by 2019-01-26 14:50:45.838729!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0785
18. set (Dataset 24) being trained for epoch 5 by 2019-01-26 14:51:03.296781!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0595
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 14:51:20.446707!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1070
20. set (Dataset 21) being trained for epoch 5 by 2019-01-26 14:51:40.713904!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1156
Epoch 5 completed!
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F0
6'), (17, 'M10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'),
(20, 'M12'), (19, 'M11'), (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 14:51:59.193524
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.48 Degree
        The absolute mean error on Yaw angle estimation: 27.58 Degree
        The absolute mean error on Roll angle estimation: 21.84 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.26 Degree
        The absolute mean error on Yaw angle estimation: 28.83 Degree
        The absolute mean error on Roll angle estimation: 4.95 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 24.45 Degree
        The absolute mean error on Yaw angle estimation: 26.92 Degree
        The absolute mean error on Roll angle estimation: 9.55 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.09 Degree
        The absolute mean error on Yaw angle estimation: 30.08 Degree
        The absolute mean error on Roll angle estimation: 14.75 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.57 Degree
        The absolute mean error on Yaw angle estimations: 28.35 Degree
        The absolute mean error on Roll angle estimations: 12.77 Degree
Exp2019-01-26_13-37-43_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 14:53:28.013376
1. set (Dataset 24) being trained for epoch 1 by 2019-01-26 14:53:32.683865!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0556
2. set (Dataset 21) being trained for epoch 1 by 2019-01-26 14:53:51.583583!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1053
3. set (Dataset 22) being trained for epoch 1 by 2019-01-26 14:54:13.858749!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0570
4. set (Dataset 16) being trained for epoch 1 by 2019-01-26 14:54:40.095091!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0724
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 14:55:10.621983!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0798
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 14:55:35.651794!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1042
7. set (Dataset 19) being trained for epoch 1 by 2019-01-26 14:55:54.796342!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0755
8. set (Dataset 18) being trained for epoch 1 by 2019-01-26 14:56:12.892481!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0896
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 14:56:35.666188!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0765
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 14:57:01.360934!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0680
11. set (Dataset 6) being trained for epoch 1 by 2019-01-26 14:57:25.408468!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0960
12. set (Dataset 2) being trained for epoch 1 by 2019-01-26 14:57:43.921788!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0760
13. set (Dataset 15) being trained for epoch 1 by 2019-01-26 14:58:03.410819!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0798
14. set (Dataset 10) being trained for epoch 1 by 2019-01-26 14:58:26.882251!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0747
15. set (Dataset 1) being trained for epoch 1 by 2019-01-26 14:58:50.511601!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0844
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 14:59:08.683661!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0666
17. set (Dataset 4) being trained for epoch 1 by 2019-01-26 14:59:30.163833!
Epoch 1/1
744/744 [==============================] - 18s 24ms/step - loss: 0.0846
18. set (Dataset 11) being trained for epoch 1 by 2019-01-26 14:59:54.010793!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0596
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 15:00:13.509759!
Epoch 1/1
485/485 [==============================] - 12s 24ms/step - loss: 0.0662
20. set (Dataset 17) being trained for epoch 1 by 2019-01-26 15:00:29.177627!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0744
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:00:44.021509
1. set (Dataset 11) being trained for epoch 2 by 2019-01-26 15:00:49.721220!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0565
2. set (Dataset 17) being trained for epoch 2 by 2019-01-26 15:01:08.042173!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0719
3. set (Dataset 6) being trained for epoch 2 by 2019-01-26 15:01:23.260939!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0997
4. set (Dataset 1) being trained for epoch 2 by 2019-01-26 15:01:41.969715!
Epoch 1/1
498/498 [==============================] - 13s 27ms/step - loss: 0.0848
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 15:02:00.744307!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1043
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 15:02:19.546123!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0668
7. set (Dataset 4) being trained for epoch 2 by 2019-01-26 15:02:39.382976!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0821
8. set (Dataset 2) being trained for epoch 2 by 2019-01-26 15:03:03.346989!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0785
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 15:03:24.184414!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0758
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 15:03:51.442214!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0734
11. set (Dataset 19) being trained for epoch 2 by 2019-01-26 15:04:15.111560!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0796
12. set (Dataset 24) being trained for epoch 2 by 2019-01-26 15:04:32.531618!
Epoch 1/1
492/492 [==============================] - 11s 22ms/step - loss: 0.0584
13. set (Dataset 10) being trained for epoch 2 by 2019-01-26 15:04:50.832725!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0698
14. set (Dataset 21) being trained for epoch 2 by 2019-01-26 15:05:15.200154!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1133
15. set (Dataset 22) being trained for epoch 2 by 2019-01-26 15:05:37.696727!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0552
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 15:05:59.711142!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0644
17. set (Dataset 15) being trained for epoch 2 by 2019-01-26 15:06:19.705067!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0777
18. set (Dataset 16) being trained for epoch 2 by 2019-01-26 15:06:45.316879!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0719
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 15:07:15.426576!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0651
20. set (Dataset 18) being trained for epoch 2 by 2019-01-26 15:07:39.970224!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0942
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:07:59.804347
1. set (Dataset 16) being trained for epoch 3 by 2019-01-26 15:08:08.531468!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0687
2. set (Dataset 18) being trained for epoch 3 by 2019-01-26 15:08:37.692824!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0860
3. set (Dataset 19) being trained for epoch 3 by 2019-01-26 15:08:58.000695!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0728
4. set (Dataset 22) being trained for epoch 3 by 2019-01-26 15:09:16.887071!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0549
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 15:09:38.574291!
Epoch 1/1
485/485 [==============================] - 13s 27ms/step - loss: 0.0667
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 15:09:58.896934!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0617
7. set (Dataset 15) being trained for epoch 3 by 2019-01-26 15:10:24.259197!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0755
8. set (Dataset 24) being trained for epoch 3 by 2019-01-26 15:10:45.263651!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0557
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 15:11:03.498922!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.1008
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 15:11:25.928887!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0757
11. set (Dataset 4) being trained for epoch 3 by 2019-01-26 15:11:53.526348!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0847
12. set (Dataset 11) being trained for epoch 3 by 2019-01-26 15:12:17.991874!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0577
13. set (Dataset 21) being trained for epoch 3 by 2019-01-26 15:12:38.579035!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1129
14. set (Dataset 17) being trained for epoch 3 by 2019-01-26 15:12:58.912190!
Epoch 1/1
395/395 [==============================] - 11s 27ms/step - loss: 0.0767
15. set (Dataset 6) being trained for epoch 3 by 2019-01-26 15:13:14.906001!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0906
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 15:13:33.759581!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0600
17. set (Dataset 10) being trained for epoch 3 by 2019-01-26 15:13:55.440825!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0734
18. set (Dataset 1) being trained for epoch 3 by 2019-01-26 15:14:18.694931!
Epoch 1/1
498/498 [==============================] - 13s 27ms/step - loss: 0.0831
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 15:14:39.758589!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0715
20. set (Dataset 2) being trained for epoch 3 by 2019-01-26 15:15:03.620327!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0723
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:15:21.093228
1. set (Dataset 1) being trained for epoch 4 by 2019-01-26 15:15:26.141843!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0784
2. set (Dataset 2) being trained for epoch 4 by 2019-01-26 15:15:44.034335!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0656
3. set (Dataset 4) being trained for epoch 4 by 2019-01-26 15:16:04.373438!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0778
4. set (Dataset 6) being trained for epoch 4 by 2019-01-26 15:16:28.173172!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0969
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 15:16:49.626348!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0581
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 15:17:15.916878!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0699
7. set (Dataset 10) being trained for epoch 4 by 2019-01-26 15:17:42.215000!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0656
8. set (Dataset 11) being trained for epoch 4 by 2019-01-26 15:18:06.302530!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0526
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 15:18:25.986049!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0638
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 15:18:43.921722!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.1043
11. set (Dataset 15) being trained for epoch 4 by 2019-01-26 15:19:04.582571!
Epoch 1/1
654/654 [==============================] - 17s 25ms/step - loss: 0.0772
12. set (Dataset 16) being trained for epoch 4 by 2019-01-26 15:19:29.985996!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0682
13. set (Dataset 17) being trained for epoch 4 by 2019-01-26 15:19:57.252664!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0717
14. set (Dataset 18) being trained for epoch 4 by 2019-01-26 15:20:13.196367!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0890
15. set (Dataset 19) being trained for epoch 4 by 2019-01-26 15:20:33.763898!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0732
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 15:20:52.082695!
Epoch 1/1
556/556 [==============================] - 14s 24ms/step - loss: 0.0595
17. set (Dataset 21) being trained for epoch 4 by 2019-01-26 15:21:11.724748!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1048
18. set (Dataset 22) being trained for epoch 4 by 2019-01-26 15:21:33.904824!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0535
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 15:21:58.527580!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0730
20. set (Dataset 24) being trained for epoch 4 by 2019-01-26 15:22:22.924578!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0574
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:22:39.761675
1. set (Dataset 22) being trained for epoch 5 by 2019-01-26 15:22:46.171505!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0514
2. set (Dataset 24) being trained for epoch 5 by 2019-01-26 15:23:08.056579!
Epoch 1/1
492/492 [==============================] - 13s 25ms/step - loss: 0.0513
3. set (Dataset 15) being trained for epoch 5 by 2019-01-26 15:23:26.987199!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0744
4. set (Dataset 19) being trained for epoch 5 by 2019-01-26 15:23:48.803197!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0723
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 15:24:09.478706!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0744
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 15:24:36.570734!
Epoch 1/1
772/772 [==============================] - 21s 27ms/step - loss: 0.0702
7. set (Dataset 21) being trained for epoch 5 by 2019-01-26 15:25:03.568212!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1103
8. set (Dataset 16) being trained for epoch 5 by 2019-01-26 15:25:27.941645!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0702
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 15:25:58.472955!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0618
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 15:26:21.856302!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0623
11. set (Dataset 10) being trained for epoch 5 by 2019-01-26 15:26:41.507061!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0709
12. set (Dataset 1) being trained for epoch 5 by 2019-01-26 15:27:04.858152!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0811
13. set (Dataset 18) being trained for epoch 5 by 2019-01-26 15:27:22.955725!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0924
14. set (Dataset 2) being trained for epoch 5 by 2019-01-26 15:27:43.988864!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0718
15. set (Dataset 4) being trained for epoch 5 by 2019-01-26 15:28:04.251771!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0815
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 15:28:28.784851!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0659
17. set (Dataset 17) being trained for epoch 5 by 2019-01-26 15:28:46.651534!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0720
18. set (Dataset 6) being trained for epoch 5 by 2019-01-26 15:29:01.677170!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0953
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 15:29:21.050426!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0983
20. set (Dataset 11) being trained for epoch 5 by 2019-01-26 15:29:41.296736!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0551
Epoch 5 completed!
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21,
'F02'), (16, 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 15:29:57.340457
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.03 Degree
        The absolute mean error on Yaw angle estimation: 32.77 Degree
        The absolute mean error on Roll angle estimation: 14.60 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 8.85 Degree
        The absolute mean error on Yaw angle estimation: 25.93 Degree
        The absolute mean error on Roll angle estimation: 4.87 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 26.39 Degree
        The absolute mean error on Yaw angle estimation: 26.14 Degree
        The absolute mean error on Roll angle estimation: 9.19 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.17 Degree
        The absolute mean error on Yaw angle estimation: 30.83 Degree
        The absolute mean error on Roll angle estimation: 14.46 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.86 Degree
        The absolute mean error on Yaw angle estimations: 28.91 Degree
        The absolute mean error on Roll angle estimations: 10.78 Degree
Exp2019-01-26_13-37-43_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 15:31:26.022829
1. set (Dataset 6) being trained for epoch 1 by 2019-01-26 15:31:31.209621!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0885
2. set (Dataset 11) being trained for epoch 1 by 2019-01-26 15:31:50.559580!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0526
3. set (Dataset 10) being trained for epoch 1 by 2019-01-26 15:32:12.645515!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0641
4. set (Dataset 4) being trained for epoch 1 by 2019-01-26 15:32:39.257999!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0763
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 15:33:06.133257!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0729
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 15:33:31.019260!
Epoch 1/1
569/569 [==============================] - 14s 24ms/step - loss: 0.1046
7. set (Dataset 17) being trained for epoch 1 by 2019-01-26 15:33:48.722368!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0718
8. set (Dataset 1) being trained for epoch 1 by 2019-01-26 15:34:04.095278!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0844
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 15:34:24.430393!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0700
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 15:34:50.428321!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0580
11. set (Dataset 21) being trained for epoch 1 by 2019-01-26 15:35:15.230250!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1072
12. set (Dataset 22) being trained for epoch 1 by 2019-01-26 15:35:37.557405!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0546
13. set (Dataset 2) being trained for epoch 1 by 2019-01-26 15:35:59.544554!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0717
14. set (Dataset 24) being trained for epoch 1 by 2019-01-26 15:36:16.864275!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0606
15. set (Dataset 15) being trained for epoch 1 by 2019-01-26 15:36:35.410444!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0729
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 15:36:56.920170!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0595
17. set (Dataset 18) being trained for epoch 1 by 2019-01-26 15:37:16.517392!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0869
18. set (Dataset 19) being trained for epoch 1 by 2019-01-26 15:37:37.477243!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0717
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 15:37:55.651253!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0637
20. set (Dataset 16) being trained for epoch 1 by 2019-01-26 15:38:16.772632!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0693
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:38:44.121402
1. set (Dataset 19) being trained for epoch 2 by 2019-01-26 15:38:49.028269!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0678
2. set (Dataset 16) being trained for epoch 2 by 2019-01-26 15:39:10.359793!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0648
3. set (Dataset 21) being trained for epoch 2 by 2019-01-26 15:39:39.705659!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1059
4. set (Dataset 15) being trained for epoch 2 by 2019-01-26 15:40:02.349958!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0736
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 15:40:24.647596!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0958
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 15:40:44.093892!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0611
7. set (Dataset 18) being trained for epoch 2 by 2019-01-26 15:41:02.327292!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0835
8. set (Dataset 22) being trained for epoch 2 by 2019-01-26 15:41:24.502344!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0504
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 15:41:49.415675!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0751
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 15:42:16.261823!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0693
11. set (Dataset 17) being trained for epoch 2 by 2019-01-26 15:42:39.304281!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0724
12. set (Dataset 6) being trained for epoch 2 by 2019-01-26 15:42:54.182426!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0920
13. set (Dataset 24) being trained for epoch 2 by 2019-01-26 15:43:12.227640!
Epoch 1/1
492/492 [==============================] - 12s 24ms/step - loss: 0.0529
14. set (Dataset 11) being trained for epoch 2 by 2019-01-26 15:43:29.707274!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0538
15. set (Dataset 10) being trained for epoch 2 by 2019-01-26 15:43:51.142722!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0646
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 15:44:15.210325!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0629
17. set (Dataset 2) being trained for epoch 2 by 2019-01-26 15:44:34.544753!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0742
18. set (Dataset 4) being trained for epoch 2 by 2019-01-26 15:44:54.992131!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0771
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 15:45:21.132024!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0604
20. set (Dataset 1) being trained for epoch 2 by 2019-01-26 15:45:44.421336!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0774
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:46:01.458284
1. set (Dataset 4) being trained for epoch 3 by 2019-01-26 15:46:08.847242!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0745
2. set (Dataset 1) being trained for epoch 3 by 2019-01-26 15:46:32.261423!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0753
3. set (Dataset 17) being trained for epoch 3 by 2019-01-26 15:46:48.748779!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0736
4. set (Dataset 10) being trained for epoch 3 by 2019-01-26 15:47:05.944224!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0633
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 15:47:28.861424!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0586
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 15:47:48.331539!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0575
7. set (Dataset 2) being trained for epoch 3 by 2019-01-26 15:48:12.147193!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0680
8. set (Dataset 6) being trained for epoch 3 by 2019-01-26 15:48:30.303864!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0920
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 15:48:49.181829!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0941
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 15:49:11.303334!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0702
11. set (Dataset 18) being trained for epoch 3 by 2019-01-26 15:49:36.432195!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0902
12. set (Dataset 19) being trained for epoch 3 by 2019-01-26 15:49:56.472461!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0699
13. set (Dataset 11) being trained for epoch 3 by 2019-01-26 15:50:14.624550!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0541
14. set (Dataset 16) being trained for epoch 3 by 2019-01-26 15:50:37.927450!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0644
15. set (Dataset 21) being trained for epoch 3 by 2019-01-26 15:51:07.133659!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1037
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 15:51:28.673899!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0594
17. set (Dataset 24) being trained for epoch 3 by 2019-01-26 15:51:47.409754!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0519
18. set (Dataset 15) being trained for epoch 3 by 2019-01-26 15:52:06.064945!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0699
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 15:52:29.918049!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0697
20. set (Dataset 22) being trained for epoch 3 by 2019-01-26 15:52:55.414448!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0506
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 15:53:16.990405
1. set (Dataset 15) being trained for epoch 4 by 2019-01-26 15:53:23.379986!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0697
2. set (Dataset 22) being trained for epoch 4 by 2019-01-26 15:53:46.599274!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0499
3. set (Dataset 18) being trained for epoch 4 by 2019-01-26 15:54:09.676822!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0848
4. set (Dataset 21) being trained for epoch 4 by 2019-01-26 15:54:31.845267!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1044
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 15:54:55.046066!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0597
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 15:55:21.534973!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0685
7. set (Dataset 24) being trained for epoch 4 by 2019-01-26 15:55:45.040976!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0544
8. set (Dataset 19) being trained for epoch 4 by 2019-01-26 15:56:02.348301!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0647
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 15:56:19.567199!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0607
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 15:56:36.983751!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0968
11. set (Dataset 2) being trained for epoch 4 by 2019-01-26 15:56:56.414260!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0721
12. set (Dataset 4) being trained for epoch 4 by 2019-01-26 15:57:16.764899!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0779
13. set (Dataset 16) being trained for epoch 4 by 2019-01-26 15:57:43.989061!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0670
14. set (Dataset 1) being trained for epoch 4 by 2019-01-26 15:58:11.981977!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0806
15. set (Dataset 17) being trained for epoch 4 by 2019-01-26 15:58:28.465115!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0678
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 15:58:44.332699!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0611
17. set (Dataset 11) being trained for epoch 4 by 2019-01-26 15:59:03.861195!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0522
18. set (Dataset 10) being trained for epoch 4 by 2019-01-26 15:59:25.516166!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0632
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 15:59:51.240423!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0710
20. set (Dataset 6) being trained for epoch 4 by 2019-01-26 16:00:15.725953!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0948
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:00:34.098364
1. set (Dataset 10) being trained for epoch 5 by 2019-01-26 16:00:41.317723!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0646
2. set (Dataset 6) being trained for epoch 5 by 2019-01-26 16:01:04.767443!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0869
3. set (Dataset 2) being trained for epoch 5 by 2019-01-26 16:01:23.669140!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0714
4. set (Dataset 17) being trained for epoch 5 by 2019-01-26 16:01:40.262674!
Epoch 1/1
395/395 [==============================] - 10s 24ms/step - loss: 0.0696
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 16:01:57.587131!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0649
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 16:02:24.143803!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0657
7. set (Dataset 11) being trained for epoch 5 by 2019-01-26 16:02:49.187910!
Epoch 1/1
572/572 [==============================] - 15s 26ms/step - loss: 0.0514
8. set (Dataset 4) being trained for epoch 5 by 2019-01-26 16:03:11.379922!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0731
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 16:03:37.521355!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0591
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 16:04:01.281888!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0598
11. set (Dataset 24) being trained for epoch 5 by 2019-01-26 16:04:18.189454!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0578
12. set (Dataset 15) being trained for epoch 5 by 2019-01-26 16:04:36.755933!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0700
13. set (Dataset 1) being trained for epoch 5 by 2019-01-26 16:04:58.299822!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0809
14. set (Dataset 22) being trained for epoch 5 by 2019-01-26 16:05:17.054063!
Epoch 1/1
665/665 [==============================] - 16s 25ms/step - loss: 0.0514
15. set (Dataset 18) being trained for epoch 5 by 2019-01-26 16:05:39.426668!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0835
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 16:06:00.170539!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0591
17. set (Dataset 16) being trained for epoch 5 by 2019-01-26 16:06:22.762796!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0643
18. set (Dataset 21) being trained for epoch 5 by 2019-01-26 16:06:51.575732!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1013
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 16:07:12.770027!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0944
20. set (Dataset 19) being trained for epoch 5 by 2019-01-26 16:07:32.081873!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0670
Epoch 5 completed!
The subjects are trained: [(10, 'M04'), (6, 'F06'), (2, 'F02'), (17, 'M10'), (7, 'M01'), (8, 'M02'), (11, 'M
05'), (4, 'F04'), (12, 'M06'), (13, 'M07'), (24, 'M14'), (15, 'F03'), (1, 'F01'), (22, 'M01'), (18, 'F05'),
(20, 'M12'), (16, 'M09'), (21, 'F02'), (23, 'M13'), (19, 'M11')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 16:07:46.712039
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 7.65 Degree
        The absolute mean error on Yaw angle estimation: 27.07 Degree
        The absolute mean error on Roll angle estimation: 15.22 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.97 Degree
        The absolute mean error on Yaw angle estimation: 26.83 Degree
        The absolute mean error on Roll angle estimation: 3.93 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 25.70 Degree
        The absolute mean error on Yaw angle estimation: 25.65 Degree
        The absolute mean error on Roll angle estimation: 6.89 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 16.03 Degree
        The absolute mean error on Yaw angle estimation: 31.47 Degree
        The absolute mean error on Roll angle estimation: 13.76 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 14.84 Degree
        The absolute mean error on Yaw angle estimations: 27.76 Degree
        The absolute mean error on Roll angle estimations: 9.95 Degree
Exp2019-01-26_13-37-43_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 16:09:15.448359
1. set (Dataset 21) being trained for epoch 1 by 2019-01-26 16:09:21.455918!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0995
2. set (Dataset 19) being trained for epoch 1 by 2019-01-26 16:09:42.330645!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0658
3. set (Dataset 24) being trained for epoch 1 by 2019-01-26 16:10:00.033674!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0510
4. set (Dataset 18) being trained for epoch 1 by 2019-01-26 16:10:19.012394!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0801
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 16:10:41.986488!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0744
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 16:11:06.864552!
Epoch 1/1
569/569 [==============================] - 15s 25ms/step - loss: 0.0925
7. set (Dataset 16) being trained for epoch 1 by 2019-01-26 16:11:30.336567!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0626
8. set (Dataset 15) being trained for epoch 1 by 2019-01-26 16:11:59.535800!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0704
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 16:12:23.652063!
Epoch 1/1
745/745 [==============================] - 20s 26ms/step - loss: 0.0663
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 16:12:50.691746!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0575
11. set (Dataset 11) being trained for epoch 1 by 2019-01-26 16:13:15.058794!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0509
12. set (Dataset 10) being trained for epoch 1 by 2019-01-26 16:13:36.809569!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0613
13. set (Dataset 22) being trained for epoch 1 by 2019-01-26 16:14:01.538245!
Epoch 1/1
665/665 [==============================] - 16s 24ms/step - loss: 0.0516
14. set (Dataset 6) being trained for epoch 1 by 2019-01-26 16:14:23.051677!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0948
15. set (Dataset 2) being trained for epoch 1 by 2019-01-26 16:14:41.637042!
Epoch 1/1
511/511 [==============================] - 12s 24ms/step - loss: 0.0706
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 16:14:59.534141!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0601
17. set (Dataset 1) being trained for epoch 1 by 2019-01-26 16:15:18.377651!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0809
18. set (Dataset 17) being trained for epoch 1 by 2019-01-26 16:15:35.102194!
Epoch 1/1
395/395 [==============================] - 10s 27ms/step - loss: 0.0687
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 16:15:50.440488!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0600
20. set (Dataset 4) being trained for epoch 1 by 2019-01-26 16:16:09.880021!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0722
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:16:33.356972
1. set (Dataset 17) being trained for epoch 2 by 2019-01-26 16:16:37.102283!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0669
2. set (Dataset 4) being trained for epoch 2 by 2019-01-26 16:16:54.547558!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0693
3. set (Dataset 11) being trained for epoch 2 by 2019-01-26 16:17:18.965668!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0507
4. set (Dataset 2) being trained for epoch 2 by 2019-01-26 16:17:38.515934!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0666
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 16:17:56.689288!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0959
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 16:18:15.945438!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0573
7. set (Dataset 1) being trained for epoch 2 by 2019-01-26 16:18:33.083658!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0760
8. set (Dataset 10) being trained for epoch 2 by 2019-01-26 16:18:52.833091!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0633
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 16:19:19.455341!
Epoch 1/1
772/772 [==============================] - 19s 24ms/step - loss: 0.0676
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 16:19:45.833543!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0651
11. set (Dataset 16) being trained for epoch 2 by 2019-01-26 16:20:13.369937!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0680
12. set (Dataset 21) being trained for epoch 2 by 2019-01-26 16:20:42.714073!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.1023
13. set (Dataset 6) being trained for epoch 2 by 2019-01-26 16:21:04.639960!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0882
14. set (Dataset 19) being trained for epoch 2 by 2019-01-26 16:21:23.361145!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0640
15. set (Dataset 24) being trained for epoch 2 by 2019-01-26 16:21:40.827094!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0530
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 16:21:58.654456!
Epoch 1/1
556/556 [==============================] - 13s 24ms/step - loss: 0.0543
17. set (Dataset 22) being trained for epoch 2 by 2019-01-26 16:22:18.416295!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0489
18. set (Dataset 18) being trained for epoch 2 by 2019-01-26 16:22:41.282349!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0825
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 16:23:03.894807!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0596
20. set (Dataset 15) being trained for epoch 2 by 2019-01-26 16:23:28.388406!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0676
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:23:49.040657
1. set (Dataset 18) being trained for epoch 3 by 2019-01-26 16:23:55.054799!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0805
2. set (Dataset 15) being trained for epoch 3 by 2019-01-26 16:24:16.651404!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0651
3. set (Dataset 16) being trained for epoch 3 by 2019-01-26 16:24:41.811652!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0629
4. set (Dataset 24) being trained for epoch 3 by 2019-01-26 16:25:09.454454!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0530
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 16:25:26.721698!
Epoch 1/1
485/485 [==============================] - 11s 24ms/step - loss: 0.0607
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 16:25:45.517937!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0551
7. set (Dataset 22) being trained for epoch 3 by 2019-01-26 16:26:10.322323!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0468
8. set (Dataset 21) being trained for epoch 3 by 2019-01-26 16:26:33.596709!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1036
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 16:26:54.912398!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0915
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 16:27:17.293376!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0705
11. set (Dataset 1) being trained for epoch 3 by 2019-01-26 16:27:42.286234!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0784
12. set (Dataset 17) being trained for epoch 3 by 2019-01-26 16:27:58.547606!
Epoch 1/1
395/395 [==============================] - 10s 25ms/step - loss: 0.0679
13. set (Dataset 19) being trained for epoch 3 by 2019-01-26 16:28:13.210919!
Epoch 1/1
502/502 [==============================] - 12s 25ms/step - loss: 0.0660
14. set (Dataset 4) being trained for epoch 3 by 2019-01-26 16:28:33.076904!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0726
15. set (Dataset 11) being trained for epoch 3 by 2019-01-26 16:28:57.614080!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0535
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 16:29:17.249998!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0623
17. set (Dataset 6) being trained for epoch 3 by 2019-01-26 16:29:36.449165!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0855
18. set (Dataset 2) being trained for epoch 3 by 2019-01-26 16:29:55.119490!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0668
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 16:30:15.373297!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0669
20. set (Dataset 10) being trained for epoch 3 by 2019-01-26 16:30:41.602066!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0631
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:31:04.298281
1. set (Dataset 2) being trained for epoch 4 by 2019-01-26 16:31:09.387196!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0695
2. set (Dataset 10) being trained for epoch 4 by 2019-01-26 16:31:29.525710!
Epoch 1/1
726/726 [==============================] - 19s 27ms/step - loss: 0.0609
3. set (Dataset 1) being trained for epoch 4 by 2019-01-26 16:31:53.873614!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0734
4. set (Dataset 11) being trained for epoch 4 by 2019-01-26 16:32:11.992026!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0506
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 16:32:33.884836!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0562
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 16:33:00.418875!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0647
7. set (Dataset 6) being trained for epoch 4 by 2019-01-26 16:33:24.563486!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0933
8. set (Dataset 17) being trained for epoch 4 by 2019-01-26 16:33:42.317598!
Epoch 1/1
395/395 [==============================] - 11s 27ms/step - loss: 0.0680
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 16:33:57.675023!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0580
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 16:34:15.327010!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0933
11. set (Dataset 22) being trained for epoch 4 by 2019-01-26 16:34:36.357546!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0486
12. set (Dataset 18) being trained for epoch 4 by 2019-01-26 16:34:59.111737!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0784
13. set (Dataset 4) being trained for epoch 4 by 2019-01-26 16:35:22.287548!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0767
14. set (Dataset 15) being trained for epoch 4 by 2019-01-26 16:35:47.059930!
Epoch 1/1
654/654 [==============================] - 16s 24ms/step - loss: 0.0685
15. set (Dataset 16) being trained for epoch 4 by 2019-01-26 16:36:11.705119!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0630
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 16:36:40.170278!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0545
17. set (Dataset 19) being trained for epoch 4 by 2019-01-26 16:36:59.155749!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0642
18. set (Dataset 24) being trained for epoch 4 by 2019-01-26 16:37:17.040741!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0513
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 16:37:37.244257!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0702
20. set (Dataset 21) being trained for epoch 4 by 2019-01-26 16:38:02.889262!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.1000
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:38:23.728616
1. set (Dataset 24) being trained for epoch 5 by 2019-01-26 16:38:28.404836!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0504
2. set (Dataset 21) being trained for epoch 5 by 2019-01-26 16:38:47.204659!
Epoch 1/1
634/634 [==============================] - 16s 26ms/step - loss: 0.0993
3. set (Dataset 22) being trained for epoch 5 by 2019-01-26 16:39:09.963603!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0481
4. set (Dataset 16) being trained for epoch 5 by 2019-01-26 16:39:35.776611!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0608
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 16:40:07.038945!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0644
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 16:40:33.245417!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0684
7. set (Dataset 19) being trained for epoch 5 by 2019-01-26 16:40:57.753317!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0652
8. set (Dataset 18) being trained for epoch 5 by 2019-01-26 16:41:16.413715!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0832
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 16:41:38.847877!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0565
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 16:42:02.026829!
Epoch 1/1
485/485 [==============================] - 12s 26ms/step - loss: 0.0573
11. set (Dataset 6) being trained for epoch 5 by 2019-01-26 16:42:19.738542!
Epoch 1/1
542/542 [==============================] - 13s 24ms/step - loss: 0.0863
12. set (Dataset 2) being trained for epoch 5 by 2019-01-26 16:42:38.074033!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0659
13. set (Dataset 15) being trained for epoch 5 by 2019-01-26 16:42:57.453970!
Epoch 1/1
654/654 [==============================] - 17s 26ms/step - loss: 0.0700
14. set (Dataset 10) being trained for epoch 5 by 2019-01-26 16:43:21.859920!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0617
15. set (Dataset 1) being trained for epoch 5 by 2019-01-26 16:43:45.313149!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0741
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 16:44:03.566733!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0595
17. set (Dataset 4) being trained for epoch 5 by 2019-01-26 16:44:25.092482!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0699
18. set (Dataset 11) being trained for epoch 5 by 2019-01-26 16:44:49.256455!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0489
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 16:45:09.311314!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0905
20. set (Dataset 17) being trained for epoch 5 by 2019-01-26 16:45:27.482368!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0677
Epoch 5 completed!
The subjects are trained: [(24, 'M14'), (21, 'F02'), (22, 'M01'), (16, 'M09'), (7, 'M01'), (8, 'M02'), (19,
'M11'), (18, 'F05'), (12, 'M06'), (13, 'M07'), (6, 'F06'), (2, 'F02'), (15, 'F03'), (10, 'M04'), (1, 'F01'),
 (20, 'M12'), (4, 'F04'), (11, 'M05'), (23, 'M13'), (17, 'M10')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 16:45:39.737107
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 7.24 Degree
        The absolute mean error on Yaw angle estimation: 30.34 Degree
        The absolute mean error on Roll angle estimation: 12.49 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 7.41 Degree
        The absolute mean error on Yaw angle estimation: 25.61 Degree
        The absolute mean error on Roll angle estimation: 4.46 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 31.00 Degree
        The absolute mean error on Yaw angle estimation: 25.10 Degree
        The absolute mean error on Roll angle estimation: 5.99 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.66 Degree
        The absolute mean error on Yaw angle estimation: 29.90 Degree
        The absolute mean error on Roll angle estimation: 14.03 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.83 Degree
        The absolute mean error on Yaw angle estimations: 27.74 Degree
        The absolute mean error on Roll angle estimations: 9.24 Degree
Exp2019-01-26_13-37-43_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-26_13-37-43
All frames and annotations from 20 datasets have been read by 2019-01-26 16:47:08.552155
1. set (Dataset 11) being trained for epoch 1 by 2019-01-26 16:47:14.254657!
Epoch 1/1
572/572 [==============================] - 14s 24ms/step - loss: 0.0502
2. set (Dataset 17) being trained for epoch 1 by 2019-01-26 16:47:31.832847!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0667
3. set (Dataset 6) being trained for epoch 1 by 2019-01-26 16:47:47.276031!
Epoch 1/1
542/542 [==============================] - 15s 27ms/step - loss: 0.0834
4. set (Dataset 1) being trained for epoch 1 by 2019-01-26 16:48:06.961337!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0765
5. set (Dataset 8) being trained for epoch 1 by 2019-01-26 16:48:27.476170!
Epoch 1/1
772/772 [==============================] - 20s 25ms/step - loss: 0.0676
6. set (Dataset 23) being trained for epoch 1 by 2019-01-26 16:48:52.670483!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0925
7. set (Dataset 4) being trained for epoch 1 by 2019-01-26 16:49:14.450330!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0708
8. set (Dataset 2) being trained for epoch 1 by 2019-01-26 16:49:38.499024!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0620
9. set (Dataset 7) being trained for epoch 1 by 2019-01-26 16:49:59.066549!
Epoch 1/1
745/745 [==============================] - 18s 25ms/step - loss: 0.0622
10. set (Dataset 12) being trained for epoch 1 by 2019-01-26 16:50:24.841588!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0530
11. set (Dataset 19) being trained for epoch 1 by 2019-01-26 16:50:48.635618!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0640
12. set (Dataset 24) being trained for epoch 1 by 2019-01-26 16:51:06.196973!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0513
13. set (Dataset 10) being trained for epoch 1 by 2019-01-26 16:51:26.567616!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0610
14. set (Dataset 21) being trained for epoch 1 by 2019-01-26 16:51:51.187248!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0976
15. set (Dataset 22) being trained for epoch 1 by 2019-01-26 16:52:13.382875!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0476
16. set (Dataset 20) being trained for epoch 1 by 2019-01-26 16:52:35.807660!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0576
17. set (Dataset 15) being trained for epoch 1 by 2019-01-26 16:52:56.353703!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0651
18. set (Dataset 16) being trained for epoch 1 by 2019-01-26 16:53:21.266610!
Epoch 1/1
914/914 [==============================] - 23s 26ms/step - loss: 0.0595
19. set (Dataset 13) being trained for epoch 1 by 2019-01-26 16:53:49.635463!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0556
20. set (Dataset 18) being trained for epoch 1 by 2019-01-26 16:54:07.900380!
Epoch 1/1
614/614 [==============================] - 15s 25ms/step - loss: 0.0792
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 16:54:27.717076
1. set (Dataset 16) being trained for epoch 2 by 2019-01-26 16:54:36.466012!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0602
2. set (Dataset 18) being trained for epoch 2 by 2019-01-26 16:55:05.585875!
Epoch 1/1
614/614 [==============================] - 16s 26ms/step - loss: 0.0758
3. set (Dataset 19) being trained for epoch 2 by 2019-01-26 16:55:26.459891!
Epoch 1/1
502/502 [==============================] - 13s 25ms/step - loss: 0.0646
4. set (Dataset 22) being trained for epoch 2 by 2019-01-26 16:55:45.617379!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0484
5. set (Dataset 23) being trained for epoch 2 by 2019-01-26 16:56:07.786053!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0891
6. set (Dataset 13) being trained for epoch 2 by 2019-01-26 16:56:26.803923!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0571
7. set (Dataset 15) being trained for epoch 2 by 2019-01-26 16:56:45.429181!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0666
8. set (Dataset 24) being trained for epoch 2 by 2019-01-26 16:57:06.433972!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0505
9. set (Dataset 8) being trained for epoch 2 by 2019-01-26 16:57:26.981123!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0671
10. set (Dataset 7) being trained for epoch 2 by 2019-01-26 16:57:54.708229!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0654
11. set (Dataset 4) being trained for epoch 2 by 2019-01-26 16:58:20.979987!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0698
12. set (Dataset 11) being trained for epoch 2 by 2019-01-26 16:58:45.827603!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0495
13. set (Dataset 21) being trained for epoch 2 by 2019-01-26 16:59:06.041720!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1027
14. set (Dataset 17) being trained for epoch 2 by 2019-01-26 16:59:25.807133!
Epoch 1/1
395/395 [==============================] - 9s 24ms/step - loss: 0.0691
15. set (Dataset 6) being trained for epoch 2 by 2019-01-26 16:59:40.558558!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0864
16. set (Dataset 20) being trained for epoch 2 by 2019-01-26 16:59:59.471893!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0555
17. set (Dataset 10) being trained for epoch 2 by 2019-01-26 17:00:21.174942!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0642
18. set (Dataset 1) being trained for epoch 2 by 2019-01-26 17:00:44.374979!
Epoch 1/1
498/498 [==============================] - 13s 25ms/step - loss: 0.0748
19. set (Dataset 12) being trained for epoch 2 by 2019-01-26 17:01:04.393295!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0525
20. set (Dataset 2) being trained for epoch 2 by 2019-01-26 17:01:28.438906!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0662
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 17:01:45.893065
1. set (Dataset 1) being trained for epoch 3 by 2019-01-26 17:01:50.934464!
Epoch 1/1
498/498 [==============================] - 12s 25ms/step - loss: 0.0790
2. set (Dataset 2) being trained for epoch 3 by 2019-01-26 17:02:08.432710!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0616
3. set (Dataset 4) being trained for epoch 3 by 2019-01-26 17:02:28.937703!
Epoch 1/1
744/744 [==============================] - 19s 26ms/step - loss: 0.0708
4. set (Dataset 6) being trained for epoch 3 by 2019-01-26 17:02:53.247164!
Epoch 1/1
542/542 [==============================] - 13s 25ms/step - loss: 0.0857
5. set (Dataset 13) being trained for epoch 3 by 2019-01-26 17:03:11.613937!
Epoch 1/1
485/485 [==============================] - 12s 25ms/step - loss: 0.0556
6. set (Dataset 12) being trained for epoch 3 by 2019-01-26 17:03:30.970740!
Epoch 1/1
732/732 [==============================] - 19s 26ms/step - loss: 0.0508
7. set (Dataset 10) being trained for epoch 3 by 2019-01-26 17:03:57.123069!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0589
8. set (Dataset 11) being trained for epoch 3 by 2019-01-26 17:04:20.910822!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0454
9. set (Dataset 23) being trained for epoch 3 by 2019-01-26 17:04:40.792617!
Epoch 1/1
569/569 [==============================] - 15s 26ms/step - loss: 0.0939
10. set (Dataset 8) being trained for epoch 3 by 2019-01-26 17:05:03.286982!
Epoch 1/1
772/772 [==============================] - 19s 25ms/step - loss: 0.0665
11. set (Dataset 15) being trained for epoch 3 by 2019-01-26 17:05:28.773460!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0680
12. set (Dataset 16) being trained for epoch 3 by 2019-01-26 17:05:54.056530!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0593
13. set (Dataset 17) being trained for epoch 3 by 2019-01-26 17:06:20.886557!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0661
14. set (Dataset 18) being trained for epoch 3 by 2019-01-26 17:06:37.021396!
Epoch 1/1
614/614 [==============================] - 15s 24ms/step - loss: 0.0780
15. set (Dataset 19) being trained for epoch 3 by 2019-01-26 17:06:56.984316!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0607
16. set (Dataset 20) being trained for epoch 3 by 2019-01-26 17:07:15.626451!
Epoch 1/1
556/556 [==============================] - 15s 26ms/step - loss: 0.0531
17. set (Dataset 21) being trained for epoch 3 by 2019-01-26 17:07:36.308932!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.0962
18. set (Dataset 22) being trained for epoch 3 by 2019-01-26 17:07:58.608925!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0471
19. set (Dataset 7) being trained for epoch 3 by 2019-01-26 17:08:22.820943!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0622
20. set (Dataset 24) being trained for epoch 3 by 2019-01-26 17:08:46.566974!
Epoch 1/1
492/492 [==============================] - 13s 26ms/step - loss: 0.0531
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 17:09:03.625996
1. set (Dataset 22) being trained for epoch 4 by 2019-01-26 17:09:10.012108!
Epoch 1/1
665/665 [==============================] - 17s 25ms/step - loss: 0.0468
2. set (Dataset 24) being trained for epoch 4 by 2019-01-26 17:09:31.345373!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0477
3. set (Dataset 15) being trained for epoch 4 by 2019-01-26 17:09:50.125437!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0636
4. set (Dataset 19) being trained for epoch 4 by 2019-01-26 17:10:11.264555!
Epoch 1/1
502/502 [==============================] - 13s 26ms/step - loss: 0.0626
5. set (Dataset 12) being trained for epoch 4 by 2019-01-26 17:10:31.695056!
Epoch 1/1
732/732 [==============================] - 19s 25ms/step - loss: 0.0550
6. set (Dataset 7) being trained for epoch 4 by 2019-01-26 17:10:57.889170!
Epoch 1/1
745/745 [==============================] - 19s 26ms/step - loss: 0.0639
7. set (Dataset 21) being trained for epoch 4 by 2019-01-26 17:11:23.283327!
Epoch 1/1
634/634 [==============================] - 16s 25ms/step - loss: 0.1014
8. set (Dataset 16) being trained for epoch 4 by 2019-01-26 17:11:47.768877!
Epoch 1/1
914/914 [==============================] - 23s 25ms/step - loss: 0.0635
9. set (Dataset 13) being trained for epoch 4 by 2019-01-26 17:12:15.977478!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0556
10. set (Dataset 23) being trained for epoch 4 by 2019-01-26 17:12:34.164758!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0883
11. set (Dataset 10) being trained for epoch 4 by 2019-01-26 17:12:55.913165!
Epoch 1/1
726/726 [==============================] - 19s 26ms/step - loss: 0.0617
12. set (Dataset 1) being trained for epoch 4 by 2019-01-26 17:13:19.661703!
Epoch 1/1
498/498 [==============================] - 13s 26ms/step - loss: 0.0721
13. set (Dataset 18) being trained for epoch 4 by 2019-01-26 17:13:38.715243!
Epoch 1/1
614/614 [==============================] - 17s 28ms/step - loss: 0.0804
14. set (Dataset 2) being trained for epoch 4 by 2019-01-26 17:14:00.879291!
Epoch 1/1
511/511 [==============================] - 13s 26ms/step - loss: 0.0667
15. set (Dataset 4) being trained for epoch 4 by 2019-01-26 17:14:21.437279!
Epoch 1/1
744/744 [==============================] - 19s 25ms/step - loss: 0.0708
16. set (Dataset 20) being trained for epoch 4 by 2019-01-26 17:14:45.611616!
Epoch 1/1
556/556 [==============================] - 14s 25ms/step - loss: 0.0558
17. set (Dataset 17) being trained for epoch 4 by 2019-01-26 17:15:03.497338!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0642
18. set (Dataset 6) being trained for epoch 4 by 2019-01-26 17:15:18.858449!
Epoch 1/1
542/542 [==============================] - 14s 26ms/step - loss: 0.0851
19. set (Dataset 8) being trained for epoch 4 by 2019-01-26 17:15:40.787378!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0696
20. set (Dataset 11) being trained for epoch 4 by 2019-01-26 17:16:06.303843!
Epoch 1/1
572/572 [==============================] - 15s 25ms/step - loss: 0.0479
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-26 17:16:25.259243
1. set (Dataset 6) being trained for epoch 5 by 2019-01-26 17:16:30.453304!
Epoch 1/1
542/542 [==============================] - 14s 25ms/step - loss: 0.0802
2. set (Dataset 11) being trained for epoch 5 by 2019-01-26 17:16:49.830887!
Epoch 1/1
572/572 [==============================] - 14s 25ms/step - loss: 0.0479
3. set (Dataset 10) being trained for epoch 5 by 2019-01-26 17:17:11.553813!
Epoch 1/1
726/726 [==============================] - 18s 25ms/step - loss: 0.0579
4. set (Dataset 4) being trained for epoch 5 by 2019-01-26 17:17:37.315399!
Epoch 1/1
744/744 [==============================] - 18s 25ms/step - loss: 0.0682
5. set (Dataset 7) being trained for epoch 5 by 2019-01-26 17:18:03.395014!
Epoch 1/1
745/745 [==============================] - 19s 25ms/step - loss: 0.0616
6. set (Dataset 8) being trained for epoch 5 by 2019-01-26 17:18:30.121674!
Epoch 1/1
772/772 [==============================] - 20s 26ms/step - loss: 0.0645
7. set (Dataset 17) being trained for epoch 5 by 2019-01-26 17:18:53.627234!
Epoch 1/1
395/395 [==============================] - 10s 26ms/step - loss: 0.0692
8. set (Dataset 1) being trained for epoch 5 by 2019-01-26 17:19:08.876277!
Epoch 1/1
498/498 [==============================] - 12s 24ms/step - loss: 0.0729
9. set (Dataset 12) being trained for epoch 5 by 2019-01-26 17:19:28.352156!
Epoch 1/1
732/732 [==============================] - 18s 25ms/step - loss: 0.0513
10. set (Dataset 13) being trained for epoch 5 by 2019-01-26 17:19:51.285722!
Epoch 1/1
485/485 [==============================] - 13s 26ms/step - loss: 0.0519
11. set (Dataset 21) being trained for epoch 5 by 2019-01-26 17:20:10.199372!
Epoch 1/1
634/634 [==============================] - 17s 26ms/step - loss: 0.0980
12. set (Dataset 22) being trained for epoch 5 by 2019-01-26 17:20:33.342570!
Epoch 1/1
665/665 [==============================] - 17s 26ms/step - loss: 0.0472
13. set (Dataset 2) being trained for epoch 5 by 2019-01-26 17:20:55.709579!
Epoch 1/1
511/511 [==============================] - 13s 25ms/step - loss: 0.0634
14. set (Dataset 24) being trained for epoch 5 by 2019-01-26 17:21:13.082804!
Epoch 1/1
492/492 [==============================] - 12s 25ms/step - loss: 0.0522
15. set (Dataset 15) being trained for epoch 5 by 2019-01-26 17:21:31.715084!
Epoch 1/1
654/654 [==============================] - 16s 25ms/step - loss: 0.0655
16. set (Dataset 20) being trained for epoch 5 by 2019-01-26 17:21:53.275933!
Epoch 1/1
556/556 [==============================] - 14s 26ms/step - loss: 0.0549
17. set (Dataset 18) being trained for epoch 5 by 2019-01-26 17:22:13.521691!
Epoch 1/1
614/614 [==============================] - 16s 25ms/step - loss: 0.0795
18. set (Dataset 19) being trained for epoch 5 by 2019-01-26 17:22:34.102147!
Epoch 1/1
502/502 [==============================] - 12s 24ms/step - loss: 0.0599
19. set (Dataset 23) being trained for epoch 5 by 2019-01-26 17:22:51.896655!
Epoch 1/1
569/569 [==============================] - 14s 25ms/step - loss: 0.0876
20. set (Dataset 16) being trained for epoch 5 by 2019-01-26 17:23:15.005906!
Epoch 1/1
914/914 [==============================] - 24s 26ms/step - loss: 0.0607
Epoch 5 completed!
The subjects are trained: [(6, 'F06'), (11, 'M05'), (10, 'M04'), (4, 'F04'), (7, 'M01'), (8, 'M02'), (17, 'M
10'), (1, 'F01'), (12, 'M06'), (13, 'M07'), (21, 'F02'), (22, 'M01'), (2, 'F02'), (24, 'M14'), (15, 'F03'),
(20, 'M12'), (18, 'F05'), (19, 'M11'), (23, 'M13'), (16, 'M09')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-26_13-37-43
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-26 17:23:40.588003
For the Subject 3 (F03):
730/730 [==============================] - 11s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.11 Degree
        The absolute mean error on Yaw angle estimation: 28.69 Degree
        The absolute mean error on Roll angle estimation: 16.96 Degree
For the Subject 5 (F05):
946/946 [==============================] - 14s 15ms/step
        The absolute mean error on Pitch angle estimation: 9.79 Degree
        The absolute mean error on Yaw angle estimation: 25.86 Degree
        The absolute mean error on Roll angle estimation: 3.97 Degree
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 27.42 Degree
        The absolute mean error on Yaw angle estimation: 26.50 Degree
        The absolute mean error on Roll angle estimation: 6.38 Degree
For the Subject 14 (M08):
797/797 [==============================] - 12s 16ms/step
        The absolute mean error on Pitch angle estimation: 15.56 Degree
        The absolute mean error on Yaw angle estimation: 31.54 Degree
        The absolute mean error on Roll angle estimation: 13.50 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 15.97 Degree
        The absolute mean error on Yaw angle estimations: 28.15 Degree
        The absolute mean error on Roll angle estimations: 10.20 Degree
Exp2019-01-26_13-37-43_part6 completed!
Exp2019-01-26_13-37-43.h5 has been saved.
subject3_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:05.490411.
subject5_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:05.689277.
subject9_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:05.885095.
subject14_Exp2019-01-26_13-37-43.png has been saved by 2019-01-26 17:25:06.100548.
Model Exp2019-01-26_13-37-43 has been evaluated successfully.
Model Exp2019-01-26_13-37-43 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-26_13-37-43 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 11:51:35.266816: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 11:51:35.364688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 11:51:35.364997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 11:51:35.365012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 11:51:35.520660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 11:51:35.520686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 11:51:35.520691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 11:51:35.520869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-26_13-37-43.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model Exp2019-01-26_13-37-43_and_2019-01-27_11-51-36
All frames and annotations from 20 datasets have been read by 2019-01-27 11:51:41.303554
1. set (Dataset 22) being trained for epoch 1 by 2019-01-27 11:51:47.702222!
Epoch 1/1
2019-01-27 11:51:48.937122: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at tensor_arra
y_ops.cc:121 : Not found: Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11TensorArrayE does no
t exist.
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327,
in _do_call
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312,
in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420,
in _call_tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line
 516, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_619)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 54, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 48, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, record = record, preprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 39, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 29, in trainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_gene
rator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on
_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482,
in __call__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, i
n run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140,
in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321,
in _do_run
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340,
in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Resource __per_step_8/_tensor_arraysinput_ta_7/N10ten
sorflow11TensorArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_619)]]

Caused by op 'training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGra
dV3', defined at:
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 41, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.p
y", line 52, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 307, in load_model
    model.model._make_train_function()
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 992, in _make_tra
in_function
    loss=self.total_loss)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 445, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/optimizers.py", line 78, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2519,
in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 48
8, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 37
9, in _MaybeCompile
    return grad_fn()  # Exit early
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py", line 62
5, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py", line
 104, in _TensorArrayReadGrad
    .grad(source=grad_source, flow=flow))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
849, in grad
    return self._implementation.grad(source, flow=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
241, in grad
    handle=self._handle, source=source, flow_in=flow, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py", line
 6221, in tensor_array_grad_v3
    name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", l
ine 787, in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, i
n create_op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, i
n __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

...which was originally created as op 'dropout025/while/TensorArrayReadV3', defined at:
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
[elided 2 identical lines from previous traceback]
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/EvaluationRecorder.p
y", line 52, in loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserializ
e
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deser
ialize_keras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 522, in add
    output_tensor = layer(self.outputs[0])
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 198, in call
    unroll=False)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2771,
in rnn
    swap_memory=True)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
3202, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2940, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py", line
2877, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2754,
in _step
    current_input = input_ta.read(time)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py", line 58
, in fn
    return method(self, *args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py", line
861, in read
    return self._implementation.read(index, name=name)

NotFoundError (see above for traceback): Resource __per_step_8/_tensor_arraysinput_ta_7/N10tensorflow11Tenso
rArrayE does not exist.
         [[Node: training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArray
GradV3 = TensorArrayGradV3[_class=["loc:@dropout025/while/TensorArrayReadV3", "loc:@dropout025/while/TensorA
rrayReadV3/Enter"], source="training/Adam/gradients", _device="/job:localhost/replica:0/task:0/device:CPU:0"
](training/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter, t
raining/Adam/gradients/dropout025/while/TensorArrayReadV3_grad/TensorArrayGrad/TensorArrayGradV3/Enter_1, ^t
raining/Adam/gradients/Sub_2/_619)]]

mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 11:56:53.344926: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 11:56:53.442347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 11:56:53.442603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 11:56:53.442616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 11:56:53.597546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 11:56:53.597572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 11:56:53.597577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 11:56:53.597719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_11-56-54 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_11-56-54
All frames and annotations from 1 datasets have been read by 2019-01-27 11:56:55.212496
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 11:57:04.107940!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 561.1528 - mean_absolute_error: 18.3496
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 11:57:29.215899
1. set (Dataset 9) being trained for epoch 2 by 2019-01-27 11:57:38.100679!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 552.5623 - mean_absolute_error: 18.1862
Epoch 2 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 11:58:01.075357
1. set (Dataset 9) being trained for epoch 3 by 2019-01-27 11:58:09.943610!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 546.9199 - mean_absolute_error: 18.0749
Epoch 3 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 11:58:33.073716
1. set (Dataset 9) being trained for epoch 4 by 2019-01-27 11:58:41.959666!
Epoch 1/1
303/882 [=========>....................] - ETA: 14s - loss: 534.0697 - mean_absolute_error: 17.7895^C
Model Exp2019-01-27_11-56-54_part1 has been interrupted.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 11:59:17.787248: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 11:59:17.885464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 11:59:17.885726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 11:59:17.885738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 11:59:18.041228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 11:59:18.041254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 11:59:18.041259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 11:59:18.041394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_11-59-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_11-59-18
All frames and annotations from 1 datasets have been read by 2019-01-27 11:59:19.688533
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 11:59:28.548652!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 561.1989 - mean_absolute_error: 18.3361
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_11-59-18
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 11:59:53.249342
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 1848.72 Degree
        The absolute mean error on Yaw angle estimation: 2441.66 Degree
        The absolute mean error on Roll angle estimation: 637.17 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 1848.72 Degree
        The absolute mean error on Yaw angle estimations: 2441.66 Degree
        The absolute mean error on Roll angle estimations: 637.17 Degree
Exp2019-01-27_11-59-18_part1 completed!
Exp2019-01-27_11-59-18.h5 has been saved.
subject9_Exp2019-01-27_11-59-18.png has been saved by 2019-01-27 12:00:15.977069.
Model Exp2019-01-27_11-59-18 has been evaluated successfully.
Model Exp2019-01-27_11-59-18 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:14:24.593442: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:14:24.689153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:14:24.689455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:14:24.689470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:14:24.844834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:14:24.844858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:14:24.844863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:14:24.845042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-14-25 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-14-25
All frames and annotations from 1 datasets have been read by 2019-01-27 13:14:26.511408
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:14:35.402054!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 571.6917 - mean_absolute_error: 18.6066
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-14-25
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:15:01.310412
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 20.74 Degree
        The absolute mean error on Yaw angle estimation: 27.23 Degree
        The absolute mean error on Roll angle estimation: 7.47 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.74 Degree
        The absolute mean error on Yaw angle estimations: 27.23 Degree
        The absolute mean error on Roll angle estimations: 7.47 Degree
Exp2019-01-27_13-14-25_part1 completed!
Exp2019-01-27_13-14-25.h5 has been saved.
subject9_Exp2019-01-27_13-14-25.png has been saved by 2019-01-27 13:15:24.025377.
Model Exp2019-01-27_13-14-25 has been evaluated successfully.
Model Exp2019-01-27_13-14-25 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:25:49.065244: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:25:49.161134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:25:49.161393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:25:49.161412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:25:49.317764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:25:49.317791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:25:49.317796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:25:49.317934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-25-50 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-25-50
All frames and annotations from 1 datasets have been read by 2019-01-27 13:25:50.872564
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:25:59.772653!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 568.6984 - mean_absolute_error: 18.4823
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-25-50
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:26:18.258004
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 20.06 Degree
        The absolute mean error on Yaw angle estimation: 27.29 Degree
        The absolute mean error on Roll angle estimation: 7.17 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.06 Degree
        The absolute mean error on Yaw angle estimations: 27.29 Degree
        The absolute mean error on Roll angle estimations: 7.17 Degree
Exp2019-01-27_13-25-50_part1 completed!
Exp2019-01-27_13-25-50.h5 has been saved.
subject9_Exp2019-01-27_13-25-50.png has been saved by 2019-01-27 13:26:35.981633.
Model Exp2019-01-27_13-25-50 has been evaluated successfully.
Model Exp2019-01-27_13-25-50 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:35:43.280212: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:35:43.378279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:35:43.378539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:35:43.378554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:35:43.533794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:35:43.533820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:35:43.533828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:35:43.533967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-35-44 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
1000_2019-01-27_13-35-44
All frames and annotations from 1 datasets have been read by 2019-01-27 13:35:45.097587
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:35:53.993757!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2081
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
001000_2019-01-27_13-35-44
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:36:12.285508
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.20 Degree
        The absolute mean error on Yaw angle estimation: 34.32 Degree
        The absolute mean error on Roll angle estimation: 10.73 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.20 Degree
        The absolute mean error on Yaw angle estimations: 34.32 Degree
        The absolute mean error on Roll angle estimations: 10.73 Degree
Exp2019-01-27_13-35-44_part1 completed!
Exp2019-01-27_13-35-44.h5 has been saved.
subject9_Exp2019-01-27_13-35-44.png has been saved by 2019-01-27 13:36:30.003511.
Model Exp2019-01-27_13-35-44 has been evaluated successfully.
Model Exp2019-01-27_13-35-44 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:36:51.400187: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:36:51.480776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:36:51.481041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:36:51.481056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:36:51.636542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:36:51.636570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:36:51.636579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:36:51.636720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-36-52 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.01
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.01
0000_2019-01-27_13-36-52
All frames and annotations from 1 datasets have been read by 2019-01-27 13:36:53.171766
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:37:02.060342!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.2109
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
010000_2019-01-27_13-36-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:37:20.526908
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.27 Degree
        The absolute mean error on Yaw angle estimation: 26.28 Degree
        The absolute mean error on Roll angle estimation: 7.14 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 18.27 Degree
        The absolute mean error on Yaw angle estimations: 26.28 Degree
        The absolute mean error on Roll angle estimations: 7.14 Degree
Exp2019-01-27_13-36-52_part1 completed!
Exp2019-01-27_13-36-52.h5 has been saved.
subject9_Exp2019-01-27_13-36-52.png has been saved by 2019-01-27 13:37:38.307345.
Model Exp2019-01-27_13-36-52 has been evaluated successfully.
Model Exp2019-01-27_13-36-52 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:39:31.969676: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:39:32.067036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:39:32.067313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:39:32.067325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:39:32.222315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:39:32.222340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:39:32.222345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:39:32.222488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-39-33 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-39-33
All frames and annotations from 1 datasets have been read by 2019-01-27 13:39:33.866209
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:39:42.750884!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.3898
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-39-33
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:40:07.461262
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 17.67 Degree
        The absolute mean error on Yaw angle estimation: 33.01 Degree
        The absolute mean error on Roll angle estimation: 11.71 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.67 Degree
        The absolute mean error on Yaw angle estimations: 33.01 Degree
        The absolute mean error on Roll angle estimations: 11.71 Degree
Exp2019-01-27_13-39-33_part1 completed!
Exp2019-01-27_13-39-33.h5 has been saved.
subject9_Exp2019-01-27_13-39-33.png has been saved by 2019-01-27 13:40:30.273942.
Model Exp2019-01-27_13-39-33 has been evaluated successfully.
Model Exp2019-01-27_13-39-33 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:41:29.649761: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:41:29.748526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:41:29.748787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:41:29.748804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:41:29.904282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:41:29.904306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:41:29.904311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:41:29.904456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-41-30 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0010_2019-01-27_13-41-30
All frames and annotations from 1 datasets have been read by 2019-01-27 13:41:31.534768
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:41:40.407342!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2185
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000010_2019-01-27_13-41-30
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:42:05.188256
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.48 Degree
        The absolute mean error on Yaw angle estimation: 19.37 Degree
        The absolute mean error on Roll angle estimation: 11.52 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.48 Degree
        The absolute mean error on Yaw angle estimations: 19.37 Degree
        The absolute mean error on Roll angle estimations: 11.52 Degree
Exp2019-01-27_13-41-30_part1 completed!
Exp2019-01-27_13-41-30.h5 has been saved.
subject9_Exp2019-01-27_13-41-30.png has been saved by 2019-01-27 13:42:27.969351.
Model Exp2019-01-27_13-41-30 has been evaluated successfully.
Model Exp2019-01-27_13-41-30 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:43:19.044773: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:43:19.143056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:43:19.143316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:43:19.143334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:43:19.298473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:43:19.298499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:43:19.298504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:43:19.298643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-43-20 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.000001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0001_2019-01-27_13-43-20
All frames and annotations from 1 datasets have been read by 2019-01-27 13:43:20.995841
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:43:29.888634!
Epoch 1/1
882/882 [==============================] - 24s 27ms/step - loss: 0.3066
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000001_2019-01-27_13-43-20
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:43:55.435424
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 16.65 Degree
        The absolute mean error on Yaw angle estimation: 58.86 Degree
        The absolute mean error on Roll angle estimation: 13.83 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 16.65 Degree
        The absolute mean error on Yaw angle estimations: 58.86 Degree
        The absolute mean error on Roll angle estimations: 13.83 Degree
Exp2019-01-27_13-43-20_part1 completed!
Exp2019-01-27_13-43-20.h5 has been saved.
subject9_Exp2019-01-27_13-43-20.png has been saved by 2019-01-27 13:44:18.221699.
Model Exp2019-01-27_13-43-20 has been evaluated successfully.
Model Exp2019-01-27_13-43-20 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:44:41.602901: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:44:41.682597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:44:41.682852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:44:41.682865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:44:41.837607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:44:41.837631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:44:41.837636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:44:41.837777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-44-42 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-44-42
All frames and annotations from 1 datasets have been read by 2019-01-27 13:44:43.446538
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:44:52.329254!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2836
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-44-42
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:45:16.772193
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 13.25 Degree
        The absolute mean error on Yaw angle estimation: 22.60 Degree
        The absolute mean error on Roll angle estimation: 8.22 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 13.25 Degree
        The absolute mean error on Yaw angle estimations: 22.60 Degree
        The absolute mean error on Roll angle estimations: 8.22 Degree
Exp2019-01-27_13-44-42_part1 completed!
Exp2019-01-27_13-44-42.h5 has been saved.
subject9_Exp2019-01-27_13-44-42.png has been saved by 2019-01-27 13:45:39.565177.
Model Exp2019-01-27_13-44-42 has been evaluated successfully.
Model Exp2019-01-27_13-44-42 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:46:18.055774: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:46:18.153497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:46:18.153762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:46:18.153775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:46:18.309810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:46:18.309837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:46:18.309842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:46:18.309980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-46-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-46-18
All frames and annotations from 1 datasets have been read by 2019-01-27 13:46:19.828002
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:46:28.727202!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1927
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-46-18
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:46:47.092500
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.64 Degree
        The absolute mean error on Yaw angle estimation: 19.06 Degree
        The absolute mean error on Roll angle estimation: 10.65 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.64 Degree
        The absolute mean error on Yaw angle estimations: 19.06 Degree
        The absolute mean error on Roll angle estimations: 10.65 Degree
Exp2019-01-27_13-46-18_part1 completed!
Exp2019-01-27_13-46-18.h5 has been saved.
subject9_Exp2019-01-27_13-46-18.png has been saved by 2019-01-27 13:47:04.915175.
Model Exp2019-01-27_13-46-18 has been evaluated successfully.
Model Exp2019-01-27_13-46-18 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:54:21.987780: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:54:22.084073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:54:22.084330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:54:22.084342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:54:22.239653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:54:22.239680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:54:22.239685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:54:22.239831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-54-23 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0100_2019-01-27_13-54-23
All frames and annotations from 1 datasets have been read by 2019-01-27 13:54:23.874366
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:54:32.746147!
Epoch 1/1
882/882 [==============================] - 23s 27ms/step - loss: 0.2940
Epoch 1 completed!
Exp2019-01-27_13-54-23_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000100_2019-01-27_13-54-23
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:54:58.244899
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 32.75 Degree
        The absolute mean error on Yaw angle estimation: 47.04 Degree
        The absolute mean error on Roll angle estimation: 7.60 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 32.75 Degree
        The absolute mean error on Yaw angle estimations: 47.04 Degree
        The absolute mean error on Roll angle estimations: 7.60 Degree
Exp2019-01-27_13-54-23_part1 completed!
Exp2019-01-27_13-54-23.h5 has been saved.
subject9_Exp2019-01-27_13-54-23.png has been saved by 2019-01-27 13:55:20.896750.
Model Exp2019-01-27_13-54-23 has been evaluated successfully.
Model Exp2019-01-27_13-54-23 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:55:58.877868: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:55:58.975696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:55:58.975951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:55:58.975964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:55:59.131706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:55:59.131732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:55:59.131737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:55:59.131875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-55-59 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 1
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.00
0010_2019-01-27_13-55-59
All frames and annotations from 1 datasets have been read by 2019-01-27 13:56:00.769742
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:56:09.636566!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2676
Epoch 1 completed!
Exp2019-01-27_13-55-59_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs1_AdamOpt_lr-0.
000010_2019-01-27_13-55-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:56:35.010867
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 32.57 Degree
        The absolute mean error on Yaw angle estimation: 22.06 Degree
        The absolute mean error on Roll angle estimation: 20.35 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 32.57 Degree
        The absolute mean error on Yaw angle estimations: 22.06 Degree
        The absolute mean error on Roll angle estimations: 20.35 Degree
Exp2019-01-27_13-55-59_part1 completed!
Exp2019-01-27_13-55-59.h5 has been saved.
subject9_Exp2019-01-27_13-55-59.png has been saved by 2019-01-27 13:56:57.626130.
Model Exp2019-01-27_13-55-59 has been evaluated successfully.
Model Exp2019-01-27_13-55-59 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 13:57:55.885697: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 13:57:55.983644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 13:57:55.983905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 13:57:55.983917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 13:57:56.138800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 13:57:56.138826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 13:57:56.138831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 13:57:56.138966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_13-57-56 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
dropout025_conv (TimeDistrib (1, 1, 4096)              0
_________________________________________________________________
fc1024 (TimeDistributed)     (1, 1, 1024)              4195328
_________________________________________________________________
dropout025 (TimeDistributed) (1, 1, 1024)              0
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 3075
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_3 (Dense)              (1, 3)                    63
=================================================================
Total params: 138,460,930
Trainable params: 4,200,386
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.00001
in_epochs = 1
out_epochs = 10
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 13:57:57.775817
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:58:06.669573!
Epoch 1/1
882/882 [==============================] - 23s 26ms/step - loss: 0.2257
Epoch 1 completed!
Exp2019-01-27_13-57-56_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:58:32.017535
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.54 Degree
        The absolute mean error on Yaw angle estimation: 23.62 Degree
        The absolute mean error on Roll angle estimation: 14.57 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 12.54 Degree
        The absolute mean error on Yaw angle estimations: 23.62 Degree
        The absolute mean error on Roll angle estimations: 14.57 Degree
Exp2019-01-27_13-57-56_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 13:58:54.976084
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:59:03.779226!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.2100
Epoch 1 completed!
Exp2019-01-27_13-57-56_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 13:59:26.953275
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.54 Degree
        The absolute mean error on Yaw angle estimation: 21.65 Degree
        The absolute mean error on Roll angle estimation: 15.12 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.54 Degree
        The absolute mean error on Yaw angle estimations: 21.65 Degree
        The absolute mean error on Roll angle estimations: 15.12 Degree
Exp2019-01-27_13-57-56_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 13:59:49.888616
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 13:59:58.688990!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.2033
Epoch 1 completed!
Exp2019-01-27_13-57-56_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:00:22.263114
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.26 Degree
        The absolute mean error on Yaw angle estimation: 20.37 Degree
        The absolute mean error on Roll angle estimation: 13.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.26 Degree
        The absolute mean error on Yaw angle estimations: 20.37 Degree
        The absolute mean error on Roll angle estimations: 13.37 Degree
Exp2019-01-27_13-57-56_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:00:45.154755
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:00:54.002816!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1953
Epoch 1 completed!
Exp2019-01-27_13-57-56_part4.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:01:16.972010
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.83 Degree
        The absolute mean error on Yaw angle estimation: 19.05 Degree
        The absolute mean error on Roll angle estimation: 9.71 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.83 Degree
        The absolute mean error on Yaw angle estimations: 19.05 Degree
        The absolute mean error on Roll angle estimations: 9.71 Degree
Exp2019-01-27_13-57-56_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:01:39.820111
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:01:48.623665!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1938
Epoch 1 completed!
Exp2019-01-27_13-57-56_part5.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:02:11.496574
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.23 Degree
        The absolute mean error on Yaw angle estimation: 20.05 Degree
        The absolute mean error on Roll angle estimation: 7.67 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 12.23 Degree
        The absolute mean error on Yaw angle estimations: 20.05 Degree
        The absolute mean error on Roll angle estimations: 7.67 Degree
Exp2019-01-27_13-57-56_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:02:34.365482
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:02:43.210671!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1874
Epoch 1 completed!
Exp2019-01-27_13-57-56_part6.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:03:06.544020
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 10.95 Degree
        The absolute mean error on Yaw angle estimation: 17.27 Degree
        The absolute mean error on Roll angle estimation: 7.98 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.95 Degree
        The absolute mean error on Yaw angle estimations: 17.27 Degree
        The absolute mean error on Roll angle estimations: 7.98 Degree
Exp2019-01-27_13-57-56_part6 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:03:29.498885
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:03:38.322744!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1814
Epoch 1 completed!
Exp2019-01-27_13-57-56_part7.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:04:01.518257
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 12.15 Degree
        The absolute mean error on Yaw angle estimation: 17.84 Degree
        The absolute mean error on Roll angle estimation: 14.80 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 12.15 Degree
        The absolute mean error on Yaw angle estimations: 17.84 Degree
        The absolute mean error on Roll angle estimations: 14.80 Degree
Exp2019-01-27_13-57-56_part7 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:04:24.432853
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:04:33.233569!
Epoch 1/1
882/882 [==============================] - 21s 24ms/step - loss: 0.1780
Epoch 1 completed!
Exp2019-01-27_13-57-56_part8.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:04:55.710285
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.23 Degree
        The absolute mean error on Yaw angle estimation: 17.05 Degree
        The absolute mean error on Roll angle estimation: 5.60 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.23 Degree
        The absolute mean error on Yaw angle estimations: 17.05 Degree
        The absolute mean error on Roll angle estimations: 5.60 Degree
Exp2019-01-27_13-57-56_part8 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:05:18.610510
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:05:27.420508!
Epoch 1/1
882/882 [==============================] - 21s 24ms/step - loss: 0.1765
Epoch 1 completed!
Exp2019-01-27_13-57-56_part9.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:05:50.080239
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.06 Degree
        The absolute mean error on Yaw angle estimation: 16.22 Degree
        The absolute mean error on Roll angle estimation: 10.89 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.06 Degree
        The absolute mean error on Yaw angle estimations: 16.22 Degree
        The absolute mean error on Roll angle estimations: 10.89 Degree
Exp2019-01-27_13-57-56_part9 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00010_2019-01-27_13-57-56
All frames and annotations from 1 datasets have been read by 2019-01-27 14:06:12.968372
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:06:21.765093!
Epoch 1/1
882/882 [==============================] - 22s 25ms/step - loss: 0.1763
Epoch 1 completed!
Exp2019-01-27_13-57-56_part10.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000010_2019-01-27_13-57-56
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:06:44.596901
For the Subject 9 (M03):
882/882 [==============================] - 13s 15ms/step
        The absolute mean error on Pitch angle estimation: 11.00 Degree
        The absolute mean error on Yaw angle estimation: 15.07 Degree
        The absolute mean error on Roll angle estimation: 8.79 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.00 Degree
        The absolute mean error on Yaw angle estimations: 15.07 Degree
        The absolute mean error on Roll angle estimations: 8.79 Degree
Exp2019-01-27_13-57-56_part10 completed!
Exp2019-01-27_13-57-56.h5 has been saved.
subject9_Exp2019-01-27_13-57-56.png has been saved by 2019-01-27 14:07:07.085305.
Model Exp2019-01-27_13-57-56 has been evaluated successfully.
Model Exp2019-01-27_13-57-56 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:12:08.064165: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:12:08.161692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 14:12:08.161949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:12:08.161962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 14:12:08.317579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 14:12:08.317603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:12:08.317607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:12:08.317750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-12-09 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 10
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:12:09.868808
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:12:18.758786!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1528
Epoch 1 completed!
Exp2019-01-27_14-12-09_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:12:37.055194
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.56 Degree
        The absolute mean error on Yaw angle estimation: 11.10 Degree
        The absolute mean error on Roll angle estimation: 3.78 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.56 Degree
        The absolute mean error on Yaw angle estimations: 11.10 Degree
        The absolute mean error on Roll angle estimations: 3.78 Degree
Exp2019-01-27_14-12-09_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:12:55.111877
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:13:03.981355!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0974
Epoch 1 completed!
Exp2019-01-27_14-12-09_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:13:20.712950
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.91 Degree
        The absolute mean error on Yaw angle estimation: 29.45 Degree
        The absolute mean error on Roll angle estimation: 4.58 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.91 Degree
        The absolute mean error on Yaw angle estimations: 29.45 Degree
        The absolute mean error on Roll angle estimations: 4.58 Degree
Exp2019-01-27_14-12-09_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:13:38.719507
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:13:47.612435!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0982
Epoch 1 completed!
Exp2019-01-27_14-12-09_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:14:04.328626
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.08 Degree
        The absolute mean error on Yaw angle estimation: 6.44 Degree
        The absolute mean error on Roll angle estimation: 3.11 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 4.08 Degree
        The absolute mean error on Yaw angle estimations: 6.44 Degree
        The absolute mean error on Roll angle estimations: 3.11 Degree
Exp2019-01-27_14-12-09_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:14:22.341498
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:14:31.227592!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 0.0778
Epoch 1 completed!
Exp2019-01-27_14-12-09_part4.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:14:47.746107
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.09 Degree
        The absolute mean error on Yaw angle estimation: 4.83 Degree
        The absolute mean error on Roll angle estimation: 2.56 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.09 Degree
        The absolute mean error on Yaw angle estimations: 4.83 Degree
        The absolute mean error on Roll angle estimations: 2.56 Degree
Exp2019-01-27_14-12-09_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:15:05.793326
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:15:14.667683!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 0.0697
Epoch 1 completed!
Exp2019-01-27_14-12-09_part5.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:15:31.070599
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 4.07 Degree
        The absolute mean error on Yaw angle estimation: 4.55 Degree
        The absolute mean error on Roll angle estimation: 2.26 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 4.07 Degree
        The absolute mean error on Yaw angle estimations: 4.55 Degree
        The absolute mean error on Roll angle estimations: 2.26 Degree
Exp2019-01-27_14-12-09_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:15:49.091542
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:15:57.967979!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0669
Epoch 1 completed!
Exp2019-01-27_14-12-09_part6.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:16:14.901958
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.64 Degree
        The absolute mean error on Yaw angle estimation: 4.86 Degree
        The absolute mean error on Roll angle estimation: 1.93 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.64 Degree
        The absolute mean error on Yaw angle estimations: 4.86 Degree
        The absolute mean error on Roll angle estimations: 1.93 Degree
Exp2019-01-27_14-12-09_part6 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:16:32.937169
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:16:41.816754!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0619
Epoch 1 completed!
Exp2019-01-27_14-12-09_part7.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:16:58.759968
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.21 Degree
        The absolute mean error on Yaw angle estimation: 6.93 Degree
        The absolute mean error on Roll angle estimation: 2.26 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.21 Degree
        The absolute mean error on Yaw angle estimations: 6.93 Degree
        The absolute mean error on Roll angle estimations: 2.26 Degree
Exp2019-01-27_14-12-09_part7 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:17:16.815489
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:17:25.690155!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0575
Epoch 1 completed!
Exp2019-01-27_14-12-09_part8.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:17:42.661223
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.90 Degree
        The absolute mean error on Yaw angle estimation: 3.49 Degree
        The absolute mean error on Roll angle estimation: 2.17 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.90 Degree
        The absolute mean error on Yaw angle estimations: 3.49 Degree
        The absolute mean error on Roll angle estimations: 2.17 Degree
Exp2019-01-27_14-12-09_part8 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:18:00.695985
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:18:09.561088!
Epoch 1/1
882/882 [==============================] - 15s 18ms/step - loss: 0.0564
Epoch 1 completed!
Exp2019-01-27_14-12-09_part9.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:18:26.137622
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.13 Degree
        The absolute mean error on Yaw angle estimation: 3.93 Degree
        The absolute mean error on Roll angle estimation: 2.71 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.13 Degree
        The absolute mean error on Yaw angle estimations: 3.93 Degree
        The absolute mean error on Roll angle estimations: 2.71 Degree
Exp2019-01-27_14-12-09_part9 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-12-09
All frames and annotations from 1 datasets have been read by 2019-01-27 14:18:44.216116
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:18:53.085128!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0537
Epoch 1 completed!
Exp2019-01-27_14-12-09_part10.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-12-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:19:10.244000
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 3.78 Degree
        The absolute mean error on Yaw angle estimation: 6.33 Degree
        The absolute mean error on Roll angle estimation: 1.94 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 3.78 Degree
        The absolute mean error on Yaw angle estimations: 6.33 Degree
        The absolute mean error on Roll angle estimations: 1.94 Degree
Exp2019-01-27_14-12-09_part10 completed!
Exp2019-01-27_14-12-09.h5 has been saved.
subject9_Exp2019-01-27_14-12-09.png has been saved by 2019-01-27 14:19:27.955675.
Model Exp2019-01-27_14-12-09 has been evaluated successfully.
Model Exp2019-01-27_14-12-09 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:22:50.513479: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:22:50.650116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 14:22:50.650383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:22:50.650398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 14:22:50.805215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 14:22:50.805239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:22:50.805244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:22:50.805379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-22-51 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
fc3 (TimeDistributed)        (1, 1, 3)                 12291
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   1920
_________________________________________________________________
dense_2 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,274,818
Trainable params: 14,274
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 10
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:22:52.336614
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:23:01.233406!
Epoch 1/1
882/882 [==============================] - 17s 20ms/step - loss: 0.1814
Epoch 1 completed!
Exp2019-01-27_14-22-51_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:23:20.524612
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.54 Degree
        The absolute mean error on Yaw angle estimation: 24.20 Degree
        The absolute mean error on Roll angle estimation: 7.94 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.54 Degree
        The absolute mean error on Yaw angle estimations: 24.20 Degree
        The absolute mean error on Roll angle estimations: 7.94 Degree
Exp2019-01-27_14-22-51_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:23:38.644955
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:23:47.535765!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1530
Epoch 1 completed!
Exp2019-01-27_14-22-51_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:24:04.341496
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.36 Degree
        The absolute mean error on Yaw angle estimation: 11.74 Degree
        The absolute mean error on Roll angle estimation: 8.61 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 10.36 Degree
        The absolute mean error on Yaw angle estimations: 11.74 Degree
        The absolute mean error on Roll angle estimations: 8.61 Degree
Exp2019-01-27_14-22-51_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:24:22.336096
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:24:31.224476!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1379
Epoch 1 completed!
Exp2019-01-27_14-22-51_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:24:48.042813
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.13 Degree
        The absolute mean error on Yaw angle estimation: 14.27 Degree
        The absolute mean error on Roll angle estimation: 5.87 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.13 Degree
        The absolute mean error on Yaw angle estimations: 14.27 Degree
        The absolute mean error on Roll angle estimations: 5.87 Degree
Exp2019-01-27_14-22-51_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:25:06.099666
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:25:14.967599!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1240
Epoch 1 completed!
Exp2019-01-27_14-22-51_part4.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:25:31.871593
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.11 Degree
        The absolute mean error on Yaw angle estimation: 14.73 Degree
        The absolute mean error on Roll angle estimation: 5.53 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.11 Degree
        The absolute mean error on Yaw angle estimations: 14.73 Degree
        The absolute mean error on Roll angle estimations: 5.53 Degree
Exp2019-01-27_14-22-51_part4 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:25:49.948082
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:25:58.877512!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1127
Epoch 1 completed!
Exp2019-01-27_14-22-51_part5.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:26:15.707754
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 13.96 Degree
        The absolute mean error on Yaw angle estimation: 13.96 Degree
        The absolute mean error on Roll angle estimation: 9.96 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 13.96 Degree
        The absolute mean error on Yaw angle estimations: 13.96 Degree
        The absolute mean error on Roll angle estimations: 9.96 Degree
Exp2019-01-27_14-22-51_part5 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:26:33.713023
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:26:42.584351!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1134
Epoch 1 completed!
Exp2019-01-27_14-22-51_part6.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:26:59.591930
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.31 Degree
        The absolute mean error on Yaw angle estimation: 13.77 Degree
        The absolute mean error on Roll angle estimation: 5.05 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 8.31 Degree
        The absolute mean error on Yaw angle estimations: 13.77 Degree
        The absolute mean error on Roll angle estimations: 5.05 Degree
Exp2019-01-27_14-22-51_part6 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:27:17.610320
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:27:26.491399!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1022
Epoch 1 completed!
Exp2019-01-27_14-22-51_part7.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:27:43.732378
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.73 Degree
        The absolute mean error on Yaw angle estimation: 12.42 Degree
        The absolute mean error on Roll angle estimation: 4.29 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.73 Degree
        The absolute mean error on Yaw angle estimations: 12.42 Degree
        The absolute mean error on Roll angle estimations: 4.29 Degree
Exp2019-01-27_14-22-51_part7 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:28:01.799529
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:28:10.669395!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1000
Epoch 1 completed!
Exp2019-01-27_14-22-51_part8.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:28:28.011063
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 11.97 Degree
        The absolute mean error on Yaw angle estimation: 10.27 Degree
        The absolute mean error on Roll angle estimation: 6.08 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.97 Degree
        The absolute mean error on Yaw angle estimations: 10.27 Degree
        The absolute mean error on Roll angle estimations: 6.08 Degree
Exp2019-01-27_14-22-51_part8 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:28:46.030915
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:28:54.916290!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0954
Epoch 1 completed!
Exp2019-01-27_14-22-51_part9.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:29:12.107028
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 7.79 Degree
        The absolute mean error on Yaw angle estimation: 12.49 Degree
        The absolute mean error on Roll angle estimation: 5.46 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 7.79 Degree
        The absolute mean error on Yaw angle estimations: 12.49 Degree
        The absolute mean error on Roll angle estimations: 5.46 Degree
Exp2019-01-27_14-22-51_part9 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0.0
00100_2019-01-27_14-22-51
All frames and annotations from 1 datasets have been read by 2019-01-27 14:29:30.425215
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:29:39.305690!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0940
Epoch 1 completed!
Exp2019-01-27_14-22-51_part10.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs10_AdamOpt_lr-0
.000100_2019-01-27_14-22-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:29:56.096427
^[[B
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 11.99 Degree
        The absolute mean error on Yaw angle estimation: 11.38 Degree
        The absolute mean error on Roll angle estimation: 6.25 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 11.99 Degree
        The absolute mean error on Yaw angle estimations: 11.38 Degree
        The absolute mean error on Roll angle estimations: 6.25 Degree
Exp2019-01-27_14-22-51_part10 completed!
Exp2019-01-27_14-22-51.h5 has been saved.
subject9_Exp2019-01-27_14-22-51.png has been saved by 2019-01-27 14:30:13.810628.
Model Exp2019-01-27_14-22-51 has been evaluated successfully.
Model Exp2019-01-27_14-22-51 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:36:59.292172: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:36:59.389454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 14:36:59.389717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:36:59.389730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 14:36:59.545699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 14:36:59.545725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:36:59.545730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:36:59.545869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-37-00 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-37-00
All frames and annotations from 1 datasets have been read by 2019-01-27 14:37:01.097217
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:37:10.135526!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1785
Epoch 1 completed!
Exp2019-01-27_14-37-00_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-37-00
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:37:28.701735
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 13.42 Degree
        The absolute mean error on Yaw angle estimation: 25.46 Degree
        The absolute mean error on Roll angle estimation: 6.02 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 13.42 Degree
        The absolute mean error on Yaw angle estimations: 25.46 Degree
        The absolute mean error on Roll angle estimations: 6.02 Degree
Exp2019-01-27_14-37-00_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-37-00
All frames and annotations from 1 datasets have been read by 2019-01-27 14:37:46.723326
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:37:55.599698!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1463
Epoch 1 completed!
Exp2019-01-27_14-37-00_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-37-00
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:38:12.734572
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.80 Degree
        The absolute mean error on Yaw angle estimation: 20.47 Degree
        The absolute mean error on Roll angle estimation: 4.20 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 8.80 Degree
        The absolute mean error on Yaw angle estimations: 20.47 Degree
        The absolute mean error on Roll angle estimations: 4.20 Degree
Exp2019-01-27_14-37-00_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-37-00
All frames and annotations from 1 datasets have been read by 2019-01-27 14:38:30.715357
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:38:39.589163!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1358
Epoch 1 completed!
Exp2019-01-27_14-37-00_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-37-00
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:38:56.811656
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 9.34 Degree
        The absolute mean error on Yaw angle estimation: 15.15 Degree
        The absolute mean error on Roll angle estimation: 3.78 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 9.34 Degree
        The absolute mean error on Yaw angle estimations: 15.15 Degree
        The absolute mean error on Roll angle estimations: 3.78 Degree
Exp2019-01-27_14-37-00_part3 completed!
Exp2019-01-27_14-37-00.h5 has been saved.
subject9_Exp2019-01-27_14-37-00.png has been saved by 2019-01-27 14:39:14.497177.
Model Exp2019-01-27_14-37-00 has been evaluated successfully.
Model Exp2019-01-27_14-37-00 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 14:44:54.499002: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 14:44:54.595898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 14:44:54.596158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 14:44:54.596174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 14:44:54.750996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 14:44:54.751022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 14:44:54.751027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 14:44:54.751169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_14-44-55 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-44-55
All frames and annotations from 1 datasets have been read by 2019-01-27 14:44:56.326835
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:45:05.231495!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 18.0210
Epoch 1 completed!
Exp2019-01-27_14-44-55_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-44-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:45:23.834406
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 17.95 Degree
        The absolute mean error on Yaw angle estimation: 27.62 Degree
        The absolute mean error on Roll angle estimation: 6.91 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 17.95 Degree
        The absolute mean error on Yaw angle estimations: 27.62 Degree
        The absolute mean error on Roll angle estimations: 6.91 Degree
Exp2019-01-27_14-44-55_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-44-55
All frames and annotations from 1 datasets have been read by 2019-01-27 14:45:41.911863
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:45:50.815900!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 15.1474
Epoch 1 completed!
Exp2019-01-27_14-44-55_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-44-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:46:07.944194
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.33 Degree
        The absolute mean error on Yaw angle estimation: 51.37 Degree
        The absolute mean error on Roll angle estimation: 11.31 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 24.33 Degree
        The absolute mean error on Yaw angle estimations: 51.37 Degree
        The absolute mean error on Roll angle estimations: 11.31 Degree
Exp2019-01-27_14-44-55_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_14-44-55
All frames and annotations from 1 datasets have been read by 2019-01-27 14:46:25.939242
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 14:46:34.826481!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 14.7312
Epoch 1 completed!
Exp2019-01-27_14-44-55_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_14-44-55
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 14:46:51.759112
7For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.61 Degree
        The absolute mean error on Yaw angle estimation: 24.38 Degree
        The absolute mean error on Roll angle estimation: 9.90 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.61 Degree
        The absolute mean error on Yaw angle estimations: 24.38 Degree
        The absolute mean error on Roll angle estimations: 9.90 Degree
Exp2019-01-27_14-44-55_part3 completed!
Exp2019-01-27_14-44-55.h5 has been saved.
subject9_Exp2019-01-27_14-44-55.png has been saved by 2019-01-27 14:47:09.458016.
Model Exp2019-01-27_14-44-55 has been evaluated successfully.
Model Exp2019-01-27_14-44-55 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:06:26.706710: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:06:26.804736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:06:26.805046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:06:26.805060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:06:26.960746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:06:26.960773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:06:26.960778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:06:26.960958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-06-27 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-06-27
All frames and annotations from 1 datasets have been read by 2019-01-27 17:06:28.499394
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:06:37.386609!
Epoch 1/1
882/882 [==============================] - 16s 19ms/step - loss: 0.2803
Epoch 1 completed!
Exp2019-01-27_17-06-27_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-06-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:06:55.392619
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.37 Degree
        The absolute mean error on Yaw angle estimation: 27.08 Degree
        The absolute mean error on Roll angle estimation: 7.21 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 18.37 Degree
        The absolute mean error on Yaw angle estimations: 27.08 Degree
        The absolute mean error on Roll angle estimations: 7.21 Degree
Exp2019-01-27_17-06-27_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-06-27
All frames and annotations from 1 datasets have been read by 2019-01-27 17:07:13.549917
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:07:22.460788!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.1686
Epoch 1 completed!
Exp2019-01-27_17-06-27_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-06-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:07:39.313559
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 15.90 Degree
        The absolute mean error on Yaw angle estimation: 26.35 Degree
        The absolute mean error on Roll angle estimation: 6.72 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 15.90 Degree
        The absolute mean error on Yaw angle estimations: 26.35 Degree
        The absolute mean error on Roll angle estimations: 6.72 Degree
Exp2019-01-27_17-06-27_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-06-27
All frames and annotations from 1 datasets have been read by 2019-01-27 17:07:57.303622
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:08:06.208615!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 0.3985
Epoch 1 completed!
Exp2019-01-27_17-06-27_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-06-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:08:22.727079
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 96.42 Degree
        The absolute mean error on Yaw angle estimation: 24.90 Degree
        The absolute mean error on Roll angle estimation: 20.49 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 96.42 Degree
        The absolute mean error on Yaw angle estimations: 24.90 Degree
        The absolute mean error on Roll angle estimations: 20.49 Degree
Exp2019-01-27_17-06-27_part3 completed!
Exp2019-01-27_17-06-27.h5 has been saved.
subject9_Exp2019-01-27_17-06-27.png has been saved by 2019-01-27 17:08:40.336369.
Model Exp2019-01-27_17-06-27 has been evaluated successfully.
Model Exp2019-01-27_17-06-27 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:11:07.981136: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:11:08.079072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:11:08.079329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:11:08.079342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:11:08.234938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:11:08.234964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:11:08.234972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:11:08.235112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-11-08 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-11-08
All frames and annotations from 1 datasets have been read by 2019-01-27 17:11:09.772831
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:11:18.668204!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 423.8770
Epoch 1 completed!
Exp2019-01-27_17-11-08_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-11-08
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:11:37.070157
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 82.24 Degree
        The absolute mean error on Yaw angle estimation: 161.83 Degree
        The absolute mean error on Roll angle estimation: 43.80 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 82.24 Degree
        The absolute mean error on Yaw angle estimations: 161.83 Degree
        The absolute mean error on Roll angle estimations: 43.80 Degree
Exp2019-01-27_17-11-08_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-11-08
All frames and annotations from 1 datasets have been read by 2019-01-27 17:11:55.115036
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:12:04.009826!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 268.4798
Epoch 1 completed!
Exp2019-01-27_17-11-08_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-11-08
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:12:20.789575
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5466.06 Degree
        The absolute mean error on Yaw angle estimation: 2227.08 Degree
        The absolute mean error on Roll angle estimation: 8169.04 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5466.06 Degree
        The absolute mean error on Yaw angle estimations: 2227.08 Degree
        The absolute mean error on Roll angle estimations: 8169.04 Degree
Exp2019-01-27_17-11-08_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-11-08
All frames and annotations from 1 datasets have been read by 2019-01-27 17:12:38.850034
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:12:47.740026!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 182.1086
Epoch 1 completed!
Exp2019-01-27_17-11-08_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-11-08
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:13:04.144654
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 21.67 Degree
        The absolute mean error on Yaw angle estimation: 53.25 Degree
        The absolute mean error on Roll angle estimation: 13.67 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 21.67 Degree
        The absolute mean error on Yaw angle estimations: 53.25 Degree
        The absolute mean error on Roll angle estimations: 13.67 Degree
Exp2019-01-27_17-11-08_part3 completed!
Exp2019-01-27_17-11-08.h5 has been saved.
subject9_Exp2019-01-27_17-11-08.png has been saved by 2019-01-27 17:13:21.784073.
Model Exp2019-01-27_17-11-08 has been evaluated successfully.
Model Exp2019-01-27_17-11-08 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:13:58.513362: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:13:58.596645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:13:58.596958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:13:58.596973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:13:58.752296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:13:58.752321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:13:58.752326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:13:58.752507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-13-59 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-13-59
All frames and annotations from 1 datasets have been read by 2019-01-27 17:14:00.270949
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:14:09.169790!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 5111.3121 - mean_absolute_error: 58.9947
Epoch 1 completed!
Exp2019-01-27_17-13-59_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-13-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:14:27.554966
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 258.08 Degree
        The absolute mean error on Yaw angle estimation: 154.25 Degree
        The absolute mean error on Roll angle estimation: 200.68 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 258.08 Degree
        The absolute mean error on Yaw angle estimations: 154.25 Degree
        The absolute mean error on Roll angle estimations: 200.68 Degree
Exp2019-01-27_17-13-59_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-13-59
All frames and annotations from 1 datasets have been read by 2019-01-27 17:14:45.661435
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:14:54.559045!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 3502.2638 - mean_absolute_error: 45.6296
Epoch 1 completed!
Exp2019-01-27_17-13-59_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-13-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:15:11.027305
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 222.44 Degree
        The absolute mean error on Yaw angle estimation: 96.18 Degree
        The absolute mean error on Roll angle estimation: 161.77 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 222.44 Degree
        The absolute mean error on Yaw angle estimations: 96.18 Degree
        The absolute mean error on Roll angle estimations: 161.77 Degree
Exp2019-01-27_17-13-59_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-13-59
All frames and annotations from 1 datasets have been read by 2019-01-27 17:15:29.051575
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:15:37.925067!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 2396.5377 - mean_absolute_error: 37.6626
Epoch 1 completed!
Exp2019-01-27_17-13-59_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-13-59
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:15:54.623638
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 127.72 Degree
        The absolute mean error on Yaw angle estimation: 36.24 Degree
        The absolute mean error on Roll angle estimation: 46.21 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 127.72 Degree
        The absolute mean error on Yaw angle estimations: 36.24 Degree
        The absolute mean error on Roll angle estimations: 46.21 Degree
Exp2019-01-27_17-13-59_part3 completed!
Exp2019-01-27_17-13-59.h5 has been saved.
subject9_Exp2019-01-27_17-13-59.png has been saved by 2019-01-27 17:16:12.281103.
Model Exp2019-01-27_17-13-59 has been evaluated successfully.
Model Exp2019-01-27_17-13-59 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:37:51.423768: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:37:51.519611: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:37:51.519866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:37:51.519880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:37:51.675047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:37:51.675074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:37:51.675082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:37:51.675220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-37-52 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 3
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-37-52
All frames and annotations from 1 datasets have been read by 2019-01-27 17:37:53.223646
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:38:02.111424!
Epoch 1/1
882/882 [==============================] - 16s 19ms/step - loss: 20.5013
Epoch 1 completed!
Exp2019-01-27_17-37-52_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-37-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:38:20.198796
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 20.21 Degree
        The absolute mean error on Yaw angle estimation: 22.74 Degree
        The absolute mean error on Roll angle estimation: 39.90 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 20.21 Degree
        The absolute mean error on Yaw angle estimations: 22.74 Degree
        The absolute mean error on Roll angle estimations: 39.90 Degree
Exp2019-01-27_17-37-52_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-37-52
All frames and annotations from 1 datasets have been read by 2019-01-27 17:38:38.216487
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:38:47.094084!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 15.5593
Epoch 1 completed!
Exp2019-01-27_17-37-52_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-37-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:39:03.953601
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: nan Degree
        The absolute mean error on Yaw angle estimation: nan Degree
        The absolute mean error on Roll angle estimation: nan Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: nan Degree
        The absolute mean error on Yaw angle estimations: nan Degree
        The absolute mean error on Roll angle estimations: nan Degree
Exp2019-01-27_17-37-52_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.00
0100_2019-01-27_17-37-52
All frames and annotations from 1 datasets have been read by 2019-01-27 17:39:21.904712
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:39:30.782926!
Epoch 1/1
882/882 [==============================] - 15s 17ms/step - loss: 14.6536
Epoch 1 completed!
Exp2019-01-27_17-37-52_part3.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs3_AdamOpt_lr-0.
000100_2019-01-27_17-37-52
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:39:47.288458
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 53.75 Degree
        The absolute mean error on Yaw angle estimation: 53.80 Degree
        The absolute mean error on Roll angle estimation: 45.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 53.75 Degree
        The absolute mean error on Yaw angle estimations: 53.80 Degree
        The absolute mean error on Roll angle estimations: 45.37 Degree
Exp2019-01-27_17-37-52_part3 completed!
Exp2019-01-27_17-37-52.h5 has been saved.
subject9_Exp2019-01-27_17-37-52.png has been saved by 2019-01-27 17:40:04.934461.
Model Exp2019-01-27_17-37-52 has been evaluated successfully.
Model Exp2019-01-27_17-37-52 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:41:44.141756: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:41:44.239114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:41:44.239370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:41:44.239382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:41:44.394416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:41:44.394442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:41:44.394446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:41:44.394584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-41-45 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-41-45
All frames and annotations from 1 datasets have been read by 2019-01-27 17:41:45.911711
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:41:54.805160!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 17.7891
Epoch 1 completed!
Exp2019-01-27_17-41-45_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-41-45
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:42:13.349871
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 19.15 Degree
        The absolute mean error on Yaw angle estimation: 26.85 Degree
        The absolute mean error on Roll angle estimation: 6.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 19.15 Degree
        The absolute mean error on Yaw angle estimations: 26.85 Degree
        The absolute mean error on Roll angle estimations: 6.37 Degree
Exp2019-01-27_17-41-45_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-41-45
All frames and annotations from 1 datasets have been read by 2019-01-27 17:42:31.474049
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:42:40.382404!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 17.3497
Epoch 1 completed!
Exp2019-01-27_17-41-45_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-41-45
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:42:57.337922
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 18.55 Degree
        The absolute mean error on Yaw angle estimation: 26.69 Degree
        The absolute mean error on Roll angle estimation: 6.13 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 18.55 Degree
        The absolute mean error on Yaw angle estimations: 26.69 Degree
        The absolute mean error on Roll angle estimations: 6.13 Degree
Exp2019-01-27_17-41-45_part2 completed!
Exp2019-01-27_17-41-45.h5 has been saved.
subject9_Exp2019-01-27_17-41-45.png has been saved by 2019-01-27 17:43:14.978960.
Model Exp2019-01-27_17-41-45 has been evaluated successfully.
Model Exp2019-01-27_17-41-45 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:44:39.001451: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:44:39.099178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:44:39.099434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:44:39.099447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:44:39.254759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:44:39.254784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:44:39.254789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:44:39.254931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-44-39 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-44-39
All frames and annotations from 1 datasets have been read by 2019-01-27 17:44:40.787081
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:44:49.683324!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1456
Epoch 1 completed!
Exp2019-01-27_17-44-39_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-44-39
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:45:07.929362
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 0.06 Degree
        The absolute mean error on Yaw angle estimation: 0.07 Degree
        The absolute mean error on Roll angle estimation: 0.04 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 0.06 Degree
        The absolute mean error on Yaw angle estimations: 0.07 Degree
        The absolute mean error on Roll angle estimations: 0.04 Degree
Exp2019-01-27_17-44-39_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-44-39
All frames and annotations from 1 datasets have been read by 2019-01-27 17:45:26.023434
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:45:34.894620!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0834
Epoch 1 completed!
Exp2019-01-27_17-44-39_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-44-39
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:45:51.901139
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 0.04 Degree
        The absolute mean error on Yaw angle estimation: 0.07 Degree
        The absolute mean error on Roll angle estimation: 0.03 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 0.04 Degree
        The absolute mean error on Yaw angle estimations: 0.07 Degree
        The absolute mean error on Roll angle estimations: 0.03 Degree
Exp2019-01-27_17-44-39_part2 completed!
Exp2019-01-27_17-44-39.h5 has been saved.
subject9_Exp2019-01-27_17-44-39.png has been saved by 2019-01-27 17:46:09.511789.
Model Exp2019-01-27_17-44-39 has been evaluated successfully.
Model Exp2019-01-27_17-44-39 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:47:11.540781: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:47:11.640112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:47:11.640371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:47:11.640383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:47:11.796872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:47:11.796897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:47:11.796901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:47:11.797041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_17-47-12 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-47-12
All frames and annotations from 1 datasets have been read by 2019-01-27 17:47:13.339605
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:47:22.252820!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.1451
Epoch 1 completed!
Exp2019-01-27_17-47-12_part1.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-47-12
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:47:40.685115
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.96 Degree
        The absolute mean error on Yaw angle estimation: 16.55 Degree
        The absolute mean error on Roll angle estimation: 7.15 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.96 Degree
        The absolute mean error on Yaw angle estimations: 16.55 Degree
        The absolute mean error on Roll angle estimations: 7.15 Degree
Exp2019-01-27_17-47-12_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.00
0100_2019-01-27_17-47-12
All frames and annotations from 1 datasets have been read by 2019-01-27 17:47:58.757568
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:48:07.651269!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0906
Epoch 1 completed!
Exp2019-01-27_17-47-12_part2.h5 has been saved.
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs2_AdamOpt_lr-0.
000100_2019-01-27_17-47-12
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:48:24.654124
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.32 Degree
        The absolute mean error on Yaw angle estimation: 5.24 Degree
        The absolute mean error on Roll angle estimation: 4.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.32 Degree
        The absolute mean error on Yaw angle estimations: 5.24 Degree
        The absolute mean error on Roll angle estimations: 4.30 Degree
Exp2019-01-27_17-47-12_part2 completed!
Exp2019-01-27_17-47-12.h5 has been saved.
subject9_Exp2019-01-27_17-47-12.png has been saved by 2019-01-27 17:48:42.299472.
Model Exp2019-01-27_17-47-12 has been evaluated successfully.
Model Exp2019-01-27_17-47-12 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_17-47-12
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:56:06.162525: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:56:06.259846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:56:06.260107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:56:06.260119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:56:06.425913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:56:06.425939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:56:06.425943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:56:06.426081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-56-07
The subjects will be tested: [(9, 'M03')]
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 64, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, record = record)
TypeError: evaluateCNN_LSTM() missing 1 required positional argument: 'angles'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_17-47-12
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:57:18.819874: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:57:18.917262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:57:18.917521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:57:18.917535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:57:19.072422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:57:19.072448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:57:19.072453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:57:19.072589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-57-19
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:57:20.878985
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 5.32 Degree
        The absolute mean error on Yaw angle estimation: 5.24 Degree
        The absolute mean error on Roll angle estimation: 4.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.32 Degree
        The absolute mean error on Yaw angle estimations: 5.24 Degree
        The absolute mean error on Roll angle estimations: 4.30 Degree
Traceback (most recent call last):
  File "continueFC_RNN_Experiment.py", line 74, in <module>
    main()
  File "continueFC_RNN_Experiment.py", line 71, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueFC_RNN_Experiment.py", line 64, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, stateful = STATEFUL, angles = angles, record =
record)
ValueError: too many values to unpack (expected 2)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_17-47-12
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:58:50.386018: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:58:50.482385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:58:50.482642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:58:50.482654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:58:50.637249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:58:50.637272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:58:50.637277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:58:50.637417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 17:58:52.478750
For the Subject 9 (M03):
882/882 [==============================] - 9s 10ms/step
        The absolute mean error on Pitch angle estimation: 5.32 Degree
        The absolute mean error on Yaw angle estimation: 5.24 Degree
        The absolute mean error on Roll angle estimation: 4.30 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.32 Degree
        The absolute mean error on Yaw angle estimations: 5.24 Degree
        The absolute mean error on Roll angle estimations: 4.30 Degree
subject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51.png has been saved by 2019-01-27 17:59:10.617293.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51 has been evaluated successfully.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_17-47-12 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 17:59:25.695772: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 17:59:25.776512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 17:59:25.776819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 17:59:25.776833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 17:59:25.931452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 17:59:25.931479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 17:59:25.931484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 17:59:25.931665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_17-47-12.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 2
eva_epoch = 1
train_batch_size = 1
test_batch_size = 1

subjectList = [9] # [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] #
testSubjects = [9] # [3, 5, 9, 14] # [9, 18, 21, 24] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26
All frames and annotations from 1 datasets have been read by 2019-01-27 17:59:27.765131
1. set (Dataset 9) being trained for epoch 1 by 2019-01-27 17:59:36.674876!
Epoch 1/1
882/882 [==============================] - 17s 19ms/step - loss: 0.0744
Epoch 1 completed!
All frames and annotations from 1 datasets have been read by 2019-01-27 17:59:54.241822
1. set (Dataset 9) being trained for epoch 2 by 2019-01-27 18:00:03.102253!
Epoch 1/1
882/882 [==============================] - 16s 18ms/step - loss: 0.0643
Epoch 2 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-27 18:00:20.122391
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 5.99 Degree
        The absolute mean error on Yaw angle estimation: 5.12 Degree
        The absolute mean error on Roll angle estimation: 2.68 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 5.99 Degree
        The absolute mean error on Yaw angle estimations: 5.12 Degree
        The absolute mean error on Roll angle estimations: 2.68 Degree
subject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26.png has been saved by 2019-01-27 18:00:37.610534.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26 has been evaluated successfully.
Model Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "keep t
anh"
[master 68e0104] keep tanh
 70 files changed, 3673 insertions(+), 2172 deletions(-)
 rename DeepRL_For_HPE/FC_RNN_Evaluater/results/{Last_Model/output_Last_Model.txt => Exp2019-01-26_04-28-49/
output_Exp2019-01-26_04-28-49.txt} (89%)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject14_Exp2019-01-26_0
4-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject3_Exp2019-01-26_04
-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject5_Exp2019-01-26_04
-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_04-28-49/subject9_Exp2019-01-26_04
-28-49.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/output_Exp2019-01-26_13-3
7-43.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject14_Exp2019-01-26_1
3-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject3_Exp2019-01-26_13
-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject5_Exp2019-01-26_13
-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-26_13-37-43/subject9_Exp2019-01-26_13
-37-43.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_11-59-18/output_Exp2019-01-27_11-5
9-18.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_11-59-18/subject9_Exp2019-01-27_11
-59-18.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-14-25/output_Exp2019-01-27_13-1
4-25.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-14-25/subject9_Exp2019-01-27_13
-14-25.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-25-50/output_Exp2019-01-27_13-2
5-50.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-25-50/subject9_Exp2019-01-27_13
-25-50.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-35-44/output_Exp2019-01-27_13-3
5-44.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-35-44/subject9_Exp2019-01-27_13
-35-44.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-36-52/output_Exp2019-01-27_13-3
6-52.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-36-52/subject9_Exp2019-01-27_13
-36-52.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-39-33/output_Exp2019-01-27_13-3
9-33.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-39-33/subject9_Exp2019-01-27_13
-39-33.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-41-30/output_Exp2019-01-27_13-4
1-30.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-41-30/subject9_Exp2019-01-27_13
-41-30.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-43-20/output_Exp2019-01-27_13-4
3-20.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-43-20/subject9_Exp2019-01-27_13
-43-20.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-44-42/output_Exp2019-01-27_13-4
4-42.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-44-42/subject9_Exp2019-01-27_13
-44-42.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-46-18/output_Exp2019-01-27_13-4
6-18.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-46-18/subject9_Exp2019-01-27_13
-46-18.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-54-23/output_Exp2019-01-27_13-5
4-23.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-54-23/subject9_Exp2019-01-27_13
-54-23.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-55-59/output_Exp2019-01-27_13-5
5-59.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-55-59/subject9_Exp2019-01-27_13
-55-59.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-57-56/output_Exp2019-01-27_13-5
7-56.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_13-57-56/subject9_Exp2019-01-27_13
-57-56.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-12-09/output_Exp2019-01-27_14-1
2-09.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-12-09/subject9_Exp2019-01-27_14
-12-09.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-22-51/output_Exp2019-01-27_14-2
2-51.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-22-51/subject9_Exp2019-01-27_14
-22-51.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-37-00/output_Exp2019-01-27_14-3
7-00.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-37-00/subject9_Exp2019-01-27_14
-37-00.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-44-55/output_Exp2019-01-27_14-4
4-55.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_14-44-55/subject9_Exp2019-01-27_14
-44-55.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-06-27/output_Exp2019-01-27_17-0
6-27.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-06-27/subject9_Exp2019-01-27_17
-06-27.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-11-08/output_Exp2019-01-27_17-1
1-08.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-11-08/subject9_Exp2019-01-27_17
-11-08.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-13-59/output_Exp2019-01-27_17-1
3-59.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-13-59/subject9_Exp2019-01-27_17
-13-59.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-37-52/output_Exp2019-01-27_17-3
7-52.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-37-52/subject9_Exp2019-01-27_17
-37-52.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-41-45/output_Exp2019-01-27_17-4
1-45.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-41-45/subject9_Exp2019-01-27_17
-41-45.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-44-39/output_Exp2019-01-27_17-4
4-39.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-44-39/subject9_Exp2019-01-27_17
-44-39.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12/output_Exp2019-01-27_17-4
7-12.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12/subject9_Exp2019-01-27_17
-47-12.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51/o
utput_Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51/s
ubject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-58-51.png
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26/o
utput_Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26/s
ubject9_Exp2019-01-27_17-47-12_and_2019-01-27_17-59-26.png
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model__/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model___/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 101, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (101/101), done.
Writing objects: 100% (101/101), 4.99 MiB | 697.00 KiB/s, done.
Total 101 (delta 34), reused 0 (delta 0)
remote: Resolving deltas: 100% (34/34), completed with 8 local objects.
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   fea1f6f..68e0104  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:15:52.548327: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:15:52.646140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:15:52.646395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:15:52.646407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:15:52.801675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:15:52.801708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:15:52.801713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:15:52.801850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-15-53 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-15-53
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, r
ecord = record)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, p
reprocess_input = preprocess_input)
TypeError: trainCNN_LSTM() got an unexpected keyword argument 'exp'
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:18:36.279976: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:18:36.377512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:18:36.377780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:18:36.377792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:18:36.533234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:18:36.533260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:18:36.533265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:18:36.533404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10471 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-18-37 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = subjectList # [s for s in subjectList if not s in testSubjects] #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-18-37
All frames and annotations from 24 datasets have been read by 2019-01-27 18:18:43.082578
1. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:18:47.765704!
Epoch 1/1
492/492 [==============================] - 10s 20ms/step - loss: 0.1347
^C
Model Exp2019-01-27_18-18-37_part1 has been interrupted.
Exp2019-01-27_18-18-37_part1.h5 has been saved.
Model Exp2019-01-27_18-18-37_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git commit -m "adding
 experiment count to output"
[master 7b03ced] adding experiment count to output
 6 files changed, 214 insertions(+), 16 deletions(-)
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Exp2019-01-27_18-18-37_part1/output_Exp2019-01-2
7_18-18-37_part1.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/FC_RNN_Evaluater/results/Last_Model_/output_Last_Model.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 14, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (11/11), done.
Writing objects: 100% (14/14), 2.33 KiB | 0 bytes/s, done.
Total 14 (delta 8), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   68e0104..7b03ced  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:21:07.912509: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:21:08.007845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:21:08.008151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:21:08.008166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:21:08.163946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:21:08.163971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:21:08.163976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:21:08.164154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-21-08 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-21-08
All frames and annotations from 20 datasets have been read by 2019-01-27 18:21:13.332003
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:21:19.740762!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.0988
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, e
xp = exp, record = record)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, p
reprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 50, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, exp = exp, record = record, preprocess_input = preprocess_in
put)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 40, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, exp = exp, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 22, in trainImageModelOnSets
    exp = ' in Experiment %d' % (exp) if exp != -1 else ''
TypeError: %d format: a number is required, not str
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:36:49.313944: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:36:49.410235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:36:49.410499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:36:49.410513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:36:49.566088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:36:49.566116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:36:49.566124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:36:49.566264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-36-50 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-36-50
All frames and annotations from 20 datasets have been read by 2019-01-27 18:36:54.600296
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:37:00.996602!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.1272
Traceback (most recent call last):
  File "runFC_RNN_Experiment.py", line 65, in <module>
    main()
  File "runFC_RNN_Experiment.py", line 62, in main
    runCNN_LSTM(record = RECORD)
  File "runFC_RNN_Experiment.py", line 48, in runCNN_LSTM
    full_model, means, results = runCNN_LSTM_ExperimentWithModel(full_model, modelID, modelStr, eva_epoch, e
xp = exp, record = record)
  File "runFC_RNN_Experiment.py", line 23, in runCNN_LSTM_ExperimentWithModel
    batch_size = train_batch_size, in_epochs = in_epochs, stateful = STATEFUL, exp = exp, record = record, p
reprocess_input = preprocess_input)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 50, in trainCNN_LSTM
    in_epochs = in_epochs, stateful = stateful, exp = exp, record = record, preprocess_input = preprocess_in
put)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 40, in trainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_out
puts, batch_size, in_epochs, exp = exp, stateful = stateful, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater/FC_RNN_Evaluater.py"
, line 22, in trainImageModelOnSets
    exp = ' in Experiment %d' % (exp) if exp != -1 else ''
TypeError: %d format: a number is required, not str
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:38:17.980728: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:38:18.079080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:38:18.079343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:38:18.079357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:38:18.234477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:38:18.234503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:38:18.234511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:38:18.234659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-38-18 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-38-18
All frames and annotations from 20 datasets have been read by 2019-01-27 18:38:23.546454
^C
Model Exp2019-01-27_18-38-18_part1 has been interrupted.
Exp2019-01-27_18-38-18_part1.h5 has been saved.
Model Exp2019-01-27_18-38-18_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:39:24.754830: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:39:24.851524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:39:24.851826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:39:24.851841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:39:25.007701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:39:25.007727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:39:25.007732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:39:25.007914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-39-25 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-39-25
All frames and annotations from 20 datasets have been read by 2019-01-27 18:39:30.089324
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:39:36.501210!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.1046
2. set (Dataset 24) being trained for epoch 1 in Experiment  in Experiment 1 by 2019-01-27 18:39:54.679586!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0894
3. set (Dataset 15) being trained for epoch 1 in Experiment  in Experiment  in Experiment 1 by 2019-01-27 18
:40:09.892744!
Epoch 1/1
585/654 [=========================>....] - ETA: 1s - loss: 0.1114^C
Model Exp2019-01-27_18-39-25_part1 has been interrupted.
Exp2019-01-27_18-39-25_part1.h5 has been saved.
Model Exp2019-01-27_18-39-25_part1 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python runFC_RNN_Expe
riment.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 18:42:46.900277: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 18:42:46.998566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 18:42:46.998830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 18:42:46.998848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 18:42:47.153779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 18:42:47.153801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 18:42:47.153805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 18:42:47.153942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-27_18-42-47 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0
_________________________________________________________________
fc1 (Dense)                  (None, 4096)              102764544
_________________________________________________________________
fc2 (Dense)                  (None, 4096)              16781312
=================================================================
Total params: 134,260,544
Trainable params: 0
Non-trainable params: 134,260,544
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 18:42:52.167787
1. set (Dataset 22) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:42:58.567546!
Epoch 1/1
665/665 [==============================] - 13s 19ms/step - loss: 0.1332
2. set (Dataset 24) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:43:16.446032!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.1030
3. set (Dataset 15) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:43:31.678422!
Epoch 1/1
654/654 [==============================] - 11s 17ms/step - loss: 0.1170
4. set (Dataset 19) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:43:47.988977!
Epoch 1/1
502/502 [==============================] - 9s 17ms/step - loss: 0.0964
5. set (Dataset 8) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:44:09.966421!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.1351
6. set (Dataset 23) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:44:28.904202!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1431
7. set (Dataset 21) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:44:45.044640!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1318
8. set (Dataset 16) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:45:05.035664!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0905
9. set (Dataset 7) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:45:34.811942!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.1055
10. set (Dataset 12) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:00.325613!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0923
11. set (Dataset 10) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:20.764274!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.1623
12. set (Dataset 1) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:41.652583!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.1402
13. set (Dataset 18) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:46:56.999856!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1333
14. set (Dataset 2) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:47:16.391078!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.1216
15. set (Dataset 4) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:47:38.782751!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.1247
16. set (Dataset 20) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:47:57.759704!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.1025
17. set (Dataset 17) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:11.897512!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0824
18. set (Dataset 6) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:24.302934!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1393
19. set (Dataset 13) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:39.105880!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0769
20. set (Dataset 11) being trained for epoch 1 in Experiment 1 by 2019-01-27 18:48:57.259225!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0916
Epoch 1 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 18:49:11.641579
1. set (Dataset 6) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:49:16.843642!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1243
2. set (Dataset 11) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:49:32.332765!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0815
3. set (Dataset 10) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:49:49.609311!
Epoch 1/1
726/726 [==============================] - 12s 17ms/step - loss: 0.0987
4. set (Dataset 4) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:09.565960!
Epoch 1/1
744/744 [==============================] - 13s 17ms/step - loss: 0.0966
5. set (Dataset 23) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:28.081221!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.1182
6. set (Dataset 13) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:43.183428!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0649
7. set (Dataset 17) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:50:55.520042!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0757
8. set (Dataset 1) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:51:07.798232!
Epoch 1/1
498/498 [==============================] - 9s 17ms/step - loss: 0.1162
9. set (Dataset 8) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:51:24.291008!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0975
10. set (Dataset 7) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:51:45.440368!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0630
11. set (Dataset 21) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:04.910538!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.1129
12. set (Dataset 22) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:22.621362!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0575
13. set (Dataset 2) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:39.854964!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.1026
14. set (Dataset 24) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:52:53.871213!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0714
15. set (Dataset 15) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:08.998806!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0879
16. set (Dataset 20) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:26.052559!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0688
17. set (Dataset 18) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:42.068783!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.1124
18. set (Dataset 19) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:53:57.886035!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0907
19. set (Dataset 12) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:54:14.117165!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0760
20. set (Dataset 16) being trained for epoch 2 in Experiment 1 by 2019-01-27 18:54:36.113482!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0702
Epoch 2 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 18:54:57.024012
1. set (Dataset 19) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:01.904439!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0744
2. set (Dataset 16) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:19.604927!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0600
3. set (Dataset 21) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:42.182104!
Epoch 1/1
634/634 [==============================] - 11s 17ms/step - loss: 0.1023
4. set (Dataset 15) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:55:59.596651!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0770
5. set (Dataset 13) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:56:16.487845!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0635
6. set (Dataset 12) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:56:32.477186!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0669
7. set (Dataset 18) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:56:51.712415!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0923
8. set (Dataset 22) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:57:09.303466!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0501
9. set (Dataset 23) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:57:26.623977!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0977
10. set (Dataset 8) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:57:44.512881!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0903
11. set (Dataset 17) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:02.272823!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0711
12. set (Dataset 6) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:14.621591!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.1086
13. set (Dataset 24) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:29.121195!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0588
14. set (Dataset 11) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:58:43.409428!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0676
15. set (Dataset 10) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:00.701733!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0780
16. set (Dataset 20) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:18.981708!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0635
17. set (Dataset 2) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:34.087527!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0843
18. set (Dataset 4) being trained for epoch 3 in Experiment 1 by 2019-01-27 18:59:50.615252!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0904
19. set (Dataset 7) being trained for epoch 3 in Experiment 1 by 2019-01-27 19:00:11.650108!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0574
20. set (Dataset 1) being trained for epoch 3 in Experiment 1 by 2019-01-27 19:00:30.196131!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0908
Epoch 3 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:00:43.906526
1. set (Dataset 4) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:00:51.292632!
Epoch 1/1
744/744 [==============================] - 13s 17ms/step - loss: 0.0823
2. set (Dataset 1) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:09.220723!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0817
3. set (Dataset 17) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:21.795295!
Epoch 1/1
395/395 [==============================] - 7s 19ms/step - loss: 0.0696
4. set (Dataset 10) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:36.474761!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0665
5. set (Dataset 12) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:01:56.701255!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0626
6. set (Dataset 7) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:02:17.237521!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0487
7. set (Dataset 2) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:02:36.093606!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0725
8. set (Dataset 6) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:02:50.623866!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0963
9. set (Dataset 13) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:05.486486!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0602
10. set (Dataset 23) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:19.718768!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0938
11. set (Dataset 18) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:35.988681!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0930
12. set (Dataset 19) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:03:51.910868!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0711
13. set (Dataset 11) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:04:06.815106!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0594
14. set (Dataset 16) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:04:25.667899!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0643
15. set (Dataset 21) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:04:47.867466!
Epoch 1/1
634/634 [==============================] - 11s 17ms/step - loss: 0.1013
16. set (Dataset 20) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:04.400685!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0604
17. set (Dataset 24) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:19.161419!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0536
18. set (Dataset 15) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:34.469924!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0665
19. set (Dataset 8) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:05:53.785055!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0807
20. set (Dataset 22) being trained for epoch 4 in Experiment 1 by 2019-01-27 19:06:13.877914!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0522
Epoch 4 for Experiment 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:06:30.065417
1. set (Dataset 15) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:06:36.424523!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0591
2. set (Dataset 22) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:06:54.411195!
Epoch 1/1
665/665 [==============================] - 12s 17ms/step - loss: 0.0525
3. set (Dataset 18) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:07:11.864472!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0834
4. set (Dataset 21) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:07:28.807315!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0908
5. set (Dataset 7) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:07:47.877576!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0462
6. set (Dataset 8) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:09.394655!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0667
7. set (Dataset 24) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:28.237416!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0523
8. set (Dataset 19) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:41.859859!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0664
9. set (Dataset 12) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:08:58.221870!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0637
10. set (Dataset 13) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:09:16.402229!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0589
11. set (Dataset 2) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:09:30.330811!
Epoch 1/1
511/511 [==============================] - 9s 17ms/step - loss: 0.0660
12. set (Dataset 4) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:09:46.495917!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0799
13. set (Dataset 16) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:08.533742!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0567
14. set (Dataset 1) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:29.877989!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0775
15. set (Dataset 17) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:42.680985!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0704
16. set (Dataset 20) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:10:54.988202!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0580
17. set (Dataset 11) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:11:10.829872!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0492
18. set (Dataset 10) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:11:28.531363!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0584
19. set (Dataset 23) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:11:47.079723!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0916
20. set (Dataset 6) being trained for epoch 5 in Experiment 1 by 2019-01-27 19:12:02.366878!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0908
Epoch 5 for Experiment 1 completed!
Exp2019-01-27_18-42-47_part1.h5 has been saved.
The subjects are trained: [(15, 'F03'), (22, 'M01'), (18, 'F05'), (21, 'F02'), (7, 'M01'), (8, 'M02'), (24,
'M14'), (19, 'M11'), (12, 'M06'), (13, 'M07'), (2, 'F02'), (4, 'F04'), (16, 'M09'), (1, 'F01'), (17, 'M10'),
 (20, 'M12'), (11, 'M05'), (10, 'M04'), (23, 'M13'), (6, 'F06')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-27_18-42-47
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 19:12:14.444907
For the Subject 3 (F03):
730/730 [==============================] - 7s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.69 Degree
        The absolute mean error on Yaw angle estimation: 29.11 Degree
        The absolute mean error on Roll angle estimation: 10.60 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 16.35 Degree
        The absolute mean error on Yaw angle estimation: 28.69 Degree
        The absolute mean error on Roll angle estimation: 4.37 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 24.51 Degree
        The absolute mean error on Yaw angle estimation: 27.13 Degree
        The absolute mean error on Roll angle estimation: 6.82 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 14.23 Degree
        The absolute mean error on Yaw angle estimation: 26.67 Degree
        The absolute mean error on Roll angle estimation: 13.22 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.95 Degree
        The absolute mean error on Yaw angle estimations: 27.90 Degree
        The absolute mean error on Roll angle estimations: 8.75 Degree
Exp2019-01-27_18-42-47_part1 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 19:13:45.881361
1. set (Dataset 10) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:13:53.105883!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0515
2. set (Dataset 6) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:11.232820!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0794
3. set (Dataset 2) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:26.018407!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0633
4. set (Dataset 17) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:38.917258!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0665
5. set (Dataset 8) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:14:53.816884!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0662
6. set (Dataset 23) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:15:13.404074!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0859
7. set (Dataset 11) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:15:29.481044!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0440
8. set (Dataset 4) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:15:46.996451!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0761
9. set (Dataset 7) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:16:08.061893!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0497
10. set (Dataset 12) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:16:28.893406!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0558
11. set (Dataset 24) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:16:47.101295!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0529
12. set (Dataset 15) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:02.157010!
Epoch 1/1
654/654 [==============================] - 11s 17ms/step - loss: 0.0630
13. set (Dataset 1) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:18.628263!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0766
14. set (Dataset 22) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:34.136817!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0493
15. set (Dataset 18) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:17:51.912838!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0826
16. set (Dataset 20) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:18:08.350942!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0552
17. set (Dataset 16) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:18:27.260683!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0558
18. set (Dataset 21) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:18:49.696981!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0912
19. set (Dataset 13) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:19:05.785579!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0575
20. set (Dataset 19) being trained for epoch 1 in Experiment 2 by 2019-01-27 19:19:19.311789!
Epoch 1/1
502/502 [==============================] - 9s 17ms/step - loss: 0.0695
Epoch 1 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:19:32.436378
1. set (Dataset 21) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:19:38.461040!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0894
2. set (Dataset 19) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:19:54.845932!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0644
3. set (Dataset 24) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:08.663520!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0496
4. set (Dataset 18) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:23.375669!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0764
5. set (Dataset 23) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:39.674408!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0852
6. set (Dataset 13) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:20:54.754717!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0561
7. set (Dataset 16) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:21:12.347048!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0519
8. set (Dataset 15) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:21:35.495497!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0623
9. set (Dataset 8) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:21:55.127988!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0654
10. set (Dataset 7) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:22:16.433160!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0499
11. set (Dataset 11) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:22:35.514674!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0438
12. set (Dataset 10) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:22:53.199325!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0512
13. set (Dataset 22) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:12.349267!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0480
14. set (Dataset 6) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:29.528190!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0818
15. set (Dataset 2) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:44.260465!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0630
16. set (Dataset 20) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:23:58.968805!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0546
17. set (Dataset 1) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:24:13.936020!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0700
18. set (Dataset 17) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:24:26.725974!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0668
19. set (Dataset 12) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:24:41.178591!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0543
20. set (Dataset 4) being trained for epoch 2 in Experiment 2 by 2019-01-27 19:25:01.567008!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0776
Epoch 2 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:25:19.202612
1. set (Dataset 17) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:25:22.957722!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0647
2. set (Dataset 4) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:25:37.684372!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0706
3. set (Dataset 11) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:25:57.095653!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0420
4. set (Dataset 2) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:26:12.445397!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0591
5. set (Dataset 13) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:26:26.658676!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0554
6. set (Dataset 12) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:26:42.694142!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0522
7. set (Dataset 1) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:01.165068!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0634
8. set (Dataset 10) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:17.541401!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0515
9. set (Dataset 23) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:36.211270!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0862
10. set (Dataset 8) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:27:54.364591!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0637
11. set (Dataset 16) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:28:17.221706!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0554
12. set (Dataset 21) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:28:39.842309!
Epoch 1/1
634/634 [==============================] - 12s 18ms/step - loss: 0.0845
13. set (Dataset 6) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:28:56.693202!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0806
14. set (Dataset 19) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:11.488908!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0634
15. set (Dataset 24) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:25.224984!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0515
16. set (Dataset 20) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:39.422913!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0500
17. set (Dataset 22) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:29:55.895098!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0489
18. set (Dataset 18) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:30:13.784069!
Epoch 1/1
614/614 [==============================] - 11s 17ms/step - loss: 0.0733
19. set (Dataset 7) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:30:32.050765!
Epoch 1/1
745/745 [==============================] - 14s 18ms/step - loss: 0.0507
20. set (Dataset 15) being trained for epoch 3 in Experiment 2 by 2019-01-27 19:30:52.081075!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0611
Epoch 3 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:31:08.185533
1. set (Dataset 18) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:31:14.067362!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0689
2. set (Dataset 15) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:31:31.660767!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0579
3. set (Dataset 16) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:31:52.515029!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0528
4. set (Dataset 24) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:32:13.452763!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0482
5. set (Dataset 12) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:32:29.640838!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0500
6. set (Dataset 7) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:32:50.356766!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0447
7. set (Dataset 22) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:10.151733!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0452
8. set (Dataset 21) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:27.908699!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0838
9. set (Dataset 13) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:44.262728!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0562
10. set (Dataset 23) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:33:58.745057!
Epoch 1/1
569/569 [==============================] - 10s 17ms/step - loss: 0.0823
11. set (Dataset 1) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:13.668643!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0615
12. set (Dataset 17) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:26.262834!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0596
13. set (Dataset 19) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:38.325985!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0669
14. set (Dataset 4) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:34:54.786776!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0718
15. set (Dataset 11) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:14.019041!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0448
16. set (Dataset 20) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:29.774079!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0518
17. set (Dataset 6) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:45.025572!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0772
18. set (Dataset 2) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:35:59.940590!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0578
19. set (Dataset 8) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:36:16.975018!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0641
20. set (Dataset 10) being trained for epoch 4 in Experiment 2 by 2019-01-27 19:36:37.983661!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0497
Epoch 4 for Experiment 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:36:55.087002
1. set (Dataset 2) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:00.175476!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0534
2. set (Dataset 10) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:16.571052!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0472
3. set (Dataset 1) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:34.387931!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0606
4. set (Dataset 11) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:37:49.327006!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0385
5. set (Dataset 7) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:38:07.289313!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0463
6. set (Dataset 8) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:38:28.359984!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0577
7. set (Dataset 6) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:38:47.577598!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0746
8. set (Dataset 17) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:01.233167!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0620
9. set (Dataset 12) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:15.800441!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0492
10. set (Dataset 13) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:34.032954!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0549
11. set (Dataset 22) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:39:49.140482!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0453
12. set (Dataset 18) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:40:06.950934!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0731
13. set (Dataset 4) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:40:25.611824!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0684
14. set (Dataset 15) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:40:45.387282!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0600
15. set (Dataset 16) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:05.911249!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0545
16. set (Dataset 20) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:27.543195!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0522
17. set (Dataset 19) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:42.451760!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0629
18. set (Dataset 24) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:41:56.228560!
Epoch 1/1
492/492 [==============================] - 9s 17ms/step - loss: 0.0485
19. set (Dataset 23) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:42:10.292714!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0821
20. set (Dataset 21) being trained for epoch 5 in Experiment 2 by 2019-01-27 19:42:26.813089!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0852
Epoch 5 for Experiment 2 completed!
Exp2019-01-27_18-42-47_part2.h5 has been saved.
The subjects are trained: [(2, 'F02'), (10, 'M04'), (1, 'F01'), (11, 'M05'), (7, 'M01'), (8, 'M02'), (6, 'F0
6'), (17, 'M10'), (12, 'M06'), (13, 'M07'), (22, 'M01'), (18, 'F05'), (4, 'F04'), (15, 'F03'), (16, 'M09'),
(20, 'M12'), (19, 'M11'), (24, 'M14'), (23, 'M13'), (21, 'F02')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-27_18-42-47
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 19:42:40.488846
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 14.47 Degree
        The absolute mean error on Yaw angle estimation: 26.24 Degree
        The absolute mean error on Roll angle estimation: 15.04 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.47 Degree
        The absolute mean error on Yaw angle estimation: 26.70 Degree
        The absolute mean error on Roll angle estimation: 4.47 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 30.05 Degree
        The absolute mean error on Yaw angle estimation: 28.99 Degree
        The absolute mean error on Roll angle estimation: 9.91 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 13.73 Degree
        The absolute mean error on Yaw angle estimation: 29.61 Degree
        The absolute mean error on Roll angle estimation: 13.48 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.18 Degree
        The absolute mean error on Yaw angle estimations: 27.89 Degree
        The absolute mean error on Roll angle estimations: 10.72 Degree
Exp2019-01-27_18-42-47_part2 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 19:43:50.425386
1. set (Dataset 24) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:43:55.096427!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0478
2. set (Dataset 21) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:44:10.022010!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0777
3. set (Dataset 22) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:44:27.924823!
Epoch 1/1
665/665 [==============================] - 12s 17ms/step - loss: 0.0424
4. set (Dataset 16) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:44:48.306731!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0502
5. set (Dataset 8) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:45:12.700088!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.0618
6. set (Dataset 23) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:45:31.680416!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0782
7. set (Dataset 19) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:45:46.840017!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0628
8. set (Dataset 18) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:01.666710!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0685
9. set (Dataset 7) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:20.535815!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0447
10. set (Dataset 12) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:40.802422!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0478
11. set (Dataset 6) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:46:59.304550!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0766
12. set (Dataset 2) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:47:14.135651!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0566
13. set (Dataset 15) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:47:29.506956!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0570
14. set (Dataset 10) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:47:48.436347!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0481
15. set (Dataset 1) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:06.394290!
Epoch 1/1
498/498 [==============================] - 9s 19ms/step - loss: 0.0608
16. set (Dataset 20) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:21.139462!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0514
17. set (Dataset 4) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:38.545496!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0633
18. set (Dataset 11) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:48:57.493891!
Epoch 1/1
572/572 [==============================] - 10s 17ms/step - loss: 0.0441
19. set (Dataset 13) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:49:12.346897!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0563
20. set (Dataset 17) being trained for epoch 1 in Experiment 3 by 2019-01-27 19:49:25.081186!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0589
Epoch 1 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:49:36.584927
1. set (Dataset 11) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:49:42.289848!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0387
2. set (Dataset 17) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:49:56.407994!
Epoch 1/1
395/395 [==============================] - 7s 17ms/step - loss: 0.0579
3. set (Dataset 6) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:08.529818!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0742
4. set (Dataset 1) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:23.311135!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0644
5. set (Dataset 23) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:37.607357!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0774
6. set (Dataset 13) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:50:52.611932!
Epoch 1/1
485/485 [==============================] - 8s 17ms/step - loss: 0.0546
7. set (Dataset 4) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:51:08.469662!
Epoch 1/1
744/744 [==============================] - 14s 18ms/step - loss: 0.0563
8. set (Dataset 2) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:51:27.223317!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0585
9. set (Dataset 8) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:51:44.445292!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0624
10. set (Dataset 7) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:06.177440!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0458
11. set (Dataset 19) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:24.295945!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0649
12. set (Dataset 24) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:38.025858!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0520
13. set (Dataset 10) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:52:54.182522!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0460
14. set (Dataset 21) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:53:13.065940!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0802
15. set (Dataset 22) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:53:30.978319!
Epoch 1/1
665/665 [==============================] - 12s 17ms/step - loss: 0.0418
16. set (Dataset 20) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:53:48.129821!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0492
17. set (Dataset 15) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:54:04.374761!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0593
18. set (Dataset 16) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:54:24.798719!
Epoch 1/1
914/914 [==============================] - 16s 17ms/step - loss: 0.0506
19. set (Dataset 12) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:54:47.871470!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0518
20. set (Dataset 18) being trained for epoch 2 in Experiment 3 by 2019-01-27 19:55:06.784013!
Epoch 1/1
614/614 [==============================] - 10s 17ms/step - loss: 0.0681
Epoch 2 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 19:55:21.594316
1. set (Dataset 16) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:55:30.326945!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0472
2. set (Dataset 18) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:55:52.580136!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0645
3. set (Dataset 19) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:08.651444!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0605
4. set (Dataset 22) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:24.298028!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0435
5. set (Dataset 13) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:41.321855!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0548
6. set (Dataset 12) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:56:57.329044!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0460
7. set (Dataset 15) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:57:17.146784!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0573
8. set (Dataset 24) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:57:33.759377!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0486
9. set (Dataset 23) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:57:48.195378!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0792
10. set (Dataset 8) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:58:06.187953!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0603
11. set (Dataset 4) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:58:27.664460!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0601
12. set (Dataset 11) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:58:46.772628!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0427
13. set (Dataset 21) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:02.915899!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0811
14. set (Dataset 17) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:17.838180!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0608
15. set (Dataset 6) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:30.103961!
Epoch 1/1
542/542 [==============================] - 10s 18ms/step - loss: 0.0735
16. set (Dataset 20) being trained for epoch 3 in Experiment 3 by 2019-01-27 19:59:45.124216!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0504
17. set (Dataset 10) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:02.223293!
Epoch 1/1
726/726 [==============================] - 13s 17ms/step - loss: 0.0485
18. set (Dataset 1) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:20.004548!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0625
19. set (Dataset 7) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:36.371809!
Epoch 1/1
745/745 [==============================] - 13s 17ms/step - loss: 0.0469
20. set (Dataset 2) being trained for epoch 3 in Experiment 3 by 2019-01-27 20:00:54.505796!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0580
Epoch 3 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:01:08.009750
1. set (Dataset 1) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:01:13.060101!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0585
2. set (Dataset 2) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:01:27.218336!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0492
3. set (Dataset 4) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:01:43.877738!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0583
4. set (Dataset 6) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:02.395349!
Epoch 1/1
542/542 [==============================] - 9s 17ms/step - loss: 0.0707
5. set (Dataset 12) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:19.140511!
Epoch 1/1
732/732 [==============================] - 13s 17ms/step - loss: 0.0489
6. set (Dataset 7) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:39.518390!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0454
7. set (Dataset 10) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:02:59.870638!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0471
8. set (Dataset 11) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:03:18.449754!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0397
9. set (Dataset 13) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:03:33.736004!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0529
10. set (Dataset 23) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:03:48.067782!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0742
11. set (Dataset 15) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:04.755729!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0604
12. set (Dataset 16) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:25.225887!
Epoch 1/1
914/914 [==============================] - 16s 18ms/step - loss: 0.0495
13. set (Dataset 17) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:45.315949!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0586
14. set (Dataset 18) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:04:58.186891!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0647
15. set (Dataset 19) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:05:14.124219!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0625
16. set (Dataset 20) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:05:28.723316!
Epoch 1/1
556/556 [==============================] - 10s 19ms/step - loss: 0.0504
17. set (Dataset 21) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:05:45.174951!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0803
18. set (Dataset 22) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:06:03.049500!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0423
19. set (Dataset 8) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:06:22.553963!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0605
20. set (Dataset 24) being trained for epoch 4 in Experiment 3 by 2019-01-27 20:06:41.050421!
Epoch 1/1
492/492 [==============================] - 9s 19ms/step - loss: 0.0502
Epoch 4 for Experiment 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-27 20:06:54.715849
1. set (Dataset 22) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:01.109039!
Epoch 1/1
665/665 [==============================] - 12s 18ms/step - loss: 0.0396
2. set (Dataset 24) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:17.505229!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0461
3. set (Dataset 15) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:32.809575!
Epoch 1/1
654/654 [==============================] - 12s 19ms/step - loss: 0.0564
4. set (Dataset 19) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:07:49.916013!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0617
5. set (Dataset 7) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:08:06.670525!
Epoch 1/1
745/745 [==============================] - 13s 18ms/step - loss: 0.0447
6. set (Dataset 8) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:08:27.687181!
Epoch 1/1
772/772 [==============================] - 14s 18ms/step - loss: 0.0591
7. set (Dataset 21) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:08:47.637308!
Epoch 1/1
634/634 [==============================] - 11s 18ms/step - loss: 0.0774
8. set (Dataset 16) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:09:07.855799!
Epoch 1/1
914/914 [==============================] - 17s 18ms/step - loss: 0.0489
9. set (Dataset 12) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:09:31.685479!
Epoch 1/1
732/732 [==============================] - 13s 18ms/step - loss: 0.0487
10. set (Dataset 13) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:09:49.699038!
Epoch 1/1
485/485 [==============================] - 9s 18ms/step - loss: 0.0522
11. set (Dataset 10) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:05.729591!
Epoch 1/1
726/726 [==============================] - 13s 18ms/step - loss: 0.0475
12. set (Dataset 1) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:24.075266!
Epoch 1/1
498/498 [==============================] - 9s 18ms/step - loss: 0.0629
13. set (Dataset 18) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:38.977272!
Epoch 1/1
614/614 [==============================] - 11s 18ms/step - loss: 0.0652
14. set (Dataset 2) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:10:54.967810!
Epoch 1/1
511/511 [==============================] - 9s 18ms/step - loss: 0.0581
15. set (Dataset 4) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:11.612821!
Epoch 1/1
744/744 [==============================] - 13s 18ms/step - loss: 0.0569
16. set (Dataset 20) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:30.109813!
Epoch 1/1
556/556 [==============================] - 10s 18ms/step - loss: 0.0537
17. set (Dataset 17) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:43.802444!
Epoch 1/1
395/395 [==============================] - 7s 18ms/step - loss: 0.0560
18. set (Dataset 6) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:11:56.252467!
Epoch 1/1
542/542 [==============================] - 9s 17ms/step - loss: 0.0721
19. set (Dataset 23) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:12:11.214010!
Epoch 1/1
569/569 [==============================] - 10s 18ms/step - loss: 0.0778
20. set (Dataset 11) being trained for epoch 5 in Experiment 3 by 2019-01-27 20:12:27.428342!
Epoch 1/1
572/572 [==============================] - 10s 18ms/step - loss: 0.0424
Epoch 5 for Experiment 3 completed!
Exp2019-01-27_18-42-47_part3.h5 has been saved.
The subjects are trained: [(22, 'M01'), (24, 'M14'), (15, 'F03'), (19, 'M11'), (7, 'M01'), (8, 'M02'), (21,
'F02'), (16, 'M09'), (12, 'M06'), (13, 'M07'), (10, 'M04'), (1, 'F01'), (18, 'F05'), (2, 'F02'), (4, 'F04'),
 (20, 'M12'), (17, 'M10'), (6, 'F06'), (23, 'M13'), (11, 'M05')]
Evaluating model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0
.000100_2019-01-27_18-42-47
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 20:12:40.011331
For the Subject 3 (F03):
730/730 [==============================] - 7s 9ms/step
        The absolute mean error on Pitch angle estimation: 12.71 Degree
        The absolute mean error on Yaw angle estimation: 39.63 Degree
        The absolute mean error on Roll angle estimation: 15.27 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 8.30 Degree
        The absolute mean error on Yaw angle estimation: 23.38 Degree
        The absolute mean error on Roll angle estimation: 3.83 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 29.03 Degree
        The absolute mean error on Yaw angle estimation: 24.58 Degree
        The absolute mean error on Roll angle estimation: 12.18 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 20.08 Degree
        The absolute mean error on Yaw angle estimation: 27.01 Degree
        The absolute mean error on Roll angle estimation: 12.92 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.53 Degree
        The absolute mean error on Yaw angle estimations: 28.65 Degree
        The absolute mean error on Roll angle estimations: 11.05 Degree
Exp2019-01-27_18-42-47_part3 completed!
Training model VGG16_inc_top_seqLen1_stateful_lstm20_output3_BatchSize1_inEpochs1_outEpochs30_AdamOpt_lr-0.0
00100_2019-01-27_18-42-47
All frames and annotations from 20 datasets have been read by 2019-01-27 20:13:49.742692
1. set (Dataset 6) being trained for epoch 1 in Experiment 4 by 2019-01-27 20:13:54.931341!
Epoch 1/1
533/542 [============================>.] - ETA: 0s - loss: 0.0684^C
Model Exp2019-01-27_18-42-47_part4 has been interrupted.
Exp2019-01-27_18-42-47_part4.h5 has been saved.
Model Exp2019-01-27_18-42-47_part4 has been recorded successfully.
Terminating...
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_18-42-47_part4
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 20:15:02.889499: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 20:15:02.987129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 20:15:02.987433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 20:15:02.987446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 20:15:03.143127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 20:15:03.143154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 20:15:03.143158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 20:15:03.143337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_18-42-47_part4.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
The subjects are trained: [(1, 'F01'), (2, 'F02'), (4, 'F04'), (6, 'F06'), (7, 'M01'), (8, 'M02'), (10, 'M04
'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (18, 'F05'), (19, 'M11'),
(20, 'M12'), (21, 'F02'), (22, 'M01'), (23, 'M13'), (24, 'M14')]
Evaluating model Exp2019-01-27_18-42-47_part4_and_2019-01-27_20-15-04
The subjects will be tested: [(3, 'F03'), (5, 'F05'), (9, 'M03'), (14, 'M08')]
All frames and annotations from 4 datasets have been read by 2019-01-27 20:15:06.137359
For the Subject 3 (F03):
730/730 [==============================] - 8s 10ms/step
        The absolute mean error on Pitch angle estimation: 14.14 Degree
        The absolute mean error on Yaw angle estimation: 42.17 Degree
        The absolute mean error on Roll angle estimation: 16.91 Degree
For the Subject 5 (F05):
946/946 [==============================] - 9s 9ms/step
        The absolute mean error on Pitch angle estimation: 10.27 Degree
        The absolute mean error on Yaw angle estimation: 25.17 Degree
        The absolute mean error on Roll angle estimation: 4.36 Degree
For the Subject 9 (M03):
882/882 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 27.32 Degree
        The absolute mean error on Yaw angle estimation: 25.95 Degree
        The absolute mean error on Roll angle estimation: 12.79 Degree
For the Subject 14 (M08):
797/797 [==============================] - 8s 9ms/step
        The absolute mean error on Pitch angle estimation: 19.02 Degree
        The absolute mean error on Yaw angle estimation: 27.32 Degree
        The absolute mean error on Roll angle estimation: 12.62 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 17.69 Degree
        The absolute mean error on Yaw angle estimations: 30.15 Degree
        The absolute mean error on Roll angle estimations: 11.67 Degree
subject3_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.239660.
subject5_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.438394.
subject9_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.637757.
subject14_Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04.png has been saved by 2019-01-27 20:16:12.855475.
Model Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04 has been evaluated successfully.
Model Exp1-27_18-42-47_part4_and_2019-01-27_20-15-04 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/FC_RNN_Evaluater$ python continueFC_RNN
_Experiment.py Exp2019-01-27_18-42-47_part4 trainMore
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the sec
ond argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np
.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-27 20:16:37.328762: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructi
ons that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-27 20:16:37.426327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node
 read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node z
ero
2019-01-27 20:16:37.426634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with pro
perties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.82GiB
2019-01-27 20:16:37.426648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devi
ces: 0
2019-01-27 20:16:37.582479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect Stre
amExecutor with strength 1 edge matrix:
2019-01-27 20:16:37.582506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-27 20:16:37.582510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-27 20:16:37.582691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow devi
ce (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeF
orce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2)
Exp2019-01-27_18-42-47_part4.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdCNN (TimeDistributed)      (1, 1, 4096)              134260544
_________________________________________________________________
lstm_1 (LSTM)                (1, 20)                   329360
_________________________________________________________________
dense_1 (Dense)              (1, 3)                    63
=================================================================
Total params: 134,589,967
Trainable params: 329,423
Non-trainable params: 134,260,544
_________________________________________________________________

######## CONF_Begins_Here ##########
confFile = 'Stateful_FC_RNN_Configuration.py'
RECORD = True # False #

output_begin = 3
num_outputs = 3

timesteps = 1 # TimeseriesGenerator Handles overlapping
learning_rate =  0.0001
in_epochs = 1
out_epochs = 30
eva_epoch = 5
train_batch_size = 1
test_batch_size = 1

subjectList = [i for i in range(1, 25)] # [1, 2, 3, 4, 5, 7, 8, 11, 12, 14] # [9] #
testSubjects = [3, 5, 9, 14] # [9, 18, 21, 24] # [9] #
trainingSubjects = [s for s in subjectList if not s in testSubjects] # subjectList #

num_datasets = len(subjectList)

lstm_nodes = 20
lstm_dropout = 0.25
lstm_recurrent_dropout = 0.25
include_vgg_top = True

angles = ['Pitch', 'Yaw', 'Roll']
use_vgg16 = True
######### CONF_ends_Here ###########
Training model Exp2019-01-27_18-42-47_part4_and_2019-01-27_20-16-38
All frames and annotations from 20 datasets have been read by 2019-01-27 20:16:42.978898
num_ou1. set (Dataset 22) being trained for epoch 1 by 2019-01-27 20:16:49.379813!
Epoch 1/1
665/665 [==============================] - 13s 20ms/step - loss: 0.0427
2. set (Dataset 24) being trained for epoch 1 by 2019-01-27 20:17:07.191014!
Epoch 1/1
492/492 [==============================] - 9s 18ms/step - loss: 0.0462
3. set (Dataset 15) being trained for epoch 1 by 2019-01-27 20:17:22.511085!
Epoch 1/1
654/654 [==============================] - 12s 18ms/step - loss: 0.0568
4. set (Dataset 19) being trained for epoch 1 by 2019-01-27 20:17:39.144240!
Epoch 1/1
502/502 [==============================] - 9s 18ms/step - loss: 0.0626
5. set (Dataset 8) being trained for epoch 1 by 2019-01-27 20:17:55.810600!
Epoch 1/1
772/772 [==============================] - 13s 17ms/step - loss: 0.0616
6. set (Dataset 23) being trained for epoch 1 by 2019-01-27 20:18:14.734283!
Epoch 1/1
569/569 [==============================] - 465s 817ms/step - loss: 0.0763
7. set (Dataset 21) being trained for epoch 1 by 2019-01-27 20:26:05.906729!
Epoch 1/1
421/634 [==================>...........] - ETA: 3s - loss: 0.0760
