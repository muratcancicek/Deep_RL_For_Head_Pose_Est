mcicek@harsimran:~$ cd Projects/deep_rl_for_head_pose_est//DeepRL_For_HPE/
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE$ cd Projects/deep_rl_for_head_pose_est//DeepRL_F
or_HPE/
-bash: cd: Projects/deep_rl_for_head_pose_est//DeepRL_For_HPE/: No such file or directory
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE$ cd LSTM_VGG16/
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-16 03:07:03.063522: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-16 03:07:03.134452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-16 03:07:03.134717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.11GiB
2019-01-16 03:07:03.134731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-16 03:07:03.288095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-16 03:07:03.288122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-16 03:07:03.288129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-16 03:07:03.288268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9782 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
Model VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-07-03 has been started to
be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 1, 7, 7, 512)      14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 1, 25088)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 26)                2611960
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 81
=================================================================
Total params: 17,326,729
Trainable params: 2,612,041
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-07-03
All frames and annotations from 4 datasets have been read by 2019-01-16 03:07:05.731009
1. set (Subject 3, F04) being trained for epoch 1!
Epoch 1/1
146/146 [==============================] - 7s 47ms/step - loss: 0.1005 - mean_absolute_error: 0.2344
2. set (Subject 1, F02) being trained for epoch 1!
Epoch 1/1
100/100 [==============================] - 4s 40ms/step - loss: 0.0916 - mean_absolute_error: 0.2300
3. set (Subject 4, F05) being trained for epoch 1!
Epoch 1/1
149/149 [==============================] - 6s 39ms/step - loss: 0.0554 - mean_absolute_error: 0.1765
4. set (Subject 2, F03) being trained for epoch 1!
Epoch 1/1
102/102 [==============================] - 4s 38ms/step - loss: 0.0744 - mean_absolute_error: 0.2095
Epoch 1 completed!
The subjects are trained: [(3, 'F04'), (1, 'F02'), (4, 'F05'), (2, 'F03')]
Evaluating model VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-07-03
The subjects will be tested: [(3, 'F04'), (1, 'F02'), (4, 'F05'), (2, 'F03')]
All frames and annotations from 2 datasets have been read by 2019-01-16 03:07:52.583886
For the Subject 9 (M04):
221/221 [==============================] - 5s 24ms/step
        The absolute mean error on Pitch angle estimation: 21.65 Degree
        The absolute mean error on Yaw angle estimation: 26.60 Degree
        The absolute mean error on Roll angle estimation: 14.08 Degree
For the Subject 18 (M11):
154/154 [==============================] - 3s 22ms/step
        The absolute mean error on Pitch angle estimation: 77.94 Degree
        The absolute mean error on Yaw angle estimation: 44.92 Degree
        The absolute mean error on Roll angle estimation: 58.78 Degree
On average in 2 test subjects:
        The absolute mean error on Pitch angle estimations: 49.80 Degree
        The absolute mean error on Yaw angle estimations: 35.76 Degree
        The absolute mean error on Roll angle estimations: 36.43 Degree
subject9_VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-07-03.png has been save
d by 2019-01-16 03:08:16.273221.
subject18_VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-07-03.png has been sav
ed by 2019-01-16 03:08:16.497809.
Model VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-07-03 has been evaluated s
uccessfully.
Model VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-07-03 has been recorded su
ccessfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-16 03:10:02.622470: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-16 03:10:02.693012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-16 03:10:02.693339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.11GiB
2019-01-16 03:10:02.693353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-16 03:10:02.846494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-16 03:10:02.846521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-16 03:10:02.846525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-16 03:10:02.846700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9782 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
Model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-10-03 has been started t
o be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
lstm_1 (LSTM)                (None, 256)               25953280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 771
=================================================================
Total params: 40,668,739
Trainable params: 25,954,051
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-10-03
All frames and annotations from 4 datasets have been read by 2019-01-16 03:10:05.195311
1. set (Subject 3, F04) being trained for epoch 1!
Epoch 1/1
143/143 [==============================] - 59s 413ms/step - loss: 0.0932 - mean_absolute_error: 0.2232
2. set (Subject 1, F02) being trained for epoch 1!
Epoch 1/1
97/97 [==============================] - 39s 397ms/step - loss: 0.0675 - mean_absolute_error: 0.1962
3. set (Subject 4, F05) being trained for epoch 1!
Epoch 1/1
146/146 [==============================] - 58s 398ms/step - loss: 0.0428 - mean_absolute_error: 0.1525
4. set (Subject 2, F03) being trained for epoch 1!
Epoch 1/1
99/99 [==============================] - 39s 390ms/step - loss: 0.0490 - mean_absolute_error: 0.1665
Epoch 1 completed!
The subjects are trained: [(3, 'F04'), (1, 'F02'), (4, 'F05'), (2, 'F03')]
Evaluating model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-10-03
The subjects will be tested: [(3, 'F04'), (1, 'F02'), (4, 'F05'), (2, 'F03')]
All frames and annotations from 2 datasets have been read by 2019-01-16 03:13:45.734802
For the Subject 9 (M04):
217/217 [==============================] - 58s 268ms/step
        The absolute mean error on Pitch angle estimation: 18.01 Degree
        The absolute mean error on Yaw angle estimation: 20.57 Degree
        The absolute mean error on Roll angle estimation: 23.10 Degree
For the Subject 18 (M11):
150/150 [==============================] - 41s 271ms/step
        The absolute mean error on Pitch angle estimation: 55.72 Degree
        The absolute mean error on Yaw angle estimation: 38.64 Degree
        The absolute mean error on Roll angle estimation: 25.39 Degree
On average in 2 test subjects:
        The absolute mean error on Pitch angle estimations: 36.87 Degree
        The absolute mean error on Yaw angle estimations: 29.61 Degree
        The absolute mean error on Roll angle estimations: 24.24 Degree
subject9_VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-10-03.png has been sa
ved by 2019-01-16 03:15:39.712884.
subject18_VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-10-03.png has been s
aved by 2019-01-16 03:15:39.933829.
Model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-10-03 has been evaluated
 successfully.
Model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-10-03 has been recorded
successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit -m "Attempting another lo
ng training"
[master 8cca94d] Attempting another long training
 9 files changed, 171 insertions(+), 1855 deletions(-)
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/filename2.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=
0.000100)_2019-01-16_03-10-03/output_VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01
-16_03-10-03.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=
0.000100)_2019-01-16_03-10-03/subject18_VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019
-01-16_03-10-03.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=
0.000100)_2019-01-16_03-10-03/subject9_VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-
01-16_03-10-03.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.
000100)_2019-01-16_03-07-03/output_VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_
03-07-03.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.
000100)_2019-01-16_03-07-03/subject18_VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-
16_03-07-03.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.
000100)_2019-01-16_03-07-03/subject9_VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-1
6_03-07-03.png
 rename DeepRL_For_HPE/LSTM_VGG16/{filename.txt => results/unsuccessful_longTraining.txt} (100%)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 14, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (14/14), done.
Writing objects: 100% (14/14), 879.95 KiB | 0 bytes/s, done.
Total 14 (delta 7), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   7f6b212..8cca94d  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-16 03:24:38.347962: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-16 03:24:38.419344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-16 03:24:38.419600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 10.11GiB
2019-01-16 03:24:38.419612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-16 03:24:38.573916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-16 03:24:38.573942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-16 03:24:38.573947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-16 03:24:38.574078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9782 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
Model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38 has been started
to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
lstm_1 (LSTM)                (None, 256)               25953280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 771
=================================================================
Total params: 40,668,739
Trainable params: 25,954,051
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38
All frames and annotations from 20 datasets have been read by 2019-01-16 03:24:44.208172
1. set (Subject 20, F02) being trained for epoch 1!
Epoch 1/1
108/108 [==============================] - 44s 407ms/step - loss: 0.1427 - mean_absolute_error: 0.2902
2. set (Subject 23, M14) being trained for epoch 1!
Epoch 1/1
111/111 [==============================] - 44s 392ms/step - loss: 0.0816 - mean_absolute_error: 0.2221
3. set (Subject 12, M07) being trained for epoch 1!
Epoch 1/1
144/144 [==============================] - 55s 382ms/step - loss: 0.0471 - mean_absolute_error: 0.1545
4. set (Subject 16, M10) being trained for epoch 1!
Epoch 1/1
180/180 [==============================] - 70s 390ms/step - loss: 0.0415 - mean_absolute_error: 0.1539
5. set (Subject 6, M01) being trained for epoch 1!
Epoch 1/1
106/106 [==============================] - 41s 387ms/step - loss: 0.0694 - mean_absolute_error: 0.1941
6. set (Subject 22, M13) being trained for epoch 1!
Epoch 1/1
130/130 [==============================] - 52s 401ms/step - loss: 0.0295 - mean_absolute_error: 0.1216
7. set (Subject 19, M12) being trained for epoch 1!
Epoch 1/1
98/98 [==============================] - 38s 388ms/step - loss: 0.0314 - mean_absolute_error: 0.1333
8. set (Subject 13, M08) being trained for epoch 1!
Epoch 1/1
94/94 [==============================] - 37s 389ms/step - loss: 0.0329 - mean_absolute_error: 0.1410
9. set (Subject 5, F06) being trained for epoch 1!
Epoch 1/1
186/186 [==============================] - 72s 390ms/step - loss: 0.0395 - mean_absolute_error: 0.1500
10. set (Subject 10, M05) being trained for epoch 1!
Epoch 1/1
142/142 [==============================] - 55s 391ms/step - loss: 0.0438 - mean_absolute_error: 0.1481
11. set (Subject 7, M02) being trained for epoch 1!
Epoch 1/1
146/146 [==============================] - 57s 389ms/step - loss: 0.0525 - mean_absolute_error: 0.1761
12. set (Subject 1, F02) being trained for epoch 1!
Epoch 1/1
97/97 [==============================] - 39s 398ms/step - loss: 0.0521 - mean_absolute_error: 0.1707
13. set (Subject 15, M09) being trained for epoch 1!
Epoch 1/1
128/128 [==============================] - 50s 388ms/step - loss: 0.0526 - mean_absolute_error: 0.1582
14. set (Subject 2, F03) being trained for epoch 1!
Epoch 1/1
99/99 [==============================] - 39s 390ms/step - loss: 0.0443 - mean_absolute_error: 0.1569
15. set (Subject 3, F04) being trained for epoch 1!
Epoch 1/1
143/143 [==============================] - 56s 389ms/step - loss: 0.0573 - mean_absolute_error: 0.1731
16. set (Subject 17, F05) being trained for epoch 1!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0207 - mean_absolute_error: 0.1080
17. set (Subject 14, F03) being trained for epoch 1!
Epoch 1/1
157/157 [==============================] - 61s 388ms/step - loss: 0.0777 - mean_absolute_error: 0.2277
18. set (Subject 4, F05) being trained for epoch 1!
Epoch 1/1
146/146 [==============================] - 57s 389ms/step - loss: 0.0457 - mean_absolute_error: 0.1678
19. set (Subject 11, M06) being trained for epoch 1!
Epoch 1/1
112/112 [==============================] - 44s 389ms/step - loss: 0.0173 - mean_absolute_error: 0.0925
20. set (Subject 8, M03) being trained for epoch 1!
Epoch 1/1
152/152 [==============================] - 59s 389ms/step - loss: 0.0326 - mean_absolute_error: 0.1322
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 03:43:36.369584
1. set (Subject 4, F05) being trained for epoch 2!
Epoch 1/1
146/146 [==============================] - 56s 387ms/step - loss: 0.0266 - mean_absolute_error: 0.1313
2. set (Subject 8, M03) being trained for epoch 2!
Epoch 1/1
152/152 [==============================] - 59s 388ms/step - loss: 0.0201 - mean_absolute_error: 0.1098
3. set (Subject 7, M02) being trained for epoch 2!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0405 - mean_absolute_error: 0.1564
4. set (Subject 3, F04) being trained for epoch 2!
Epoch 1/1
143/143 [==============================] - 56s 389ms/step - loss: 0.0238 - mean_absolute_error: 0.1156
5. set (Subject 22, M13) being trained for epoch 2!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0251 - mean_absolute_error: 0.1171
6. set (Subject 11, M06) being trained for epoch 2!
Epoch 1/1
112/112 [==============================] - 44s 388ms/step - loss: 0.0138 - mean_absolute_error: 0.0884
7. set (Subject 14, F03) being trained for epoch 2!
Epoch 1/1
157/157 [==============================] - 61s 388ms/step - loss: 0.0516 - mean_absolute_error: 0.1808
8. set (Subject 1, F02) being trained for epoch 2!
Epoch 1/1
97/97 [==============================] - 38s 388ms/step - loss: 0.0212 - mean_absolute_error: 0.1087
9. set (Subject 6, M01) being trained for epoch 2!
Epoch 1/1
106/106 [==============================] - 41s 387ms/step - loss: 0.0620 - mean_absolute_error: 0.1806
10. set (Subject 5, F06) being trained for epoch 2!
Epoch 1/1
186/186 [==============================] - 72s 390ms/step - loss: 0.0140 - mean_absolute_error: 0.0882
11. set (Subject 19, M12) being trained for epoch 2!
Epoch 1/1
98/98 [==============================] - 38s 388ms/step - loss: 0.0325 - mean_absolute_error: 0.1386
12. set (Subject 20, F02) being trained for epoch 2!
Epoch 1/1
108/108 [==============================] - 42s 390ms/step - loss: 0.0372 - mean_absolute_error: 0.1467
13. set (Subject 2, F03) being trained for epoch 2!
Epoch 1/1
99/99 [==============================] - 39s 390ms/step - loss: 0.0298 - mean_absolute_error: 0.1334
14. set (Subject 23, M14) being trained for epoch 2!
Epoch 1/1
111/111 [==============================] - 43s 389ms/step - loss: 0.0655 - mean_absolute_error: 0.1910
15. set (Subject 12, M07) being trained for epoch 2!
Epoch 1/1
144/144 [==============================] - 56s 388ms/step - loss: 0.0108 - mean_absolute_error: 0.0770
16. set (Subject 17, F05) being trained for epoch 2!
Epoch 1/1
76/76 [==============================] - 30s 391ms/step - loss: 0.0218 - mean_absolute_error: 0.1094
17. set (Subject 15, M09) being trained for epoch 2!
Epoch 1/1
128/128 [==============================] - 50s 389ms/step - loss: 0.0449 - mean_absolute_error: 0.1410
18. set (Subject 16, M10) being trained for epoch 2!
Epoch 1/1
180/180 [==============================] - 70s 391ms/step - loss: 0.0385 - mean_absolute_error: 0.1434
19. set (Subject 10, M05) being trained for epoch 2!
Epoch 1/1
142/142 [==============================] - 55s 390ms/step - loss: 0.0210 - mean_absolute_error: 0.1056
20. set (Subject 13, M08) being trained for epoch 2!
Epoch 1/1
94/94 [==============================] - 37s 389ms/step - loss: 0.0123 - mean_absolute_error: 0.0810
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 04:02:24.898631
1. set (Subject 16, M10) being trained for epoch 3!
Epoch 1/1
180/180 [==============================] - 70s 388ms/step - loss: 0.0285 - mean_absolute_error: 0.1151
2. set (Subject 13, M08) being trained for epoch 3!
Epoch 1/1
94/94 [==============================] - 37s 391ms/step - loss: 0.0117 - mean_absolute_error: 0.0784
3. set (Subject 19, M12) being trained for epoch 3!
Epoch 1/1
98/98 [==============================] - 38s 387ms/step - loss: 0.0276 - mean_absolute_error: 0.1239
4. set (Subject 12, M07) being trained for epoch 3!
Epoch 1/1
144/144 [==============================] - 56s 388ms/step - loss: 0.0085 - mean_absolute_error: 0.0696
5. set (Subject 11, M06) being trained for epoch 3!
Epoch 1/1
112/112 [==============================] - 43s 388ms/step - loss: 0.0120 - mean_absolute_error: 0.0790
6. set (Subject 10, M05) being trained for epoch 3!
Epoch 1/1
142/142 [==============================] - 55s 390ms/step - loss: 0.0148 - mean_absolute_error: 0.0887
7. set (Subject 15, M09) being trained for epoch 3!
Epoch 1/1
128/128 [==============================] - 50s 390ms/step - loss: 0.0449 - mean_absolute_error: 0.1405
8. set (Subject 20, F02) being trained for epoch 3!
Epoch 1/1
108/108 [==============================] - 42s 390ms/step - loss: 0.0278 - mean_absolute_error: 0.1245
9. set (Subject 22, M13) being trained for epoch 3!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0204 - mean_absolute_error: 0.1042
10. set (Subject 6, M01) being trained for epoch 3!
Epoch 1/1
106/106 [==============================] - 41s 389ms/step - loss: 0.0501 - mean_absolute_error: 0.1580
11. set (Subject 14, F03) being trained for epoch 3!
Epoch 1/1
157/157 [==============================] - 61s 388ms/step - loss: 0.0289 - mean_absolute_error: 0.1355
12. set (Subject 4, F05) being trained for epoch 3!
Epoch 1/1
146/146 [==============================] - 57s 389ms/step - loss: 0.0162 - mean_absolute_error: 0.0983
13. set (Subject 23, M14) being trained for epoch 3!
Epoch 1/1
111/111 [==============================] - 43s 389ms/step - loss: 0.0582 - mean_absolute_error: 0.1782
14. set (Subject 8, M03) being trained for epoch 3!
Epoch 1/1
152/152 [==============================] - 59s 388ms/step - loss: 0.0159 - mean_absolute_error: 0.0962
15. set (Subject 7, M02) being trained for epoch 3!
Epoch 1/1
146/146 [==============================] - 57s 389ms/step - loss: 0.0249 - mean_absolute_error: 0.1233
16. set (Subject 17, F05) being trained for epoch 3!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0206 - mean_absolute_error: 0.1028
17. set (Subject 2, F03) being trained for epoch 3!
Epoch 1/1
99/99 [==============================] - 39s 390ms/step - loss: 0.0177 - mean_absolute_error: 0.1012
18. set (Subject 3, F04) being trained for epoch 3!
Epoch 1/1
143/143 [==============================] - 56s 389ms/step - loss: 0.0175 - mean_absolute_error: 0.1001
19. set (Subject 5, F06) being trained for epoch 3!
Epoch 1/1
186/186 [==============================] - 73s 390ms/step - loss: 0.0113 - mean_absolute_error: 0.0813
20. set (Subject 1, F02) being trained for epoch 3!
Epoch 1/1
97/97 [==============================] - 38s 388ms/step - loss: 0.0128 - mean_absolute_error: 0.0823
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 04:21:13.344990
1. set (Subject 3, F04) being trained for epoch 4!
Epoch 1/1
143/143 [==============================] - 55s 387ms/step - loss: 0.0148 - mean_absolute_error: 0.0915
2. set (Subject 1, F02) being trained for epoch 4!
Epoch 1/1
97/97 [==============================] - 38s 389ms/step - loss: 0.0134 - mean_absolute_error: 0.0869
3. set (Subject 14, F03) being trained for epoch 4!
Epoch 1/1
157/157 [==============================] - 61s 389ms/step - loss: 0.0250 - mean_absolute_error: 0.1243
4. set (Subject 7, M02) being trained for epoch 4!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0167 - mean_absolute_error: 0.1043
5. set (Subject 10, M05) being trained for epoch 4!
Epoch 1/1
142/142 [==============================] - 55s 391ms/step - loss: 0.0114 - mean_absolute_error: 0.0779
6. set (Subject 5, F06) being trained for epoch 4!
Epoch 1/1
186/186 [==============================] - 73s 390ms/step - loss: 0.0089 - mean_absolute_error: 0.0710
7. set (Subject 2, F03) being trained for epoch 4!
Epoch 1/1
99/99 [==============================] - 39s 390ms/step - loss: 0.0137 - mean_absolute_error: 0.0913
8. set (Subject 4, F05) being trained for epoch 4!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0096 - mean_absolute_error: 0.0749
9. set (Subject 11, M06) being trained for epoch 4!
Epoch 1/1
112/112 [==============================] - 43s 387ms/step - loss: 0.0070 - mean_absolute_error: 0.0630
10. set (Subject 22, M13) being trained for epoch 4!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0237 - mean_absolute_error: 0.1082
11. set (Subject 15, M09) being trained for epoch 4!
Epoch 1/1
128/128 [==============================] - 50s 390ms/step - loss: 0.0402 - mean_absolute_error: 0.1406
12. set (Subject 16, M10) being trained for epoch 4!
Epoch 1/1
180/180 [==============================] - 70s 391ms/step - loss: 0.0340 - mean_absolute_error: 0.1241
13. set (Subject 8, M03) being trained for epoch 4!
Epoch 1/1
152/152 [==============================] - 59s 389ms/step - loss: 0.0119 - mean_absolute_error: 0.0837
14. set (Subject 13, M08) being trained for epoch 4!
Epoch 1/1
94/94 [==============================] - 37s 390ms/step - loss: 0.0115 - mean_absolute_error: 0.0744
15. set (Subject 19, M12) being trained for epoch 4!
Epoch 1/1
98/98 [==============================] - 38s 388ms/step - loss: 0.0312 - mean_absolute_error: 0.1260
16. set (Subject 17, F05) being trained for epoch 4!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0188 - mean_absolute_error: 0.1014
17. set (Subject 23, M14) being trained for epoch 4!
Epoch 1/1
111/111 [==============================] - 43s 389ms/step - loss: 0.0622 - mean_absolute_error: 0.1850
18. set (Subject 12, M07) being trained for epoch 4!
Epoch 1/1
144/144 [==============================] - 56s 388ms/step - loss: 0.0099 - mean_absolute_error: 0.0763
19. set (Subject 6, M01) being trained for epoch 4!
Epoch 1/1
106/106 [==============================] - 41s 389ms/step - loss: 0.0603 - mean_absolute_error: 0.1605
20. set (Subject 20, F02) being trained for epoch 4!
Epoch 1/1
108/108 [==============================] - 42s 390ms/step - loss: 0.0286 - mean_absolute_error: 0.1256
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 04:40:02.255516
1. set (Subject 12, M07) being trained for epoch 5!
Epoch 1/1
144/144 [==============================] - 56s 386ms/step - loss: 0.0064 - mean_absolute_error: 0.0593
2. set (Subject 20, F02) being trained for epoch 5!
Epoch 1/1
108/108 [==============================] - 42s 391ms/step - loss: 0.0232 - mean_absolute_error: 0.1118
3. set (Subject 15, M09) being trained for epoch 5!
Epoch 1/1
128/128 [==============================] - 50s 389ms/step - loss: 0.0444 - mean_absolute_error: 0.1444
4. set (Subject 19, M12) being trained for epoch 5!
Epoch 1/1
98/98 [==============================] - 38s 388ms/step - loss: 0.0262 - mean_absolute_error: 0.1194
5. set (Subject 5, F06) being trained for epoch 5!
Epoch 1/1
186/186 [==============================] - 73s 390ms/step - loss: 0.0101 - mean_absolute_error: 0.0762
6. set (Subject 6, M01) being trained for epoch 5!
Epoch 1/1
106/106 [==============================] - 41s 389ms/step - loss: 0.0483 - mean_absolute_error: 0.1465
7. set (Subject 23, M14) being trained for epoch 5!
Epoch 1/1
111/111 [==============================] - 43s 389ms/step - loss: 0.0525 - mean_absolute_error: 0.1720
8. set (Subject 16, M10) being trained for epoch 5!
Epoch 1/1
180/180 [==============================] - 70s 389ms/step - loss: 0.0198 - mean_absolute_error: 0.1038
9. set (Subject 10, M05) being trained for epoch 5!
Epoch 1/1
142/142 [==============================] - 55s 390ms/step - loss: 0.0095 - mean_absolute_error: 0.0733
10. set (Subject 11, M06) being trained for epoch 5!
Epoch 1/1
112/112 [==============================] - 43s 388ms/step - loss: 0.0062 - mean_absolute_error: 0.0600
11. set (Subject 2, F03) being trained for epoch 5!
Epoch 1/1
99/99 [==============================] - 39s 391ms/step - loss: 0.0159 - mean_absolute_error: 0.0984
12. set (Subject 3, F04) being trained for epoch 5!
Epoch 1/1
143/143 [==============================] - 56s 390ms/step - loss: 0.0128 - mean_absolute_error: 0.0876
13. set (Subject 13, M08) being trained for epoch 5!
Epoch 1/1
94/94 [==============================] - 37s 391ms/step - loss: 0.0135 - mean_absolute_error: 0.0813
14. set (Subject 1, F02) being trained for epoch 5!
Epoch 1/1
97/97 [==============================] - 38s 389ms/step - loss: 0.0147 - mean_absolute_error: 0.0869
15. set (Subject 14, F03) being trained for epoch 5!
Epoch 1/1
157/157 [==============================] - 61s 389ms/step - loss: 0.0190 - mean_absolute_error: 0.1096
16. set (Subject 17, F05) being trained for epoch 5!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0182 - mean_absolute_error: 0.1029
17. set (Subject 8, M03) being trained for epoch 5!
Epoch 1/1
152/152 [==============================] - 59s 388ms/step - loss: 0.0116 - mean_absolute_error: 0.0829
18. set (Subject 7, M02) being trained for epoch 5!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0193 - mean_absolute_error: 0.1042
19. set (Subject 22, M13) being trained for epoch 5!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0219 - mean_absolute_error: 0.1035
20. set (Subject 4, F05) being trained for epoch 5!
Epoch 1/1
146/146 [==============================] - 57s 389ms/step - loss: 0.0114 - mean_absolute_error: 0.0809
Epoch 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 04:58:51.037181
1. set (Subject 7, M02) being trained for epoch 6!
Epoch 1/1
146/146 [==============================] - 57s 388ms/step - loss: 0.0137 - mean_absolute_error: 0.0909
2. set (Subject 4, F05) being trained for epoch 6!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0093 - mean_absolute_error: 0.0760
3. set (Subject 2, F03) being trained for epoch 6!
Epoch 1/1
99/99 [==============================] - 39s 391ms/step - loss: 0.0142 - mean_absolute_error: 0.0925
4. set (Subject 14, F03) being trained for epoch 6!
Epoch 1/1
157/157 [==============================] - 61s 389ms/step - loss: 0.0161 - mean_absolute_error: 0.1013
5. set (Subject 6, M01) being trained for epoch 6!
Epoch 1/1
106/106 [==============================] - 41s 388ms/step - loss: 0.0413 - mean_absolute_error: 0.1378
6. set (Subject 22, M13) being trained for epoch 6!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0226 - mean_absolute_error: 0.1058
7. set (Subject 8, M03) being trained for epoch 6!
Epoch 1/1
152/152 [==============================] - 59s 388ms/step - loss: 0.0105 - mean_absolute_error: 0.0784
8. set (Subject 3, F04) being trained for epoch 6!
Epoch 1/1
143/143 [==============================] - 56s 390ms/step - loss: 0.0124 - mean_absolute_error: 0.0861
9. set (Subject 5, F06) being trained for epoch 6!
Epoch 1/1
186/186 [==============================] - 72s 390ms/step - loss: 0.0087 - mean_absolute_error: 0.0701
10. set (Subject 10, M05) being trained for epoch 6!
Epoch 1/1
142/142 [==============================] - 55s 390ms/step - loss: 0.0081 - mean_absolute_error: 0.0679
11. set (Subject 23, M14) being trained for epoch 6!
Epoch 1/1
111/111 [==============================] - 43s 390ms/step - loss: 0.0456 - mean_absolute_error: 0.1600
12. set (Subject 12, M07) being trained for epoch 6!
Epoch 1/1
144/144 [==============================] - 56s 389ms/step - loss: 0.0101 - mean_absolute_error: 0.0761
13. set (Subject 1, F02) being trained for epoch 6!
Epoch 1/1
97/97 [==============================] - 38s 388ms/step - loss: 0.0118 - mean_absolute_error: 0.0797
14. set (Subject 20, F02) being trained for epoch 6!
Epoch 1/1
108/108 [==============================] - 42s 390ms/step - loss: 0.0203 - mean_absolute_error: 0.1028
15. set (Subject 15, M09) being trained for epoch 6!
Epoch 1/1
128/128 [==============================] - 50s 389ms/step - loss: 0.0332 - mean_absolute_error: 0.1275
16. set (Subject 17, F05) being trained for epoch 6!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0153 - mean_absolute_error: 0.0939
17. set (Subject 13, M08) being trained for epoch 6!
Epoch 1/1
94/94 [==============================] - 37s 389ms/step - loss: 0.0108 - mean_absolute_error: 0.0744
18. set (Subject 19, M12) being trained for epoch 6!
Epoch 1/1
98/98 [==============================] - 38s 388ms/step - loss: 0.0167 - mean_absolute_error: 0.0913
19. set (Subject 11, M06) being trained for epoch 6!
Epoch 1/1
112/112 [==============================] - 43s 388ms/step - loss: 0.0066 - mean_absolute_error: 0.0590
20. set (Subject 16, M10) being trained for epoch 6!
Epoch 1/1
180/180 [==============================] - 70s 389ms/step - loss: 0.0200 - mean_absolute_error: 0.0998
Epoch 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 05:17:39.620072
1. set (Subject 19, M12) being trained for epoch 7!
Epoch 1/1
98/98 [==============================] - 38s 384ms/step - loss: 0.0165 - mean_absolute_error: 0.0941
2. set (Subject 16, M10) being trained for epoch 7!
Epoch 1/1
180/180 [==============================] - 70s 390ms/step - loss: 0.0156 - mean_absolute_error: 0.0909
3. set (Subject 23, M14) being trained for epoch 7!
Epoch 1/1
111/111 [==============================] - 43s 390ms/step - loss: 0.0331 - mean_absolute_error: 0.1369
4. set (Subject 15, M09) being trained for epoch 7!
Epoch 1/1
128/128 [==============================] - 50s 390ms/step - loss: 0.0300 - mean_absolute_error: 0.1207
5. set (Subject 22, M13) being trained for epoch 7!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0178 - mean_absolute_error: 0.0984
6. set (Subject 11, M06) being trained for epoch 7!
Epoch 1/1
112/112 [==============================] - 43s 388ms/step - loss: 0.0050 - mean_absolute_error: 0.0521
7. set (Subject 13, M08) being trained for epoch 7!
Epoch 1/1
94/94 [==============================] - 37s 391ms/step - loss: 0.0099 - mean_absolute_error: 0.0699
8. set (Subject 12, M07) being trained for epoch 7!
Epoch 1/1
144/144 [==============================] - 56s 388ms/step - loss: 0.0073 - mean_absolute_error: 0.0617
9. set (Subject 6, M01) being trained for epoch 7!
Epoch 1/1
106/106 [==============================] - 41s 389ms/step - loss: 0.0354 - mean_absolute_error: 0.1300
10. set (Subject 5, F06) being trained for epoch 7!
Epoch 1/1
186/186 [==============================] - 73s 390ms/step - loss: 0.0079 - mean_absolute_error: 0.0676
11. set (Subject 8, M03) being trained for epoch 7!
Epoch 1/1
152/152 [==============================] - 59s 389ms/step - loss: 0.0134 - mean_absolute_error: 0.0891
12. set (Subject 7, M02) being trained for epoch 7!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0185 - mean_absolute_error: 0.1041
13. set (Subject 20, F02) being trained for epoch 7!
Epoch 1/1
108/108 [==============================] - 42s 392ms/step - loss: 0.0144 - mean_absolute_error: 0.0900
14. set (Subject 4, F05) being trained for epoch 7!
Epoch 1/1
146/146 [==============================] - 57s 389ms/step - loss: 0.0090 - mean_absolute_error: 0.0739
15. set (Subject 2, F03) being trained for epoch 7!
Epoch 1/1
99/99 [==============================] - 39s 391ms/step - loss: 0.0119 - mean_absolute_error: 0.0829
16. set (Subject 17, F05) being trained for epoch 7!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0173 - mean_absolute_error: 0.0996
17. set (Subject 1, F02) being trained for epoch 7!
Epoch 1/1
97/97 [==============================] - 38s 388ms/step - loss: 0.0102 - mean_absolute_error: 0.0750
18. set (Subject 14, F03) being trained for epoch 7!
Epoch 1/1
157/157 [==============================] - 61s 389ms/step - loss: 0.0199 - mean_absolute_error: 0.1109
19. set (Subject 10, M05) being trained for epoch 7!
Epoch 1/1
142/142 [==============================] - 55s 391ms/step - loss: 0.0079 - mean_absolute_error: 0.0686
20. set (Subject 3, F04) being trained for epoch 7!
Epoch 1/1
143/143 [==============================] - 56s 390ms/step - loss: 0.0097 - mean_absolute_error: 0.0768
Epoch 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 05:36:28.591500
1. set (Subject 14, F03) being trained for epoch 8!
Epoch 1/1
157/157 [==============================] - 61s 387ms/step - loss: 0.0172 - mean_absolute_error: 0.1017
2. set (Subject 3, F04) being trained for epoch 8!
Epoch 1/1
143/143 [==============================] - 56s 391ms/step - loss: 0.0087 - mean_absolute_error: 0.0729
3. set (Subject 8, M03) being trained for epoch 8!
Epoch 1/1
152/152 [==============================] - 59s 389ms/step - loss: 0.0098 - mean_absolute_error: 0.0756
4. set (Subject 2, F03) being trained for epoch 8!
Epoch 1/1
99/99 [==============================] - 39s 390ms/step - loss: 0.0167 - mean_absolute_error: 0.0977
5. set (Subject 11, M06) being trained for epoch 8!
Epoch 1/1
112/112 [==============================] - 43s 388ms/step - loss: 0.0082 - mean_absolute_error: 0.0633
6. set (Subject 10, M05) being trained for epoch 8!
Epoch 1/1
142/142 [==============================] - 55s 390ms/step - loss: 0.0088 - mean_absolute_error: 0.0698
7. set (Subject 1, F02) being trained for epoch 8!
Epoch 1/1
97/97 [==============================] - 38s 388ms/step - loss: 0.0109 - mean_absolute_error: 0.0780
8. set (Subject 7, M02) being trained for epoch 8!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0177 - mean_absolute_error: 0.1010
9. set (Subject 22, M13) being trained for epoch 8!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0159 - mean_absolute_error: 0.0912
10. set (Subject 6, M01) being trained for epoch 8!
Epoch 1/1
106/106 [==============================] - 41s 388ms/step - loss: 0.0434 - mean_absolute_error: 0.1394
11. set (Subject 13, M08) being trained for epoch 8!
Epoch 1/1
94/94 [==============================] - 37s 389ms/step - loss: 0.0092 - mean_absolute_error: 0.0666
12. set (Subject 19, M12) being trained for epoch 8!
Epoch 1/1
98/98 [==============================] - 38s 387ms/step - loss: 0.0165 - mean_absolute_error: 0.0938
13. set (Subject 4, F05) being trained for epoch 8!
Epoch 1/1
146/146 [==============================] - 57s 389ms/step - loss: 0.0094 - mean_absolute_error: 0.0749
14. set (Subject 16, M10) being trained for epoch 8!
Epoch 1/1
180/180 [==============================] - 70s 390ms/step - loss: 0.0166 - mean_absolute_error: 0.0973
15. set (Subject 23, M14) being trained for epoch 8!
Epoch 1/1
111/111 [==============================] - 43s 389ms/step - loss: 0.0376 - mean_absolute_error: 0.1460
16. set (Subject 17, F05) being trained for epoch 8!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0181 - mean_absolute_error: 0.1031
17. set (Subject 20, F02) being trained for epoch 8!
Epoch 1/1
108/108 [==============================] - 42s 390ms/step - loss: 0.0164 - mean_absolute_error: 0.0966
18. set (Subject 15, M09) being trained for epoch 8!
Epoch 1/1
128/128 [==============================] - 50s 389ms/step - loss: 0.0219 - mean_absolute_error: 0.1056
19. set (Subject 5, F06) being trained for epoch 8!
Epoch 1/1
186/186 [==============================] - 73s 391ms/step - loss: 0.0077 - mean_absolute_error: 0.0669
20. set (Subject 12, M07) being trained for epoch 8!
Epoch 1/1
144/144 [==============================] - 56s 389ms/step - loss: 0.0075 - mean_absolute_error: 0.0634
Epoch 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 05:55:17.702407
1. set (Subject 15, M09) being trained for epoch 9!
Epoch 1/1
128/128 [==============================] - 50s 387ms/step - loss: 0.0177 - mean_absolute_error: 0.0925
2. set (Subject 12, M07) being trained for epoch 9!
Epoch 1/1
144/144 [==============================] - 56s 389ms/step - loss: 0.0057 - mean_absolute_error: 0.0556
3. set (Subject 13, M08) being trained for epoch 9!
Epoch 1/1
94/94 [==============================] - 37s 390ms/step - loss: 0.0087 - mean_absolute_error: 0.0663
4. set (Subject 23, M14) being trained for epoch 9!
Epoch 1/1
111/111 [==============================] - 43s 389ms/step - loss: 0.0297 - mean_absolute_error: 0.1346
5. set (Subject 10, M05) being trained for epoch 9!
Epoch 1/1
142/142 [==============================] - 55s 391ms/step - loss: 0.0070 - mean_absolute_error: 0.0631
6. set (Subject 5, F06) being trained for epoch 9!
Epoch 1/1
186/186 [==============================] - 73s 391ms/step - loss: 0.0066 - mean_absolute_error: 0.0614
7. set (Subject 20, F02) being trained for epoch 9!
Epoch 1/1
108/108 [==============================] - 42s 391ms/step - loss: 0.0162 - mean_absolute_error: 0.0983
8. set (Subject 19, M12) being trained for epoch 9!
Epoch 1/1
98/98 [==============================] - 38s 388ms/step - loss: 0.0142 - mean_absolute_error: 0.0877
9. set (Subject 11, M06) being trained for epoch 9!
Epoch 1/1
112/112 [==============================] - 43s 388ms/step - loss: 0.0065 - mean_absolute_error: 0.0606
10. set (Subject 22, M13) being trained for epoch 9!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0162 - mean_absolute_error: 0.0952
11. set (Subject 1, F02) being trained for epoch 9!
Epoch 1/1
97/97 [==============================] - 38s 389ms/step - loss: 0.0084 - mean_absolute_error: 0.0709
12. set (Subject 14, F03) being trained for epoch 9!
Epoch 1/1
157/157 [==============================] - 61s 389ms/step - loss: 0.0224 - mean_absolute_error: 0.1184
13. set (Subject 16, M10) being trained for epoch 9!
Epoch 1/1
180/180 [==============================] - 70s 390ms/step - loss: 0.0117 - mean_absolute_error: 0.0816
14. set (Subject 3, F04) being trained for epoch 9!
Epoch 1/1
143/143 [==============================] - 56s 390ms/step - loss: 0.0135 - mean_absolute_error: 0.0910
15. set (Subject 8, M03) being trained for epoch 9!
Epoch 1/1
152/152 [==============================] - 59s 388ms/step - loss: 0.0151 - mean_absolute_error: 0.0918
16. set (Subject 17, F05) being trained for epoch 9!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0155 - mean_absolute_error: 0.0968
17. set (Subject 4, F05) being trained for epoch 9!
Epoch 1/1
146/146 [==============================] - 57s 391ms/step - loss: 0.0127 - mean_absolute_error: 0.0860
18. set (Subject 2, F03) being trained for epoch 9!
Epoch 1/1
99/99 [==============================] - 39s 390ms/step - loss: 0.0120 - mean_absolute_error: 0.0798
19. set (Subject 6, M01) being trained for epoch 9!
Epoch 1/1
106/106 [==============================] - 41s 388ms/step - loss: 0.0312 - mean_absolute_error: 0.1238
20. set (Subject 7, M02) being trained for epoch 9!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0163 - mean_absolute_error: 0.0964
Epoch 9 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 06:14:07.064366
1. set (Subject 2, F03) being trained for epoch 10!
Epoch 1/1
99/99 [==============================] - 38s 387ms/step - loss: 0.0075 - mean_absolute_error: 0.0663
2. set (Subject 7, M02) being trained for epoch 10!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0178 - mean_absolute_error: 0.1028
3. set (Subject 1, F02) being trained for epoch 10!
Epoch 1/1
97/97 [==============================] - 38s 388ms/step - loss: 0.0130 - mean_absolute_error: 0.0883
4. set (Subject 8, M03) being trained for epoch 10!
Epoch 1/1
152/152 [==============================] - 59s 388ms/step - loss: 0.0108 - mean_absolute_error: 0.0778
5. set (Subject 5, F06) being trained for epoch 10!
Epoch 1/1
186/186 [==============================] - 73s 390ms/step - loss: 0.0080 - mean_absolute_error: 0.0676
6. set (Subject 6, M01) being trained for epoch 10!
Epoch 1/1
106/106 [==============================] - 41s 389ms/step - loss: 0.0280 - mean_absolute_error: 0.1174
7. set (Subject 4, F05) being trained for epoch 10!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0087 - mean_absolute_error: 0.0732
8. set (Subject 14, F03) being trained for epoch 10!
Epoch 1/1
157/157 [==============================] - 61s 389ms/step - loss: 0.0171 - mean_absolute_error: 0.1017
9. set (Subject 10, M05) being trained for epoch 10!
Epoch 1/1
142/142 [==============================] - 55s 391ms/step - loss: 0.0068 - mean_absolute_error: 0.0631
10. set (Subject 11, M06) being trained for epoch 10!
Epoch 1/1
112/112 [==============================] - 43s 388ms/step - loss: 0.0057 - mean_absolute_error: 0.0540
11. set (Subject 20, F02) being trained for epoch 10!
Epoch 1/1
108/108 [==============================] - 42s 389ms/step - loss: 0.0157 - mean_absolute_error: 0.0946
12. set (Subject 15, M09) being trained for epoch 10!
Epoch 1/1
128/128 [==============================] - 50s 388ms/step - loss: 0.0192 - mean_absolute_error: 0.0993
13. set (Subject 3, F04) being trained for epoch 10!
Epoch 1/1
143/143 [==============================] - 56s 390ms/step - loss: 0.0105 - mean_absolute_error: 0.0817
14. set (Subject 12, M07) being trained for epoch 10!
Epoch 1/1
144/144 [==============================] - 56s 389ms/step - loss: 0.0074 - mean_absolute_error: 0.0655
15. set (Subject 13, M08) being trained for epoch 10!
Epoch 1/1
94/94 [==============================] - 37s 390ms/step - loss: 0.0107 - mean_absolute_error: 0.0747
16. set (Subject 17, F05) being trained for epoch 10!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0127 - mean_absolute_error: 0.0894
17. set (Subject 16, M10) being trained for epoch 10!
Epoch 1/1
180/180 [==============================] - 70s 389ms/step - loss: 0.0124 - mean_absolute_error: 0.0845
18. set (Subject 23, M14) being trained for epoch 10!
Epoch 1/1
111/111 [==============================] - 43s 390ms/step - loss: 0.0314 - mean_absolute_error: 0.1348
19. set (Subject 22, M13) being trained for epoch 10!
Epoch 1/1
130/130 [==============================] - 51s 391ms/step - loss: 0.0133 - mean_absolute_error: 0.0861
20. set (Subject 19, M12) being trained for epoch 10!
Epoch 1/1
98/98 [==============================] - 38s 388ms/step - loss: 0.0178 - mean_absolute_error: 0.1018
Epoch 10 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 06:32:55.772489
1. set (Subject 23, M14) being trained for epoch 11!
Epoch 1/1
111/111 [==============================] - 43s 386ms/step - loss: 0.0322 - mean_absolute_error: 0.1337
2. set (Subject 19, M12) being trained for epoch 11!
Epoch 1/1
98/98 [==============================] - 38s 388ms/step - loss: 0.0119 - mean_absolute_error: 0.0784
3. set (Subject 20, F02) being trained for epoch 11!
Epoch 1/1
108/108 [==============================] - 42s 391ms/step - loss: 0.0124 - mean_absolute_error: 0.0836
4. set (Subject 13, M08) being trained for epoch 11!
Epoch 1/1
94/94 [==============================] - 37s 390ms/step - loss: 0.0083 - mean_absolute_error: 0.0659
5. set (Subject 6, M01) being trained for epoch 11!
Epoch 1/1
106/106 [==============================] - 41s 388ms/step - loss: 0.0268 - mean_absolute_error: 0.1157
6. set (Subject 22, M13) being trained for epoch 11!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0100 - mean_absolute_error: 0.0770
7. set (Subject 16, M10) being trained for epoch 11!
Epoch 1/1
180/180 [==============================] - 70s 390ms/step - loss: 0.0123 - mean_absolute_error: 0.0845
8. set (Subject 15, M09) being trained for epoch 11!
Epoch 1/1
128/128 [==============================] - 50s 390ms/step - loss: 0.0136 - mean_absolute_error: 0.0844
9. set (Subject 5, F06) being trained for epoch 11!
Epoch 1/1
186/186 [==============================] - 73s 391ms/step - loss: 0.0079 - mean_absolute_error: 0.0669
10. set (Subject 10, M05) being trained for epoch 11!
Epoch 1/1
142/142 [==============================] - 55s 391ms/step - loss: 0.0073 - mean_absolute_error: 0.0642
11. set (Subject 4, F05) being trained for epoch 11!
Epoch 1/1
146/146 [==============================] - 57s 389ms/step - loss: 0.0089 - mean_absolute_error: 0.0734
12. set (Subject 2, F03) being trained for epoch 11!
Epoch 1/1
99/99 [==============================] - 39s 390ms/step - loss: 0.0101 - mean_absolute_error: 0.0765
13. set (Subject 12, M07) being trained for epoch 11!
Epoch 1/1
144/144 [==============================] - 56s 388ms/step - loss: 0.0065 - mean_absolute_error: 0.0588
14. set (Subject 7, M02) being trained for epoch 11!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0151 - mean_absolute_error: 0.0939
15. set (Subject 1, F02) being trained for epoch 11!
Epoch 1/1
97/97 [==============================] - 38s 388ms/step - loss: 0.0088 - mean_absolute_error: 0.0724
16. set (Subject 17, F05) being trained for epoch 11!
Epoch 1/1
76/76 [==============================] - 30s 391ms/step - loss: 0.0116 - mean_absolute_error: 0.0805
17. set (Subject 3, F04) being trained for epoch 11!
Epoch 1/1
143/143 [==============================] - 56s 390ms/step - loss: 0.0087 - mean_absolute_error: 0.0727
18. set (Subject 8, M03) being trained for epoch 11!
Epoch 1/1
152/152 [==============================] - 59s 389ms/step - loss: 0.0114 - mean_absolute_error: 0.0804
19. set (Subject 11, M06) being trained for epoch 11!
Epoch 1/1
112/112 [==============================] - 43s 388ms/step - loss: 0.0061 - mean_absolute_error: 0.0581
20. set (Subject 14, F03) being trained for epoch 11!
Epoch 1/1
157/157 [==============================] - 61s 389ms/step - loss: 0.0181 - mean_absolute_error: 0.1052
Epoch 11 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 06:51:44.819430
1. set (Subject 8, M03) being trained for epoch 12!
Epoch 1/1
152/152 [==============================] - 59s 387ms/step - loss: 0.0116 - mean_absolute_error: 0.0823
2. set (Subject 14, F03) being trained for epoch 12!
Epoch 1/1
157/157 [==============================] - 61s 388ms/step - loss: 0.0156 - mean_absolute_error: 0.0987
3. set (Subject 4, F05) being trained for epoch 12!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0099 - mean_absolute_error: 0.0777
4. set (Subject 1, F02) being trained for epoch 12!
Epoch 1/1
97/97 [==============================] - 38s 389ms/step - loss: 0.0112 - mean_absolute_error: 0.0804
5. set (Subject 22, M13) being trained for epoch 12!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0106 - mean_absolute_error: 0.0773
6. set (Subject 11, M06) being trained for epoch 12!
Epoch 1/1
112/112 [==============================] - 44s 389ms/step - loss: 0.0054 - mean_absolute_error: 0.0544
7. set (Subject 3, F04) being trained for epoch 12!
Epoch 1/1
143/143 [==============================] - 56s 390ms/step - loss: 0.0089 - mean_absolute_error: 0.0741
8. set (Subject 2, F03) being trained for epoch 12!
Epoch 1/1
99/99 [==============================] - 39s 390ms/step - loss: 0.0090 - mean_absolute_error: 0.0728
9. set (Subject 6, M01) being trained for epoch 12!
Epoch 1/1
106/106 [==============================] - 41s 389ms/step - loss: 0.0267 - mean_absolute_error: 0.1185
10. set (Subject 5, F06) being trained for epoch 12!
Epoch 1/1
186/186 [==============================] - 73s 391ms/step - loss: 0.0069 - mean_absolute_error: 0.0642
11. set (Subject 16, M10) being trained for epoch 12!
Epoch 1/1
180/180 [==============================] - 70s 390ms/step - loss: 0.0104 - mean_absolute_error: 0.0769
12. set (Subject 23, M14) being trained for epoch 12!
Epoch 1/1
111/111 [==============================] - 43s 390ms/step - loss: 0.0236 - mean_absolute_error: 0.1199
13. set (Subject 7, M02) being trained for epoch 12!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0135 - mean_absolute_error: 0.0883
14. set (Subject 19, M12) being trained for epoch 12!
Epoch 1/1
98/98 [==============================] - 38s 389ms/step - loss: 0.0128 - mean_absolute_error: 0.0832
15. set (Subject 20, F02) being trained for epoch 12!
Epoch 1/1
108/108 [==============================] - 42s 391ms/step - loss: 0.0139 - mean_absolute_error: 0.0882
16. set (Subject 17, F05) being trained for epoch 12!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0118 - mean_absolute_error: 0.0833
17. set (Subject 12, M07) being trained for epoch 12!
Epoch 1/1
144/144 [==============================] - 56s 389ms/step - loss: 0.0061 - mean_absolute_error: 0.0590
18. set (Subject 13, M08) being trained for epoch 12!
Epoch 1/1
94/94 [==============================] - 37s 390ms/step - loss: 0.0129 - mean_absolute_error: 0.0796
19. set (Subject 10, M05) being trained for epoch 12!
Epoch 1/1
142/142 [==============================] - 55s 390ms/step - loss: 0.0076 - mean_absolute_error: 0.0645
20. set (Subject 15, M09) being trained for epoch 12!
Epoch 1/1
128/128 [==============================] - 50s 389ms/step - loss: 0.0131 - mean_absolute_error: 0.0854
Epoch 12 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 07:10:34.263427
1. set (Subject 13, M08) being trained for epoch 13!
Epoch 1/1
94/94 [==============================] - 36s 386ms/step - loss: 0.0110 - mean_absolute_error: 0.0790
2. set (Subject 15, M09) being trained for epoch 13!
Epoch 1/1
128/128 [==============================] - 50s 389ms/step - loss: 0.0118 - mean_absolute_error: 0.0815
3. set (Subject 16, M10) being trained for epoch 13!
Epoch 1/1
180/180 [==============================] - 70s 390ms/step - loss: 0.0096 - mean_absolute_error: 0.0767
4. set (Subject 20, F02) being trained for epoch 13!
Epoch 1/1
108/108 [==============================] - 42s 392ms/step - loss: 0.0105 - mean_absolute_error: 0.0780
5. set (Subject 11, M06) being trained for epoch 13!
Epoch 1/1
112/112 [==============================] - 43s 388ms/step - loss: 0.0056 - mean_absolute_error: 0.0555
6. set (Subject 10, M05) being trained for epoch 13!
Epoch 1/1
142/142 [==============================] - 55s 390ms/step - loss: 0.0067 - mean_absolute_error: 0.0621
7. set (Subject 12, M07) being trained for epoch 13!
Epoch 1/1
144/144 [==============================] - 56s 389ms/step - loss: 0.0066 - mean_absolute_error: 0.0641
8. set (Subject 23, M14) being trained for epoch 13!
Epoch 1/1
111/111 [==============================] - 43s 390ms/step - loss: 0.0212 - mean_absolute_error: 0.1116
9. set (Subject 22, M13) being trained for epoch 13!
Epoch 1/1
130/130 [==============================] - 51s 391ms/step - loss: 0.0131 - mean_absolute_error: 0.0897
10. set (Subject 6, M01) being trained for epoch 13!
Epoch 1/1
106/106 [==============================] - 41s 390ms/step - loss: 0.0243 - mean_absolute_error: 0.1133
11. set (Subject 3, F04) being trained for epoch 13!
Epoch 1/1
143/143 [==============================] - 56s 390ms/step - loss: 0.0096 - mean_absolute_error: 0.0759
12. set (Subject 8, M03) being trained for epoch 13!
Epoch 1/1
152/152 [==============================] - 59s 390ms/step - loss: 0.0114 - mean_absolute_error: 0.0821
13. set (Subject 19, M12) being trained for epoch 13!
Epoch 1/1
98/98 [==============================] - 38s 389ms/step - loss: 0.0133 - mean_absolute_error: 0.0879
14. set (Subject 14, F03) being trained for epoch 13!
Epoch 1/1
157/157 [==============================] - 61s 389ms/step - loss: 0.0178 - mean_absolute_error: 0.1043
15. set (Subject 4, F05) being trained for epoch 13!
Epoch 1/1
146/146 [==============================] - 57s 389ms/step - loss: 0.0073 - mean_absolute_error: 0.0680
16. set (Subject 17, F05) being trained for epoch 13!
Epoch 1/1
76/76 [==============================] - 30s 390ms/step - loss: 0.0118 - mean_absolute_error: 0.0841
17. set (Subject 7, M02) being trained for epoch 13!
Epoch 1/1
146/146 [==============================] - 57s 389ms/step - loss: 0.0118 - mean_absolute_error: 0.0828
18. set (Subject 1, F02) being trained for epoch 13!
Epoch 1/1
97/97 [==============================] - 38s 389ms/step - loss: 0.0093 - mean_absolute_error: 0.0722
19. set (Subject 5, F06) being trained for epoch 13!
Epoch 1/1
186/186 [==============================] - 73s 390ms/step - loss: 0.0089 - mean_absolute_error: 0.0710
20. set (Subject 2, F03) being trained for epoch 13!
Epoch 1/1
99/99 [==============================] - 39s 391ms/step - loss: 0.0106 - mean_absolute_error: 0.0757
Epoch 13 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 07:29:24.028559
1. set (Subject 1, F02) being trained for epoch 14!
Epoch 1/1
97/97 [==============================] - 37s 384ms/step - loss: 0.0085 - mean_absolute_error: 0.0661
2. set (Subject 2, F03) being trained for epoch 14!
Epoch 1/1
99/99 [==============================] - 39s 392ms/step - loss: 0.0076 - mean_absolute_error: 0.0686
3. set (Subject 3, F04) being trained for epoch 14!
Epoch 1/1
143/143 [==============================] - 56s 390ms/step - loss: 0.0085 - mean_absolute_error: 0.0714
4. set (Subject 4, F05) being trained for epoch 14!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0080 - mean_absolute_error: 0.0720
5. set (Subject 10, M05) being trained for epoch 14!
Epoch 1/1
142/142 [==============================] - 55s 391ms/step - loss: 0.0066 - mean_absolute_error: 0.0615
6. set (Subject 5, F06) being trained for epoch 14!
Epoch 1/1
186/186 [==============================] - 73s 391ms/step - loss: 0.0066 - mean_absolute_error: 0.0614
7. set (Subject 7, M02) being trained for epoch 14!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0114 - mean_absolute_error: 0.0814
8. set (Subject 8, M03) being trained for epoch 14!
Epoch 1/1
152/152 [==============================] - 59s 389ms/step - loss: 0.0086 - mean_absolute_error: 0.0684
9. set (Subject 11, M06) being trained for epoch 14!
Epoch 1/1
112/112 [==============================] - 44s 390ms/step - loss: 0.0057 - mean_absolute_error: 0.0552
10. set (Subject 22, M13) being trained for epoch 14!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0110 - mean_absolute_error: 0.0808
11. set (Subject 12, M07) being trained for epoch 14!
Epoch 1/1
144/144 [==============================] - 56s 389ms/step - loss: 0.0058 - mean_absolute_error: 0.0579
12. set (Subject 13, M08) being trained for epoch 14!
Epoch 1/1
94/94 [==============================] - 37s 390ms/step - loss: 0.0090 - mean_absolute_error: 0.0691
13. set (Subject 14, F03) being trained for epoch 14!
Epoch 1/1
157/157 [==============================] - 61s 389ms/step - loss: 0.0140 - mean_absolute_error: 0.0920
14. set (Subject 15, M09) being trained for epoch 14!
Epoch 1/1
128/128 [==============================] - 50s 390ms/step - loss: 0.0130 - mean_absolute_error: 0.0847
15. set (Subject 16, M10) being trained for epoch 14!
Epoch 1/1
180/180 [==============================] - 70s 390ms/step - loss: 0.0135 - mean_absolute_error: 0.0892
16. set (Subject 17, F05) being trained for epoch 14!
Epoch 1/1
76/76 [==============================] - 30s 391ms/step - loss: 0.0136 - mean_absolute_error: 0.0902
17. set (Subject 19, M12) being trained for epoch 14!
Epoch 1/1
98/98 [==============================] - 38s 388ms/step - loss: 0.0142 - mean_absolute_error: 0.0910
18. set (Subject 20, F02) being trained for epoch 14!
Epoch 1/1
108/108 [==============================] - 42s 390ms/step - loss: 0.0108 - mean_absolute_error: 0.0799
19. set (Subject 6, M01) being trained for epoch 14!
Epoch 1/1
106/106 [==============================] - 41s 388ms/step - loss: 0.0248 - mean_absolute_error: 0.1123
20. set (Subject 23, M14) being trained for epoch 14!
Epoch 1/1
111/111 [==============================] - 43s 389ms/step - loss: 0.0234 - mean_absolute_error: 0.1188
Epoch 14 completed!
All frames and annotations from 20 datasets have been read by 2019-01-16 07:48:13.822934
1. set (Subject 20, F02) being trained for epoch 15!
Epoch 1/1
108/108 [==============================] - 42s 388ms/step - loss: 0.0086 - mean_absolute_error: 0.0706
2. set (Subject 23, M14) being trained for epoch 15!
Epoch 1/1
111/111 [==============================] - 43s 390ms/step - loss: 0.0197 - mean_absolute_error: 0.1067
3. set (Subject 12, M07) being trained for epoch 15!
Epoch 1/1
144/144 [==============================] - 56s 389ms/step - loss: 0.0054 - mean_absolute_error: 0.0567
4. set (Subject 16, M10) being trained for epoch 15!
Epoch 1/1
180/180 [==============================] - 70s 391ms/step - loss: 0.0093 - mean_absolute_error: 0.0744
5. set (Subject 5, F06) being trained for epoch 15!
Epoch 1/1
186/186 [==============================] - 73s 391ms/step - loss: 0.0062 - mean_absolute_error: 0.0617
6. set (Subject 6, M01) being trained for epoch 15!
Epoch 1/1
106/106 [==============================] - 41s 389ms/step - loss: 0.0234 - mean_absolute_error: 0.1069
7. set (Subject 19, M12) being trained for epoch 15!
Epoch 1/1
98/98 [==============================] - 38s 388ms/step - loss: 0.0108 - mean_absolute_error: 0.0778
8. set (Subject 13, M08) being trained for epoch 15!
Epoch 1/1
94/94 [==============================] - 37s 390ms/step - loss: 0.0090 - mean_absolute_error: 0.0702
9. set (Subject 10, M05) being trained for epoch 15!
Epoch 1/1
142/142 [==============================] - 56s 392ms/step - loss: 0.0103 - mean_absolute_error: 0.0767
10. set (Subject 11, M06) being trained for epoch 15!
Epoch 1/1
112/112 [==============================] - 44s 389ms/step - loss: 0.0052 - mean_absolute_error: 0.0548
11. set (Subject 7, M02) being trained for epoch 15!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0126 - mean_absolute_error: 0.0868
12. set (Subject 1, F02) being trained for epoch 15!
Epoch 1/1
97/97 [==============================] - 38s 389ms/step - loss: 0.0079 - mean_absolute_error: 0.0632
13. set (Subject 15, M09) being trained for epoch 15!
Epoch 1/1
128/128 [==============================] - 50s 390ms/step - loss: 0.0118 - mean_absolute_error: 0.0817
14. set (Subject 2, F03) being trained for epoch 15!
Epoch 1/1
99/99 [==============================] - 39s 390ms/step - loss: 0.0081 - mean_absolute_error: 0.0662
15. set (Subject 3, F04) being trained for epoch 15!
Epoch 1/1
143/143 [==============================] - 56s 390ms/step - loss: 0.0080 - mean_absolute_error: 0.0672
16. set (Subject 17, F05) being trained for epoch 15!
Epoch 1/1
76/76 [==============================] - 30s 391ms/step - loss: 0.0107 - mean_absolute_error: 0.0803
17. set (Subject 14, F03) being trained for epoch 15!
Epoch 1/1
157/157 [==============================] - 61s 389ms/step - loss: 0.0143 - mean_absolute_error: 0.0952
18. set (Subject 4, F05) being trained for epoch 15!
Epoch 1/1
146/146 [==============================] - 57s 390ms/step - loss: 0.0087 - mean_absolute_error: 0.0714
19. set (Subject 22, M13) being trained for epoch 15!
Epoch 1/1
130/130 [==============================] - 51s 390ms/step - loss: 0.0086 - mean_absolute_error: 0.0706
20. set (Subject 8, M03) being trained for epoch 15!
Epoch 1/1
152/152 [==============================] - 59s 389ms/step - loss: 0.0078 - mean_absolute_error: 0.0671
Epoch 15 completed!
VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38.h5 has been saved.
The subjects are trained: [(20, 'F02'), (23, 'M14'), (12, 'M07'), (16, 'M10'), (5, 'F06'), (6, 'M01'), (19, 'M12'), (
13, 'M08'), (10, 'M05'), (11, 'M06'), (7, 'M02'), (1, 'F02'), (15, 'M09'), (2, 'F03'), (3, 'F04'), (17, 'F05'), (14,
'F03'), (4, 'F05'), (22, 'M13'), (8, 'M03')]
Evaluating model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38
The subjects will be tested: [(20, 'F02'), (23, 'M14'), (12, 'M07'), (16, 'M10'), (5, 'F06'), (6, 'M01'), (19, 'M12')
, (13, 'M08'), (10, 'M05'), (11, 'M06'), (7, 'M02'), (1, 'F02'), (15, 'M09'), (2, 'F03'), (3, 'F04'), (17, 'F05'), (1
4, 'F03'), (4, 'F05'), (22, 'M13'), (8, 'M03')]
All frames and annotations from 4 datasets have been read by 2019-01-16 08:07:00.361955
For the Subject 9 (M04):
217/217 [==============================] - 58s 268ms/step
        The absolute mean error on Pitch angle estimation: 12.58 Degree
        The absolute mean error on Yaw angle estimation: 20.57 Degree
        The absolute mean error on Roll angle estimation: 6.23 Degree
For the Subject 18 (M11):
150/150 [==============================] - 41s 271ms/step
        The absolute mean error on Pitch angle estimation: 11.66 Degree
        The absolute mean error on Yaw angle estimation: 17.86 Degree
        The absolute mean error on Roll angle estimation: 8.63 Degree
For the Subject 21 (M01):
155/155 [==============================] - 42s 271ms/step
        The absolute mean error on Pitch angle estimation: 15.25 Degree
        The absolute mean error on Yaw angle estimation: 18.45 Degree
        The absolute mean error on Roll angle estimation: 12.81 Degree
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 217, in <module>
    main()
  File "runCNN_LSTM.py", line 214, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 207, in runCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, record = record)
  File "runCNN_LSTM.py", line 150, in evaluateCNN_LSTM
    outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, record = re
cord)
  File "runCNN_LSTM.py", line 116, in evaluateSubject
    printLog('For the Subject %d (%s):' % (subject, BIWI_Subject_IDs[subject]), record = record)
IndexError: list index out of range
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ ls
continueTrainigCNN_LSTM.py  LSTM_VGG16Helper.pyc       NeighborFolderimporter.pyc  runCNN_LSTM.py         trainLSTM_V
GG16.py
EvaluationRecorder.py       LSTM_VGG16.pyproj          __pycache__                 scrollback.txt         VGG_model.t
xt
LSTM_VGG16Helper.py         NeighborFolderimporter.py  results                     tf_trainLSTM_VGG16.py
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit -m "saving scrollback"
[master d6674d8] saving scrollback
 2 files changed, 1445 insertions(+)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model/output_Last_Model.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/scrollback.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
remote: Counting objects: 11, done.
remote: Compressing objects: 100% (11/11), done.
remote: Total 11 (delta 9), reused 0 (delta 0)
Unpacking objects: 100% (11/11), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   8cca94d..f8fbe7c  master     -> origin/master
Removing foo.png
Removing DeepRL_For_HPE/LSTM_VGG16/results/VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_201
9-01-16_03-07-03/output_VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-07-03.tx
t
Merge made by the 'recursive' strategy.
 ..._lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_03-07-03.txt |  82 -------------------------
---------------
 foo.png                                                                              | Bin 148226 -> 0 bytes
 2 files changed, 82 deletions(-)
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.
000100)_2019-01-16_03-07-03/output_VGG16_seqLen1_lstm26_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-16_
03-07-03.txt
 delete mode 100644 foo.png
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-19 18:23:38.794143: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-19 18:23:38.864704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-19 18:23:38.864957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-19 18:23:38.864968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-19 18:23:39.018982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-19 18:23:39.019008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-19 18:23:39.019013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-19 18:23:39.019142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9536 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
Model 19-01-19_18-23-39 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 1, 7, 7, 512)      14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 1, 25088)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 2)                 200728
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 9
=================================================================
Total params: 14,915,425
Trainable params: 200,737
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen1_lstm2_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-19_18-23-39
All frames and annotations from 1 datasets have been read by 2019-01-19 18:23:40.271224
1. set (Subject 9, M04) being trained for epoch 1!
Epoch 1/1
36/36 [==============================] - 6s 172ms/step - loss: 0.0811 - mean_absolute_error: 0.2190
Epoch 1 completed!
The subjects are trained: [(9, 'M04')]
Evaluating model VGG16_seqLen1_lstm2_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-19_18-23-39
The subjects will be tested: [(9, 'M04')]
All frames and annotations from 1 datasets have been read by 2019-01-19 18:23:56.361884
Traceback (most recent call last):
  File "runCNN_LSTM.py", line 148, in <module>
    main()
  File "runCNN_LSTM.py", line 145, in main
    runCNN_LSTM(record = RECORD)
  File "runCNN_LSTM.py", line 138, in runCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, record = record)
  File "runCNN_LSTM.py", line 80, in evaluateCNN_LSTM
    outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, record = re
cord)
  File "runCNN_LSTM.py", line 46, in evaluateSubject
    printLog('For the Subject %d (%s):' % (subject, BIWI_Subject_IDs[subject]), record = record)
IndexError: list index out of range
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-19 18:49:59.659282: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-19 18:49:59.730702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-19 18:49:59.730955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-19 18:49:59.730967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-19 18:49:59.884276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-19 18:49:59.884302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-19 18:49:59.884311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-19 18:49:59.884442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9536 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
Model 19-01-19_18-50-00 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 1, 7, 7, 512)      14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 1, 25088)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 2)                 200728
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 9
=================================================================
Total params: 14,915,425
Trainable params: 200,737
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen1_lstm2_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-19_18-50-00
All frames and annotations from 1 datasets have been read by 2019-01-19 18:50:01.116532
1. set (Subject 9, M03) being trained for epoch 1!
Epoch 1/1
36/36 [==============================] - 6s 172ms/step - loss: 0.0833 - mean_absolute_error: 0.2155
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_seqLen1_lstm2_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-19_18-50-00
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-19 18:50:17.113935
For the Subject 24 (M14):
21/21 [==============================] - 3s 159ms/step
        The absolute mean error on Pitch angle estimation: 27.73 Degree
        The absolute mean error on Yaw angle estimation: 12.11 Degree
        The absolute mean error on Roll angle estimation: 6.37 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 27.73 Degree
        The absolute mean error on Yaw angle estimations: 12.11 Degree
        The absolute mean error on Roll angle estimations: 6.37 Degree
subject24_VGG16_seqLen1_lstm2_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-19_18-50-00.png has been save
d by 2019-01-19 18:50:25.275727.
Model 19-01-19_18-50-00 has been evaluated successfully.
Model 19-01-19_18-50-00 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-19 18:57:08.425383: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-19 18:57:08.496921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-19 18:57:08.497177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-19 18:57:08.497190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-19 18:57:08.651732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-19 18:57:08.651759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-19 18:57:08.651764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-19 18:57:08.651895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9537 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp19-01-19_18-57-09 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 1, 7, 7, 512)      14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 1, 25088)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 2)                 200728
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 9
=================================================================
Total params: 14,915,425
Trainable params: 200,737
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen1_lstm2_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-19_18-57-09
All frames and annotations from 1 datasets have been read by 2019-01-19 18:57:09.866795
1. set (Subject 9, M03) being trained for epoch 1!
Epoch 1/1
36/36 [==============================] - 6s 169ms/step - loss: 0.0762 - mean_absolute_error: 0.2126
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_seqLen1_lstm2_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-19_18-57-09
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-19 18:57:25.789556
For the Subject 24 (M14):
21/21 [==============================] - 3s 159ms/step
        The absolute mean error on Pitch angle estimation: 28.57 Degree
        The absolute mean error on Yaw angle estimation: 12.28 Degree
        The absolute mean error on Roll angle estimation: 7.72 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 28.57 Degree
        The absolute mean error on Yaw angle estimations: 12.28 Degree
        The absolute mean error on Roll angle estimations: 7.72 Degree
subject24_Exp19-01-19_18-57-09.png has been saved by 2019-01-19 18:57:33.995806.
Model Exp19-01-19_18-57-09 has been evaluated successfully.
Model Exp19-01-19_18-57-09 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-19 18:59:26.717949: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-19 18:59:26.792890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-19 18:59:26.793148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-19 18:59:26.793161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-19 18:59:26.947219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-19 18:59:26.947243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-19 18:59:26.947248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-19 18:59:26.947381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9537 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-19_18-59-27 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 1, 7, 7, 512)      14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 1, 25088)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 2)                 200728
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 9
=================================================================
Total params: 14,915,425
Trainable params: 200,737
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen1_lstm2_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-19_18-59-27
All frames and annotations from 1 datasets have been read by 2019-01-19 18:59:28.152572
1. set (Subject 9, M03) being trained for epoch 1!
Epoch 1/1
36/36 [==============================] - 6s 169ms/step - loss: 0.0752 - mean_absolute_error: 0.2089
Epoch 1 completed!
The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_seqLen1_lstm2_output3_inEpochs1_outEpochs1_AdamOpt(lr=0.000100)_2019-01-19_18-59-27
The subjects will be tested: [(9, 'M03')]
All frames and annotations from 1 datasets have been read by 2019-01-19 18:59:44.050468
For the Subject 24 (M14):
21/21 [==============================] - 3s 159ms/step
        The absolute mean error on Pitch angle estimation: 28.09 Degree
        The absolute mean error on Yaw angle estimation: 12.06 Degree
        The absolute mean error on Roll angle estimation: 6.40 Degree
On average in 1 test subjects:
        The absolute mean error on Pitch angle estimations: 28.09 Degree
        The absolute mean error on Yaw angle estimations: 12.06 Degree
        The absolute mean error on Roll angle estimations: 6.40 Degree
subject24_Exp2019-01-19_18-59-27.png has been saved by 2019-01-19 18:59:52.278717.
Model Exp2019-01-19_18-59-27 has been evaluated successfully.
Model Exp2019-01-19_18-59-27 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_LSTM.py VG
G16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38
-bash: syntax error near unexpected token `('
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_LSTM.py "V
GG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38"
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-19 21:24:15.712213: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-19 21:24:15.783491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-19 21:24:15.783803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-19 21:24:15.783819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-19 21:24:15.937837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-19 21:24:15.937864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-19 21:24:15.937869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-19 21:24:15.938043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9536 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
lstm_1 (LSTM)                (None, 256)               25953280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 771
=================================================================
Total params: 40,668,739
Trainable params: 25,954,051
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38_and_2019
-01-19_21-24-16
All frames and annotations from 1 datasets have been read by 2019-01-19 21:24:17.879426
1. set (Subject 9, M03) being trained for epoch 1!
Epoch 1/7
Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 61, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 58, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 41, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 39, in trainC
NN_LSTM
    in_epochs = in_epochs, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line 52, in t
rainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_outputs, bat
ch_size, in_epochs, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line 44, in t
rainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_generator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1877, in train_on_batch
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1476, in _standardize_user
_data
    exception_prefix='input')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 123, in _standardize_input
_data
    str(data_shape))
ValueError: Error when checking input: expected tdVGG16_input to have shape (16, 224, 224, 3) but got array with shap
e (1, 224, 224, 3)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_LSTM.py "V
GG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38"
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-19 21:28:16.726330: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-19 21:28:16.796913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-19 21:28:16.797169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-19 21:28:16.797181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-19 21:28:16.950754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-19 21:28:16.950779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-19 21:28:16.950783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-19 21:28:16.950911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9536 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
lstm_1 (LSTM)                (None, 256)               25953280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 771
=================================================================
Total params: 40,668,739
Trainable params: 25,954,051
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38_and_2019
-01-19_21-28-17
All frames and annotations from 1 datasets have been read by 2019-01-19 21:28:18.882459
1. set (Subject 9, M03) being trained for epoch 1!
Epoch 1/7
^C2019-01-19 21:28:38.647198: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of
 memory trying to allocate 4.79GiB.  Current allocation summary follows.
2019-01-19 21:28:38.647286: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 77, Chu
nks in use: 77. 19.2KiB allocated for chunks. 19.2KiB in use in bin. 1.2KiB client-requested in use in bin.
2019-01-19 21:28:38.647339: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 4, Chun
ks in use: 4. 2.0KiB allocated for chunks. 2.0KiB in use in bin. 1.8KiB client-requested in use in bin.
2019-01-19 21:28:38.647365: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 9, Chun
ks in use: 9. 9.2KiB allocated for chunks. 9.2KiB in use in bin. 9.0KiB client-requested in use in bin.
2019-01-19 21:28:38.647390: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 13, Chu
nks in use: 13. 32.8KiB allocated for chunks. 32.8KiB in use in bin. 32.0KiB client-requested in use in bin.
2019-01-19 21:28:38.647414: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 9, Chun
ks in use: 9. 41.5KiB allocated for chunks. 41.5KiB in use in bin. 41.5KiB client-requested in use in bin.
2019-01-19 21:28:38.647439: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chun
ks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:28:38.647463: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:28:38.647484: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:28:38.647506: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks:
 1, Chunks in use: 0. 90.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:28:38.647531: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks:
 1, Chunks in use: 1. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.
2019-01-19 21:28:38.647556: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks:
 9, Chunks in use: 9. 2.38MiB allocated for chunks. 2.38MiB in use in bin. 2.28MiB client-requested in use in bin.
2019-01-19 21:28:38.647582: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks:
 1, Chunks in use: 1. 576.0KiB allocated for chunks. 576.0KiB in use in bin. 576.0KiB client-requested in use in bin.
2019-01-19 21:28:38.647604: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks:
 9, Chunks in use: 8. 9.75MiB allocated for chunks. 8.50MiB in use in bin. 8.12MiB client-requested in use in bin.
2019-01-19 21:28:38.647626: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks:
 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-19 21:28:38.647651: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks:
 1, Chunks in use: 1. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-19 21:28:38.647676: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks:
 6, Chunks in use: 5. 54.00MiB allocated for chunks. 45.00MiB in use in bin. 45.00MiB client-requested in use in bin.
2019-01-19 21:28:38.647704: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks:
 8, Chunks in use: 8. 196.00MiB allocated for chunks. 196.00MiB in use in bin. 196.00MiB client-requested in use in b
in.
2019-01-19 21:28:38.647727: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:28:38.647754: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks:
 5, Chunks in use: 5. 490.00MiB allocated for chunks. 490.00MiB in use in bin. 490.00MiB client-requested in use in b
in.
2019-01-19 21:28:38.647783: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks:
 2, Chunks in use: 2. 361.38MiB allocated for chunks. 361.38MiB in use in bin. 327.69MiB client-requested in use in b
in.
2019-01-19 21:28:38.647809: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks:
 2, Chunks in use: 1. 8.22GiB allocated for chunks. 4.79GiB in use in bin. 4.79GiB client-requested in use in bin.
2019-01-19 21:28:38.647833: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 4.79GiB was 256.00MiB, Chu
nk State:
2019-01-19 21:28:38.647864: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 3.43GiB | Requested Size:
0B | in_use: 0, prev:   Size: 24.50MiB | Requested Size: 24.50MiB | in_use: 1
2019-01-19 21:28:38.647887: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500000 of size 1280
2019-01-19 21:28:38.647907: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500500 of size 256
2019-01-19 21:28:38.647925: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500600 of size 256
2019-01-19 21:28:38.647943: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500700 of size 256
2019-01-19 21:28:38.647961: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500800 of size 256
2019-01-19 21:28:38.647979: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500900 of size 256
2019-01-19 21:28:38.647996: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500a00 of size 256
2019-01-19 21:28:38.648014: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500b00 of size 256
2019-01-19 21:28:38.648034: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500c00 of size 512
2019-01-19 21:28:38.648052: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500e00 of size 256
2019-01-19 21:28:38.648070: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500f00 of size 256
2019-01-19 21:28:38.648088: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501000 of size 256
2019-01-19 21:28:38.648105: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501100 of size 256
2019-01-19 21:28:38.648124: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501200 of size 256
2019-01-19 21:28:38.648143: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501300 of size 1024
2019-01-19 21:28:38.648160: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501700 of size 256
2019-01-19 21:28:38.648178: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501800 of size 256
2019-01-19 21:28:38.648196: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501900 of size 256
2019-01-19 21:28:38.648214: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501a00 of size 256
2019-01-19 21:28:38.648232: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501b00 of size 256
2019-01-19 21:28:38.648250: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501c00 of size 2048
2019-01-19 21:28:38.648268: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502400 of size 256
2019-01-19 21:28:38.648286: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502500 of size 256
2019-01-19 21:28:38.648305: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502600 of size 10485
76
2019-01-19 21:28:38.648325: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a602600 of size 4096
2019-01-19 21:28:38.648343: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603600 of size 256
2019-01-19 21:28:38.648361: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603700 of size 256
2019-01-19 21:28:38.648378: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603800 of size 256
2019-01-19 21:28:38.648397: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603900 of size 3072
2019-01-19 21:28:38.648417: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a604500 of size 3840
2019-01-19 21:28:38.648435: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a605400 of size 256
2019-01-19 21:28:38.648453: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a605500 of size 4096
2019-01-19 21:28:38.648471: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a606500 of size 4096
2019-01-19 21:28:38.648489: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a607500 of size 3072
2019-01-19 21:28:38.648507: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a608100 of size 256
2019-01-19 21:28:38.648525: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a608200 of size 4096
2019-01-19 21:28:38.648542: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609200 of size 3072
2019-01-19 21:28:38.648560: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609e00 of size 256
2019-01-19 21:28:38.648578: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609f00 of size 256
2019-01-19 21:28:38.648596: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a000 of size 256
2019-01-19 21:28:38.648614: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a100 of size 256
2019-01-19 21:28:38.648632: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a200 of size 256
2019-01-19 21:28:38.648649: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a300 of size 256
2019-01-19 21:28:38.648667: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a400 of size 256
2019-01-19 21:28:38.648684: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a500 of size 256
2019-01-19 21:28:38.648702: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a600 of size 256
2019-01-19 21:28:38.648720: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a700 of size 256
2019-01-19 21:28:38.648738: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a800 of size 256
2019-01-19 21:28:38.648755: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a900 of size 256
2019-01-19 21:28:38.648773: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60aa00 of size 256
2019-01-19 21:28:38.648791: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60ab00 of size 1024
2019-01-19 21:28:38.648809: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60af00 of size 1024
2019-01-19 21:28:38.648827: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60b300 of size 1024
2019-01-19 21:28:38.648845: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60b700 of size 1024
2019-01-19 21:28:38.648863: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60bb00 of size 256
2019-01-19 21:28:38.648881: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60bc00 of size 256
2019-01-19 21:28:38.648899: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60bd00 of size 256
2019-01-19 21:28:38.648917: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60be00 of size 256
2019-01-19 21:28:38.648935: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60bf00 of size 256
2019-01-19 21:28:38.648952: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60c000 of size 256
2019-01-19 21:28:38.648970: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60c100 of size 256
2019-01-19 21:28:38.648988: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60c200 of size 256
2019-01-19 21:28:38.649005: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60c300 of size 256
2019-01-19 21:28:38.649023: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60c400 of size 256
2019-01-19 21:28:38.649041: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60c500 of size 256
2019-01-19 21:28:38.649058: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60c600 of size 256
2019-01-19 21:28:38.649076: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60c700 of size 256
2019-01-19 21:28:38.649094: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60c800 of size 256
2019-01-19 21:28:38.649112: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60c900 of size 256
2019-01-19 21:28:38.649130: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60ca00 of size 256
2019-01-19 21:28:38.649147: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60cb00 of size 256
2019-01-19 21:28:38.649165: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60cc00 of size 256
2019-01-19 21:28:38.649183: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60cd00 of size 512
2019-01-19 21:28:38.649200: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60cf00 of size 256
2019-01-19 21:28:38.649218: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60d000 of size 256
2019-01-19 21:28:38.649236: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60d100 of size 256
2019-01-19 21:28:38.649253: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60d200 of size 256
2019-01-19 21:28:38.649272: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60d300 of size 6912
2019-01-19 21:28:38.649290: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60ee00 of size 256
2019-01-19 21:28:38.649308: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60ef00 of size 4096
2019-01-19 21:28:38.649326: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60ff00 of size 3072
2019-01-19 21:28:38.649344: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a610b00 of size 256
2019-01-19 21:28:38.649361: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a610c00 of size 256
2019-01-19 21:28:38.649379: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a610d00 of size 4096
2019-01-19 21:28:38.649397: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a611d00 of size 3072
2019-01-19 21:28:38.649416: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a612900 of size 256
2019-01-19 21:28:38.649434: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a612a00 of size 92928
2019-01-19 21:28:38.649452: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a629500 of size 256
2019-01-19 21:28:38.649471: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a629600 of size 29491
2
2019-01-19 21:28:38.649489: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a671600 of size 512
2019-01-19 21:28:38.649508: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a671800 of size 26214
4
2019-01-19 21:28:38.649528: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a6b1800 of size 32768
0
2019-01-19 21:28:38.649546: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a701800 of size 512
2019-01-19 21:28:38.649565: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a701a00 of size 11796
48
2019-01-19 21:28:38.649583: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a821a00 of size 1024
2019-01-19 21:28:38.649602: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a821e00 of size 23592
96
2019-01-19 21:28:38.649620: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aa61e00 of size 10485
76
2019-01-19 21:28:38.649639: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ab61e00 of size 13107
20
2019-01-19 21:28:38.649658: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca1e00 of size 1024
2019-01-19 21:28:38.649675: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca2200 of size 1024
2019-01-19 21:28:38.649713: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca2600 of size 26214
4
2019-01-19 21:28:38.649732: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ace2600 of size 26214
4
2019-01-19 21:28:38.649750: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ad22600 of size 26214
4
2019-01-19 21:28:38.649768: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ad62600 of size 26214
4
2019-01-19 21:28:38.649786: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ada2600 of size 26214
4
2019-01-19 21:28:38.649804: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ade2600 of size 10485
76
2019-01-19 21:28:38.649822: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aee2600 of size 10485
76
2019-01-19 21:28:38.649840: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0afe2600 of size 13107
20
2019-01-19 21:28:38.649858: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b122600 of size 2048
2019-01-19 21:28:38.649878: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b122e00 of size 94371
84
2019-01-19 21:28:38.649897: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ba22e00 of size 94371
84
2019-01-19 21:28:38.649915: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0c322e00 of size 94371
84
2019-01-19 21:28:38.649933: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0cc22e00 of size 94371
84
2019-01-19 21:28:38.649951: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0d522e00 of size 94371
84
2019-01-19 21:28:38.649970: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0de22e00 of size 2048
2019-01-19 21:28:38.649988: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0de23600 of size 10485
76
2019-01-19 21:28:38.650006: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df23600 of size 4096
2019-01-19 21:28:38.709413: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24600 of size 2048
2019-01-19 21:28:38.709437: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24e00 of size 256
2019-01-19 21:28:38.709447: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24f00 of size 256
2019-01-19 21:28:38.709454: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25000 of size 256
2019-01-19 21:28:38.709459: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25100 of size 256
2019-01-19 21:28:38.709463: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25200 of size 256
2019-01-19 21:28:38.709468: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25300 of size 256
2019-01-19 21:28:38.709473: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25400 of size 256
2019-01-19 21:28:38.709478: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25500 of size 256
2019-01-19 21:28:38.709483: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25600 of size 256
2019-01-19 21:28:38.709488: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25700 of size 256
2019-01-19 21:28:38.709494: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25800 of size 256
2019-01-19 21:28:38.709499: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25900 of size 256
2019-01-19 21:28:38.709503: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25a00 of size 2048
2019-01-19 21:28:38.709509: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26200 of size 2048
2019-01-19 21:28:38.709513: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26a00 of size 256
2019-01-19 21:28:38.709518: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26b00 of size 2048
2019-01-19 21:28:38.709525: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df27300 of size 10276
0448
2019-01-19 21:28:38.709530: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14127300 of size 6912
2019-01-19 21:28:38.709536: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14128e00 of size 14745
6
2019-01-19 21:28:38.709542: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1414ce00 of size 29491
2
2019-01-19 21:28:38.709547: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14194e00 of size 58982
4
2019-01-19 21:28:38.709553: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14224e00 of size 11796
48
2019-01-19 21:28:38.709558: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14344e00 of size 23592
96
2019-01-19 21:28:38.709564: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14584e00 of size 47185
92
2019-01-19 21:28:38.709570: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14a04e00 of size 94371
84
2019-01-19 21:28:38.709575: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb15304e00 of size 10276
0448
2019-01-19 21:28:38.709580: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1b504e00 of size 10276
0448
2019-01-19 21:28:38.709586: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb21704e00 of size 10276
0448
2019-01-19 21:28:38.709591: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb27904e00 of size 25690
112
2019-01-19 21:28:38.709597: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb29184e00 of size 25690
112
2019-01-19 21:28:38.709602: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2aa04e00 of size 25690
112
2019-01-19 21:28:38.709608: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2c284e00 of size 25690
112
2019-01-19 21:28:38.709613: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb2db04e00 of size 10276
0448
2019-01-19 21:28:38.709619: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb33d04e00 of size 13808
4352
2019-01-19 21:28:38.709625: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb3c0b4e00 of size 24084
4800
2019-01-19 21:28:38.709631: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb4a664e00 of size 51380
22400
2019-01-19 21:28:38.709636: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xc7ca64e00 of size 25690
112
2019-01-19 21:28:38.709642: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xc7e2e4e00 of size 25690
112
2019-01-19 21:28:38.709647: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xc7fb64e00 of size 25690
112
2019-01-19 21:28:38.709652: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xc813e4e00 of size 25690
112
2019-01-19 21:28:38.709658: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xc82c64e00 of size 36837
12512
2019-01-19 21:28:38.709663: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by s
ize:
2019-01-19 21:28:38.709673: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 77 Chunks of size 256 totalling 19
.2KiB
2019-01-19 21:28:38.709685: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 512 totalling 2.0
KiB
2019-01-19 21:28:38.709692: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 8 Chunks of size 1024 totalling 8.
0KiB
2019-01-19 21:28:38.709698: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.
2KiB
2019-01-19 21:28:38.709704: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 7 Chunks of size 2048 totalling 14
.0KiB
2019-01-19 21:28:38.709710: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 3072 totalling 15
.0KiB
2019-01-19 21:28:38.709716: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3840 totalling 3.
8KiB
2019-01-19 21:28:38.709722: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 7 Chunks of size 4096 totalling 28
.0KiB
2019-01-19 21:28:38.709728: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 6912 totalling 13
.5KiB
2019-01-19 21:28:38.709734: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 147456 totalling
144.0KiB
2019-01-19 21:28:38.709741: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 6 Chunks of size 262144 totalling
1.50MiB
2019-01-19 21:28:38.709747: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 294912 totalling
576.0KiB
2019-01-19 21:28:38.709753: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 327680 totalling
320.0KiB
2019-01-19 21:28:38.709759: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 589824 totalling
576.0KiB
2019-01-19 21:28:38.709765: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 1048576 totalling
 5.00MiB
2019-01-19 21:28:38.709770: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 1179648 totalling
 2.25MiB
2019-01-19 21:28:38.709776: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1310720 totalling
 1.25MiB
2019-01-19 21:28:38.709782: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 2359296 totalling
 4.50MiB
2019-01-19 21:28:38.709788: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 4718592 totalling
 4.50MiB
2019-01-19 21:28:38.709794: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 9437184 totalling
 45.00MiB
2019-01-19 21:28:38.709800: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 8 Chunks of size 25690112 totallin
g 196.00MiB
2019-01-19 21:28:38.709806: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 102760448 totalli
ng 490.00MiB
2019-01-19 21:28:38.709812: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 138084352 totalli
ng 131.69MiB
2019-01-19 21:28:38.709819: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 240844800 totalli
ng 229.69MiB
2019-01-19 21:28:38.709825: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 5138022400 totall
ing 4.79GiB
2019-01-19 21:28:38.709830: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 5.87Gi
B
2019-01-19 21:28:38.709838: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                  9999699149
InUse:                  6305145600
MaxInUse:               6305145856
NumAllocs:                     221
MaxAllocSize:           5138022400

2019-01-19 21:28:38.709857: W tensorflow/core/common_runtime/bfc_allocator.cc:279] **********************************
******************************____________________________________
2019-01-19 21:28:38.709877: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_ops.cc:672 : Re
source exhausted: OOM when allocating tensor with shape[400,64,224,224] and type float on /job:localhost/replica:0/ta
sk:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_
tf_sessionrun
    status, run_metadata)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_ca
ll
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_f
n
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_
tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in
__exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[400,64,224,224]
 and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.

         [[Node: loss/mul/_367 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:C
PU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_2159_
loss/mul", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 61, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 58, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 41, in continueTrainigCNN_LSTM
    batch_size = train_batch_size, in_epochs = in_epochs, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 39, in trainC
NN_LSTM
    in_epochs = in_epochs, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line 52, in t
rainImageModelForEpochs
    model = trainImageModelOnSets(model, e, trainingSubjects, trainingBiwi, timesteps, output_begin, num_outputs, bat
ch_size, in_epochs, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/LSTM_VGG16Helper.py", line 44, in t
rainImageModelOnSets
    model.fit_generator(data_gen, steps_per_epoch=len(data_gen), epochs=in_epochs, verbose=1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1315, in fit_generator
    initial_epoch=initial_epoch)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2230, in fit_generator
    class_weight=class_weight)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1883, in train_on_batch
    outputs = self.train_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482, in __call
__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_ru
n
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _do_ca
ll
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[400,64,224,224]
 and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.

         [[Node: loss/mul/_367 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:C
PU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_2159_
loss/mul", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


Caused by op 'tdVGG16/block1_conv1/convolution', defined at:
  File "continueTrainigCNN_LSTM.py", line 61, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 58, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 30, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", line 57, in
 loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserialize
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deserialize_ke
ras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 497, in add
    layer(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 213, in call
    y = self.layer.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2085, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2235, in run_internal_grap
h
    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py", line 168, in call
    dilation_rate=self.dilation_rate)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 3341, in conv2d
    data_format=tf_data_format)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 782, in convolution
    return op(input, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 870, in __call__
    return self.conv_op(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 522, in __call__
    return self.call(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 206, in __call__
    name=self.name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 953, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787,
in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, in create_
op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, in __init_
_
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[400,64,224,224] and type floa
t on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.

         [[Node: loss/mul/_367 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:C
PU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_2159_
loss/mul", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


^CError in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/multiprocessing/util.py", line 262, in _run_finalizers
    finalizer()
  File "/home/mcicek/anaconda3/lib/python3.6/multiprocessing/util.py", line 186, in __call__
    res = self._callback(*self._args, **self._kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/multiprocessing/pool.py", line 582, in _terminate_pool
    worker_handler.join()
  File "/home/mcicek/anaconda3/lib/python3.6/threading.py", line 1056, in join
    self._wait_for_tstate_lock()
  File "/home/mcicek/anaconda3/lib/python3.6/threading.py", line 1072, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
KeyboardInterrupt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_LSTM.py "V
GG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38"
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-19 21:30:20.893934: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-19 21:30:20.966161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-19 21:30:20.966478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-19 21:30:20.966494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-19 21:30:21.120531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-19 21:30:21.120557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-19 21:30:21.120562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-19 21:30:21.120739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9537 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
lstm_1 (LSTM)                (None, 256)               25953280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 771
=================================================================
Total params: 40,668,739
Trainable params: 25,954,051
Non-trainable params: 14,714,688
_________________________________________________________________

The subjects are trained: [(9, 'M03')]
Evaluating model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38_and_20
19-01-19_21-30-22
The subjects will be tested: [(24, 'M14')]
All frames and annotations from 1 datasets have been read by 2019-01-19 21:30:22.736250
For the Subject 24 (M14):
2019-01-19 21:30:37.834180: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of m
emory trying to allocate 4.59GiB.  Current allocation summary follows.
2019-01-19 21:30:37.834269: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 51, Chu
nks in use: 51. 12.8KiB allocated for chunks. 12.8KiB in use in bin. 1008B client-requested in use in bin.
2019-01-19 21:30:37.834328: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 3, Chun
ks in use: 3. 1.5KiB allocated for chunks. 1.5KiB in use in bin. 1.5KiB client-requested in use in bin.
2019-01-19 21:30:37.834356: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 5, Chun
ks in use: 5. 5.2KiB allocated for chunks. 5.2KiB in use in bin. 5.0KiB client-requested in use in bin.
2019-01-19 21:30:37.834383: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 11, Chu
nks in use: 11. 26.8KiB allocated for chunks. 26.8KiB in use in bin. 26.0KiB client-requested in use in bin.
2019-01-19 21:30:37.834410: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 7, Chun
ks in use: 7. 33.5KiB allocated for chunks. 33.5KiB in use in bin. 33.5KiB client-requested in use in bin.
2019-01-19 21:30:37.834433: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chun
ks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:30:37.834458: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:30:37.834482: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:30:37.834509: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks:
 1, Chunks in use: 0. 115.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:30:37.834539: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks:
 1, Chunks in use: 1. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.
2019-01-19 21:30:37.834566: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks:
 5, Chunks in use: 5. 1.38MiB allocated for chunks. 1.38MiB in use in bin. 1.28MiB client-requested in use in bin.
2019-01-19 21:30:37.834597: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks:
 1, Chunks in use: 1. 576.0KiB allocated for chunks. 576.0KiB in use in bin. 576.0KiB client-requested in use in bin.
2019-01-19 21:30:37.834621: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks:
 6, Chunks in use: 6. 6.50MiB allocated for chunks. 6.50MiB in use in bin. 6.12MiB client-requested in use in bin.
2019-01-19 21:30:37.834648: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks:
 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-19 21:30:37.834675: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks:
 2, Chunks in use: 1. 8.75MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-19 21:30:37.834702: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks:
 6, Chunks in use: 5. 54.00MiB allocated for chunks. 45.00MiB in use in bin. 45.00MiB client-requested in use in bin.
2019-01-19 21:30:37.834727: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks:
 4, Chunks in use: 4. 98.00MiB allocated for chunks. 98.00MiB in use in bin. 98.00MiB client-requested in use in bin.
2019-01-19 21:30:37.834756: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:30:37.834785: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks:
 4, Chunks in use: 4. 392.00MiB allocated for chunks. 392.00MiB in use in bin. 392.00MiB client-requested in use in b
in.
2019-01-19 21:30:37.834813: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks:
 2, Chunks in use: 1. 441.00MiB allocated for chunks. 220.50MiB in use in bin. 220.50MiB client-requested in use in b
in.
2019-01-19 21:30:37.834840: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks:
 2, Chunks in use: 1. 8.33GiB allocated for chunks. 4.59GiB in use in bin. 4.59GiB client-requested in use in bin.
2019-01-19 21:30:37.834866: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 4.59GiB was 256.00MiB, Chu
nk State:
2019-01-19 21:30:37.834898: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 3.74GiB | Requested Size:
0B | in_use: 0, prev:   Size: 4.59GiB | Requested Size: 4.59GiB | in_use: 1
2019-01-19 21:30:37.834923: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500000 of size 1280
2019-01-19 21:30:37.834951: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500500 of size 256
2019-01-19 21:30:37.834980: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500600 of size 256
2019-01-19 21:30:37.835006: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500700 of size 256
2019-01-19 21:30:37.835035: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500800 of size 256
2019-01-19 21:30:37.835065: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500900 of size 256
2019-01-19 21:30:37.835093: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500a00 of size 256
2019-01-19 21:30:37.835122: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500b00 of size 256
2019-01-19 21:30:37.835144: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500c00 of size 512
2019-01-19 21:30:37.835164: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500e00 of size 256
2019-01-19 21:30:37.835182: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500f00 of size 256
2019-01-19 21:30:37.835200: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501000 of size 256
2019-01-19 21:30:37.835219: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501100 of size 256
2019-01-19 21:30:37.835237: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501200 of size 256
2019-01-19 21:30:37.835256: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501300 of size 1024
2019-01-19 21:30:37.835275: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501700 of size 256
2019-01-19 21:30:37.835294: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501800 of size 256
2019-01-19 21:30:37.835312: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501900 of size 256
2019-01-19 21:30:37.835330: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501a00 of size 256
2019-01-19 21:30:37.835349: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501b00 of size 256
2019-01-19 21:30:37.835368: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501c00 of size 2048
2019-01-19 21:30:37.835387: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502400 of size 256
2019-01-19 21:30:37.835405: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502500 of size 256
2019-01-19 21:30:37.835425: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502600 of size 10485
76
2019-01-19 21:30:37.835445: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a602600 of size 4096
2019-01-19 21:30:37.835464: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603600 of size 256
2019-01-19 21:30:37.835482: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603700 of size 256
2019-01-19 21:30:37.835500: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603800 of size 256
2019-01-19 21:30:37.835520: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603900 of size 3072
2019-01-19 21:30:37.835540: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a604500 of size 3840
2019-01-19 21:30:37.835559: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a605400 of size 256
2019-01-19 21:30:37.835577: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a605500 of size 4096
2019-01-19 21:30:37.835596: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a606500 of size 4096
2019-01-19 21:30:37.835615: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a607500 of size 3072
2019-01-19 21:30:37.835633: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a608100 of size 256
2019-01-19 21:30:37.835652: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a608200 of size 4096
2019-01-19 21:30:37.835671: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609200 of size 3072
2019-01-19 21:30:37.835690: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609e00 of size 256
2019-01-19 21:30:37.835708: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609f00 of size 256
2019-01-19 21:30:37.835727: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a000 of size 256
2019-01-19 21:30:37.835745: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a100 of size 256
2019-01-19 21:30:37.835764: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a200 of size 256
2019-01-19 21:30:37.835782: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a300 of size 256
2019-01-19 21:30:37.835800: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a400 of size 256
2019-01-19 21:30:37.835819: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a500 of size 256
2019-01-19 21:30:37.835837: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a600 of size 256
2019-01-19 21:30:37.835855: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a700 of size 256
2019-01-19 21:30:37.835874: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a800 of size 256
2019-01-19 21:30:37.835892: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a900 of size 256
2019-01-19 21:30:37.835911: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60aa00 of size 256
2019-01-19 21:30:37.835930: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60ab00 of size 6912
2019-01-19 21:30:37.835949: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a60c600 of size 11852
8
2019-01-19 21:30:37.835967: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a629500 of size 256
2019-01-19 21:30:37.835988: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a629600 of size 29491
2
2019-01-19 21:30:37.836008: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a671600 of size 512
2019-01-19 21:30:37.836028: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a671800 of size 26214
4
2019-01-19 21:30:37.836048: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a6b1800 of size 32768
0
2019-01-19 21:30:37.836067: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a701800 of size 512
2019-01-19 21:30:37.836087: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a701a00 of size 11796
48
2019-01-19 21:30:37.836106: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a821a00 of size 1024
2019-01-19 21:30:37.836125: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a821e00 of size 23592
96
2019-01-19 21:30:37.836145: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aa61e00 of size 10485
76
2019-01-19 21:30:37.836165: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ab61e00 of size 13107
20
2019-01-19 21:30:37.836184: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca1e00 of size 1024
2019-01-19 21:30:37.836203: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca2200 of size 1024
2019-01-19 21:30:37.836223: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca2600 of size 26214
4
2019-01-19 21:30:37.836242: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0ace2600 of size 44564
48
2019-01-19 21:30:37.836261: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b122600 of size 2048
2019-01-19 21:30:37.836280: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b122e00 of size 94371
84
2019-01-19 21:30:37.836300: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ba22e00 of size 94371
84
2019-01-19 21:30:37.836319: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0c322e00 of size 94371
84
2019-01-19 21:30:37.836338: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0cc22e00 of size 94371
84
2019-01-19 21:30:37.836356: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0d522e00 of size 94371
84
2019-01-19 21:30:37.836376: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0de22e00 of size 2048
2019-01-19 21:30:37.836394: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0de23600 of size 10485
76
2019-01-19 21:30:37.836414: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df23600 of size 4096
2019-01-19 21:30:37.836433: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24600 of size 2048
2019-01-19 21:30:37.836452: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24e00 of size 256
2019-01-19 21:30:37.836471: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24f00 of size 256
2019-01-19 21:30:37.836489: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25000 of size 256
2019-01-19 21:30:37.836508: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25100 of size 256
2019-01-19 21:30:37.836527: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25200 of size 256
2019-01-19 21:30:37.836546: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25300 of size 256
2019-01-19 21:30:37.836564: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25400 of size 256
2019-01-19 21:30:37.836583: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25500 of size 256
2019-01-19 21:30:37.836602: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25600 of size 256
2019-01-19 21:30:37.836620: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25700 of size 256
2019-01-19 21:30:37.836639: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25800 of size 256
2019-01-19 21:30:37.836657: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25900 of size 256
2019-01-19 21:30:37.836677: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25a00 of size 2048
2019-01-19 21:30:37.836696: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26200 of size 2048
2019-01-19 21:30:37.836714: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26a00 of size 256
2019-01-19 21:30:37.836733: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26b00 of size 2048
2019-01-19 21:30:37.836753: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df27300 of size 10276
0448
2019-01-19 21:30:37.836772: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14127300 of size 6912
2019-01-19 21:30:37.836792: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14128e00 of size 14745
6
2019-01-19 21:30:37.836811: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1414ce00 of size 29491
2
2019-01-19 21:30:37.836832: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14194e00 of size 58982
4
2019-01-19 21:30:37.836851: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14224e00 of size 11796
48
2019-01-19 21:30:37.836870: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14344e00 of size 23592
96
2019-01-19 21:30:37.836890: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14584e00 of size 47185
92
2019-01-19 21:30:37.836910: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14a04e00 of size 94371
84
2019-01-19 21:30:37.836929: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb15304e00 of size 10276
0448
2019-01-19 21:30:37.836948: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1b504e00 of size 10276
0448
2019-01-19 21:30:37.836966: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb21704e00 of size 10276
0448
2019-01-19 21:30:37.836986: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb27904e00 of size 23121
1008
2019-01-19 21:30:37.837005: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb35584e00 of size 25690
112
2019-01-19 21:30:37.837025: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb36e04e00 of size 25690
112
2019-01-19 21:30:37.837043: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb38684e00 of size 25690
112
2019-01-19 21:30:37.837063: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb39f04e00 of size 25690
112
2019-01-19 21:30:37.837082: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb3b784e00 of size 23121
1008
2019-01-19 21:30:37.837102: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb49404e00 of size 49325
01504
2019-01-19 21:30:37.837122: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xc6f404e00 of size 40122
57792
2019-01-19 21:30:37.837140: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by s
ize:
2019-01-19 21:30:37.837166: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 51 Chunks of size 256 totalling 12
.8KiB
2019-01-19 21:30:37.837188: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 512 totalling 1.5
KiB
2019-01-19 21:30:37.837210: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 1024 totalling 4.
0KiB
2019-01-19 21:30:37.837231: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.
2KiB
2019-01-19 21:30:37.837253: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 7 Chunks of size 2048 totalling 14
.0KiB
2019-01-19 21:30:37.837274: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 3072 totalling 9.
0KiB
2019-01-19 21:30:37.837294: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3840 totalling 3.
8KiB
2019-01-19 21:30:37.864238: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 4096 totalling 20
.0KiB
2019-01-19 21:30:37.864250: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 6912 totalling 13
.5KiB
2019-01-19 21:30:37.864256: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 147456 totalling
144.0KiB
2019-01-19 21:30:37.864262: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 262144 totalling
512.0KiB
2019-01-19 21:30:37.864267: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 294912 totalling
576.0KiB
2019-01-19 21:30:37.864272: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 327680 totalling
320.0KiB
2019-01-19 21:30:37.864277: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 589824 totalling
576.0KiB
2019-01-19 21:30:37.864282: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 1048576 totalling
 3.00MiB
2019-01-19 21:30:37.864287: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 1179648 totalling
 2.25MiB
2019-01-19 21:30:37.864292: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1310720 totalling
 1.25MiB
2019-01-19 21:30:37.864296: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 2359296 totalling
 4.50MiB
2019-01-19 21:30:37.864301: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 4718592 totalling
 4.50MiB
2019-01-19 21:30:37.864306: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 9437184 totalling
 45.00MiB
2019-01-19 21:30:37.864311: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 25690112 totallin
g 98.00MiB
2019-01-19 21:30:37.864316: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 102760448 totalli
ng 392.00MiB
2019-01-19 21:30:37.864321: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 231211008 totalli
ng 220.50MiB
2019-01-19 21:30:37.864325: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 4932501504 totall
ing 4.59GiB
2019-01-19 21:30:37.864330: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 5.35Gi
B
2019-01-19 21:30:37.864336: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                 10000695296
InUse:                  5743214336
MaxInUse:               5743214336
NumAllocs:                     171
MaxAllocSize:           4932501504

2019-01-19 21:30:37.864346: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *****__***************************
**************************________________________________________
2019-01-19 21:30:37.864360: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_ops.cc:672 : Re
source exhausted: OOM when allocating tensor with shape[384,64,224,224] and type float on /job:localhost/replica:0/ta
sk:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_ca
ll
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_f
n
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_
tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in
__exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[384,64,224,224]
 and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 62, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 59, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 52, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 80, in evalua
teCNN_LSTM
    outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, record = re
cord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 47, in evalua
teSubject
    predictions = full_model.predict_generator(test_gen, verbose = 1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1392, in predict_generator
    verbose=verbose)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2540, in predict_generator
    outs = self.predict_on_batch(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1945, in predict_on_batch
    outputs = self.predict_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482, in __call
__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_ru
n
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _do_ca
ll
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[384,64,224,224]
 and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


Caused by op 'tdVGG16/block1_conv1/convolution', defined at:
  File "continueTrainigCNN_LSTM.py", line 62, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 59, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 30, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", line 57, in
 loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserialize
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deserialize_ke
ras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 497, in add
    layer(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 213, in call
    y = self.layer.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2085, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2235, in run_internal_grap
h
    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py", line 168, in call
    dilation_rate=self.dilation_rate)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 3341, in conv2d
    data_format=tf_data_format)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 782, in convolution
    return op(input, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 870, in __call__
    return self.conv_op(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 522, in __call__
    return self.call(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 206, in __call__
    name=self.name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 953, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787,
in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, in create_
op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, in __init_
_
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[384,64,224,224] and type floa
t on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_LSTM.py "V
GG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38"
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-19 21:32:42.231663: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-19 21:32:42.305773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-19 21:32:42.306026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-19 21:32:42.306037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-19 21:32:42.459353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-19 21:32:42.459377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-19 21:32:42.459381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-19 21:32:42.459511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9537 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
lstm_1 (LSTM)                (None, 256)               25953280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 771
=================================================================
Total params: 40,668,739
Trainable params: 25,954,051
Non-trainable params: 14,714,688
_________________________________________________________________

The subjects are trained: [(1, 'F01'), (2, 'F02'), (3, 'F03'), (4, 'F04'), (5, 'F05'), (6, 'F06'), (7, 'M01'), (8, 'M
02'), (10, 'M04'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (14, 'M08'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (19, 'M
11'), (20, 'M12'), (22, 'M01'), (23, 'M13')]
Evaluating model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38_and_20
19-01-19_21-32-43
The subjects will be tested: [(9, 'M03'), (18, 'F05'), (21, 'F02'), (24, 'M14')]
All frames and annotations from 4 datasets have been read by 2019-01-19 21:32:44.604427
For the Subject 9 (M03):
2019-01-19 21:33:20.434651: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of m
emory trying to allocate 4.59GiB.  Current allocation summary follows.
2019-01-19 21:33:20.434738: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 51, Chu
nks in use: 51. 12.8KiB allocated for chunks. 12.8KiB in use in bin. 1008B client-requested in use in bin.
2019-01-19 21:33:20.434804: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 3, Chun
ks in use: 3. 1.5KiB allocated for chunks. 1.5KiB in use in bin. 1.5KiB client-requested in use in bin.
2019-01-19 21:33:20.434836: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 5, Chun
ks in use: 5. 5.2KiB allocated for chunks. 5.2KiB in use in bin. 5.0KiB client-requested in use in bin.
2019-01-19 21:33:20.434863: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 11, Chu
nks in use: 11. 26.8KiB allocated for chunks. 26.8KiB in use in bin. 26.0KiB client-requested in use in bin.
2019-01-19 21:33:20.434892: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 7, Chun
ks in use: 7. 33.5KiB allocated for chunks. 33.5KiB in use in bin. 33.5KiB client-requested in use in bin.
2019-01-19 21:33:20.434915: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chun
ks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:33:20.434936: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:33:20.434955: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:33:20.434981: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks:
 1, Chunks in use: 0. 115.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:33:20.435009: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks:
 1, Chunks in use: 1. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.
2019-01-19 21:33:20.435034: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks:
 5, Chunks in use: 5. 1.38MiB allocated for chunks. 1.38MiB in use in bin. 1.28MiB client-requested in use in bin.
2019-01-19 21:33:20.435058: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks:
 1, Chunks in use: 1. 576.0KiB allocated for chunks. 576.0KiB in use in bin. 576.0KiB client-requested in use in bin.
2019-01-19 21:33:20.435086: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks:
 6, Chunks in use: 6. 6.50MiB allocated for chunks. 6.50MiB in use in bin. 6.12MiB client-requested in use in bin.
2019-01-19 21:33:20.435110: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks:
 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-19 21:33:20.435132: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks:
 2, Chunks in use: 1. 8.75MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-19 21:33:20.435165: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks:
 6, Chunks in use: 5. 54.00MiB allocated for chunks. 45.00MiB in use in bin. 45.00MiB client-requested in use in bin.
2019-01-19 21:33:20.435205: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks:
 4, Chunks in use: 4. 98.00MiB allocated for chunks. 98.00MiB in use in bin. 98.00MiB client-requested in use in bin.
2019-01-19 21:33:20.435237: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-19 21:33:20.435275: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks:
 4, Chunks in use: 4. 392.00MiB allocated for chunks. 392.00MiB in use in bin. 392.00MiB client-requested in use in b
in.
2019-01-19 21:33:20.435313: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks:
 2, Chunks in use: 1. 441.00MiB allocated for chunks. 220.50MiB in use in bin. 220.50MiB client-requested in use in b
in.
2019-01-19 21:33:20.435348: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks:
 2, Chunks in use: 1. 8.33GiB allocated for chunks. 4.59GiB in use in bin. 4.59GiB client-requested in use in bin.
2019-01-19 21:33:20.435380: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 4.59GiB was 256.00MiB, Chu
nk State:
2019-01-19 21:33:20.435417: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 3.74GiB | Requested Size:
0B | in_use: 0, prev:   Size: 4.59GiB | Requested Size: 4.59GiB | in_use: 1
2019-01-19 21:33:20.435446: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500000 of size 1280
2019-01-19 21:33:20.435470: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500500 of size 256
2019-01-19 21:33:20.435495: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500600 of size 256
2019-01-19 21:33:20.435519: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500700 of size 256
2019-01-19 21:33:20.435543: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500800 of size 256
2019-01-19 21:33:20.435566: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500900 of size 256
2019-01-19 21:33:20.435590: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500a00 of size 256
2019-01-19 21:33:20.435613: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500b00 of size 256
2019-01-19 21:33:20.435639: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500c00 of size 512
2019-01-19 21:33:20.435662: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500e00 of size 256
2019-01-19 21:33:20.435686: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500f00 of size 256
2019-01-19 21:33:20.435710: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501000 of size 256
2019-01-19 21:33:20.435733: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501100 of size 256
2019-01-19 21:33:20.435757: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501200 of size 256
2019-01-19 21:33:20.435782: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501300 of size 1024
2019-01-19 21:33:20.435805: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501700 of size 256
2019-01-19 21:33:20.435829: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501800 of size 256
2019-01-19 21:33:20.435853: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501900 of size 256
2019-01-19 21:33:20.435876: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501a00 of size 256
2019-01-19 21:33:20.435900: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501b00 of size 256
2019-01-19 21:33:20.435924: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501c00 of size 2048
2019-01-19 21:33:20.435947: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502400 of size 256
2019-01-19 21:33:20.435971: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502500 of size 256
2019-01-19 21:33:20.435995: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502600 of size 10485
76
2019-01-19 21:33:20.436020: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a602600 of size 4096
2019-01-19 21:33:20.436044: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603600 of size 256
2019-01-19 21:33:20.436068: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603700 of size 256
2019-01-19 21:33:20.436091: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603800 of size 256
2019-01-19 21:33:20.436117: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603900 of size 3072
2019-01-19 21:33:20.436143: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a604500 of size 3840
2019-01-19 21:33:20.436166: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a605400 of size 256
2019-01-19 21:33:20.436190: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a605500 of size 4096
2019-01-19 21:33:20.436214: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a606500 of size 4096
2019-01-19 21:33:20.436238: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a607500 of size 3072
2019-01-19 21:33:20.436261: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a608100 of size 256
2019-01-19 21:33:20.436285: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a608200 of size 4096
2019-01-19 21:33:20.436308: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609200 of size 3072
2019-01-19 21:33:20.436332: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609e00 of size 256
2019-01-19 21:33:20.436356: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609f00 of size 256
2019-01-19 21:33:20.436379: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a000 of size 256
2019-01-19 21:33:20.436402: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a100 of size 256
2019-01-19 21:33:20.436426: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a200 of size 256
2019-01-19 21:33:20.436449: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a300 of size 256
2019-01-19 21:33:20.436473: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a400 of size 256
2019-01-19 21:33:20.436496: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a500 of size 256
2019-01-19 21:33:20.436520: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a600 of size 256
2019-01-19 21:33:20.436543: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a700 of size 256
2019-01-19 21:33:20.436566: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a800 of size 256
2019-01-19 21:33:20.436590: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a900 of size 256
2019-01-19 21:33:20.436613: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60aa00 of size 256
2019-01-19 21:33:20.436638: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60ab00 of size 6912
2019-01-19 21:33:20.436662: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a60c600 of size 11852
8
2019-01-19 21:33:20.436685: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a629500 of size 256
2019-01-19 21:33:20.436710: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a629600 of size 29491
2
2019-01-19 21:33:20.436734: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a671600 of size 512
2019-01-19 21:33:20.436758: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a671800 of size 26214
4
2019-01-19 21:33:20.436783: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a6b1800 of size 32768
0
2019-01-19 21:33:20.436806: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a701800 of size 512
2019-01-19 21:33:20.436832: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a701a00 of size 11796
48
2019-01-19 21:33:20.436855: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a821a00 of size 1024
2019-01-19 21:33:20.436880: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a821e00 of size 23592
96
2019-01-19 21:33:20.436904: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aa61e00 of size 10485
76
2019-01-19 21:33:20.436929: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ab61e00 of size 13107
20
2019-01-19 21:33:20.436953: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca1e00 of size 1024
2019-01-19 21:33:20.436976: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca2200 of size 1024
2019-01-19 21:33:20.437000: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca2600 of size 26214
4
2019-01-19 21:33:20.437024: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0ace2600 of size 44564
48
2019-01-19 21:33:20.437048: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b122600 of size 2048
2019-01-19 21:33:20.437072: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b122e00 of size 94371
84
2019-01-19 21:33:20.437096: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ba22e00 of size 94371
84
2019-01-19 21:33:20.437120: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0c322e00 of size 94371
84
2019-01-19 21:33:20.437143: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0cc22e00 of size 94371
84
2019-01-19 21:33:20.437167: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0d522e00 of size 94371
84
2019-01-19 21:33:20.437190: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0de22e00 of size 2048
2019-01-19 21:33:20.437214: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0de23600 of size 10485
76
2019-01-19 21:33:20.437237: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df23600 of size 4096
2019-01-19 21:33:20.437261: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24600 of size 2048
2019-01-19 21:33:20.437284: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24e00 of size 256
2019-01-19 21:33:20.437308: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24f00 of size 256
2019-01-19 21:33:20.437332: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25000 of size 256
2019-01-19 21:33:20.437355: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25100 of size 256
2019-01-19 21:33:20.437378: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25200 of size 256
2019-01-19 21:33:20.437402: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25300 of size 256
2019-01-19 21:33:20.437425: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25400 of size 256
2019-01-19 21:33:20.437449: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25500 of size 256
2019-01-19 21:33:20.437472: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25600 of size 256
2019-01-19 21:33:20.437496: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25700 of size 256
2019-01-19 21:33:20.437519: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25800 of size 256
2019-01-19 21:33:20.437542: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25900 of size 256
2019-01-19 21:33:20.437566: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25a00 of size 2048
2019-01-19 21:33:20.437590: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26200 of size 2048
2019-01-19 21:33:20.437614: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26a00 of size 256
2019-01-19 21:33:20.437637: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26b00 of size 2048
2019-01-19 21:33:20.437662: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df27300 of size 10276
0448
2019-01-19 21:33:20.437705: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14127300 of size 6912
2019-01-19 21:33:20.437731: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14128e00 of size 14745
6
2019-01-19 21:33:20.437756: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1414ce00 of size 29491
2
2019-01-19 21:33:20.437781: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14194e00 of size 58982
4
2019-01-19 21:33:20.437805: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14224e00 of size 11796
48
2019-01-19 21:33:20.437830: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14344e00 of size 23592
96
2019-01-19 21:33:20.437854: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14584e00 of size 47185
92
2019-01-19 21:33:20.437878: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14a04e00 of size 94371
84
2019-01-19 21:33:20.437903: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb15304e00 of size 10276
0448
2019-01-19 21:33:20.437927: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1b504e00 of size 10276
0448
2019-01-19 21:33:20.437951: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb21704e00 of size 10276
0448
2019-01-19 21:33:20.437975: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb27904e00 of size 23121
1008
2019-01-19 21:33:20.437999: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb35584e00 of size 25690
112
2019-01-19 21:33:20.438023: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb36e04e00 of size 25690
112
2019-01-19 21:33:20.438047: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb38684e00 of size 25690
112
2019-01-19 21:33:20.438071: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb39f04e00 of size 25690
112
2019-01-19 21:33:20.438095: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb3b784e00 of size 23121
1008
2019-01-19 21:33:20.438120: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb49404e00 of size 49325
01504
2019-01-19 21:33:20.438144: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xc6f404e00 of size 40122
57792
2019-01-19 21:33:20.438167: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by s
ize:
2019-01-19 21:33:20.438198: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 51 Chunks of size 256 totalling 12
.8KiB
2019-01-19 21:33:20.438224: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 512 totalling 1.5
KiB
2019-01-19 21:33:20.438251: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 1024 totalling 4.
0KiB
2019-01-19 21:33:20.438277: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.
2KiB
2019-01-19 21:33:20.465754: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 7 Chunks of size 2048 totalling 14
.0KiB
2019-01-19 21:33:20.465766: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 3072 totalling 9.
0KiB
2019-01-19 21:33:20.465771: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3840 totalling 3.
8KiB
2019-01-19 21:33:20.465777: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 4096 totalling 20
.0KiB
2019-01-19 21:33:20.465784: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 6912 totalling 13
.5KiB
2019-01-19 21:33:20.465792: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 147456 totalling
144.0KiB
2019-01-19 21:33:20.465799: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 262144 totalling
512.0KiB
2019-01-19 21:33:20.465805: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 294912 totalling
576.0KiB
2019-01-19 21:33:20.465809: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 327680 totalling
320.0KiB
2019-01-19 21:33:20.465812: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 589824 totalling
576.0KiB
2019-01-19 21:33:20.465816: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 1048576 totalling
 3.00MiB
2019-01-19 21:33:20.465819: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 1179648 totalling
 2.25MiB
2019-01-19 21:33:20.465823: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1310720 totalling
 1.25MiB
2019-01-19 21:33:20.465827: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 2359296 totalling
 4.50MiB
2019-01-19 21:33:20.465830: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 4718592 totalling
 4.50MiB
2019-01-19 21:33:20.465834: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 9437184 totalling
 45.00MiB
2019-01-19 21:33:20.465838: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 25690112 totallin
g 98.00MiB
2019-01-19 21:33:20.465842: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 102760448 totalli
ng 392.00MiB
2019-01-19 21:33:20.465846: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 231211008 totalli
ng 220.50MiB
2019-01-19 21:33:20.465850: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 4932501504 totall
ing 4.59GiB
2019-01-19 21:33:20.465854: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 5.35Gi
B
2019-01-19 21:33:20.465860: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                 10000695296
InUse:                  5743214336
MaxInUse:               5743214336
NumAllocs:                     171
MaxAllocSize:           4932501504

2019-01-19 21:33:20.465871: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *****__***************************
**************************________________________________________
2019-01-19 21:33:20.465883: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_ops.cc:672 : Re
source exhausted: OOM when allocating tensor with shape[384,64,224,224] and type float on /job:localhost/replica:0/ta
sk:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_ca
ll
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_f
n
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_
tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in
__exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[384,64,224,224]
 and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.

         [[Node: dense_1/BiasAdd/_329 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/d
evice:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edg
e_507_dense_1/BiasAdd", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 62, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 59, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 52, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 80, in evalua
teCNN_LSTM
    outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, record = re
cord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 47, in evalua
teSubject
    predictions = full_model.predict_generator(test_gen, verbose = 1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1392, in predict_generator
    verbose=verbose)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2540, in predict_generator
    outs = self.predict_on_batch(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1945, in predict_on_batch
    outputs = self.predict_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482, in __call
__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_ru
n
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _do_ca
ll
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[384,64,224,224]
 and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.

         [[Node: dense_1/BiasAdd/_329 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/d
evice:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edg
e_507_dense_1/BiasAdd", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


Caused by op 'tdVGG16/block1_conv1/convolution', defined at:
  File "continueTrainigCNN_LSTM.py", line 62, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 59, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 30, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", line 57, in
 loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserialize
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deserialize_ke
ras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 497, in add
    layer(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 213, in call
    y = self.layer.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2085, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2235, in run_internal_grap
h
    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py", line 168, in call
    dilation_rate=self.dilation_rate)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 3341, in conv2d
    data_format=tf_data_format)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 782, in convolution
    return op(input, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 870, in __call__
    return self.conv_op(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 522, in __call__
    return self.call(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 206, in __call__
    name=self.name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 953, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787,
in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, in create_
op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, in __init_
_
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[384,64,224,224] and type floa
t on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.

         [[Node: dense_1/BiasAdd/_329 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/d
evice:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edg
e_507_dense_1/BiasAdd", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_LSTM.py "V
GG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38"

/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-20 00:39:35.059705: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-20 00:39:35.130177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-20 00:39:35.130430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-20 00:39:35.130441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-20 00:39:35.287031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-20 00:39:35.287057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-20 00:39:35.287061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-20 00:39:35.287191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9536 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
lstm_1 (LSTM)                (None, 256)               25953280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 771
=================================================================
Total params: 40,668,739
Trainable params: 25,954,051
Non-trainable params: 14,714,688
_________________________________________________________________

The subjects are trained: [(1, 'F01'), (2, 'F02'), (3, 'F03'), (4, 'F04'), (5, 'F05'), (6, 'F06'), (7, 'M01'), (8, 'M
02'), (10, 'M04'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (14, 'M08'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (19, 'M
11'), (20, 'M12'), (22, 'M01'), (23, 'M13')]
Evaluating model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38_and_20
19-01-20_00-39-36
The subjects will be tested: [(9, 'M03'), (18, 'F05'), (21, 'F02'), (24, 'M14')]
All frames and annotations from 4 datasets have been read by 2019-01-20 00:39:37.431587
For the Subject 9 (M03):
2019-01-20 00:40:13.251969: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of m
emory trying to allocate 4.59GiB.  Current allocation summary follows.
2019-01-20 00:40:13.252053: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 51, Chu
nks in use: 51. 12.8KiB allocated for chunks. 12.8KiB in use in bin. 1008B client-requested in use in bin.
2019-01-20 00:40:13.252105: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 3, Chun
ks in use: 3. 1.5KiB allocated for chunks. 1.5KiB in use in bin. 1.5KiB client-requested in use in bin.
2019-01-20 00:40:13.252131: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 5, Chun
ks in use: 5. 5.2KiB allocated for chunks. 5.2KiB in use in bin. 5.0KiB client-requested in use in bin.
2019-01-20 00:40:13.252156: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 11, Chu
nks in use: 11. 26.0KiB allocated for chunks. 26.0KiB in use in bin. 26.0KiB client-requested in use in bin.
2019-01-20 00:40:13.252189: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 7, Chun
ks in use: 7. 33.5KiB allocated for chunks. 33.5KiB in use in bin. 33.5KiB client-requested in use in bin.
2019-01-20 00:40:13.252213: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chun
ks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-20 00:40:13.252238: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-20 00:40:13.252261: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-20 00:40:13.252287: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks:
 1, Chunks in use: 0. 112.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-20 00:40:13.252314: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks:
 1, Chunks in use: 1. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.
2019-01-20 00:40:13.252337: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks:
 5, Chunks in use: 5. 1.38MiB allocated for chunks. 1.38MiB in use in bin. 1.28MiB client-requested in use in bin.
2019-01-20 00:40:13.252361: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks:
 1, Chunks in use: 1. 576.0KiB allocated for chunks. 576.0KiB in use in bin. 576.0KiB client-requested in use in bin.
2019-01-20 00:40:13.252387: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks:
 6, Chunks in use: 6. 6.50MiB allocated for chunks. 6.50MiB in use in bin. 6.12MiB client-requested in use in bin.
2019-01-20 00:40:13.252413: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks:
 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-20 00:40:13.252438: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks:
 2, Chunks in use: 1. 8.88MiB allocated for chunks. 4.63MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-20 00:40:13.252465: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks:
 5, Chunks in use: 5. 45.00MiB allocated for chunks. 45.00MiB in use in bin. 45.00MiB client-requested in use in bin.
2019-01-20 00:40:13.252493: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks:
 4, Chunks in use: 4. 98.00MiB allocated for chunks. 98.00MiB in use in bin. 98.00MiB client-requested in use in bin.
2019-01-20 00:40:13.252517: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-20 00:40:13.252546: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks:
 4, Chunks in use: 4. 392.01MiB allocated for chunks. 392.01MiB in use in bin. 392.00MiB client-requested in use in b
in.
2019-01-20 00:40:13.252574: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks:
 2, Chunks in use: 1. 441.00MiB allocated for chunks. 220.50MiB in use in bin. 220.50MiB client-requested in use in b
in.
2019-01-20 00:40:13.252601: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks:
 2, Chunks in use: 1. 8.34GiB allocated for chunks. 4.59GiB in use in bin. 4.59GiB client-requested in use in bin.
2019-01-20 00:40:13.252625: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 4.59GiB was 256.00MiB, Chu
nk State:
2019-01-20 00:40:13.252656: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 3.74GiB | Requested Size:
0B | in_use: 0, prev:   Size: 4.59GiB | Requested Size: 4.59GiB | in_use: 1
2019-01-20 00:40:13.252681: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500000 of size 1280
2019-01-20 00:40:13.252701: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500500 of size 256
2019-01-20 00:40:13.252720: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500600 of size 256
2019-01-20 00:40:13.252738: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500700 of size 256
2019-01-20 00:40:13.252757: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500800 of size 256
2019-01-20 00:40:13.252774: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500900 of size 256
2019-01-20 00:40:13.252793: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500a00 of size 256
2019-01-20 00:40:13.252810: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500b00 of size 256
2019-01-20 00:40:13.252830: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500c00 of size 512
2019-01-20 00:40:13.252849: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500e00 of size 256
2019-01-20 00:40:13.252867: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500f00 of size 256
2019-01-20 00:40:13.252885: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501000 of size 256
2019-01-20 00:40:13.252903: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501100 of size 256
2019-01-20 00:40:13.252921: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501200 of size 256
2019-01-20 00:40:13.252941: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501300 of size 1024
2019-01-20 00:40:13.252960: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501700 of size 256
2019-01-20 00:40:13.252978: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501800 of size 256
2019-01-20 00:40:13.252996: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501900 of size 256
2019-01-20 00:40:13.253015: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501a00 of size 256
2019-01-20 00:40:13.253033: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501b00 of size 256
2019-01-20 00:40:13.253052: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501c00 of size 2048
2019-01-20 00:40:13.253070: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502400 of size 256
2019-01-20 00:40:13.253090: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502500 of size 256
2019-01-20 00:40:13.253111: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502600 of size 10485
76
2019-01-20 00:40:13.253128: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a602600 of size 4096
2019-01-20 00:40:13.253144: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603600 of size 256
2019-01-20 00:40:13.253174: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603700 of size 256
2019-01-20 00:40:13.253193: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603800 of size 256
2019-01-20 00:40:13.253209: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603900 of size 94371
84
2019-01-20 00:40:13.253227: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0af03900 of size 94371
84
2019-01-20 00:40:13.253247: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b803900 of size 94371
84
2019-01-20 00:40:13.253265: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0c103900 of size 94371
84
2019-01-20 00:40:13.253286: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ca03900 of size 3072
2019-01-20 00:40:13.253305: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ca04500 of size 6912
2019-01-20 00:40:13.253325: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ca06000 of size 14745
6
2019-01-20 00:40:13.253345: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ca2a000 of size 29491
2
2019-01-20 00:40:13.253365: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ca72000 of size 58982
4
2019-01-20 00:40:13.253384: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0cb02000 of size 11796
48
2019-01-20 00:40:13.253404: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0cc22000 of size 23592
96
2019-01-20 00:40:13.253423: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ce62000 of size 48560
64
2019-01-20 00:40:13.253442: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d303900 of size 4096
2019-01-20 00:40:13.253460: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d304900 of size 256
2019-01-20 00:40:13.253479: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d304a00 of size 256
2019-01-20 00:40:13.253497: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d304b00 of size 256
2019-01-20 00:40:13.253515: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d304c00 of size 256
2019-01-20 00:40:13.253533: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d304d00 of size 256
2019-01-20 00:40:13.253552: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d304e00 of size 256
2019-01-20 00:40:13.253570: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d304f00 of size 256
2019-01-20 00:40:13.253588: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d305000 of size 256
2019-01-20 00:40:13.253606: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d305100 of size 256
2019-01-20 00:40:13.253624: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d305200 of size 256
2019-01-20 00:40:13.253642: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d305300 of size 256
2019-01-20 00:40:13.253660: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d305400 of size 256
2019-01-20 00:40:13.253678: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d305500 of size 256
2019-01-20 00:40:13.253726: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0d305600 of size 10276
7360
2019-01-20 00:40:13.253745: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13507100 of size 256
2019-01-20 00:40:13.253763: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13507200 of size 4096
2019-01-20 00:40:13.253782: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13508200 of size 3072
2019-01-20 00:40:13.253801: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13508e00 of size 4096
2019-01-20 00:40:13.253819: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13509e00 of size 3072
2019-01-20 00:40:13.253837: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350aa00 of size 256
2019-01-20 00:40:13.253856: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350ab00 of size 4096
2019-01-20 00:40:13.253874: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350bb00 of size 3072
2019-01-20 00:40:13.253893: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350c700 of size 256
2019-01-20 00:40:13.253911: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350c800 of size 256
2019-01-20 00:40:13.253929: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350c900 of size 256
2019-01-20 00:40:13.253947: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350ca00 of size 256
2019-01-20 00:40:13.253966: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350cb00 of size 256
2019-01-20 00:40:13.253984: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350cc00 of size 256
2019-01-20 00:40:13.254002: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350cd00 of size 256
2019-01-20 00:40:13.254020: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350ce00 of size 256
2019-01-20 00:40:13.254038: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350cf00 of size 256
2019-01-20 00:40:13.254056: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350d000 of size 256
2019-01-20 00:40:13.254074: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350d100 of size 256
2019-01-20 00:40:13.254092: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350d200 of size 256
2019-01-20 00:40:13.254110: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350d300 of size 256
2019-01-20 00:40:13.254129: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1350d400 of size 6912
2019-01-20 00:40:13.254147: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb1350ef00 of size 11545
6
2019-01-20 00:40:13.254166: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1352b200 of size 256
2019-01-20 00:40:13.254185: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1352b300 of size 29491
2
2019-01-20 00:40:13.254203: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13573300 of size 512
2019-01-20 00:40:13.254222: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13573500 of size 26214
4
2019-01-20 00:40:13.254241: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb135b3500 of size 32768
0
2019-01-20 00:40:13.254260: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13603500 of size 512
2019-01-20 00:40:13.254279: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13603700 of size 11796
48
2019-01-20 00:40:13.254297: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13723700 of size 1024
2019-01-20 00:40:13.254316: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13723b00 of size 23592
96
2019-01-20 00:40:13.254335: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13963b00 of size 10485
76
2019-01-20 00:40:13.254354: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13a63b00 of size 13107
20
2019-01-20 00:40:13.254372: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13ba3b00 of size 1024
2019-01-20 00:40:13.254392: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13ba3f00 of size 1024
2019-01-20 00:40:13.254410: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13ba4300 of size 10485
76
2019-01-20 00:40:13.254428: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb13ca4300 of size 26214
4
2019-01-20 00:40:13.254447: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb13ce4300 of size 44564
48
2019-01-20 00:40:13.254466: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14124300 of size 2048
2019-01-20 00:40:13.254484: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14124b00 of size 2048
2019-01-20 00:40:13.254502: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14125300 of size 2048
2019-01-20 00:40:13.254520: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14125b00 of size 2048
2019-01-20 00:40:13.254539: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14126300 of size 2048
2019-01-20 00:40:13.254557: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14126b00 of size 2048
2019-01-20 00:40:13.254576: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14127300 of size 94371
84
2019-01-20 00:40:13.254595: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14a27300 of size 10276
0448
2019-01-20 00:40:13.254615: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1ac27300 of size 10276
0448
2019-01-20 00:40:13.254633: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb20e27300 of size 10276
0448
2019-01-20 00:40:13.254652: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb27027300 of size 23121
1008
2019-01-20 00:40:13.254671: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb34ca7300 of size 25690
112
2019-01-20 00:40:13.254690: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb36527300 of size 25690
112
2019-01-20 00:40:13.254709: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb37da7300 of size 25690
112
2019-01-20 00:40:13.254727: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb39627300 of size 25690
112
2019-01-20 00:40:13.254746: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb3aea7300 of size 23121
1008
2019-01-20 00:40:13.254766: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb48b27300 of size 49325
01504
2019-01-20 00:40:13.254785: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xc6eb27300 of size 40208
07168
2019-01-20 00:40:13.254803: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by s
ize:
2019-01-20 00:40:13.254827: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 51 Chunks of size 256 totalling 12
.8KiB
2019-01-20 00:40:13.254848: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 512 totalling 1.5
KiB
2019-01-20 00:40:13.254868: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 1024 totalling 4.
0KiB
2019-01-20 00:40:13.254888: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.
2KiB
2019-01-20 00:40:13.254908: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 7 Chunks of size 2048 totalling 14
.0KiB
2019-01-20 00:40:13.306262: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 3072 totalling 12
.0KiB
2019-01-20 00:40:13.306288: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 4096 totalling 20
.0KiB
2019-01-20 00:40:13.306301: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 6912 totalling 13
.5KiB
2019-01-20 00:40:13.306313: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 147456 totalling
144.0KiB
2019-01-20 00:40:13.306325: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 262144 totalling
512.0KiB
2019-01-20 00:40:13.306336: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 294912 totalling
576.0KiB
2019-01-20 00:40:13.306346: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 327680 totalling
320.0KiB
2019-01-20 00:40:13.306357: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 589824 totalling
576.0KiB
2019-01-20 00:40:13.306368: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 1048576 totalling
 3.00MiB
2019-01-20 00:40:13.306378: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 1179648 totalling
 2.25MiB
2019-01-20 00:40:13.306388: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1310720 totalling
 1.25MiB
2019-01-20 00:40:13.306400: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 2359296 totalling
 4.50MiB
2019-01-20 00:40:13.306410: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 4856064 totalling
 4.63MiB
2019-01-20 00:40:13.306421: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 9437184 totalling
 45.00MiB
2019-01-20 00:40:13.306431: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 25690112 totallin
g 98.00MiB
2019-01-20 00:40:13.306442: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 102760448 totalli
ng 294.00MiB
2019-01-20 00:40:13.306454: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 102767360 totalli
ng 98.01MiB
2019-01-20 00:40:13.306465: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 231211008 totalli
ng 220.50MiB
2019-01-20 00:40:13.306475: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 4932501504 totall
ing 4.59GiB
2019-01-20 00:40:13.306485: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 5.35Gi
B
2019-01-20 00:40:13.306500: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                  9999948186
InUse:                  5743357952
MaxInUse:               5743357952
NumAllocs:                     171
MaxAllocSize:           4932501504

2019-01-20 00:40:13.306527: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *****__***************************
**************************________________________________________
2019-01-20 00:40:13.306553: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_ops.cc:672 : Re
source exhausted: OOM when allocating tensor with shape[384,64,224,224] and type float on /job:localhost/replica:0/ta
sk:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_ca
ll
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_f
n
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_
tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in
__exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[384,64,224,224]
 and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.

         [[Node: dense_1/BiasAdd/_329 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/d
evice:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edg
e_507_dense_1/BiasAdd", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 62, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 59, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 52, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 80, in evalua
teCNN_LSTM
    outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, record = re
cord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 47, in evalua
teSubject
    predictions = full_model.predict_generator(test_gen, verbose = 1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1392, in predict_generator
    verbose=verbose)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2540, in predict_generator
    outs = self.predict_on_batch(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1945, in predict_on_batch
    outputs = self.predict_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482, in __call
__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_ru
n
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _do_ca
ll
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[384,64,224,224]
 and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.

         [[Node: dense_1/BiasAdd/_329 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/d
evice:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edg
e_507_dense_1/BiasAdd", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


Caused by op 'tdVGG16/block1_conv1/convolution', defined at:
  File "continueTrainigCNN_LSTM.py", line 62, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 59, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 30, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", line 57, in
 loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserialize
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deserialize_ke
ras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 497, in add
    layer(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 213, in call
    y = self.layer.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2085, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2235, in run_internal_grap
h
    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py", line 168, in call
    dilation_rate=self.dilation_rate)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 3341, in conv2d
    data_format=tf_data_format)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 782, in convolution
    return op(input, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 870, in __call__
    return self.conv_op(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 522, in __call__
    return self.call(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 206, in __call__
    name=self.name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 953, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787,
in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, in create_
op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, in __init_
_
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[384,64,224,224] and type floa
t on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.

         [[Node: dense_1/BiasAdd/_329 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/d
evice:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edg
e_507_dense_1/BiasAdd", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ tmux attach -t 0
sessions should be nested with care, unset $TMUX to force
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_LSTM.py "V
GG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38"
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-20 00:43:51.502729: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-20 00:43:51.577205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-20 00:43:51.577467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-20 00:43:51.577483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-20 00:43:51.731762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-20 00:43:51.731789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-20 00:43:51.731798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-20 00:43:51.731932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9536 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
lstm_1 (LSTM)                (None, 256)               25953280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 771
=================================================================
Total params: 40,668,739
Trainable params: 25,954,051
Non-trainable params: 14,714,688
_________________________________________________________________

The subjects are trained: [(1, 'F01'), (2, 'F02'), (3, 'F03'), (4, 'F04'), (5, 'F05'), (6, 'F06'), (7, 'M01'), (8, 'M
02'), (10, 'M04'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (14, 'M08'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (19, 'M
11'), (20, 'M12'), (22, 'M01'), (23, 'M13')]
Evaluating model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38_and_20
19-01-20_00-43-52
The subjects will be tested: [(9, 'M03'), (18, 'F05'), (21, 'F02'), (24, 'M14')]
All frames and annotations from 4 datasets have been read by 2019-01-20 00:43:53.898988
For the Subject 9 (M03):
2019-01-20 00:44:29.729725: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of m
emory trying to allocate 4.59GiB.  Current allocation summary follows.
2019-01-20 00:44:29.729815: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256):   Total Chunks: 51, Chu
nks in use: 51. 12.8KiB allocated for chunks. 12.8KiB in use in bin. 1008B client-requested in use in bin.
2019-01-20 00:44:29.729869: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512):   Total Chunks: 3, Chun
ks in use: 3. 1.5KiB allocated for chunks. 1.5KiB in use in bin. 1.5KiB client-requested in use in bin.
2019-01-20 00:44:29.729897: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024):  Total Chunks: 5, Chun
ks in use: 5. 5.2KiB allocated for chunks. 5.2KiB in use in bin. 5.0KiB client-requested in use in bin.
2019-01-20 00:44:29.729924: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048):  Total Chunks: 11, Chu
nks in use: 11. 26.8KiB allocated for chunks. 26.8KiB in use in bin. 26.0KiB client-requested in use in bin.
2019-01-20 00:44:29.729969: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096):  Total Chunks: 7, Chun
ks in use: 7. 33.5KiB allocated for chunks. 33.5KiB in use in bin. 33.5KiB client-requested in use in bin.
2019-01-20 00:44:29.730004: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192):  Total Chunks: 0, Chun
ks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-20 00:44:29.730039: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384):         Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-20 00:44:29.730071: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768):         Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-20 00:44:29.730107: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536):         Total Chunks:
 1, Chunks in use: 0. 115.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-20 00:44:29.730145: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072):        Total Chunks:
 1, Chunks in use: 1. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.
2019-01-20 00:44:29.730180: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144):        Total Chunks:
 5, Chunks in use: 5. 1.38MiB allocated for chunks. 1.38MiB in use in bin. 1.28MiB client-requested in use in bin.
2019-01-20 00:44:29.730216: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288):        Total Chunks:
 1, Chunks in use: 1. 576.0KiB allocated for chunks. 576.0KiB in use in bin. 576.0KiB client-requested in use in bin.
2019-01-20 00:44:29.730249: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576):       Total Chunks:
 6, Chunks in use: 6. 6.50MiB allocated for chunks. 6.50MiB in use in bin. 6.12MiB client-requested in use in bin.
2019-01-20 00:44:29.730282: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152):       Total Chunks:
 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-20 00:44:29.730316: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304):       Total Chunks:
 2, Chunks in use: 1. 8.75MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.
2019-01-20 00:44:29.730351: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608):       Total Chunks:
 6, Chunks in use: 5. 54.00MiB allocated for chunks. 45.00MiB in use in bin. 45.00MiB client-requested in use in bin.
2019-01-20 00:44:29.730387: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216):      Total Chunks:
 4, Chunks in use: 4. 98.00MiB allocated for chunks. 98.00MiB in use in bin. 98.00MiB client-requested in use in bin.
2019-01-20 00:44:29.730418: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432):      Total Chunks:
 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-01-20 00:44:29.730454: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864):      Total Chunks:
 4, Chunks in use: 4. 392.00MiB allocated for chunks. 392.00MiB in use in bin. 392.00MiB client-requested in use in b
in.
2019-01-20 00:44:29.730491: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728):     Total Chunks:
 2, Chunks in use: 1. 441.00MiB allocated for chunks. 220.50MiB in use in bin. 220.50MiB client-requested in use in b
in.
2019-01-20 00:44:29.730525: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456):     Total Chunks:
 2, Chunks in use: 1. 8.33GiB allocated for chunks. 4.59GiB in use in bin. 4.59GiB client-requested in use in bin.
2019-01-20 00:44:29.730556: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 4.59GiB was 256.00MiB, Chu
nk State:
2019-01-20 00:44:29.730594: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 3.74GiB | Requested Size:
0B | in_use: 0, prev:   Size: 4.59GiB | Requested Size: 4.59GiB | in_use: 1
2019-01-20 00:44:29.730623: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500000 of size 1280
2019-01-20 00:44:29.730649: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500500 of size 256
2019-01-20 00:44:29.730674: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500600 of size 256
2019-01-20 00:44:29.730698: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500700 of size 256
2019-01-20 00:44:29.730721: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500800 of size 256
2019-01-20 00:44:29.730745: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500900 of size 256
2019-01-20 00:44:29.730769: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500a00 of size 256
2019-01-20 00:44:29.730793: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500b00 of size 256
2019-01-20 00:44:29.730822: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500c00 of size 512
2019-01-20 00:44:29.730845: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500e00 of size 256
2019-01-20 00:44:29.730868: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a500f00 of size 256
2019-01-20 00:44:29.730887: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501000 of size 256
2019-01-20 00:44:29.730902: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501100 of size 256
2019-01-20 00:44:29.730920: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501200 of size 256
2019-01-20 00:44:29.730942: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501300 of size 1024
2019-01-20 00:44:29.730960: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501700 of size 256
2019-01-20 00:44:29.730979: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501800 of size 256
2019-01-20 00:44:29.730997: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501900 of size 256
2019-01-20 00:44:29.731021: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501a00 of size 256
2019-01-20 00:44:29.731044: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501b00 of size 256
2019-01-20 00:44:29.731070: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a501c00 of size 2048
2019-01-20 00:44:29.731093: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502400 of size 256
2019-01-20 00:44:29.731118: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502500 of size 256
2019-01-20 00:44:29.731143: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a502600 of size 10485
76
2019-01-20 00:44:29.731168: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a602600 of size 4096
2019-01-20 00:44:29.731191: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603600 of size 256
2019-01-20 00:44:29.731215: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603700 of size 256
2019-01-20 00:44:29.731239: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603800 of size 256
2019-01-20 00:44:29.731264: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a603900 of size 3072
2019-01-20 00:44:29.731290: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a604500 of size 3840
2019-01-20 00:44:29.731313: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a605400 of size 256
2019-01-20 00:44:29.731337: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a605500 of size 4096
2019-01-20 00:44:29.731361: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a606500 of size 4096
2019-01-20 00:44:29.731384: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a607500 of size 3072
2019-01-20 00:44:29.731408: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a608100 of size 256
2019-01-20 00:44:29.731432: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a608200 of size 4096
2019-01-20 00:44:29.731456: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609200 of size 3072
2019-01-20 00:44:29.731479: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609e00 of size 256
2019-01-20 00:44:29.731503: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a609f00 of size 256
2019-01-20 00:44:29.731526: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a000 of size 256
2019-01-20 00:44:29.731550: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a100 of size 256
2019-01-20 00:44:29.731573: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a200 of size 256
2019-01-20 00:44:29.731597: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a300 of size 256
2019-01-20 00:44:29.731620: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a400 of size 256
2019-01-20 00:44:29.731643: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a500 of size 256
2019-01-20 00:44:29.731667: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a600 of size 256
2019-01-20 00:44:29.731690: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a700 of size 256
2019-01-20 00:44:29.731713: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a800 of size 256
2019-01-20 00:44:29.731736: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60a900 of size 256
2019-01-20 00:44:29.731760: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60aa00 of size 256
2019-01-20 00:44:29.731785: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a60ab00 of size 6912
2019-01-20 00:44:29.731809: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0a60c600 of size 11852
8
2019-01-20 00:44:29.731832: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a629500 of size 256
2019-01-20 00:44:29.731857: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a629600 of size 29491
2
2019-01-20 00:44:29.731881: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a671600 of size 512
2019-01-20 00:44:29.731906: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a671800 of size 26214
4
2019-01-20 00:44:29.731930: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a6b1800 of size 32768
0
2019-01-20 00:44:29.731954: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a701800 of size 512
2019-01-20 00:44:29.731979: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a701a00 of size 11796
48
2019-01-20 00:44:29.732002: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a821a00 of size 1024
2019-01-20 00:44:29.732027: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0a821e00 of size 23592
96
2019-01-20 00:44:29.732051: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aa61e00 of size 10485
76
2019-01-20 00:44:29.732076: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ab61e00 of size 13107
20
2019-01-20 00:44:29.732100: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca1e00 of size 1024
2019-01-20 00:44:29.732124: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca2200 of size 1024
2019-01-20 00:44:29.732147: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0aca2600 of size 26214
4
2019-01-20 00:44:29.732177: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0ace2600 of size 44564
48
2019-01-20 00:44:29.732201: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b122600 of size 2048
2019-01-20 00:44:29.732230: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0b122e00 of size 94371
84
2019-01-20 00:44:29.732254: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0ba22e00 of size 94371
84
2019-01-20 00:44:29.732278: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0c322e00 of size 94371
84
2019-01-20 00:44:29.732302: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0cc22e00 of size 94371
84
2019-01-20 00:44:29.732326: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb0d522e00 of size 94371
84
2019-01-20 00:44:29.732350: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0de22e00 of size 2048
2019-01-20 00:44:29.732374: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0de23600 of size 10485
76
2019-01-20 00:44:29.732398: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df23600 of size 4096
2019-01-20 00:44:29.732421: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24600 of size 2048
2019-01-20 00:44:29.732445: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24e00 of size 256
2019-01-20 00:44:29.732469: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df24f00 of size 256
2019-01-20 00:44:29.732492: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25000 of size 256
2019-01-20 00:44:29.732516: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25100 of size 256
2019-01-20 00:44:29.732539: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25200 of size 256
2019-01-20 00:44:29.732563: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25300 of size 256
2019-01-20 00:44:29.732587: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25400 of size 256
2019-01-20 00:44:29.732611: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25500 of size 256
2019-01-20 00:44:29.732634: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25600 of size 256
2019-01-20 00:44:29.732657: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25700 of size 256
2019-01-20 00:44:29.732681: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25800 of size 256
2019-01-20 00:44:29.732704: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25900 of size 256
2019-01-20 00:44:29.732728: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df25a00 of size 2048
2019-01-20 00:44:29.732752: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26200 of size 2048
2019-01-20 00:44:29.732775: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26a00 of size 256
2019-01-20 00:44:29.732799: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df26b00 of size 2048
2019-01-20 00:44:29.732825: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb0df27300 of size 10276
0448
2019-01-20 00:44:29.732849: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14127300 of size 6912
2019-01-20 00:44:29.732874: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14128e00 of size 14745
6
2019-01-20 00:44:29.732898: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1414ce00 of size 29491
2
2019-01-20 00:44:29.732923: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14194e00 of size 58982
4
2019-01-20 00:44:29.732947: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14224e00 of size 11796
48
2019-01-20 00:44:29.732971: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14344e00 of size 23592
96
2019-01-20 00:44:29.732996: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14584e00 of size 47185
92
2019-01-20 00:44:29.733020: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb14a04e00 of size 94371
84
2019-01-20 00:44:29.733045: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb15304e00 of size 10276
0448
2019-01-20 00:44:29.733068: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb1b504e00 of size 10276
0448
2019-01-20 00:44:29.733092: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb21704e00 of size 10276
0448
2019-01-20 00:44:29.733115: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb27904e00 of size 23121
1008
2019-01-20 00:44:29.733140: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb35584e00 of size 25690
112
2019-01-20 00:44:29.733164: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb36e04e00 of size 25690
112
2019-01-20 00:44:29.733188: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb38684e00 of size 25690
112
2019-01-20 00:44:29.733212: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb39f04e00 of size 25690
112
2019-01-20 00:44:29.733237: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb3b784e00 of size 23121
1008
2019-01-20 00:44:29.733262: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb49404e00 of size 49325
01504
2019-01-20 00:44:29.733287: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xc6f404e00 of size 40117
59616
2019-01-20 00:44:29.733309: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by s
ize:
2019-01-20 00:44:29.733341: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 51 Chunks of size 256 totalling 12
.8KiB
2019-01-20 00:44:29.733368: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 512 totalling 1.5
KiB
2019-01-20 00:44:29.733395: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 1024 totalling 4.
0KiB
2019-01-20 00:44:29.733421: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.
2KiB
2019-01-20 00:44:29.733449: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 7 Chunks of size 2048 totalling 14
.0KiB
2019-01-20 00:44:29.733477: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 3072 totalling 9.
0KiB
2019-01-20 00:44:29.733503: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3840 totalling 3.
8KiB
2019-01-20 00:44:29.733531: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 4096 totalling 20
.0KiB
2019-01-20 00:44:29.733559: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 6912 totalling 13
.5KiB
2019-01-20 00:44:29.733587: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 147456 totalling
144.0KiB
2019-01-20 00:44:29.733616: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 262144 totalling
512.0KiB
2019-01-20 00:44:29.733644: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 294912 totalling
576.0KiB
2019-01-20 00:44:29.733673: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 327680 totalling
320.0KiB
2019-01-20 00:44:29.733718: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 589824 totalling
576.0KiB
2019-01-20 00:44:29.733742: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 3 Chunks of size 1048576 totalling
 3.00MiB
2019-01-20 00:44:29.733761: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 1179648 totalling
 2.25MiB
2019-01-20 00:44:29.733778: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1310720 totalling
 1.25MiB
2019-01-20 00:44:29.733800: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 2 Chunks of size 2359296 totalling
 4.50MiB
2019-01-20 00:44:29.733821: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 4718592 totalling
 4.50MiB
2019-01-20 00:44:29.733843: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 5 Chunks of size 9437184 totalling
 45.00MiB
2019-01-20 00:44:29.733865: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 25690112 totallin
g 98.00MiB
2019-01-20 00:44:29.733887: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 102760448 totalli
ng 392.00MiB
2019-01-20 00:44:29.733909: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 231211008 totalli
ng 220.50MiB
2019-01-20 00:44:29.733930: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 4932501504 totall
ing 4.59GiB
2019-01-20 00:44:29.733951: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 5.35Gi
B
2019-01-20 00:44:29.733978: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats:
Limit:                 10000197223
InUse:                  5743214336
MaxInUse:               5743214336
NumAllocs:                     171
MaxAllocSize:           4932501504

2019-01-20 00:44:29.765508: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *****__***************************
**************************________________________________________
2019-01-20 00:44:29.765594: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_ops.cc:672 : Re
source exhausted: OOM when allocating tensor with shape[384,64,224,224] and type float on /job:localhost/replica:0/ta
sk:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1327, in _do_ca
ll
    return fn(*args)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1312, in _run_f
n
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1420, in _call_
tf_sessionrun
    status, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py", line 516, in
__exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[384,64,224,224]
 and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "continueTrainigCNN_LSTM.py", line 62, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 59, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 52, in continueTrainigCNN_LSTM
    num_outputs = num_outputs, batch_size = test_batch_size, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 80, in evalua
teCNN_LSTM
    outputs = evaluateSubject(full_model, subject, test_gen, test_labels, timesteps, num_outputs, angles, record = re
cord)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/runCNN_LSTM.py", line 47, in evalua
teSubject
    predictions = full_model.predict_generator(test_gen, verbose = 1)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1392, in predict_generator
    verbose=verbose)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 2540, in predict_generator
    outs = self.predict_on_batch(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/training.py", line 1945, in predict_on_batch
    outputs = self.predict_function(ins)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 2482, in __call
__
    **self.session_kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 905, in run
    run_metadata_ptr)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1140, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1321, in _do_ru
n
    run_metadata)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1340, in _do_ca
ll
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[384,64,224,224]
 and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


Caused by op 'tdVGG16/block1_conv1/convolution', defined at:
  File "continueTrainigCNN_LSTM.py", line 62, in <module>
    main()
  File "continueTrainigCNN_LSTM.py", line 59, in main
    continueTrainigCNN_LSTM(record = RECORD)
  File "continueTrainigCNN_LSTM.py", line 30, in continueTrainigCNN_LSTM
    full_model = loadKerasModel(modelID, record = record)
  File "/home/mcicek/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16/EvaluationRecorder.py", line 57, in
 loadKerasModel
    model = load_model(Keras_Models_Folder + fileName)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 270, in load_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 347, in model_from_config
    return layer_module.deserialize(config, custom_objects=custom_objects)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py", line 55, in deserialize
    printable_module_name='layer')
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py", line 144, in deserialize_ke
ras_object
    list(custom_objects.items())))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 1412, in from_config
    model.add(layer)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/models.py", line 497, in add
    layer(x)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 619, in __call__
    output = self.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/wrappers.py", line 213, in call
    y = self.layer.call(inputs, **kwargs)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2085, in call
    output_tensors, _, _ = self.run_internal_graph(inputs, masks)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py", line 2235, in run_internal_grap
h
    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py", line 168, in call
    dilation_rate=self.dilation_rate)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py", line 3341, in conv2d
    data_format=tf_data_format)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 782, in convolution
    return op(input, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 870, in __call__
    return self.conv_op(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 522, in __call__
    return self.call(inp, filter)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 206, in __call__
    name=self.name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 953, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787,
in _apply_op_helper
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3290, in create_
op
    op_def=op_def)
  File "/home/mcicek/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1654, in __init_
_
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[384,64,224,224] and type floa
t on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[Node: tdVGG16/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], pa
dding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](td
VGG16/block1_conv1/convolution-0-TransposeNHWCToNCHW-LayoutOptimizer, block1_conv1/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunO
ptions for current allocation info.


mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python continueTrainigCNN_LSTM.py "V
GG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38"
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-20 00:46:24.814828: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-20 00:46:24.886712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-20 00:46:24.886969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-20 00:46:24.886982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-20 00:46:25.041668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-20 00:46:25.041701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-20 00:46:25.041706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-20 00:46:25.041838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9536 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38.h5 has been saved.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
lstm_1 (LSTM)                (None, 256)               25953280
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 771
=================================================================
Total params: 40,668,739
Trainable params: 25,954,051
Non-trainable params: 14,714,688
_________________________________________________________________

The subjects are trained: [(1, 'F01'), (2, 'F02'), (3, 'F03'), (4, 'F04'), (5, 'F05'), (6, 'F06'), (7, 'M01'), (8, 'M
02'), (10, 'M04'), (11, 'M05'), (12, 'M06'), (13, 'M07'), (14, 'M08'), (15, 'F03'), (16, 'M09'), (17, 'M10'), (19, 'M
11'), (20, 'M12'), (22, 'M01'), (23, 'M13')]
Evaluating model VGG16_seqLen16_lstm256_output3_inEpochs1_outEpochs15_AdamOpt(lr=0.000100)_2019-01-16_03-24-38_and_20
19-01-20_00-46-26
The subjects will be tested: [(9, 'M03'), (18, 'F05'), (21, 'F02'), (24, 'M14')]
All frames and annotations from 4 datasets have been read by 2019-01-20 00:46:27.222529
For the Subject 9 (M03):
217/217 [==============================] - 61s 279ms/step
        The absolute mean error on Pitch angle estimation: 12.58 Degree
        The absolute mean error on Yaw angle estimation: 20.57 Degree
        The absolute mean error on Roll angle estimation: 6.23 Degree
For the Subject 18 (F05):
150/150 [==============================] - 41s 272ms/step
        The absolute mean error on Pitch angle estimation: 11.66 Degree
        The absolute mean error on Yaw angle estimation: 17.86 Degree
        The absolute mean error on Roll angle estimation: 8.63 Degree
For the Subject 21 (F02):
155/155 [==============================] - 42s 272ms/step
        The absolute mean error on Pitch angle estimation: 15.25 Degree
        The absolute mean error on Yaw angle estimation: 18.45 Degree
        The absolute mean error on Roll angle estimation: 12.81 Degree
For the Subject 24 (M14):
119/119 [==============================] - 33s 273ms/step
        The absolute mean error on Pitch angle estimation: 12.37 Degree
        The absolute mean error on Yaw angle estimation: 16.71 Degree
        The absolute mean error on Roll angle estimation: 10.91 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 12.96 Degree
        The absolute mean error on Yaw angle estimations: 18.40 Degree
        The absolute mean error on Roll angle estimations: 9.65 Degree
subject9_Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26.png has been saved by 2019-01-20 00:49:49.194518.
subject18_Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26.png has been saved by 2019-01-20 00:49:49.398399.
subject21_Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26.png has been saved by 2019-01-20 00:49:49.605350.
subject24_Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26.png has been saved by 2019-01-20 00:49:49.801949.
Model Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26 has been evaluated successfully.
Model Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit "Complating last training
"
error: pathspec 'Complating last training' did not match any file(s) known to git.
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit -m "Complating last train
ing
"
[master f727829] Complating last training
 13 files changed, 149 insertions(+), 22 deletions(-)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26/output_Exp2019-0
1-16_03-24-38_and_2019-01-20_00-46-26.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26/subject18_Exp201
9-01-16_03-24-38_and_2019-01-20_00-46-26.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26/subject21_Exp201
9-01-16_03-24-38_and_2019-01-20_00-46-26.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26/subject24_Exp201
9-01-16_03-24-38_and_2019-01-20_00-46-26.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26/subject9_Exp2019
-01-16_03-24-38_and_2019-01-20_00-46-26.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-19_18-59-27/output_Exp2019-01-19_18-59-27.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-19_18-59-27/subject24_Exp2019-01-19_18-59-27.png
 rename DeepRL_For_HPE/LSTM_VGG16/results/{Last_Model => Last_Model__}/output_Last_Model.txt (100%)
 rename DeepRL_For_HPE/LSTM_VGG16/{ => results/Last_Model__}/scrollback.txt (100%)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 38, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (37/37), done.
Writing objects: 100% (38/38), 778.97 KiB | 0 bytes/s, done.
Total 38 (delta 22), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   05a019f..f727829  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-20 04:23:27.271888: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-20 04:23:27.343516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-20 04:23:27.343806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-20 04:23:27.343820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-20 04:23:27.497494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-20 04:23:27.497516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-20 04:23:27.497521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-20 04:23:27.497695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9536 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-20_04-23-27 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
time_distributed_2 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
fc1024 (TimeDistributed)     (None, 16, 4096)          102764544
_________________________________________________________________
time_distributed_3 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
fc104 (TimeDistributed)      (None, 16, 4096)          16781312
_________________________________________________________________
time_distributed_4 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
fc10 (TimeDistributed)       (None, 16, 1024)          4195328
_________________________________________________________________
time_distributed_5 (TimeDist (None, 16, 1024)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 10)                41400
_________________________________________________________________
dense_4 (Dense)              (None, 3)                 33
=================================================================
Total params: 138,497,305
Trainable params: 123,782,617
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen16_lstm10_output3_inEpochs1_outEpochs20_AdamOpt_lr-0.000100__2019-01-20_04-23-27
All frames and annotations from 20 datasets have been read by 2019-01-20 04:23:33.179827
1. set (Subject 20, M12) being trained for epoch 1!
Epoch 1/1
108/108 [==============================] - 49s 452ms/step - loss: 0.1017 - mean_absolute_error: 0.2374
2. set (Subject 23, M13) being trained for epoch 1!
Epoch 1/1
111/111 [==============================] - 48s 433ms/step - loss: 0.0805 - mean_absolute_error: 0.2174
3. set (Subject 12, M06) being trained for epoch 1!
Epoch 1/1
144/144 [==============================] - 62s 429ms/step - loss: 0.0670 - mean_absolute_error: 0.2003
4. set (Subject 16, M09) being trained for epoch 1!
Epoch 1/1
180/180 [==============================] - 77s 428ms/step - loss: 0.0687 - mean_absolute_error: 0.1876
5. set (Subject 6, F06) being trained for epoch 1!
Epoch 1/1
106/106 [==============================] - 45s 427ms/step - loss: 0.0884 - mean_absolute_error: 0.2051
6. set (Subject 22, M01) being trained for epoch 1!
Epoch 1/1
130/130 [==============================] - 57s 441ms/step - loss: 0.0337 - mean_absolute_error: 0.1218
7. set (Subject 19, M11) being trained for epoch 1!
Epoch 1/1
98/98 [==============================] - 42s 427ms/step - loss: 0.0562 - mean_absolute_error: 0.1779
8. set (Subject 13, M07) being trained for epoch 1!
Epoch 1/1
94/94 [==============================] - 40s 428ms/step - loss: 0.0485 - mean_absolute_error: 0.1656
9. set (Subject 5, F05) being trained for epoch 1!
Epoch 1/1
186/186 [==============================] - 78s 421ms/step - loss: 0.0836 - mean_absolute_error: 0.2096
10. set (Subject 10, M04) being trained for epoch 1!
Epoch 1/1
142/142 [==============================] - 60s 421ms/step - loss: 0.1111 - mean_absolute_error: 0.2787
11. set (Subject 7, M01) being trained for epoch 1!
Epoch 1/1
146/146 [==============================] - 62s 423ms/step - loss: 0.0950 - mean_absolute_error: 0.2297
12. set (Subject 1, F01) being trained for epoch 1!
Epoch 1/1
97/97 [==============================] - 42s 431ms/step - loss: 0.1017 - mean_absolute_error: 0.2306
13. set (Subject 15, F03) being trained for epoch 1!
Epoch 1/1
128/128 [==============================] - 54s 422ms/step - loss: 0.0706 - mean_absolute_error: 0.1883
14. set (Subject 2, F02) being trained for epoch 1!
Epoch 1/1
99/99 [==============================] - 42s 423ms/step - loss: 0.1040 - mean_absolute_error: 0.2416
15. set (Subject 3, F03) being trained for epoch 1!
Epoch 1/1
143/143 [==============================] - 60s 422ms/step - loss: 0.0823 - mean_absolute_error: 0.2115
16. set (Subject 17, M10) being trained for epoch 1!
Epoch 1/1
76/76 [==============================] - 32s 421ms/step - loss: 0.0330 - mean_absolute_error: 0.1230
17. set (Subject 14, M08) being trained for epoch 1!
Epoch 1/1
157/157 [==============================] - 66s 420ms/step - loss: 0.1202 - mean_absolute_error: 0.2763
18. set (Subject 4, F04) being trained for epoch 1!
Epoch 1/1
146/146 [==============================] - 62s 422ms/step - loss: 0.0923 - mean_absolute_error: 0.2309
19. set (Subject 11, M05) being trained for epoch 1!
Epoch 1/1
112/112 [==============================] - 47s 420ms/step - loss: 0.0453 - mean_absolute_error: 0.1446
20. set (Subject 8, M02) being trained for epoch 1!
Epoch 1/1
152/152 [==============================] - 64s 419ms/step - loss: 0.0743 - mean_absolute_error: 0.2010
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 04:43:55.596837
1. set (Subject 4, F04) being trained for epoch 2!
Epoch 1/1
146/146 [==============================] - 61s 418ms/step - loss: 0.0886 - mean_absolute_error: 0.2268
2. set (Subject 8, M02) being trained for epoch 2!
Epoch 1/1
152/152 [==============================] - 64s 419ms/step - loss: 0.0715 - mean_absolute_error: 0.1964
3. set (Subject 7, M01) being trained for epoch 2!
Epoch 1/1
146/146 [==============================] - 62s 422ms/step - loss: 0.0823 - mean_absolute_error: 0.2165
4. set (Subject 3, F03) being trained for epoch 2!
Epoch 1/1
143/143 [==============================] - 60s 422ms/step - loss: 0.0772 - mean_absolute_error: 0.2099
5. set (Subject 22, M01) being trained for epoch 2!
Epoch 1/1
130/130 [==============================] - 55s 422ms/step - loss: 0.0361 - mean_absolute_error: 0.1255
6. set (Subject 11, M05) being trained for epoch 2!
Epoch 1/1
112/112 [==============================] - 47s 418ms/step - loss: 0.0471 - mean_absolute_error: 0.1467
7. set (Subject 14, M08) being trained for epoch 2!
Epoch 1/1
157/157 [==============================] - 66s 418ms/step - loss: 0.1206 - mean_absolute_error: 0.2748
8. set (Subject 1, F01) being trained for epoch 2!
Epoch 1/1
97/97 [==============================] - 41s 418ms/step - loss: 0.1075 - mean_absolute_error: 0.2333
9. set (Subject 6, F06) being trained for epoch 2!
Epoch 1/1
106/106 [==============================] - 44s 417ms/step - loss: 0.0875 - mean_absolute_error: 0.1971
10. set (Subject 5, F05) being trained for epoch 2!
Epoch 1/1
186/186 [==============================] - 78s 421ms/step - loss: 0.0815 - mean_absolute_error: 0.2021
11. set (Subject 19, M11) being trained for epoch 2!
Epoch 1/1
98/98 [==============================] - 41s 418ms/step - loss: 0.0524 - mean_absolute_error: 0.1802
12. set (Subject 20, M12) being trained for epoch 2!
Epoch 1/1
108/108 [==============================] - 45s 421ms/step - loss: 0.0453 - mean_absolute_error: 0.1433
13. set (Subject 2, F02) being trained for epoch 2!
Epoch 1/1
99/99 [==============================] - 42s 423ms/step - loss: 0.1030 - mean_absolute_error: 0.2407
14. set (Subject 23, M13) being trained for epoch 2!
Epoch 1/1
111/111 [==============================] - 47s 421ms/step - loss: 0.0884 - mean_absolute_error: 0.2204
15. set (Subject 12, M06) being trained for epoch 2!
Epoch 1/1
144/144 [==============================] - 60s 420ms/step - loss: 0.1090 - mean_absolute_error: 0.2770
16. set (Subject 17, M10) being trained for epoch 2!
Epoch 1/1
76/76 [==============================] - 32s 420ms/step - loss: 0.1785 - mean_absolute_error: 0.3265
17. set (Subject 15, F03) being trained for epoch 2!
Epoch 1/1
128/128 [==============================] - 54s 422ms/step - loss: 0.1416 - mean_absolute_error: 0.2927
18. set (Subject 16, M09) being trained for epoch 2!
Epoch 1/1
180/180 [==============================] - 76s 421ms/step - loss: 0.1006 - mean_absolute_error: 0.2364
19. set (Subject 10, M04) being trained for epoch 2!
Epoch 1/1
142/142 [==============================] - 60s 421ms/step - loss: 0.0780 - mean_absolute_error: 0.2229
20. set (Subject 13, M07) being trained for epoch 2!
Epoch 1/1
94/94 [==============================] - 40s 421ms/step - loss: 0.0528 - mean_absolute_error: 0.1761
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 05:04:01.660601
1. set (Subject 16, M09) being trained for epoch 3!
Epoch 1/1
180/180 [==============================] - 75s 419ms/step - loss: 0.0567 - mean_absolute_error: 0.1745
2. set (Subject 13, M07) being trained for epoch 3!
Epoch 1/1
94/94 [==============================] - 40s 423ms/step - loss: 0.0513 - mean_absolute_error: 0.1641
3. set (Subject 19, M11) being trained for epoch 3!
Epoch 1/1
98/98 [==============================] - 41s 419ms/step - loss: 0.0430 - mean_absolute_error: 0.1625
4. set (Subject 12, M06) being trained for epoch 3!
Epoch 1/1
144/144 [==============================] - 61s 421ms/step - loss: 0.0728 - mean_absolute_error: 0.1979
5. set (Subject 11, M05) being trained for epoch 3!
Epoch 1/1
112/112 [==============================] - 47s 421ms/step - loss: 0.0472 - mean_absolute_error: 0.1496
6. set (Subject 10, M04) being trained for epoch 3!
Epoch 1/1
142/142 [==============================] - 60s 422ms/step - loss: 0.0701 - mean_absolute_error: 0.1908
7. set (Subject 15, F03) being trained for epoch 3!
Epoch 1/1
128/128 [==============================] - 54s 421ms/step - loss: 0.0633 - mean_absolute_error: 0.1720
8. set (Subject 20, M12) being trained for epoch 3!
Epoch 1/1
108/108 [==============================] - 46s 422ms/step - loss: 0.0412 - mean_absolute_error: 0.1346
9. set (Subject 22, M01) being trained for epoch 3!
Epoch 1/1
130/130 [==============================] - 55s 421ms/step - loss: 0.0314 - mean_absolute_error: 0.1107
10. set (Subject 6, F06) being trained for epoch 3!
Epoch 1/1
106/106 [==============================] - 45s 420ms/step - loss: 0.0777 - mean_absolute_error: 0.1834
11. set (Subject 14, M08) being trained for epoch 3!
Epoch 1/1
157/157 [==============================] - 66s 420ms/step - loss: 0.1146 - mean_absolute_error: 0.2673
12. set (Subject 4, F04) being trained for epoch 3!
Epoch 1/1
146/146 [==============================] - 61s 421ms/step - loss: 0.0917 - mean_absolute_error: 0.2322
13. set (Subject 23, M13) being trained for epoch 3!
Epoch 1/1
111/111 [==============================] - 47s 421ms/step - loss: 0.0721 - mean_absolute_error: 0.1943
14. set (Subject 8, M02) being trained for epoch 3!
Epoch 1/1
152/152 [==============================] - 64s 419ms/step - loss: 0.0796 - mean_absolute_error: 0.2114
15. set (Subject 7, M01) being trained for epoch 3!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0852 - mean_absolute_error: 0.2197
16. set (Subject 17, M10) being trained for epoch 3!
Epoch 1/1
76/76 [==============================] - 32s 420ms/step - loss: 0.0295 - mean_absolute_error: 0.1141
17. set (Subject 2, F02) being trained for epoch 3!
Epoch 1/1
99/99 [==============================] - 42s 421ms/step - loss: 0.1090 - mean_absolute_error: 0.2443
18. set (Subject 3, F03) being trained for epoch 3!
Epoch 1/1
143/143 [==============================] - 60s 420ms/step - loss: 0.0770 - mean_absolute_error: 0.2074
19. set (Subject 5, F05) being trained for epoch 3!
Epoch 1/1
186/186 [==============================] - 79s 422ms/step - loss: 0.0813 - mean_absolute_error: 0.1996
20. set (Subject 1, F01) being trained for epoch 3!
Epoch 1/1
97/97 [==============================] - 41s 420ms/step - loss: 0.1025 - mean_absolute_error: 0.2297
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 05:24:08.557698
1. set (Subject 3, F03) being trained for epoch 4!
Epoch 1/1
143/143 [==============================] - 60s 420ms/step - loss: 0.0767 - mean_absolute_error: 0.2037
2. set (Subject 1, F01) being trained for epoch 4!
Epoch 1/1
97/97 [==============================] - 41s 419ms/step - loss: 0.1026 - mean_absolute_error: 0.2325
3. set (Subject 14, M08) being trained for epoch 4!
Epoch 1/1
157/157 [==============================] - 66s 420ms/step - loss: 0.1225 - mean_absolute_error: 0.2794
4. set (Subject 7, M01) being trained for epoch 4!
Epoch 1/1
146/146 [==============================] - 62s 421ms/step - loss: 0.0826 - mean_absolute_error: 0.2144
5. set (Subject 10, M04) being trained for epoch 4!
Epoch 1/1
142/142 [==============================] - 60s 422ms/step - loss: 0.0630 - mean_absolute_error: 0.1787
6. set (Subject 5, F05) being trained for epoch 4!
Epoch 1/1
186/186 [==============================] - 78s 421ms/step - loss: 0.0778 - mean_absolute_error: 0.1971
7. set (Subject 2, F02) being trained for epoch 4!
Epoch 1/1
99/99 [==============================] - 42s 420ms/step - loss: 0.0907 - mean_absolute_error: 0.2274
8. set (Subject 4, F04) being trained for epoch 4!
Epoch 1/1
146/146 [==============================] - 61s 419ms/step - loss: 0.0885 - mean_absolute_error: 0.2227
9. set (Subject 11, M05) being trained for epoch 4!
Epoch 1/1
112/112 [==============================] - 47s 419ms/step - loss: 0.0498 - mean_absolute_error: 0.1566
10. set (Subject 22, M01) being trained for epoch 4!
Epoch 1/1
130/130 [==============================] - 55s 420ms/step - loss: 0.0373 - mean_absolute_error: 0.1281
11. set (Subject 15, F03) being trained for epoch 4!
Epoch 1/1
128/128 [==============================] - 54s 420ms/step - loss: 0.0733 - mean_absolute_error: 0.1938
12. set (Subject 16, M09) being trained for epoch 4!
Epoch 1/1
180/180 [==============================] - 76s 423ms/step - loss: 0.0625 - mean_absolute_error: 0.1793
13. set (Subject 8, M02) being trained for epoch 4!
Epoch 1/1
152/152 [==============================] - 64s 421ms/step - loss: 0.0741 - mean_absolute_error: 0.2007
14. set (Subject 13, M07) being trained for epoch 4!
Epoch 1/1
94/94 [==============================] - 40s 422ms/step - loss: 0.0393 - mean_absolute_error: 0.1479
15. set (Subject 19, M11) being trained for epoch 4!
Epoch 1/1
98/98 [==============================] - 41s 417ms/step - loss: 0.0473 - mean_absolute_error: 0.1717
16. set (Subject 17, M10) being trained for epoch 4!
Epoch 1/1
76/76 [==============================] - 32s 422ms/step - loss: 0.0318 - mean_absolute_error: 0.1218
17. set (Subject 23, M13) being trained for epoch 4!
Epoch 1/1
111/111 [==============================] - 47s 420ms/step - loss: 0.0733 - mean_absolute_error: 0.1984
18. set (Subject 12, M06) being trained for epoch 4!
Epoch 1/1
144/144 [==============================] - 60s 419ms/step - loss: 0.0600 - mean_absolute_error: 0.1784
19. set (Subject 6, F06) being trained for epoch 4!
Epoch 1/1
106/106 [==============================] - 44s 418ms/step - loss: 0.0848 - mean_absolute_error: 0.1939
20. set (Subject 20, M12) being trained for epoch 4!
Epoch 1/1
108/108 [==============================] - 45s 420ms/step - loss: 0.0421 - mean_absolute_error: 0.1373
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 05:44:14.889901
1. set (Subject 12, M06) being trained for epoch 5!
Epoch 1/1
144/144 [==============================] - 60s 417ms/step - loss: 0.0600 - mean_absolute_error: 0.1780
2. set (Subject 20, M12) being trained for epoch 5!
Epoch 1/1
108/108 [==============================] - 46s 423ms/step - loss: 0.0431 - mean_absolute_error: 0.1407
3. set (Subject 15, F03) being trained for epoch 5!
Epoch 1/1
128/128 [==============================] - 54s 420ms/step - loss: 0.0672 - mean_absolute_error: 0.1824
4. set (Subject 19, M11) being trained for epoch 5!
Epoch 1/1
98/98 [==============================] - 41s 420ms/step - loss: 0.0449 - mean_absolute_error: 0.1637
5. set (Subject 5, F05) being trained for epoch 5!
Epoch 1/1
186/186 [==============================] - 78s 422ms/step - loss: 0.0846 - mean_absolute_error: 0.2073
6. set (Subject 6, F06) being trained for epoch 5!
Epoch 1/1
106/106 [==============================] - 44s 418ms/step - loss: 0.0837 - mean_absolute_error: 0.1921
7. set (Subject 23, M13) being trained for epoch 5!
Epoch 1/1
111/111 [==============================] - 47s 420ms/step - loss: 0.0717 - mean_absolute_error: 0.1953
8. set (Subject 16, M09) being trained for epoch 5!
Epoch 1/1
180/180 [==============================] - 76s 422ms/step - loss: 0.0573 - mean_absolute_error: 0.1667
9. set (Subject 10, M04) being trained for epoch 5!
Epoch 1/1
142/142 [==============================] - 60s 424ms/step - loss: 0.0675 - mean_absolute_error: 0.1866
10. set (Subject 11, M05) being trained for epoch 5!
Epoch 1/1
112/112 [==============================] - 47s 418ms/step - loss: 0.0442 - mean_absolute_error: 0.1375
11. set (Subject 2, F02) being trained for epoch 5!
Epoch 1/1
99/99 [==============================] - 42s 422ms/step - loss: 0.1077 - mean_absolute_error: 0.2448
12. set (Subject 3, F03) being trained for epoch 5!
Epoch 1/1
143/143 [==============================] - 60s 421ms/step - loss: 0.0787 - mean_absolute_error: 0.2104
13. set (Subject 13, M07) being trained for epoch 5!
Epoch 1/1
94/94 [==============================] - 40s 420ms/step - loss: 0.0399 - mean_absolute_error: 0.1429
14. set (Subject 1, F01) being trained for epoch 5!
Epoch 1/1
97/97 [==============================] - 41s 419ms/step - loss: 0.1031 - mean_absolute_error: 0.2301
15. set (Subject 14, M08) being trained for epoch 5!
Epoch 1/1
157/157 [==============================] - 66s 419ms/step - loss: 0.1206 - mean_absolute_error: 0.2760
16. set (Subject 17, M10) being trained for epoch 5!
Epoch 1/1
76/76 [==============================] - 32s 423ms/step - loss: 0.0319 - mean_absolute_error: 0.1186
17. set (Subject 8, M02) being trained for epoch 5!
Epoch 1/1
152/152 [==============================] - 64s 419ms/step - loss: 0.0740 - mean_absolute_error: 0.2010
18. set (Subject 7, M01) being trained for epoch 5!
Epoch 1/1
146/146 [==============================] - 62s 421ms/step - loss: 0.0825 - mean_absolute_error: 0.2159
19. set (Subject 22, M01) being trained for epoch 5!
Epoch 1/1
130/130 [==============================] - 55s 422ms/step - loss: 0.0335 - mean_absolute_error: 0.1161
20. set (Subject 4, F04) being trained for epoch 5!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0880 - mean_absolute_error: 0.2264
Epoch 5 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 06:04:21.777849
1. set (Subject 7, M01) being trained for epoch 6!
Epoch 1/1
146/146 [==============================] - 61s 419ms/step - loss: 0.0832 - mean_absolute_error: 0.2165
2. set (Subject 4, F04) being trained for epoch 6!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0859 - mean_absolute_error: 0.2232
3. set (Subject 2, F02) being trained for epoch 6!
Epoch 1/1
99/99 [==============================] - 42s 422ms/step - loss: 0.0974 - mean_absolute_error: 0.2326
4. set (Subject 14, M08) being trained for epoch 6!
Epoch 1/1
157/157 [==============================] - 66s 419ms/step - loss: 0.1234 - mean_absolute_error: 0.2794
5. set (Subject 6, F06) being trained for epoch 6!
Epoch 1/1
106/106 [==============================] - 44s 417ms/step - loss: 0.0889 - mean_absolute_error: 0.2013
6. set (Subject 22, M01) being trained for epoch 6!
Epoch 1/1
130/130 [==============================] - 55s 421ms/step - loss: 0.0342 - mean_absolute_error: 0.1174
7. set (Subject 8, M02) being trained for epoch 6!
Epoch 1/1
152/152 [==============================] - 64s 421ms/step - loss: 0.0730 - mean_absolute_error: 0.1993
8. set (Subject 3, F03) being trained for epoch 6!
Epoch 1/1
143/143 [==============================] - 60s 420ms/step - loss: 0.0770 - mean_absolute_error: 0.2104
9. set (Subject 5, F05) being trained for epoch 6!
Epoch 1/1
186/186 [==============================] - 78s 421ms/step - loss: 0.0800 - mean_absolute_error: 0.1975
10. set (Subject 10, M04) being trained for epoch 6!
Epoch 1/1
142/142 [==============================] - 60s 423ms/step - loss: 0.0624 - mean_absolute_error: 0.1783
11. set (Subject 23, M13) being trained for epoch 6!
Epoch 1/1
111/111 [==============================] - 47s 421ms/step - loss: 0.0759 - mean_absolute_error: 0.2063
12. set (Subject 12, M06) being trained for epoch 6!
Epoch 1/1
144/144 [==============================] - 61s 420ms/step - loss: 0.0553 - mean_absolute_error: 0.1726
13. set (Subject 1, F01) being trained for epoch 6!
Epoch 1/1
97/97 [==============================] - 41s 421ms/step - loss: 0.0995 - mean_absolute_error: 0.2262
14. set (Subject 20, M12) being trained for epoch 6!
Epoch 1/1
108/108 [==============================] - 46s 422ms/step - loss: 0.0475 - mean_absolute_error: 0.1534
15. set (Subject 15, F03) being trained for epoch 6!
Epoch 1/1
128/128 [==============================] - 54s 420ms/step - loss: 0.0735 - mean_absolute_error: 0.1945
16. set (Subject 17, M10) being trained for epoch 6!
Epoch 1/1
76/76 [==============================] - 32s 422ms/step - loss: 0.0336 - mean_absolute_error: 0.1240
17. set (Subject 13, M07) being trained for epoch 6!
Epoch 1/1
94/94 [==============================] - 40s 421ms/step - loss: 0.0394 - mean_absolute_error: 0.1450
18. set (Subject 19, M11) being trained for epoch 6!
Epoch 1/1
98/98 [==============================] - 41s 418ms/step - loss: 0.0493 - mean_absolute_error: 0.1760
19. set (Subject 11, M05) being trained for epoch 6!
Epoch 1/1
112/112 [==============================] - 47s 420ms/step - loss: 0.0463 - mean_absolute_error: 0.1425
20. set (Subject 16, M09) being trained for epoch 6!
Epoch 1/1
180/180 [==============================] - 76s 420ms/step - loss: 0.0598 - mean_absolute_error: 0.1739
Epoch 6 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 06:24:28.291078
1. set (Subject 19, M11) being trained for epoch 7!
Epoch 1/1
98/98 [==============================] - 41s 417ms/step - loss: 0.0441 - mean_absolute_error: 0.1617
2. set (Subject 16, M09) being trained for epoch 7!
Epoch 1/1
180/180 [==============================] - 76s 421ms/step - loss: 0.0564 - mean_absolute_error: 0.1663
3. set (Subject 23, M13) being trained for epoch 7!
Epoch 1/1
111/111 [==============================] - 47s 420ms/step - loss: 0.0713 - mean_absolute_error: 0.1925
4. set (Subject 15, F03) being trained for epoch 7!
Epoch 1/1
128/128 [==============================] - 54s 422ms/step - loss: 0.0611 - mean_absolute_error: 0.1663
5. set (Subject 22, M01) being trained for epoch 7!
Epoch 1/1
130/130 [==============================] - 54s 418ms/step - loss: 0.0305 - mean_absolute_error: 0.1094
6. set (Subject 11, M05) being trained for epoch 7!
Epoch 1/1
112/112 [==============================] - 47s 419ms/step - loss: 0.0450 - mean_absolute_error: 0.1386
7. set (Subject 13, M07) being trained for epoch 7!
Epoch 1/1
94/94 [==============================] - 40s 422ms/step - loss: 0.0466 - mean_absolute_error: 0.1562
8. set (Subject 12, M06) being trained for epoch 7!
Epoch 1/1
144/144 [==============================] - 60s 419ms/step - loss: 0.0649 - mean_absolute_error: 0.1846
9. set (Subject 6, F06) being trained for epoch 7!
Epoch 1/1
106/106 [==============================] - 44s 419ms/step - loss: 0.0807 - mean_absolute_error: 0.1870
10. set (Subject 5, F05) being trained for epoch 7!
Epoch 1/1
186/186 [==============================] - 78s 421ms/step - loss: 0.0846 - mean_absolute_error: 0.2059
11. set (Subject 8, M02) being trained for epoch 7!
Epoch 1/1
152/152 [==============================] - 64s 419ms/step - loss: 0.0750 - mean_absolute_error: 0.2015
12. set (Subject 7, M01) being trained for epoch 7!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0834 - mean_absolute_error: 0.2175
13. set (Subject 20, M12) being trained for epoch 7!
Epoch 1/1
108/108 [==============================] - 45s 421ms/step - loss: 0.0450 - mean_absolute_error: 0.1450
14. set (Subject 4, F04) being trained for epoch 7!
Epoch 1/1
146/146 [==============================] - 61s 418ms/step - loss: 0.0897 - mean_absolute_error: 0.2289
15. set (Subject 2, F02) being trained for epoch 7!
Epoch 1/1
99/99 [==============================] - 42s 421ms/step - loss: 0.0983 - mean_absolute_error: 0.2346
16. set (Subject 17, M10) being trained for epoch 7!
Epoch 1/1
76/76 [==============================] - 32s 423ms/step - loss: 0.0355 - mean_absolute_error: 0.1314
17. set (Subject 1, F01) being trained for epoch 7!
Epoch 1/1
97/97 [==============================] - 41s 422ms/step - loss: 0.1026 - mean_absolute_error: 0.2296
18. set (Subject 14, M08) being trained for epoch 7!
Epoch 1/1
157/157 [==============================] - 66s 420ms/step - loss: 0.1251 - mean_absolute_error: 0.2796
19. set (Subject 10, M04) being trained for epoch 7!
Epoch 1/1
142/142 [==============================] - 60s 422ms/step - loss: 0.0633 - mean_absolute_error: 0.1793
20. set (Subject 3, F03) being trained for epoch 7!
Epoch 1/1
143/143 [==============================] - 60s 420ms/step - loss: 0.0784 - mean_absolute_error: 0.2099
Epoch 7 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 06:44:34.198492
1. set (Subject 14, M08) being trained for epoch 8!
Epoch 1/1
157/157 [==============================] - 66s 418ms/step - loss: 0.1218 - mean_absolute_error: 0.2775
2. set (Subject 3, F03) being trained for epoch 8!
Epoch 1/1
143/143 [==============================] - 60s 423ms/step - loss: 0.0773 - mean_absolute_error: 0.2056
3. set (Subject 8, M02) being trained for epoch 8!
Epoch 1/1
152/152 [==============================] - 64s 421ms/step - loss: 0.0735 - mean_absolute_error: 0.2035
4. set (Subject 2, F02) being trained for epoch 8!
Epoch 1/1
99/99 [==============================] - 42s 423ms/step - loss: 0.0976 - mean_absolute_error: 0.2333
5. set (Subject 11, M05) being trained for epoch 8!
Epoch 1/1
112/112 [==============================] - 47s 418ms/step - loss: 0.0480 - mean_absolute_error: 0.1513
6. set (Subject 10, M04) being trained for epoch 8!
Epoch 1/1
142/142 [==============================] - 60s 420ms/step - loss: 0.0626 - mean_absolute_error: 0.1785
7. set (Subject 1, F01) being trained for epoch 8!
Epoch 1/1
97/97 [==============================] - 41s 419ms/step - loss: 0.1012 - mean_absolute_error: 0.2274
8. set (Subject 7, M01) being trained for epoch 8!
Epoch 1/1
146/146 [==============================] - 61s 421ms/step - loss: 0.0831 - mean_absolute_error: 0.2156
9. set (Subject 22, M01) being trained for epoch 8!
Epoch 1/1
130/130 [==============================] - 55s 420ms/step - loss: 0.0366 - mean_absolute_error: 0.1261
10. set (Subject 6, F06) being trained for epoch 8!
Epoch 1/1
106/106 [==============================] - 44s 420ms/step - loss: 0.0903 - mean_absolute_error: 0.2015
11. set (Subject 13, M07) being trained for epoch 8!
Epoch 1/1
94/94 [==============================] - 40s 422ms/step - loss: 0.0580 - mean_absolute_error: 0.1718
12. set (Subject 19, M11) being trained for epoch 8!
Epoch 1/1
98/98 [==============================] - 41s 418ms/step - loss: 0.3679 - mean_absolute_error: 0.5278
13. set (Subject 4, F04) being trained for epoch 8!
Epoch 1/1
146/146 [==============================] - 61s 421ms/step - loss: 0.3223 - mean_absolute_error: 0.5022
14. set (Subject 16, M09) being trained for epoch 8!
Epoch 1/1
180/180 [==============================] - 76s 422ms/step - loss: 0.3616 - mean_absolute_error: 0.5500
15. set (Subject 23, M13) being trained for epoch 8!
Epoch 1/1
111/111 [==============================] - 47s 420ms/step - loss: 0.2965 - mean_absolute_error: 0.4818
16. set (Subject 17, M10) being trained for epoch 8!
Epoch 1/1
76/76 [==============================] - 32s 423ms/step - loss: 0.2395 - mean_absolute_error: 0.4495
17. set (Subject 20, M12) being trained for epoch 8!
Epoch 1/1
108/108 [==============================] - 46s 421ms/step - loss: 0.2027 - mean_absolute_error: 0.4036
18. set (Subject 15, F03) being trained for epoch 8!
Epoch 1/1
128/128 [==============================] - 54s 420ms/step - loss: 0.2430 - mean_absolute_error: 0.4288
19. set (Subject 5, F05) being trained for epoch 8!
Epoch 1/1
186/186 [==============================] - 78s 422ms/step - loss: 0.1432 - mean_absolute_error: 0.3265
20. set (Subject 12, M06) being trained for epoch 8!
Epoch 1/1
144/144 [==============================] - 60s 419ms/step - loss: 0.1003 - mean_absolute_error: 0.2678
Epoch 8 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 07:04:41.320148
1. set (Subject 15, F03) being trained for epoch 9!
Epoch 1/1
128/128 [==============================] - 54s 418ms/step - loss: 0.1869 - mean_absolute_error: 0.3628
2. set (Subject 12, M06) being trained for epoch 9!
Epoch 1/1
144/144 [==============================] - 61s 420ms/step - loss: 0.0832 - mean_absolute_error: 0.2417
3. set (Subject 13, M07) being trained for epoch 9!
Epoch 1/1
94/94 [==============================] - 40s 421ms/step - loss: 0.0698 - mean_absolute_error: 0.2251
4. set (Subject 23, M13) being trained for epoch 9!
Epoch 1/1
111/111 [==============================] - 46s 419ms/step - loss: 0.1450 - mean_absolute_error: 0.3120
5. set (Subject 10, M04) being trained for epoch 9!
Epoch 1/1
142/142 [==============================] - 60s 421ms/step - loss: 0.0862 - mean_absolute_error: 0.2368
6. set (Subject 5, F05) being trained for epoch 9!
Epoch 1/1
186/186 [==============================] - 78s 421ms/step - loss: 0.0894 - mean_absolute_error: 0.2456
7. set (Subject 20, M12) being trained for epoch 9!
Epoch 1/1
108/108 [==============================] - 46s 421ms/step - loss: 0.0867 - mean_absolute_error: 0.2392
8. set (Subject 19, M11) being trained for epoch 9!
Epoch 1/1
98/98 [==============================] - 41s 419ms/step - loss: 0.0873 - mean_absolute_error: 0.2315
9. set (Subject 11, M05) being trained for epoch 9!
Epoch 1/1
112/112 [==============================] - 47s 418ms/step - loss: 0.0739 - mean_absolute_error: 0.2109
10. set (Subject 22, M01) being trained for epoch 9!
Epoch 1/1
130/130 [==============================] - 55s 423ms/step - loss: 0.0624 - mean_absolute_error: 0.1966
11. set (Subject 1, F01) being trained for epoch 9!
Epoch 1/1
97/97 [==============================] - 41s 418ms/step - loss: 0.1016 - mean_absolute_error: 0.2389
12. set (Subject 14, M08) being trained for epoch 9!
Epoch 1/1
157/157 [==============================] - 66s 419ms/step - loss: 0.1469 - mean_absolute_error: 0.3131
13. set (Subject 16, M09) being trained for epoch 9!
Epoch 1/1
180/180 [==============================] - 76s 421ms/step - loss: 0.0872 - mean_absolute_error: 0.2256
14. set (Subject 3, F03) being trained for epoch 9!
Epoch 1/1
143/143 [==============================] - 60s 422ms/step - loss: 0.0829 - mean_absolute_error: 0.2309
15. set (Subject 8, M02) being trained for epoch 9!
Epoch 1/1
152/152 [==============================] - 64s 419ms/step - loss: 0.0665 - mean_absolute_error: 0.1851
16. set (Subject 17, M10) being trained for epoch 9!
Epoch 1/1
76/76 [==============================] - 32s 421ms/step - loss: 0.0464 - mean_absolute_error: 0.1611
17. set (Subject 4, F04) being trained for epoch 9!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0859 - mean_absolute_error: 0.2222
18. set (Subject 2, F02) being trained for epoch 9!
Epoch 1/1
99/99 [==============================] - 42s 422ms/step - loss: 0.0859 - mean_absolute_error: 0.2222
19. set (Subject 6, F06) being trained for epoch 9!
Epoch 1/1
106/106 [==============================] - 44s 419ms/step - loss: 0.1023 - mean_absolute_error: 0.2226
20. set (Subject 7, M01) being trained for epoch 9!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0828 - mean_absolute_error: 0.2155
Epoch 9 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 07:24:47.652919
1. set (Subject 2, F02) being trained for epoch 10!
Epoch 1/1
99/99 [==============================] - 41s 419ms/step - loss: 0.0863 - mean_absolute_error: 0.2225
2. set (Subject 7, M01) being trained for epoch 10!
Epoch 1/1
146/146 [==============================] - 62s 422ms/step - loss: 0.0829 - mean_absolute_error: 0.2140
3. set (Subject 1, F01) being trained for epoch 10!
Epoch 1/1
97/97 [==============================] - 41s 421ms/step - loss: 0.0996 - mean_absolute_error: 0.2251
4. set (Subject 8, M02) being trained for epoch 10!
Epoch 1/1
152/152 [==============================] - 64s 421ms/step - loss: 0.0676 - mean_absolute_error: 0.1875
5. set (Subject 5, F05) being trained for epoch 10!
Epoch 1/1
186/186 [==============================] - 79s 423ms/step - loss: 0.0754 - mean_absolute_error: 0.2007
6. set (Subject 6, F06) being trained for epoch 10!
Epoch 1/1
106/106 [==============================] - 44s 419ms/step - loss: 0.1055 - mean_absolute_error: 0.2288
7. set (Subject 4, F04) being trained for epoch 10!
Epoch 1/1
146/146 [==============================] - 61s 421ms/step - loss: 0.0937 - mean_absolute_error: 0.2267
8. set (Subject 14, M08) being trained for epoch 10!
Epoch 1/1
157/157 [==============================] - 66s 420ms/step - loss: 0.1284 - mean_absolute_error: 0.2872
9. set (Subject 10, M04) being trained for epoch 10!
Epoch 1/1
142/142 [==============================] - 60s 421ms/step - loss: 0.0623 - mean_absolute_error: 0.1778
10. set (Subject 11, M05) being trained for epoch 10!
Epoch 1/1
112/112 [==============================] - 47s 421ms/step - loss: 0.0503 - mean_absolute_error: 0.1538
11. set (Subject 20, M12) being trained for epoch 10!
Epoch 1/1
108/108 [==============================] - 46s 422ms/step - loss: 0.0469 - mean_absolute_error: 0.1552
12. set (Subject 15, F03) being trained for epoch 10!
Epoch 1/1
128/128 [==============================] - 54s 419ms/step - loss: 0.0729 - mean_absolute_error: 0.1950
13. set (Subject 3, F03) being trained for epoch 10!
Epoch 1/1
143/143 [==============================] - 60s 420ms/step - loss: 0.0770 - mean_absolute_error: 0.2102
14. set (Subject 12, M06) being trained for epoch 10!
Epoch 1/1
144/144 [==============================] - 60s 417ms/step - loss: 0.0580 - mean_absolute_error: 0.1758
15. set (Subject 13, M07) being trained for epoch 10!
Epoch 1/1
94/94 [==============================] - 39s 419ms/step - loss: 0.0385 - mean_absolute_error: 0.1431
16. set (Subject 17, M10) being trained for epoch 10!
Epoch 1/1
76/76 [==============================] - 32s 419ms/step - loss: 0.0359 - mean_absolute_error: 0.1344
17. set (Subject 16, M09) being trained for epoch 10!
Epoch 1/1
180/180 [==============================] - 76s 420ms/step - loss: 0.0639 - mean_absolute_error: 0.1836
18. set (Subject 23, M13) being trained for epoch 10!
Epoch 1/1
111/111 [==============================] - 47s 422ms/step - loss: 0.0736 - mean_absolute_error: 0.1994
19. set (Subject 22, M01) being trained for epoch 10!
Epoch 1/1
130/130 [==============================] - 55s 420ms/step - loss: 0.0326 - mean_absolute_error: 0.1134
20. set (Subject 19, M11) being trained for epoch 10!
Epoch 1/1
98/98 [==============================] - 41s 418ms/step - loss: 0.0452 - mean_absolute_error: 0.1663
Epoch 10 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 07:44:54.537999
1. set (Subject 23, M13) being trained for epoch 11!
Epoch 1/1
111/111 [==============================] - 46s 418ms/step - loss: 0.0724 - mean_absolute_error: 0.1957
2. set (Subject 19, M11) being trained for epoch 11!
Epoch 1/1
98/98 [==============================] - 41s 419ms/step - loss: 0.0437 - mean_absolute_error: 0.1617
3. set (Subject 20, M12) being trained for epoch 11!
Epoch 1/1
108/108 [==============================] - 46s 424ms/step - loss: 0.0411 - mean_absolute_error: 0.1334
4. set (Subject 13, M07) being trained for epoch 11!
Epoch 1/1
94/94 [==============================] - 40s 422ms/step - loss: 0.0435 - mean_absolute_error: 0.1529
5. set (Subject 6, F06) being trained for epoch 11!
Epoch 1/1
106/106 [==============================] - 44s 419ms/step - loss: 0.0806 - mean_absolute_error: 0.1875
6. set (Subject 22, M01) being trained for epoch 11!
Epoch 1/1
130/130 [==============================] - 55s 422ms/step - loss: 0.0314 - mean_absolute_error: 0.1098
7. set (Subject 16, M09) being trained for epoch 11!
Epoch 1/1
180/180 [==============================] - 76s 421ms/step - loss: 0.0551 - mean_absolute_error: 0.1621
8. set (Subject 15, F03) being trained for epoch 11!
Epoch 1/1
128/128 [==============================] - 54s 421ms/step - loss: 0.0607 - mean_absolute_error: 0.1650
9. set (Subject 5, F05) being trained for epoch 11!
Epoch 1/1
186/186 [==============================] - 78s 422ms/step - loss: 0.0914 - mean_absolute_error: 0.2142
10. set (Subject 10, M04) being trained for epoch 11!
Epoch 1/1
142/142 [==============================] - 60s 420ms/step - loss: 0.0676 - mean_absolute_error: 0.1868
11. set (Subject 4, F04) being trained for epoch 11!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0905 - mean_absolute_error: 0.2298
12. set (Subject 2, F02) being trained for epoch 11!
Epoch 1/1
99/99 [==============================] - 42s 421ms/step - loss: 0.1049 - mean_absolute_error: 0.2401
13. set (Subject 12, M06) being trained for epoch 11!
Epoch 1/1
144/144 [==============================] - 61s 420ms/step - loss: 0.0577 - mean_absolute_error: 0.1759
14. set (Subject 7, M01) being trained for epoch 11!
Epoch 1/1
146/146 [==============================] - 62s 421ms/step - loss: 0.0822 - mean_absolute_error: 0.2143
15. set (Subject 1, F01) being trained for epoch 11!
Epoch 1/1
97/97 [==============================] - 41s 420ms/step - loss: 0.1015 - mean_absolute_error: 0.2282
16. set (Subject 17, M10) being trained for epoch 11!
Epoch 1/1
76/76 [==============================] - 32s 421ms/step - loss: 0.0363 - mean_absolute_error: 0.1310
17. set (Subject 3, F03) being trained for epoch 11!
Epoch 1/1
143/143 [==============================] - 60s 420ms/step - loss: 0.0775 - mean_absolute_error: 0.2075
18. set (Subject 8, M02) being trained for epoch 11!
Epoch 1/1
152/152 [==============================] - 64s 422ms/step - loss: 0.0718 - mean_absolute_error: 0.1987
19. set (Subject 11, M05) being trained for epoch 11!
Epoch 1/1
112/112 [==============================] - 47s 420ms/step - loss: 0.0482 - mean_absolute_error: 0.1504
20. set (Subject 14, M08) being trained for epoch 11!
Epoch 1/1
157/157 [==============================] - 66s 421ms/step - loss: 0.1218 - mean_absolute_error: 0.2768
Epoch 11 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 08:05:01.933075
1. set (Subject 8, M02) being trained for epoch 12!
Epoch 1/1
152/152 [==============================] - 63s 416ms/step - loss: 0.0715 - mean_absolute_error: 0.1960
2. set (Subject 14, M08) being trained for epoch 12!
Epoch 1/1
157/157 [==============================] - 66s 420ms/step - loss: 0.1205 - mean_absolute_error: 0.2750
3. set (Subject 4, F04) being trained for epoch 12!
Epoch 1/1
146/146 [==============================] - 61s 421ms/step - loss: 0.0863 - mean_absolute_error: 0.2252
4. set (Subject 1, F01) being trained for epoch 12!
Epoch 1/1
97/97 [==============================] - 41s 419ms/step - loss: 0.1051 - mean_absolute_error: 0.2291
5. set (Subject 22, M01) being trained for epoch 12!
Epoch 1/1
130/130 [==============================] - 55s 421ms/step - loss: 0.0344 - mean_absolute_error: 0.1194
6. set (Subject 11, M05) being trained for epoch 12!
Epoch 1/1
112/112 [==============================] - 47s 419ms/step - loss: 0.0464 - mean_absolute_error: 0.1434
7. set (Subject 3, F03) being trained for epoch 12!
Epoch 1/1
143/143 [==============================] - 60s 422ms/step - loss: 0.0772 - mean_absolute_error: 0.2105
8. set (Subject 2, F02) being trained for epoch 12!
Epoch 1/1
99/99 [==============================] - 42s 422ms/step - loss: 0.0998 - mean_absolute_error: 0.2353
9. set (Subject 6, F06) being trained for epoch 12!
Epoch 1/1
106/106 [==============================] - 44s 419ms/step - loss: 0.0890 - mean_absolute_error: 0.2031
10. set (Subject 5, F05) being trained for epoch 12!
Epoch 1/1
186/186 [==============================] - 78s 422ms/step - loss: 0.0798 - mean_absolute_error: 0.1978
11. set (Subject 16, M09) being trained for epoch 12!
Epoch 1/1
180/180 [==============================] - 76s 421ms/step - loss: 0.0650 - mean_absolute_error: 0.1845
12. set (Subject 23, M13) being trained for epoch 12!
Epoch 1/1
111/111 [==============================] - 47s 423ms/step - loss: 0.0725 - mean_absolute_error: 0.1981
13. set (Subject 7, M01) being trained for epoch 12!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0834 - mean_absolute_error: 0.2153
14. set (Subject 19, M11) being trained for epoch 12!
Epoch 1/1
98/98 [==============================] - 41s 419ms/step - loss: 0.0487 - mean_absolute_error: 0.1753
15. set (Subject 20, M12) being trained for epoch 12!
Epoch 1/1
108/108 [==============================] - 46s 421ms/step - loss: 0.0424 - mean_absolute_error: 0.1386
16. set (Subject 17, M10) being trained for epoch 12!
Epoch 1/1
76/76 [==============================] - 32s 422ms/step - loss: 0.0304 - mean_absolute_error: 0.1153
17. set (Subject 12, M06) being trained for epoch 12!
Epoch 1/1
144/144 [==============================] - 60s 417ms/step - loss: 0.0607 - mean_absolute_error: 0.1789
18. set (Subject 13, M07) being trained for epoch 12!
Epoch 1/1
94/94 [==============================] - 40s 421ms/step - loss: 0.0403 - mean_absolute_error: 0.1471
19. set (Subject 10, M04) being trained for epoch 12!
Epoch 1/1
142/142 [==============================] - 60s 421ms/step - loss: 0.0636 - mean_absolute_error: 0.1795
20. set (Subject 15, F03) being trained for epoch 12!
Epoch 1/1
128/128 [==============================] - 54s 420ms/step - loss: 0.0698 - mean_absolute_error: 0.1872
Epoch 12 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 08:25:08.318562
1. set (Subject 13, M07) being trained for epoch 13!
Epoch 1/1
94/94 [==============================] - 39s 417ms/step - loss: 0.0401 - mean_absolute_error: 0.1462
2. set (Subject 15, F03) being trained for epoch 13!
Epoch 1/1
128/128 [==============================] - 54s 422ms/step - loss: 0.0681 - mean_absolute_error: 0.1833
3. set (Subject 16, M09) being trained for epoch 13!
Epoch 1/1
180/180 [==============================] - 76s 421ms/step - loss: 0.0584 - mean_absolute_error: 0.1703
4. set (Subject 20, M12) being trained for epoch 13!
Epoch 1/1
108/108 [==============================] - 46s 422ms/step - loss: 0.0411 - mean_absolute_error: 0.1324
5. set (Subject 11, M05) being trained for epoch 13!
Epoch 1/1
112/112 [==============================] - 47s 418ms/step - loss: 0.0445 - mean_absolute_error: 0.1374
6. set (Subject 10, M04) being trained for epoch 13!
Epoch 1/1
142/142 [==============================] - 60s 422ms/step - loss: 0.0672 - mean_absolute_error: 0.1861
7. set (Subject 12, M06) being trained for epoch 13!
Epoch 1/1
144/144 [==============================] - 60s 419ms/step - loss: 0.0610 - mean_absolute_error: 0.1803
8. set (Subject 23, M13) being trained for epoch 13!
Epoch 1/1
111/111 [==============================] - 47s 421ms/step - loss: 0.0720 - mean_absolute_error: 0.1968
9. set (Subject 22, M01) being trained for epoch 13!
Epoch 1/1
130/130 [==============================] - 55s 420ms/step - loss: 0.0317 - mean_absolute_error: 0.1100
10. set (Subject 6, F06) being trained for epoch 13!
Epoch 1/1
106/106 [==============================] - 45s 421ms/step - loss: 0.0819 - mean_absolute_error: 0.1887
11. set (Subject 3, F03) being trained for epoch 13!
Epoch 1/1
143/143 [==============================] - 60s 421ms/step - loss: 0.0798 - mean_absolute_error: 0.2126
12. set (Subject 8, M02) being trained for epoch 13!
Epoch 1/1
152/152 [==============================] - 64s 421ms/step - loss: 0.0766 - mean_absolute_error: 0.2067
13. set (Subject 19, M11) being trained for epoch 13!
Epoch 1/1
98/98 [==============================] - 41s 419ms/step - loss: 0.0471 - mean_absolute_error: 0.1702
14. set (Subject 14, M08) being trained for epoch 13!
Epoch 1/1
157/157 [==============================] - 66s 419ms/step - loss: 0.1168 - mean_absolute_error: 0.2705
15. set (Subject 4, F04) being trained for epoch 13!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0880 - mean_absolute_error: 0.2278
16. set (Subject 17, M10) being trained for epoch 13!
Epoch 1/1
76/76 [==============================] - 32s 421ms/step - loss: 0.0296 - mean_absolute_error: 0.1160
17. set (Subject 7, M01) being trained for epoch 13!
Epoch 1/1
146/146 [==============================] - 62s 422ms/step - loss: 0.0849 - mean_absolute_error: 0.2190
18. set (Subject 1, F01) being trained for epoch 13!
Epoch 1/1
97/97 [==============================] - 41s 418ms/step - loss: 0.1078 - mean_absolute_error: 0.2323
19. set (Subject 5, F05) being trained for epoch 13!
Epoch 1/1
186/186 [==============================] - 79s 423ms/step - loss: 0.0815 - mean_absolute_error: 0.2009
20. set (Subject 2, F02) being trained for epoch 13!
Epoch 1/1
99/99 [==============================] - 42s 422ms/step - loss: 0.0962 - mean_absolute_error: 0.2325
Epoch 13 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 08:45:15.262168
1. set (Subject 1, F01) being trained for epoch 14!
Epoch 1/1
97/97 [==============================] - 40s 414ms/step - loss: 0.0999 - mean_absolute_error: 0.2266
2. set (Subject 2, F02) being trained for epoch 14!
Epoch 1/1
99/99 [==============================] - 42s 422ms/step - loss: 0.0897 - mean_absolute_error: 0.2264
3. set (Subject 3, F03) being trained for epoch 14!
Epoch 1/1
143/143 [==============================] - 60s 423ms/step - loss: 0.0783 - mean_absolute_error: 0.2081
4. set (Subject 4, F04) being trained for epoch 14!
Epoch 1/1
146/146 [==============================] - 62s 422ms/step - loss: 0.0874 - mean_absolute_error: 0.2219
5. set (Subject 10, M04) being trained for epoch 14!
Epoch 1/1
142/142 [==============================] - 60s 421ms/step - loss: 0.0622 - mean_absolute_error: 0.1794
6. set (Subject 5, F05) being trained for epoch 14!
Epoch 1/1
186/186 [==============================] - 79s 422ms/step - loss: 0.0754 - mean_absolute_error: 0.1962
7. set (Subject 7, M01) being trained for epoch 14!
Epoch 1/1
146/146 [==============================] - 61s 421ms/step - loss: 0.0822 - mean_absolute_error: 0.2115
8. set (Subject 8, M02) being trained for epoch 14!
Epoch 1/1
152/152 [==============================] - 64s 419ms/step - loss: 0.0686 - mean_absolute_error: 0.1905
9. set (Subject 11, M05) being trained for epoch 14!
Epoch 1/1
112/112 [==============================] - 47s 418ms/step - loss: 0.0509 - mean_absolute_error: 0.1561
10. set (Subject 22, M01) being trained for epoch 14!
Epoch 1/1
130/130 [==============================] - 55s 420ms/step - loss: 0.0383 - mean_absolute_error: 0.1312
11. set (Subject 12, M06) being trained for epoch 14!
Epoch 1/1
144/144 [==============================] - 60s 420ms/step - loss: 0.0538 - mean_absolute_error: 0.1705
12. set (Subject 13, M07) being trained for epoch 14!
Epoch 1/1
94/94 [==============================] - 39s 419ms/step - loss: 0.0376 - mean_absolute_error: 0.1463
13. set (Subject 14, M08) being trained for epoch 14!
Epoch 1/1
157/157 [==============================] - 66s 421ms/step - loss: 0.1265 - mean_absolute_error: 0.2846
14. set (Subject 15, F03) being trained for epoch 14!
Epoch 1/1
128/128 [==============================] - 54s 420ms/step - loss: 0.0731 - mean_absolute_error: 0.1941
15. set (Subject 16, M09) being trained for epoch 14!
Epoch 1/1
180/180 [==============================] - 76s 421ms/step - loss: 0.0620 - mean_absolute_error: 0.1785
16. set (Subject 17, M10) being trained for epoch 14!
Epoch 1/1
76/76 [==============================] - 32s 420ms/step - loss: 0.0307 - mean_absolute_error: 0.1165
17. set (Subject 19, M11) being trained for epoch 14!
Epoch 1/1
98/98 [==============================] - 41s 416ms/step - loss: 0.0455 - mean_absolute_error: 0.1672
18. set (Subject 20, M12) being trained for epoch 14!
Epoch 1/1
108/108 [==============================] - 45s 420ms/step - loss: 0.0415 - mean_absolute_error: 0.1352
19. set (Subject 6, F06) being trained for epoch 14!
Epoch 1/1
106/106 [==============================] - 45s 421ms/step - loss: 0.0805 - mean_absolute_error: 0.1873
20. set (Subject 23, M13) being trained for epoch 14!
Epoch 1/1
111/111 [==============================] - 47s 421ms/step - loss: 0.0716 - mean_absolute_error: 0.1937
Epoch 14 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 09:05:21.595584
1. set (Subject 20, M12) being trained for epoch 15!
Epoch 1/1
108/108 [==============================] - 45s 420ms/step - loss: 0.0404 - mean_absolute_error: 0.1306
2. set (Subject 23, M13) being trained for epoch 15!
Epoch 1/1
111/111 [==============================] - 47s 422ms/step - loss: 0.0712 - mean_absolute_error: 0.1926
3. set (Subject 12, M06) being trained for epoch 15!
Epoch 1/1
144/144 [==============================] - 60s 419ms/step - loss: 0.0647 - mean_absolute_error: 0.1847
4. set (Subject 16, M09) being trained for epoch 15!
Epoch 1/1
180/180 [==============================] - 76s 421ms/step - loss: 0.0569 - mean_absolute_error: 0.1655
5. set (Subject 5, F05) being trained for epoch 15!
Epoch 1/1
186/186 [==============================] - 78s 421ms/step - loss: 0.0858 - mean_absolute_error: 0.2071
6. set (Subject 6, F06) being trained for epoch 15!
Epoch 1/1
106/106 [==============================] - 45s 421ms/step - loss: 0.0820 - mean_absolute_error: 0.1893
7. set (Subject 19, M11) being trained for epoch 15!
Epoch 1/1
98/98 [==============================] - 41s 418ms/step - loss: 0.0470 - mean_absolute_error: 0.1677
8. set (Subject 13, M07) being trained for epoch 15!
Epoch 1/1
94/94 [==============================] - 40s 421ms/step - loss: 0.0438 - mean_absolute_error: 0.1516
9. set (Subject 10, M04) being trained for epoch 15!
Epoch 1/1
142/142 [==============================] - 60s 421ms/step - loss: 0.0665 - mean_absolute_error: 0.1845
10. set (Subject 11, M05) being trained for epoch 15!
Epoch 1/1
112/112 [==============================] - 47s 418ms/step - loss: 0.0448 - mean_absolute_error: 0.1395
11. set (Subject 7, M01) being trained for epoch 15!
Epoch 1/1
146/146 [==============================] - 62s 421ms/step - loss: 0.0847 - mean_absolute_error: 0.2193
12. set (Subject 1, F01) being trained for epoch 15!
Epoch 1/1
97/97 [==============================] - 41s 420ms/step - loss: 0.1042 - mean_absolute_error: 0.2302
13. set (Subject 15, F03) being trained for epoch 15!
Epoch 1/1
128/128 [==============================] - 54s 421ms/step - loss: 0.0685 - mean_absolute_error: 0.1848
14. set (Subject 2, F02) being trained for epoch 15!
Epoch 1/1
99/99 [==============================] - 42s 420ms/step - loss: 0.1040 - mean_absolute_error: 0.2411
15. set (Subject 3, F03) being trained for epoch 15!
Epoch 1/1
143/143 [==============================] - 60s 421ms/step - loss: 0.0785 - mean_absolute_error: 0.2092
16. set (Subject 17, M10) being trained for epoch 15!
Epoch 1/1
76/76 [==============================] - 32s 420ms/step - loss: 0.0330 - mean_absolute_error: 0.1241
17. set (Subject 14, M08) being trained for epoch 15!
Epoch 1/1
157/157 [==============================] - 66s 420ms/step - loss: 0.1192 - mean_absolute_error: 0.2742
18. set (Subject 4, F04) being trained for epoch 15!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0881 - mean_absolute_error: 0.2268
19. set (Subject 22, M01) being trained for epoch 15!
Epoch 1/1
130/130 [==============================] - 55s 420ms/step - loss: 0.0331 - mean_absolute_error: 0.1158
20. set (Subject 8, M02) being trained for epoch 15!
Epoch 1/1
152/152 [==============================] - 64s 419ms/step - loss: 0.0751 - mean_absolute_error: 0.2039
Epoch 15 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 09:25:27.808724
1. set (Subject 4, F04) being trained for epoch 16!
Epoch 1/1
146/146 [==============================] - 61s 419ms/step - loss: 0.0867 - mean_absolute_error: 0.2249
2. set (Subject 8, M02) being trained for epoch 16!
Epoch 1/1
152/152 [==============================] - 64s 420ms/step - loss: 0.0715 - mean_absolute_error: 0.1977
3. set (Subject 7, M01) being trained for epoch 16!
Epoch 1/1
146/146 [==============================] - 62s 422ms/step - loss: 0.0823 - mean_absolute_error: 0.2167
4. set (Subject 3, F03) being trained for epoch 16!
Epoch 1/1
143/143 [==============================] - 60s 420ms/step - loss: 0.0764 - mean_absolute_error: 0.2086
5. set (Subject 6, F06) being trained for epoch 16!
Epoch 1/1
106/106 [==============================] - 44s 418ms/step - loss: 0.0890 - mean_absolute_error: 0.2034
6. set (Subject 22, M01) being trained for epoch 16!
Epoch 1/1
130/130 [==============================] - 55s 421ms/step - loss: 0.0348 - mean_absolute_error: 0.1206
7. set (Subject 14, M08) being trained for epoch 16!
Epoch 1/1
157/157 [==============================] - 66s 419ms/step - loss: 0.1183 - mean_absolute_error: 0.2723
8. set (Subject 1, F01) being trained for epoch 16!
Epoch 1/1
97/97 [==============================] - 41s 420ms/step - loss: 0.1078 - mean_absolute_error: 0.2325
9. set (Subject 5, F05) being trained for epoch 16!
Epoch 1/1
186/186 [==============================] - 78s 422ms/step - loss: 0.0816 - mean_absolute_error: 0.2006
10. set (Subject 10, M04) being trained for epoch 16!
Epoch 1/1
142/142 [==============================] - 60s 421ms/step - loss: 0.0633 - mean_absolute_error: 0.1791
11. set (Subject 19, M11) being trained for epoch 16!
Epoch 1/1
98/98 [==============================] - 41s 418ms/step - loss: 0.0505 - mean_absolute_error: 0.1799
12. set (Subject 20, M12) being trained for epoch 16!
Epoch 1/1
108/108 [==============================] - 46s 421ms/step - loss: 0.0435 - mean_absolute_error: 0.1435
13. set (Subject 2, F02) being trained for epoch 16!
Epoch 1/1
99/99 [==============================] - 42s 421ms/step - loss: 0.1004 - mean_absolute_error: 0.2378
14. set (Subject 23, M13) being trained for epoch 16!
Epoch 1/1
111/111 [==============================] - 47s 419ms/step - loss: 0.0752 - mean_absolute_error: 0.2042
15. set (Subject 12, M06) being trained for epoch 16!
Epoch 1/1
144/144 [==============================] - 61s 421ms/step - loss: 0.0561 - mean_absolute_error: 0.1730
16. set (Subject 17, M10) being trained for epoch 16!
Epoch 1/1
76/76 [==============================] - 32s 419ms/step - loss: 0.0357 - mean_absolute_error: 0.1300
17. set (Subject 15, F03) being trained for epoch 16!
Epoch 1/1
128/128 [==============================] - 54s 420ms/step - loss: 0.0711 - mean_absolute_error: 0.1896
18. set (Subject 16, M09) being trained for epoch 16!
Epoch 1/1
180/180 [==============================] - 76s 422ms/step - loss: 0.0607 - mean_absolute_error: 0.1749
19. set (Subject 11, M05) being trained for epoch 16!
Epoch 1/1
112/112 [==============================] - 47s 420ms/step - loss: 0.0448 - mean_absolute_error: 0.1398
20. set (Subject 13, M07) being trained for epoch 16!
Epoch 1/1
94/94 [==============================] - 40s 422ms/step - loss: 0.0416 - mean_absolute_error: 0.1480
Epoch 16 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 09:45:34.065162
1. set (Subject 16, M09) being trained for epoch 17!
Epoch 1/1
180/180 [==============================] - 75s 419ms/step - loss: 0.0585 - mean_absolute_error: 0.1702
2. set (Subject 13, M07) being trained for epoch 17!
Epoch 1/1
94/94 [==============================] - 39s 420ms/step - loss: 0.0432 - mean_absolute_error: 0.1493
3. set (Subject 19, M11) being trained for epoch 17!
Epoch 1/1
98/98 [==============================] - 41s 420ms/step - loss: 0.0459 - mean_absolute_error: 0.1657
4. set (Subject 12, M06) being trained for epoch 17!
Epoch 1/1
144/144 [==============================] - 61s 421ms/step - loss: 0.0630 - mean_absolute_error: 0.1820
5. set (Subject 22, M01) being trained for epoch 17!
Epoch 1/1
130/130 [==============================] - 55s 420ms/step - loss: 0.0321 - mean_absolute_error: 0.1108
6. set (Subject 11, M05) being trained for epoch 17!
Epoch 1/1
112/112 [==============================] - 47s 420ms/step - loss: 0.0448 - mean_absolute_error: 0.1386
7. set (Subject 15, F03) being trained for epoch 17!
Epoch 1/1
128/128 [==============================] - 54s 422ms/step - loss: 0.0652 - mean_absolute_error: 0.1772
8. set (Subject 20, M12) being trained for epoch 17!
Epoch 1/1
108/108 [==============================] - 46s 422ms/step - loss: 0.0407 - mean_absolute_error: 0.1314
9. set (Subject 6, F06) being trained for epoch 17!
Epoch 1/1
106/106 [==============================] - 44s 418ms/step - loss: 0.0795 - mean_absolute_error: 0.1849
10. set (Subject 5, F05) being trained for epoch 17!
Epoch 1/1
186/186 [==============================] - 78s 420ms/step - loss: 0.0873 - mean_absolute_error: 0.2104
11. set (Subject 14, M08) being trained for epoch 17!
Epoch 1/1
157/157 [==============================] - 66s 420ms/step - loss: 0.1168 - mean_absolute_error: 0.2709
12. set (Subject 4, F04) being trained for epoch 17!
Epoch 1/1
146/146 [==============================] - 61s 420ms/step - loss: 0.0906 - mean_absolute_error: 0.2303
13. set (Subject 23, M13) being trained for epoch 17!
Epoch 1/1
111/111 [==============================] - 47s 420ms/step - loss: 0.0717 - mean_absolute_error: 0.1947
14. set (Subject 8, M02) being trained for epoch 17!
Epoch 1/1
152/152 [==============================] - 64s 419ms/step - loss: 0.0762 - mean_absolute_error: 0.2052
15. set (Subject 7, M01) being trained for epoch 17!
Epoch 1/1
146/146 [==============================] - 61s 421ms/step - loss: 0.0834 - mean_absolute_error: 0.2177
16. set (Subject 17, M10) being trained for epoch 17!
Epoch 1/1
76/76 [==============================] - 32s 422ms/step - loss: 0.0312 - mean_absolute_error: 0.1182
17. set (Subject 2, F02) being trained for epoch 17!
Epoch 1/1
99/99 [==============================] - 42s 422ms/step - loss: 0.1032 - mean_absolute_error: 0.2393
18. set (Subject 3, F03) being trained for epoch 17!
Epoch 1/1
143/143 [==============================] - 60s 421ms/step - loss: 0.0771 - mean_absolute_error: 0.2068
19. set (Subject 10, M04) being trained for epoch 17!
Epoch 1/1
142/142 [==============================] - 60s 421ms/step - loss: 0.0633 - mean_absolute_error: 0.1807
20. set (Subject 1, F01) being trained for epoch 17!
Epoch 1/1
97/97 [==============================] - 41s 420ms/step - loss: 0.1022 - mean_absolute_error: 0.2290
Epoch 17 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 10:05:40.337440
1. set (Subject 3, F03) being trained for epoch 18!
Epoch 1/1
143/143 [==============================] - 60s 420ms/step - loss: 0.0771 - mean_absolute_error: 0.2059
2. set (Subject 1, F01) being trained for epoch 18!
Epoch 1/1
97/97 [==============================] - 41s 421ms/step - loss: 0.1022 - mean_absolute_error: 0.2313
3. set (Subject 14, M08) being trained for epoch 18!
Epoch 1/1
157/157 [==============================] - 66s 420ms/step - loss: 0.1232 - mean_absolute_error: 0.2800
4. set (Subject 7, M01) being trained for epoch 18!
Epoch 1/1
146/146 [==============================] - 61s 421ms/step - loss: 0.0824 - mean_absolute_error: 0.2138
5. set (Subject 11, M05) being trained for epoch 18!
Epoch 1/1
112/112 [==============================] - 47s 417ms/step - loss: 0.0466 - mean_absolute_error: 0.1470
6. set (Subject 10, M04) being trained for epoch 18!
Epoch 1/1
142/142 [==============================] - 60s 420ms/step - loss: 0.0628 - mean_absolute_error: 0.1780
7. set (Subject 2, F02) being trained for epoch 18!
Epoch 1/1
99/99 [==============================] - 42s 421ms/step - loss: 0.0949 - mean_absolute_error: 0.2321
8. set (Subject 4, F04) being trained for epoch 18!
Epoch 1/1
146/146 [==============================] - 61s 419ms/step - loss: 0.0879 - mean_absolute_error: 0.2235
9. set (Subject 22, M01) being trained for epoch 18!
Epoch 1/1
130/130 [==============================] - 55s 421ms/step - loss: 0.0365 - mean_absolute_error: 0.1268
10. set (Subject 6, F06) being trained for epoch 18!
Epoch 1/1
106/106 [==============================] - 44s 418ms/step - loss: 0.0903 - mean_absolute_error: 0.2025
11. set (Subject 15, F03) being trained for epoch 18!
Epoch 1/1
128/128 [==============================] - 54s 418ms/step - loss: 0.0695 - mean_absolute_error: 0.1868
12. set (Subject 16, M09) being trained for epoch 18!
Epoch 1/1
180/180 [==============================] - 76s 420ms/step - loss: 0.0598 - mean_absolute_error: 0.1734
13. set (Subject 8, M02) being trained for epoch 18!
Epoch 1/1
152/152 [==============================] - 64s 420ms/step - loss: 0.0757 - mean_absolute_error: 0.2041
14. set (Subject 13, M07) being trained for epoch 18!
Epoch 1/1
94/94 [==============================] - 40s 422ms/step - loss: 0.0403 - mean_absolute_error: 0.1485
15. set (Subject 19, M11) being trained for epoch 18!
Epoch 1/1
98/98 [==============================] - 41s 418ms/step - loss: 0.0458 - mean_absolute_error: 0.1677
16. set (Subject 17, M10) being trained for epoch 18!
Epoch 1/1
76/76 [==============================] - 32s 421ms/step - loss: 0.0307 - mean_absolute_error: 0.1187
17. set (Subject 23, M13) being trained for epoch 18!
Epoch 1/1
111/111 [==============================] - 46s 419ms/step - loss: 0.0730 - mean_absolute_error: 0.1972
18. set (Subject 12, M06) being trained for epoch 18!
Epoch 1/1
144/144 [==============================] - 60s 418ms/step - loss: 0.0618 - mean_absolute_error: 0.1806
19. set (Subject 5, F05) being trained for epoch 18!
Epoch 1/1
186/186 [==============================] - 79s 423ms/step - loss: 0.0814 - mean_absolute_error: 0.2016
20. set (Subject 20, M12) being trained for epoch 18!
Epoch 1/1
108/108 [==============================] - 46s 422ms/step - loss: 0.0442 - mean_absolute_error: 0.1446
Epoch 18 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 10:25:45.774783
1. set (Subject 12, M06) being trained for epoch 19!
Epoch 1/1
144/144 [==============================] - 60s 417ms/step - loss: 0.0568 - mean_absolute_error: 0.1738
2. set (Subject 20, M12) being trained for epoch 19!
Epoch 1/1
108/108 [==============================] - 46s 423ms/step - loss: 0.0446 - mean_absolute_error: 0.1454
3. set (Subject 15, F03) being trained for epoch 19!
Epoch 1/1
128/128 [==============================] - 54s 421ms/step - loss: 0.0702 - mean_absolute_error: 0.1882
4. set (Subject 19, M11) being trained for epoch 19!
Epoch 1/1
98/98 [==============================] - 41s 419ms/step - loss: 0.0472 - mean_absolute_error: 0.1700
5. set (Subject 10, M04) being trained for epoch 19!
Epoch 1/1
142/142 [==============================] - 59s 418ms/step - loss: 0.0648 - mean_absolute_error: 0.1820
6. set (Subject 5, F05) being trained for epoch 19!
Epoch 1/1
186/186 [==============================] - 78s 422ms/step - loss: 0.0807 - mean_absolute_error: 0.2011
7. set (Subject 23, M13) being trained for epoch 19!
Epoch 1/1
111/111 [==============================] - 47s 421ms/step - loss: 0.0735 - mean_absolute_error: 0.2009
8. set (Subject 16, M09) being trained for epoch 19!
Epoch 1/1
180/180 [==============================] - 76s 421ms/step - loss: 0.0616 - mean_absolute_error: 0.1761
9. set (Subject 11, M05) being trained for epoch 19!
Epoch 1/1
112/112 [==============================] - 47s 420ms/step - loss: 0.0456 - mean_absolute_error: 0.1420
10. set (Subject 22, M01) being trained for epoch 19!
Epoch 1/1
130/130 [==============================] - 55s 421ms/step - loss: 0.0315 - mean_absolute_error: 0.1092
11. set (Subject 2, F02) being trained for epoch 19!
Epoch 1/1
99/99 [==============================] - 42s 420ms/step - loss: 0.1061 - mean_absolute_error: 0.2431
12. set (Subject 3, F03) being trained for epoch 19!
Epoch 1/1
143/143 [==============================] - 60s 420ms/step - loss: 0.0789 - mean_absolute_error: 0.2092
13. set (Subject 13, M07) being trained for epoch 19!
Epoch 1/1
94/94 [==============================] - 40s 422ms/step - loss: 0.0399 - mean_absolute_error: 0.1430
14. set (Subject 1, F01) being trained for epoch 19!
Epoch 1/1
97/97 [==============================] - 41s 420ms/step - loss: 0.1027 - mean_absolute_error: 0.2302
15. set (Subject 14, M08) being trained for epoch 19!
Epoch 1/1
157/157 [==============================] - 66s 419ms/step - loss: 0.1215 - mean_absolute_error: 0.2777
16. set (Subject 17, M10) being trained for epoch 19!
Epoch 1/1
76/76 [==============================] - 32s 423ms/step - loss: 0.0321 - mean_absolute_error: 0.1199
17. set (Subject 8, M02) being trained for epoch 19!
Epoch 1/1
152/152 [==============================] - 64s 419ms/step - loss: 0.0738 - mean_absolute_error: 0.2006
18. set (Subject 7, M01) being trained for epoch 19!
Epoch 1/1
146/146 [==============================] - 61s 421ms/step - loss: 0.0826 - mean_absolute_error: 0.2162
19. set (Subject 6, F06) being trained for epoch 19!
Epoch 1/1
106/106 [==============================] - 45s 420ms/step - loss: 0.0862 - mean_absolute_error: 0.1959
20. set (Subject 4, F04) being trained for epoch 19!
Epoch 1/1
146/146 [==============================] - 62s 421ms/step - loss: 0.0884 - mean_absolute_error: 0.2274
Epoch 19 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 10:45:52.534411
1. set (Subject 7, M01) being trained for epoch 20!
Epoch 1/1
146/146 [==============================] - 61s 419ms/step - loss: 0.0828 - mean_absolute_error: 0.2164
2. set (Subject 4, F04) being trained for epoch 20!
Epoch 1/1
146/146 [==============================] - 62s 423ms/step - loss: 0.0864 - mean_absolute_error: 0.2240
3. set (Subject 2, F02) being trained for epoch 20!
Epoch 1/1
99/99 [==============================] - 42s 420ms/step - loss: 0.0980 - mean_absolute_error: 0.2333
4. set (Subject 14, M08) being trained for epoch 20!
Epoch 1/1
157/157 [==============================] - 66s 420ms/step - loss: 0.1227 - mean_absolute_error: 0.2785
5. set (Subject 5, F05) being trained for epoch 20!
Epoch 1/1
186/186 [==============================] - 79s 423ms/step - loss: 0.0797 - mean_absolute_error: 0.1979
6. set (Subject 6, F06) being trained for epoch 20!
Epoch 1/1
106/106 [==============================] - 44s 419ms/step - loss: 0.0902 - mean_absolute_error: 0.2039
7. set (Subject 8, M02) being trained for epoch 20!
Epoch 1/1
152/152 [==============================] - 64s 420ms/step - loss: 0.0709 - mean_absolute_error: 0.1960
8. set (Subject 3, F03) being trained for epoch 20!
Epoch 1/1
143/143 [==============================] - 60s 420ms/step - loss: 0.0771 - mean_absolute_error: 0.2100
9. set (Subject 10, M04) being trained for epoch 20!
Epoch 1/1
142/142 [==============================] - 60s 421ms/step - loss: 0.0629 - mean_absolute_error: 0.1794
10. set (Subject 11, M05) being trained for epoch 20!
Epoch 1/1
112/112 [==============================] - 47s 421ms/step - loss: 0.0479 - mean_absolute_error: 0.1503
11. set (Subject 23, M13) being trained for epoch 20!
Epoch 1/1
111/111 [==============================] - 46s 418ms/step - loss: 0.0753 - mean_absolute_error: 0.2042
12. set (Subject 12, M06) being trained for epoch 20!
Epoch 1/1
144/144 [==============================] - 60s 419ms/step - loss: 0.0566 - mean_absolute_error: 0.1741
13. set (Subject 1, F01) being trained for epoch 20!
Epoch 1/1
97/97 [==============================] - 41s 419ms/step - loss: 0.1003 - mean_absolute_error: 0.2266
14. set (Subject 20, M12) being trained for epoch 20!
Epoch 1/1
108/108 [==============================] - 45s 420ms/step - loss: 0.0465 - mean_absolute_error: 0.1511
15. set (Subject 15, F03) being trained for epoch 20!
Epoch 1/1
128/128 [==============================] - 54s 419ms/step - loss: 0.0724 - mean_absolute_error: 0.1926
16. set (Subject 17, M10) being trained for epoch 20!
Epoch 1/1
76/76 [==============================] - 32s 419ms/step - loss: 0.0333 - mean_absolute_error: 0.1229
17. set (Subject 13, M07) being trained for epoch 20!
Epoch 1/1
94/94 [==============================] - 40s 421ms/step - loss: 0.0397 - mean_absolute_error: 0.1455
18. set (Subject 19, M11) being trained for epoch 20!
Epoch 1/1
98/98 [==============================] - 41s 420ms/step - loss: 0.0485 - mean_absolute_error: 0.1740
19. set (Subject 22, M01) being trained for epoch 20!
Epoch 1/1
130/130 [==============================] - 55s 422ms/step - loss: 0.0327 - mean_absolute_error: 0.1132
20. set (Subject 16, M09) being trained for epoch 20!
Epoch 1/1
180/180 [==============================] - 76s 421ms/step - loss: 0.0588 - mean_absolute_error: 0.1714
Epoch 20 completed!
Exp2019-01-20_04-23-27.h5 has been saved.
The subjects are trained: [(7, 'M01'), (4, 'F04'), (2, 'F02'), (14, 'M08'), (5, 'F05'), (6, 'F06'), (8, 'M02'), (3, '
F03'), (10, 'M04'), (11, 'M05'), (23, 'M13'), (12, 'M06'), (1, 'F01'), (20, 'M12'), (15, 'F03'), (17, 'M10'), (13, 'M
07'), (19, 'M11'), (22, 'M01'), (16, 'M09')]
Evaluating model VGG16_seqLen16_lstm10_output3_inEpochs1_outEpochs20_AdamOpt_lr-0.000100__2019-01-20_04-23-27
The subjects will be tested: [(9, 'M03'), (18, 'F05'), (21, 'F02'), (24, 'M14')]
All frames and annotations from 4 datasets have been read by 2019-01-20 11:05:55.709299
For the Subject 9 (M03):
217/217 [==============================] - 54s 251ms/step
        The absolute mean error on Pitch angle estimation: 18.53 Degree
        The absolute mean error on Yaw angle estimation: 26.39 Degree
        The absolute mean error on Roll angle estimation: 8.39 Degree
For the Subject 18 (F05):
150/150 [==============================] - 38s 254ms/step
        The absolute mean error on Pitch angle estimation: 21.75 Degree
        The absolute mean error on Yaw angle estimation: 20.85 Degree
        The absolute mean error on Roll angle estimation: 11.38 Degree
For the Subject 21 (F02):
155/155 [==============================] - 39s 253ms/step
        The absolute mean error on Pitch angle estimation: 22.68 Degree
        The absolute mean error on Yaw angle estimation: 16.16 Degree
        The absolute mean error on Roll angle estimation: 16.54 Degree
For the Subject 24 (M14):
119/119 [==============================] - 30s 254ms/step
        The absolute mean error on Pitch angle estimation: 18.50 Degree
        The absolute mean error on Yaw angle estimation: 11.64 Degree
        The absolute mean error on Roll angle estimation: 5.45 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 20.36 Degree
        The absolute mean error on Yaw angle estimations: 18.76 Degree
        The absolute mean error on Roll angle estimations: 10.44 Degree
subject9_Exp2019-01-20_04-23-27.png has been saved by 2019-01-20 11:09:03.077748.
subject18_Exp2019-01-20_04-23-27.png has been saved by 2019-01-20 11:09:03.272954.
subject21_Exp2019-01-20_04-23-27.png has been saved by 2019-01-20 11:09:03.471265.
subject24_Exp2019-01-20_04-23-27.png has been saved by 2019-01-20 11:09:03.656507.
Model Exp2019-01-20_04-23-27 has been evaluated successfully.
Model Exp2019-01-20_04-23-27 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git add --all
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git commit -m "Results of Exp2019-01
-20_04-23-27"
[master f679e72] Results of Exp2019-01-20_04-23-27
 9 files changed, 1908 insertions(+), 1448 deletions(-)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-20_04-23-27/2019-01-20_04-23-27_scrollback.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-20_04-23-27/output_Exp2019-01-20_04-23-27.txt
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-20_04-23-27/subject18_Exp2019-01-20_04-23-27.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-20_04-23-27/subject21_Exp2019-01-20_04-23-27.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-20_04-23-27/subject24_Exp2019-01-20_04-23-27.png
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-20_04-23-27/subject9_Exp2019-01-20_04-23-27.png
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__/output_Last_Model.txt
 delete mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Last_Model__/scrollback.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git push
Password for 'https://muratcancicek@bitbucket.org':
Counting objects: 13, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (13/13), done.
Writing objects: 100% (13/13), 455.20 KiB | 0 bytes/s, done.
Total 13 (delta 6), reused 0 (delta 0)
To https://muratcancicek@bitbucket.org/muratcancicek/deep_rl_for_head_pose_est.git
   f727829..f679e72  master -> master
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
remote: Counting objects: 5, done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 5 (delta 3), reused 0 (delta 0)
Unpacking objects: 100% (5/5), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   f679e72..e0ae20d  master     -> origin/master
Updating f679e72..e0ae20d
Fast-forward
 DeepRL_For_HPE/Note_Files/commands.txt | 4 ++++
 1 file changed, 4 insertions(+)
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ git pull
remote: Counting objects: 7, done.
remote: Compressing objects: 100% (7/7), done.
remote: Total 7 (delta 5), reused 0 (delta 0)
Unpacking objects: 100% (7/7), done.
From https://bitbucket.org/muratcancicek/deep_rl_for_head_pose_est
   e0ae20d..38f0110  master     -> origin/master
Updating e0ae20d..38f0110
Fast-forward
 .../scrollback_Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26.txt           | 1051 +++++++++++++++++++++++++++++++++
++
 1 file changed, 1051 insertions(+)
 create mode 100644 DeepRL_For_HPE/LSTM_VGG16/results/Exp2019-01-16_03-24-38_and_2019-01-20_00-46-26/scrollback_Exp20
19-01-16_03-24-38_and_2019-01-20_00-46-26.txt
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$ python runCNN_LSTM.py
/home/mcicek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argum
ent of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dty
pe(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
2019-01-20 20:04:05.422963: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that
this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-01-20 20:04:05.494204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read fro
m SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-20 20:04:05.494459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties:
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:01:00.0
totalMemory: 11.92GiB freeMemory: 9.86GiB
2019-01-20 20:04:05.494472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0
2019-01-20 20:04:05.648119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecuto
r with strength 1 edge matrix:
2019-01-20 20:04:05.648143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0
2019-01-20 20:04:05.648148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N
2019-01-20 20:04:05.648279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:
localhost/replica:0/task:0/device:GPU:0 with 9537 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, p
ci bus id: 0000:01:00.0, compute capability: 5.2)
Model Exp2019-01-20_20-04-06 has been started to be evaluated.
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 224, 224, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688
_________________________________________________________________

_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
tdVGG16 (TimeDistributed)    (None, 16, 7, 7, 512)     14714688
_________________________________________________________________
time_distributed_1 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
time_distributed_2 (TimeDist (None, 16, 25088)         0
_________________________________________________________________
fc1024 (TimeDistributed)     (None, 16, 4096)          102764544
_________________________________________________________________
time_distributed_3 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
fc104 (TimeDistributed)      (None, 16, 4096)          16781312
_________________________________________________________________
time_distributed_4 (TimeDist (None, 16, 4096)          0
_________________________________________________________________
fc10 (TimeDistributed)       (None, 16, 1024)          4195328
_________________________________________________________________
time_distributed_5 (TimeDist (None, 16, 1024)          0
_________________________________________________________________
lstm_1 (LSTM)                (None, 32)                135296
_________________________________________________________________
dense_4 (Dense)              (None, 3)                 99
=================================================================
Total params: 138,591,267
Trainable params: 123,876,579
Non-trainable params: 14,714,688
_________________________________________________________________

Training model VGG16_seqLen16_lstm32_output3_inEpochs1_outEpochs5_AdamOpt_lr-0.000010__2019-01-20_20-04-06
All frames and annotations from 20 datasets have been read by 2019-01-20 20:04:11.364694
1. set (Subject 20, M12) being trained for epoch 1!
Epoch 1/1
540/540 [==============================] - 88s 162ms/step - loss: 0.2188 - mean_absolute_error: 0.3687
2. set (Subject 23, M13) being trained for epoch 1!
Epoch 1/1
553/553 [==============================] - 88s 160ms/step - loss: 0.1233 - mean_absolute_error: 0.2759
3. set (Subject 12, M06) being trained for epoch 1!
Epoch 1/1
716/716 [==============================] - 115s 160ms/step - loss: 0.0814 - mean_absolute_error: 0.2203
4. set (Subject 16, M09) being trained for epoch 1!
Epoch 1/1
898/898 [==============================] - 144s 160ms/step - loss: 0.0695 - mean_absolute_error: 0.2017
5. set (Subject 6, F06) being trained for epoch 1!
Epoch 1/1
526/526 [==============================] - 83s 159ms/step - loss: 0.0971 - mean_absolute_error: 0.2361
6. set (Subject 22, M01) being trained for epoch 1!
Epoch 1/1
649/649 [==============================] - 103s 159ms/step - loss: 0.0435 - mean_absolute_error: 0.1571
7. set (Subject 19, M11) being trained for epoch 1!
Epoch 1/1
486/486 [==============================] - 78s 160ms/step - loss: 0.0528 - mean_absolute_error: 0.1763
8. set (Subject 13, M07) being trained for epoch 1!
Epoch 1/1
469/469 [==============================] - 75s 160ms/step - loss: 0.0446 - mean_absolute_error: 0.1573
9. set (Subject 5, F05) being trained for epoch 1!
Epoch 1/1
930/930 [==============================] - 149s 160ms/step - loss: 0.0852 - mean_absolute_error: 0.2092
10. set (Subject 10, M04) being trained for epoch 1!
Epoch 1/1
710/710 [==============================] - 114s 160ms/step - loss: 0.0687 - mean_absolute_error: 0.1884
11. set (Subject 7, M01) being trained for epoch 1!
Epoch 1/1
729/729 [==============================] - 116s 159ms/step - loss: 0.0910 - mean_absolute_error: 0.2237
12. set (Subject 1, F01) being trained for epoch 1!
Epoch 1/1
482/482 [==============================] - 77s 160ms/step - loss: 0.0989 - mean_absolute_error: 0.2308
13. set (Subject 15, F03) being trained for epoch 1!
Epoch 1/1
638/638 [==============================] - 101s 159ms/step - loss: 0.0741 - mean_absolute_error: 0.1961
14. set (Subject 2, F02) being trained for epoch 1!
Epoch 1/1
495/495 [==============================] - 79s 159ms/step - loss: 0.0842 - mean_absolute_error: 0.2219
15. set (Subject 3, F03) being trained for epoch 1!
Epoch 1/1
714/714 [==============================] - 114s 160ms/step - loss: 0.0905 - mean_absolute_error: 0.2289
16. set (Subject 17, M10) being trained for epoch 1!
Epoch 1/1
379/379 [==============================] - 60s 159ms/step - loss: 0.0383 - mean_absolute_error: 0.1402
17. set (Subject 14, M08) being trained for epoch 1!
Epoch 1/1
781/781 [==============================] - 124s 159ms/step - loss: 0.1633 - mean_absolute_error: 0.3274
18. set (Subject 4, F04) being trained for epoch 1!
Epoch 1/1
728/728 [==============================] - 116s 160ms/step - loss: 0.0897 - mean_absolute_error: 0.2259
19. set (Subject 11, M05) being trained for epoch 1!
Epoch 1/1
556/556 [==============================] - 89s 160ms/step - loss: 0.0525 - mean_absolute_error: 0.1552
20. set (Subject 8, M02) being trained for epoch 1!
Epoch 1/1
756/756 [==============================] - 121s 160ms/step - loss: 0.0822 - mean_absolute_error: 0.2172
Epoch 1 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 20:40:19.814034
1. set (Subject 4, F04) being trained for epoch 2!
Epoch 1/1
728/728 [==============================] - 116s 159ms/step - loss: 0.0809 - mean_absolute_error: 0.2150
2. set (Subject 8, M02) being trained for epoch 2!
Epoch 1/1
756/756 [==============================] - 121s 160ms/step - loss: 0.0770 - mean_absolute_error: 0.2078
3. set (Subject 7, M01) being trained for epoch 2!
Epoch 1/1
729/729 [==============================] - 117s 161ms/step - loss: 0.0854 - mean_absolute_error: 0.2212
4. set (Subject 3, F03) being trained for epoch 2!
Epoch 1/1
714/714 [==============================] - 114s 159ms/step - loss: 0.0768 - mean_absolute_error: 0.2078
5. set (Subject 22, M01) being trained for epoch 2!
Epoch 1/1
649/649 [==============================] - 104s 160ms/step - loss: 0.0357 - mean_absolute_error: 0.1218
6. set (Subject 11, M05) being trained for epoch 2!
Epoch 1/1
556/556 [==============================] - 89s 160ms/step - loss: 0.0559 - mean_absolute_error: 0.1560
7. set (Subject 14, M08) being trained for epoch 2!
Epoch 1/1
781/781 [==============================] - 125s 160ms/step - loss: 0.1207 - mean_absolute_error: 0.2742
8. set (Subject 1, F01) being trained for epoch 2!
Epoch 1/1
482/482 [==============================] - 77s 159ms/step - loss: 0.1081 - mean_absolute_error: 0.2350
9. set (Subject 6, F06) being trained for epoch 2!
Epoch 1/1
526/526 [==============================] - 84s 159ms/step - loss: 0.0855 - mean_absolute_error: 0.1966
10. set (Subject 5, F05) being trained for epoch 2!
Epoch 1/1
930/930 [==============================] - 149s 160ms/step - loss: 0.0826 - mean_absolute_error: 0.2049
11. set (Subject 19, M11) being trained for epoch 2!
Epoch 1/1
486/486 [==============================] - 77s 159ms/step - loss: 0.0495 - mean_absolute_error: 0.1756
12. set (Subject 20, M12) being trained for epoch 2!
Epoch 1/1
540/540 [==============================] - 87s 160ms/step - loss: 0.0429 - mean_absolute_error: 0.1396
13. set (Subject 2, F02) being trained for epoch 2!
Epoch 1/1
495/495 [==============================] - 79s 159ms/step - loss: 0.0977 - mean_absolute_error: 0.2340
14. set (Subject 23, M13) being trained for epoch 2!
Epoch 1/1
553/553 [==============================] - 88s 159ms/step - loss: 0.0752 - mean_absolute_error: 0.2016
15. set (Subject 12, M06) being trained for epoch 2!
Epoch 1/1
716/716 [==============================] - 115s 160ms/step - loss: 0.0568 - mean_absolute_error: 0.1745
16. set (Subject 17, M10) being trained for epoch 2!
Epoch 1/1
379/379 [==============================] - 61s 160ms/step - loss: 0.0343 - mean_absolute_error: 0.1240
17. set (Subject 15, F03) being trained for epoch 2!
Epoch 1/1
638/638 [==============================] - 102s 160ms/step - loss: 0.0694 - mean_absolute_error: 0.1843
18. set (Subject 16, M09) being trained for epoch 2!
Epoch 1/1
898/898 [==============================] - 143s 159ms/step - loss: 0.0592 - mean_absolute_error: 0.1706
19. set (Subject 10, M04) being trained for epoch 2!
Epoch 1/1
710/710 [==============================] - 114s 161ms/step - loss: 0.0653 - mean_absolute_error: 0.1819
20. set (Subject 13, M07) being trained for epoch 2!
Epoch 1/1
469/469 [==============================] - 75s 159ms/step - loss: 0.0387 - mean_absolute_error: 0.1469
Epoch 2 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 21:16:26.535090
1. set (Subject 16, M09) being trained for epoch 3!
Epoch 1/1
898/898 [==============================] - 145s 161ms/step - loss: 0.0581 - mean_absolute_error: 0.1677
2. set (Subject 13, M07) being trained for epoch 3!
Epoch 1/1
469/469 [==============================] - 75s 161ms/step - loss: 0.0386 - mean_absolute_error: 0.1462
3. set (Subject 19, M11) being trained for epoch 3!
Epoch 1/1
486/486 [==============================] - 77s 159ms/step - loss: 0.0469 - mean_absolute_error: 0.1677
4. set (Subject 12, M06) being trained for epoch 3!
Epoch 1/1
716/716 [==============================] - 115s 160ms/step - loss: 0.0573 - mean_absolute_error: 0.1748
5. set (Subject 11, M05) being trained for epoch 3!
Epoch 1/1
556/556 [==============================] - 89s 160ms/step - loss: 0.0543 - mean_absolute_error: 0.1594
6. set (Subject 10, M04) being trained for epoch 3!
Epoch 1/1
710/710 [==============================] - 113s 159ms/step - loss: 0.0649 - mean_absolute_error: 0.1830
7. set (Subject 15, F03) being trained for epoch 3!
Epoch 1/1
638/638 [==============================] - 101s 159ms/step - loss: 0.0648 - mean_absolute_error: 0.1754
8. set (Subject 20, M12) being trained for epoch 3!
Epoch 1/1
540/540 [==============================] - 86s 159ms/step - loss: 0.0414 - mean_absolute_error: 0.1324
9. set (Subject 22, M01) being trained for epoch 3!
Epoch 1/1
649/649 [==============================] - 103s 159ms/step - loss: 0.0306 - mean_absolute_error: 0.1078
10. set (Subject 6, F06) being trained for epoch 3!
Epoch 1/1
526/526 [==============================] - 84s 160ms/step - loss: 0.0791 - mean_absolute_error: 0.1845
11. set (Subject 14, M08) being trained for epoch 3!
Epoch 1/1
781/781 [==============================] - 124s 159ms/step - loss: 0.1165 - mean_absolute_error: 0.2712
12. set (Subject 4, F04) being trained for epoch 3!
Epoch 1/1
728/728 [==============================] - 117s 160ms/step - loss: 0.0945 - mean_absolute_error: 0.2357
13. set (Subject 23, M13) being trained for epoch 3!
Epoch 1/1
553/553 [==============================] - 88s 159ms/step - loss: 0.0706 - mean_absolute_error: 0.1912
14. set (Subject 8, M02) being trained for epoch 3!
Epoch 1/1
756/756 [==============================] - 121s 160ms/step - loss: 0.0775 - mean_absolute_error: 0.2062
15. set (Subject 7, M01) being trained for epoch 3!
Epoch 1/1
729/729 [==============================] - 117s 161ms/step - loss: 0.0860 - mean_absolute_error: 0.2219
16. set (Subject 17, M10) being trained for epoch 3!
Epoch 1/1
379/379 [==============================] - 61s 160ms/step - loss: 0.0284 - mean_absolute_error: 0.1081
17. set (Subject 2, F02) being trained for epoch 3!
Epoch 1/1
495/495 [==============================] - 79s 159ms/step - loss: 0.0950 - mean_absolute_error: 0.2306
18. set (Subject 3, F03) being trained for epoch 3!
Epoch 1/1
714/714 [==============================] - 114s 160ms/step - loss: 0.0801 - mean_absolute_error: 0.2176
19. set (Subject 5, F05) being trained for epoch 3!
Epoch 1/1
930/930 [==============================] - 149s 160ms/step - loss: 0.0807 - mean_absolute_error: 0.2081
20. set (Subject 1, F01) being trained for epoch 3!
Epoch 1/1
482/482 [==============================] - 77s 159ms/step - loss: 0.1042 - mean_absolute_error: 0.2301
Epoch 3 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 21:52:33.566590
1. set (Subject 3, F03) being trained for epoch 4!
Epoch 1/1
714/714 [==============================] - 115s 160ms/step - loss: 0.0866 - mean_absolute_error: 0.2281
2. set (Subject 1, F01) being trained for epoch 4!
Epoch 1/1
482/482 [==============================] - 77s 159ms/step - loss: 0.1043 - mean_absolute_error: 0.2296
3. set (Subject 14, M08) being trained for epoch 4!
Epoch 1/1
781/781 [==============================] - 124s 159ms/step - loss: 0.1421 - mean_absolute_error: 0.3076
4. set (Subject 7, M01) being trained for epoch 4!
Epoch 1/1
729/729 [==============================] - 116s 159ms/step - loss: 0.0862 - mean_absolute_error: 0.2178
5. set (Subject 10, M04) being trained for epoch 4!
Epoch 1/1
710/710 [==============================] - 114s 161ms/step - loss: 0.0660 - mean_absolute_error: 0.1849
6. set (Subject 5, F05) being trained for epoch 4!
Epoch 1/1
930/930 [==============================] - 148s 159ms/step - loss: 0.0754 - mean_absolute_error: 0.2004
7. set (Subject 2, F02) being trained for epoch 4!
Epoch 1/1
495/495 [==============================] - 79s 160ms/step - loss: 0.0798 - mean_absolute_error: 0.2169
8. set (Subject 4, F04) being trained for epoch 4!
Epoch 1/1
728/728 [==============================] - 116s 159ms/step - loss: 0.0944 - mean_absolute_error: 0.2352
9. set (Subject 11, M05) being trained for epoch 4!
Epoch 1/1
556/556 [==============================] - 88s 159ms/step - loss: 0.0539 - mean_absolute_error: 0.1727
10. set (Subject 22, M01) being trained for epoch 4!
Epoch 1/1
649/649 [==============================] - 104s 160ms/step - loss: 0.0420 - mean_absolute_error: 0.1480
11. set (Subject 15, F03) being trained for epoch 4!
Epoch 1/1
638/638 [==============================] - 101s 159ms/step - loss: 0.0876 - mean_absolute_error: 0.2205
12. set (Subject 16, M09) being trained for epoch 4!
Epoch 1/1
898/898 [==============================] - 144s 160ms/step - loss: 0.0772 - mean_absolute_error: 0.2035
13. set (Subject 8, M02) being trained for epoch 4!
Epoch 1/1
756/756 [==============================] - 120s 159ms/step - loss: 0.0827 - mean_absolute_error: 0.2100
14. set (Subject 13, M07) being trained for epoch 4!
Epoch 1/1
469/469 [==============================] - 75s 160ms/step - loss: 0.0485 - mean_absolute_error: 0.1675
15. set (Subject 19, M11) being trained for epoch 4!
Epoch 1/1
486/486 [==============================] - 77s 159ms/step - loss: 0.0704 - mean_absolute_error: 0.2150
16. set (Subject 17, M10) being trained for epoch 4!
Epoch 1/1
379/379 [==============================] - 61s 160ms/step - loss: 0.0460 - mean_absolute_error: 0.1583
17. set (Subject 23, M13) being trained for epoch 4!
Epoch 1/1
553/553 [==============================] - 88s 160ms/step - loss: 0.0757 - mean_absolute_error: 0.2064
18. set (Subject 12, M06) being trained for epoch 4!
Epoch 1/1
716/716 [==============================] - 115s 160ms/step - loss: 0.0583 - mean_absolute_error: 0.1811
19. set (Subject 6, F06) being trained for epoch 4!
Epoch 1/1
526/526 [==============================] - 84s 159ms/step - loss: 0.1028 - mean_absolute_error: 0.2249
20. set (Subject 20, M12) being trained for epoch 4!
Epoch 1/1
540/540 [==============================] - 87s 160ms/step - loss: 0.0535 - mean_absolute_error: 0.1721
Epoch 4 completed!
All frames and annotations from 20 datasets have been read by 2019-01-20 22:28:39.519998
1. set (Subject 12, M06) being trained for epoch 5!
Epoch 1/1
716/716 [==============================] - 115s 161ms/step - loss: 0.0534 - mean_absolute_error: 0.1715
2. set (Subject 20, M12) being trained for epoch 5!
Epoch 1/1
540/540 [==============================] - 87s 161ms/step - loss: 0.0520 - mean_absolute_error: 0.1686
3. set (Subject 15, F03) being trained for epoch 5!
Epoch 1/1
638/638 [==============================] - 102s 159ms/step - loss: 0.0816 - mean_absolute_error: 0.2097
4. set (Subject 19, M11) being trained for epoch 5!
Epoch 1/1
486/486 [==============================] - 78s 160ms/step - loss: 0.0545 - mean_absolute_error: 0.1895
5. set (Subject 5, F05) being trained for epoch 5!
Epoch 1/1
930/930 [==============================] - 148s 159ms/step - loss: 0.0782 - mean_absolute_error: 0.2002
6. set (Subject 6, F06) being trained for epoch 5!
Epoch 1/1
526/526 [==============================] - 84s 160ms/step - loss: 0.0971 - mean_absolute_error: 0.2145
7. set (Subject 23, M13) being trained for epoch 5!
Epoch 1/1
553/553 [==============================] - 89s 160ms/step - loss: 0.0805 - mean_absolute_error: 0.2143
8. set (Subject 16, M09) being trained for epoch 5!
Epoch 1/1
898/898 [==============================] - 144s 160ms/step - loss: 0.0632 - mean_absolute_error: 0.1797
9. set (Subject 10, M04) being trained for epoch 5!
Epoch 1/1
710/710 [==============================] - 113s 159ms/step - loss: 0.0683 - mean_absolute_error: 0.1899
10. set (Subject 11, M05) being trained for epoch 5!
Epoch 1/1
556/556 [==============================] - 88s 159ms/step - loss: 0.0508 - mean_absolute_error: 0.1548
11. set (Subject 2, F02) being trained for epoch 5!
Epoch 1/1
495/495 [==============================] - 79s 160ms/step - loss: 0.0916 - mean_absolute_error: 0.2276
12. set (Subject 3, F03) being trained for epoch 5!
Epoch 1/1
714/714 [==============================] - 114s 160ms/step - loss: 0.0774 - mean_absolute_error: 0.2108
13. set (Subject 13, M07) being trained for epoch 5!
Epoch 1/1
469/469 [==============================] - 75s 160ms/step - loss: 0.0374 - mean_absolute_error: 0.1429
14. set (Subject 1, F01) being trained for epoch 5!
Epoch 1/1
482/482 [==============================] - 77s 160ms/step - loss: 0.0999 - mean_absolute_error: 0.2255
15. set (Subject 14, M08) being trained for epoch 5!
Epoch 1/1
781/781 [==============================] - 125s 160ms/step - loss: 0.1258 - mean_absolute_error: 0.2845
16. set (Subject 17, M10) being trained for epoch 5!
Epoch 1/1
379/379 [==============================] - 61s 160ms/step - loss: 0.0299 - mean_absolute_error: 0.1263
17. set (Subject 8, M02) being trained for epoch 5!
Epoch 1/1
756/756 [==============================] - 121s 161ms/step - loss: 0.0711 - mean_absolute_error: 0.1960
18. set (Subject 7, M01) being trained for epoch 5!
Epoch 1/1
729/729 [==============================] - 117s 160ms/step - loss: 0.0824 - mean_absolute_error: 0.2154
19. set (Subject 22, M01) being trained for epoch 5!
Epoch 1/1
649/649 [==============================] - 104s 160ms/step - loss: 0.0324 - mean_absolute_error: 0.1270
20. set (Subject 4, F04) being trained for epoch 5!
Epoch 1/1
728/728 [==============================] - 117s 160ms/step - loss: 0.0859 - mean_absolute_error: 0.2217
Epoch 5 completed!
Exp2019-01-20_20-04-06.h5 has been saved.
The subjects are trained: [(12, 'M06'), (20, 'M12'), (15, 'F03'), (19, 'M11'), (5, 'F05'), (6, 'F06'), (23, 'M13'), (
16, 'M09'), (10, 'M04'), (11, 'M05'), (2, 'F02'), (3, 'F03'), (13, 'M07'), (1, 'F01'), (14, 'M08'), (17, 'M10'), (8,
'M02'), (7, 'M01'), (22, 'M01'), (4, 'F04')]
Evaluating model VGG16_seqLen16_lstm32_output3_inEpochs1_outEpochs5_AdamOpt_lr-0.000010__2019-01-20_20-04-06
The subjects will be tested: [(9, 'M03'), (18, 'F05'), (21, 'F02'), (24, 'M14')]
All frames and annotations from 4 datasets have been read by 2019-01-20 23:04:46.878877
For the Subject 9 (M03):
866/866 [==============================] - 60s 69ms/step
        The absolute mean error on Pitch angle estimation: 19.87 Degree
        The absolute mean error on Yaw angle estimation: 26.17 Degree
        The absolute mean error on Roll angle estimation: 8.12 Degree
For the Subject 18 (F05):
598/598 [==============================] - 42s 70ms/step
        The absolute mean error on Pitch angle estimation: 18.57 Degree
        The absolute mean error on Yaw angle estimation: 18.12 Degree
        The absolute mean error on Roll angle estimation: 10.73 Degree
For the Subject 21 (F02):
618/618 [==============================] - 43s 70ms/step
        The absolute mean error on Pitch angle estimation: 20.47 Degree
        The absolute mean error on Yaw angle estimation: 14.32 Degree
        The absolute mean error on Roll angle estimation: 17.82 Degree
For the Subject 24 (M14):
476/476 [==============================] - 33s 70ms/step
        The absolute mean error on Pitch angle estimation: 14.05 Degree
        The absolute mean error on Yaw angle estimation: 12.67 Degree
        The absolute mean error on Roll angle estimation: 6.88 Degree
On average in 4 test subjects:
        The absolute mean error on Pitch angle estimations: 18.24 Degree
        The absolute mean error on Yaw angle estimations: 17.82 Degree
        The absolute mean error on Roll angle estimations: 10.89 Degree
subject9_Exp2019-01-20_20-04-06.png has been saved by 2019-01-20 23:08:11.418969.
subject18_Exp2019-01-20_20-04-06.png has been saved by 2019-01-20 23:08:11.613728.
subject21_Exp2019-01-20_20-04-06.png has been saved by 2019-01-20 23:08:11.810639.
subject24_Exp2019-01-20_20-04-06.png has been saved by 2019-01-20 23:08:11.996856.
Model Exp2019-01-20_20-04-06 has been evaluated successfully.
Model Exp2019-01-20_20-04-06 has been recorded successfully.
Done
mcicek@harsimran:~/Projects/deep_rl_for_head_pose_est/DeepRL_For_HPE/LSTM_VGG16$
